/*
 Navicat Premium Data Transfer

 Source Server         : 阿里云
 Source Server Type    : MySQL
 Source Server Version : 80022
 Source Host           : 101.132.148.7:3306
 Source Schema         : myblog

 Target Server Type    : MySQL
 Target Server Version : 80022
 File Encoding         : 65001

 Date: 27/01/2021 20:12:10
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for account_articleviewhistory
-- ----------------------------
DROP TABLE IF EXISTS `account_articleviewhistory`;
CREATE TABLE `account_articleviewhistory` (
  `id` int NOT NULL AUTO_INCREMENT,
  `time` datetime(6) NOT NULL,
  `article_id` int DEFAULT NULL,
  `user_id` int DEFAULT NULL,
  `is_like` tinyint(1) NOT NULL,
  `category_id` int DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  KEY `account_articleviewh_article_id_daf2cd55_fk_blog_arti` (`article_id`) USING BTREE,
  KEY `account_articleviewhistory_user_id_f8a6a58c_fk_auth_user_id` (`user_id`) USING BTREE,
  KEY `account_articleviewh_category_id_d5f5a56b_fk_blog_cate` (`category_id`),
  CONSTRAINT `account_articleviewh_article_id_daf2cd55_fk_blog_arti` FOREIGN KEY (`article_id`) REFERENCES `blog_article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `account_articleviewh_category_id_d5f5a56b_fk_blog_cate` FOREIGN KEY (`category_id`) REFERENCES `blog_category` (`id`),
  CONSTRAINT `account_articleviewhistory_user_id_f8a6a58c_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=214 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of account_articleviewhistory
-- ----------------------------
BEGIN;
INSERT INTO `account_articleviewhistory` VALUES (194, '2021-01-12 09:09:28.889540', 124, 1, 0, 8);
INSERT INTO `account_articleviewhistory` VALUES (195, '2021-01-26 16:52:06.927868', 125, 1, 0, 8);
INSERT INTO `account_articleviewhistory` VALUES (196, '2020-12-25 11:06:44.942045', 126, 1, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (197, '2020-12-24 06:54:41.033230', 127, 1, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (199, '2020-12-23 08:56:37.285632', 128, 1, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (201, '2020-12-26 23:05:48.517220', 129, 1, 0, 3);
INSERT INTO `account_articleviewhistory` VALUES (206, '2020-12-27 20:11:12.988372', 130, 1, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (207, '2020-12-27 21:06:40.235627', 130, 76, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (208, '2020-12-27 21:15:54.112443', 129, 76, 0, 3);
INSERT INTO `account_articleviewhistory` VALUES (209, '2020-12-27 21:17:30.740621', 126, 76, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (210, '2020-12-27 21:18:07.319739', 124, 76, 0, 8);
INSERT INTO `account_articleviewhistory` VALUES (211, '2020-12-28 08:01:09.451953', 130, 72, 0, 4);
INSERT INTO `account_articleviewhistory` VALUES (212, '2020-12-28 08:08:19.179718', 11, 1, 0, 6);
INSERT INTO `account_articleviewhistory` VALUES (213, '2021-01-15 17:24:52.480978', 131, 1, 0, 3);
COMMIT;

-- ----------------------------
-- Table structure for account_commentmessage
-- ----------------------------
DROP TABLE IF EXISTS `account_commentmessage`;
CREATE TABLE `account_commentmessage` (
  `id` int NOT NULL AUTO_INCREMENT,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci,
  `time` datetime(6) NOT NULL,
  `level` int NOT NULL,
  `like` int NOT NULL,
  `reply_id` int DEFAULT NULL,
  `root_id` int DEFAULT NULL,
  `user_id` int DEFAULT NULL,
  `article_id` int DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  KEY `account_commentmessage_article_id_17f25e54_fk_blog_article_id` (`article_id`) USING BTREE,
  KEY `account_commentmessage_user_id_d2eb3fa3_fk_auth_user_id` (`user_id`) USING BTREE,
  CONSTRAINT `account_commentmessage_article_id_17f25e54_fk_blog_article_id` FOREIGN KEY (`article_id`) REFERENCES `blog_article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `account_commentmessage_user_id_d2eb3fa3_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of account_commentmessage
-- ----------------------------
BEGIN;
INSERT INTO `account_commentmessage` VALUES (2, '写的非常好', '2020-09-30 16:10:45.011835', 0, 13, NULL, NULL, 45, 28);
INSERT INTO `account_commentmessage` VALUES (6, '该内容已被用户删除', '2020-09-30 17:28:36.470302', 1, 0, 2, 2, 1, 28);
INSERT INTO `account_commentmessage` VALUES (7, '该内容已被用户删除', '2020-09-30 17:29:01.360800', 1, 6, 2, 2, 1, 28);
INSERT INTO `account_commentmessage` VALUES (8, '该内容已被用户删除', '2020-09-30 17:30:23.552869', 0, 3, NULL, NULL, 1, 28);
INSERT INTO `account_commentmessage` VALUES (9, '12345678<img src=\"http://127.0.0.1:8000/static/layui/images/face/51.gif\" alt=\"[兔子]\"><img src=\"media/comment/2020_09_30_17_31_23_023828.jpg\" alt=\"undefined\" style=\"display:inline-block;max-width:50%;height:auto\" class=\"upload-img\">', '2020-09-30 17:31:24.793460', 2, 0, 6, 2, 45, 28);
INSERT INTO `account_commentmessage` VALUES (10, '评论在此<img src=\"media/comment/2020_09_30_17_52_45_340492.jpg\" alt=\"undefined\" style=\"display:inline-block;max-width:50%;height:auto\" class=\"upload-img\"><img src=\"http://127.0.0.1:8000/static/layui/images/face/13.gif\" alt=\"[偷笑]\">', '2020-09-30 17:52:50.292256', 0, 24, NULL, NULL, 45, 11);
INSERT INTO `account_commentmessage` VALUES (17, '写得好', '2020-10-17 23:00:22.028546', 0, 0, NULL, NULL, 45, 9);
INSERT INTO `account_commentmessage` VALUES (18, '啦啦啦 我来了', '2020-10-17 23:00:30.768633', 0, 0, NULL, NULL, 45, 7);
INSERT INTO `account_commentmessage` VALUES (20, '该内容已被用户删除', '2020-10-17 23:01:30.126833', 1, 0, 8, 8, 45, 28);
INSERT INTO `account_commentmessage` VALUES (21, '该内容已被用户删除', '2020-10-17 23:01:41.229830', 1, 2, 8, 8, 45, 28);
INSERT INTO `account_commentmessage` VALUES (22, '该内容已被用户删除', '2020-10-17 23:01:45.510840', 2, 0, 6, 2, 45, 28);
INSERT INTO `account_commentmessage` VALUES (23, '该内容已被用户删除', '2020-10-17 23:01:49.215572', 2, 2, 7, 2, 45, 28);
COMMIT;

-- ----------------------------
-- Table structure for account_leavemessage
-- ----------------------------
DROP TABLE IF EXISTS `account_leavemessage`;
CREATE TABLE `account_leavemessage` (
  `id` int NOT NULL AUTO_INCREMENT,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci,
  `time` datetime(6) NOT NULL,
  `level` int NOT NULL,
  `user_id` int DEFAULT NULL,
  `like` int NOT NULL,
  `reply_id` int DEFAULT NULL,
  `root_id` int DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  KEY `account_leavemessage_user_id_947e2704_fk_auth_user_id` (`user_id`) USING BTREE,
  CONSTRAINT `account_leavemessage_user_id_947e2704_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=32 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of account_leavemessage
-- ----------------------------
BEGIN;
INSERT INTO `account_leavemessage` VALUES (2, '嗯呢，我也觉得是', '2020-11-09 22:06:43.000000', 1, 45, 0, 1, 1);
INSERT INTO `account_leavemessage` VALUES (3, '那必须的啊', '1900-01-20 22:07:44.956948', 3, 45, 0, 6, 1);
INSERT INTO `account_leavemessage` VALUES (4, '该内容已被用户删除', '1900-01-20 22:08:28.265735', 4, 1, 0, 3, 1);
INSERT INTO `account_leavemessage` VALUES (5, '支持支持，鼎力支持！', '1900-01-20 22:09:07.918721', 5, 45, 4, 4, 1);
INSERT INTO `account_leavemessage` VALUES (6, '该内容已被用户删除', '1900-01-20 09:59:55.265053', 2, 1, 0, 2, 1);
INSERT INTO `account_leavemessage` VALUES (7, '该内容已被用户删除', '1900-01-20 11:11:25.905584', 0, 1, 0, NULL, NULL);
INSERT INTO `account_leavemessage` VALUES (8, '留言测试2-2', '1900-01-20 11:11:58.979330', 1, 45, 0, 7, 7);
INSERT INTO `account_leavemessage` VALUES (9, '留言测试2-1', '1900-01-20 11:13:51.710481', 1, 45, 0, 7, 7);
INSERT INTO `account_leavemessage` VALUES (10, '该内容已被用户删除', '1900-01-20 11:15:23.155168', 2, 1, 6, 8, 7);
INSERT INTO `account_leavemessage` VALUES (11, '大家都来留言啊', '1900-01-20 15:18:30.510672', 0, 45, 32, NULL, NULL);
INSERT INTO `account_leavemessage` VALUES (19, '那必须的啊', '1900-01-20 15:39:39.534346', 1, 1, 0, 11, 11);
INSERT INTO `account_leavemessage` VALUES (20, '该内容已被用户删除', '1900-01-20 15:43:38.736344', 2, 1, 0, 9, 7);
INSERT INTO `account_leavemessage` VALUES (21, '该内容已被用户删除', '1900-01-20 15:45:09.639632', 2, 1, 0, 8, 7);
INSERT INTO `account_leavemessage` VALUES (22, '必须顶起！<img src=\"http://127.0.0.1:8000/static/layui/images/face/40.gif\" alt=\"[晕]\"><img src=\"media/comment/2020_09_30_15_45_43_122882.jpg\" alt=\"undefined\" style=\"display:inline-block;max-width:50%;height:auto\" class=\"upload-img\">', '1900-01-20 15:45:46.859937', 1, 1, 0, 11, 11);
INSERT INTO `account_leavemessage` VALUES (23, '支持支持<img src=\"http://127.0.0.1:8000/static/layui/images/face/1.gif\" alt=\"[嘻嘻]\">', '1900-01-20 16:06:00.411972', 0, 1, 0, NULL, NULL);
INSERT INTO `account_leavemessage` VALUES (24, '该内容已被用户删除', '1900-01-20 17:25:27.389129', 1, 1, 0, 2, 2);
INSERT INTO `account_leavemessage` VALUES (25, '该内容已被用户删除', '1900-01-20 17:26:17.409297', 1, 1, 0, 2, 2);
INSERT INTO `account_leavemessage` VALUES (27, '来啦来嘞', '1900-01-20 23:00:52.090010', 0, 45, 0, NULL, NULL);
INSERT INTO `account_leavemessage` VALUES (28, '嗯嗯嗯', '1900-01-20 23:00:58.611691', 1, 45, 0, 23, 23);
INSERT INTO `account_leavemessage` VALUES (29, '必须的啊', '1900-01-20 23:01:06.353090', 2, 45, 0, 22, 11);
INSERT INTO `account_leavemessage` VALUES (31, '<img src=\"media/comment/2020_10_25_21_42_34_691922.jpg\" alt=\"undefined\" style=\"display:inline-block;max-width:50%;height:auto\" class=\"upload-img\">我也来留个言', '1900-01-20 21:42:45.186817', 0, 1, 0, NULL, NULL);
COMMIT;

-- ----------------------------
-- Table structure for account_sectionmessage
-- ----------------------------
DROP TABLE IF EXISTS `account_sectionmessage`;
CREATE TABLE `account_sectionmessage` (
  `id` int NOT NULL AUTO_INCREMENT,
  `content` longtext,
  `time` datetime(6) NOT NULL,
  `level` int NOT NULL,
  `like` int NOT NULL,
  `reply_id` int DEFAULT NULL,
  `root_id` int DEFAULT NULL,
  `section_id` int DEFAULT NULL,
  `user_id` int DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `account_notemessage_user_id_aaa777bb_fk_auth_user_id` (`user_id`),
  KEY `account_sectionmessage_section_id_05c45792_fk_blog_section_id` (`section_id`),
  CONSTRAINT `account_notemessage_user_id_aaa777bb_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`),
  CONSTRAINT `account_sectionmessage_section_id_05c45792_fk_blog_section_id` FOREIGN KEY (`section_id`) REFERENCES `blog_section` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of account_sectionmessage
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for account_sectionviewhistory
-- ----------------------------
DROP TABLE IF EXISTS `account_sectionviewhistory`;
CREATE TABLE `account_sectionviewhistory` (
  `id` int NOT NULL AUTO_INCREMENT,
  `time` datetime(6) NOT NULL,
  `is_like` tinyint(1) NOT NULL,
  `section_id` int DEFAULT NULL,
  `user_id` int DEFAULT NULL,
  `note_id` int DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `account_sectionviewh_section_id_aaee88fa_fk_blog_sect` (`section_id`),
  KEY `account_sectionviewhistory_user_id_10c5e55f_fk_auth_user_id` (`user_id`),
  KEY `account_sectionviewhistory_note_id_029f11f1_fk_blog_note_id` (`note_id`),
  CONSTRAINT `account_sectionviewh_section_id_aaee88fa_fk_blog_sect` FOREIGN KEY (`section_id`) REFERENCES `blog_section` (`id`),
  CONSTRAINT `account_sectionviewhistory_note_id_029f11f1_fk_blog_note_id` FOREIGN KEY (`note_id`) REFERENCES `blog_note` (`id`),
  CONSTRAINT `account_sectionviewhistory_user_id_10c5e55f_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=77 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of account_sectionviewhistory
-- ----------------------------
BEGIN;
INSERT INTO `account_sectionviewhistory` VALUES (12, '2020-12-20 16:25:14.463806', 0, 18, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (34, '2020-12-15 13:09:20.204002', 1, 52, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (35, '2020-12-15 23:52:30.639301', 0, 64, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (36, '2020-12-15 23:53:37.155763', 0, 63, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (37, '2020-12-16 22:53:48.971091', 0, 69, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (38, '2020-12-17 10:00:37.494786', 0, 74, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (39, '2020-12-20 16:05:41.721091', 0, 54, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (40, '2020-12-20 16:05:47.907927', 0, 55, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (41, '2020-12-20 16:05:56.886737', 0, 56, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (42, '2020-12-20 16:23:13.811848', 0, 53, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (43, '2020-12-25 12:56:55.993618', 0, 75, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (44, '2020-12-20 23:24:56.763483', 0, 76, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (45, '2020-12-20 23:25:07.932241', 0, 77, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (46, '2020-12-20 23:33:29.647660', 0, 80, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (47, '2020-12-20 23:33:35.722848', 0, 81, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (48, '2020-12-20 23:33:42.946452', 0, 82, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (49, '2020-12-24 08:12:00.432975', 0, 85, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (50, '2021-01-09 16:48:20.549487', 0, 95, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (51, '2020-12-25 12:56:48.701993', 0, 88, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (52, '2020-12-25 22:51:13.538316', 0, 101, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (53, '2020-12-25 22:51:20.681549', 0, 100, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (54, '2020-12-31 13:47:03.321007', 0, 99, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (55, '2020-12-25 22:52:03.731581', 0, 98, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (56, '2021-01-26 13:02:59.039115', 0, 96, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (57, '2021-01-26 13:03:12.569604', 0, 97, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (58, '2020-12-26 22:24:38.254617', 0, 1, 74, 1);
INSERT INTO `account_sectionviewhistory` VALUES (59, '2020-12-26 22:27:23.628128', 0, 2, 74, 1);
INSERT INTO `account_sectionviewhistory` VALUES (60, '2020-12-29 23:13:04.354785', 0, 102, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (61, '2020-12-29 23:13:27.943583', 0, 103, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (62, '2021-01-06 10:57:54.513196', 0, 121, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (63, '2021-01-06 10:58:11.626920', 0, 118, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (64, '2021-01-06 10:58:26.253293', 0, 1, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (65, '2021-01-06 10:58:37.085795', 0, 4, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (66, '2021-01-06 10:58:54.093268', 0, 5, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (67, '2021-01-06 10:59:02.383567', 0, 6, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (68, '2021-01-06 10:59:28.386956', 0, 7, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (69, '2021-01-08 21:00:08.053578', 0, 136, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (70, '2021-01-08 21:12:54.328579', 0, 145, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (71, '2021-01-08 21:13:00.191757', 0, 146, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (72, '2021-01-08 22:34:15.858591', 0, 147, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (73, '2021-01-08 23:15:12.515577', 0, 148, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (74, '2021-01-10 01:39:33.817596', 0, 164, 1, 2);
INSERT INTO `account_sectionviewhistory` VALUES (75, '2021-01-20 20:52:50.073005', 0, 35, 1, 1);
INSERT INTO `account_sectionviewhistory` VALUES (76, '2021-01-20 20:52:56.093363', 0, 34, 1, 1);
COMMIT;

-- ----------------------------
-- Table structure for account_userinfo
-- ----------------------------
DROP TABLE IF EXISTS `account_userinfo`;
CREATE TABLE `account_userinfo` (
  `id` int NOT NULL AUTO_INCREMENT,
  `phone` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `sex` varchar(1) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `web` varchar(50) DEFAULT NULL,
  `aboutme` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `photo` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `user_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `user_id` (`user_id`) USING BTREE,
  CONSTRAINT `account_userinfo_user_id_2e7aaeee_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=54 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of account_userinfo
-- ----------------------------
BEGIN;
INSERT INTO `account_userinfo` VALUES (1, '17793666234', '1', 'https://www.cuiliangblog.cn/', '心中有火，眼里有光', 'photo/2020_12_26_23_12_52_183626.jpg', 1);
INSERT INTO `account_userinfo` VALUES (22, '保密', '1', 'http://layuimini.99php.cn/', '保密', 'photo/2020_10_24_14_49_58_826886.jpg', 45);
INSERT INTO `account_userinfo` VALUES (38, '保密', '2', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_18_49_51_751774.jpg', 62);
INSERT INTO `account_userinfo` VALUES (39, '保密', '2', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_18_52_54_397647.jpg', 63);
INSERT INTO `account_userinfo` VALUES (43, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_04_32_770264.jpg', 67);
INSERT INTO `account_userinfo` VALUES (44, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_08_03_302743.jpg', 68);
INSERT INTO `account_userinfo` VALUES (45, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_09_56_521600.jpg', 69);
INSERT INTO `account_userinfo` VALUES (46, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_10_16_740592.jpg', 70);
INSERT INTO `account_userinfo` VALUES (47, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/default.jpg', 71);
INSERT INTO `account_userinfo` VALUES (48, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_44_19_166903.jpg', 72);
INSERT INTO `account_userinfo` VALUES (49, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_26_21_47_08_682774.jpg', 73);
INSERT INTO `account_userinfo` VALUES (50, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/default.jpg', 74);
INSERT INTO `account_userinfo` VALUES (51, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/default.jpg', 75);
INSERT INTO `account_userinfo` VALUES (52, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2020_12_27_20_23_40_993819.jpg', 76);
INSERT INTO `account_userinfo` VALUES (53, '保密', '1', NULL, '这个人很懒，什么都没留下！', 'photo/2021_01_04_14_30_12_132533.jpg', 77);
COMMIT;

-- ----------------------------
-- Table structure for auth_group
-- ----------------------------
DROP TABLE IF EXISTS `auth_group`;
CREATE TABLE `auth_group` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `name` (`name`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_group
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for auth_group_permissions
-- ----------------------------
DROP TABLE IF EXISTS `auth_group_permissions`;
CREATE TABLE `auth_group_permissions` (
  `id` int NOT NULL AUTO_INCREMENT,
  `group_id` int NOT NULL,
  `permission_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `auth_group_permissions_group_id_permission_id_0cd325b0_uniq` (`group_id`,`permission_id`) USING BTREE,
  KEY `auth_group_permissio_permission_id_84c5c92e_fk_auth_perm` (`permission_id`) USING BTREE,
  CONSTRAINT `auth_group_permissio_permission_id_84c5c92e_fk_auth_perm` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `auth_group_permissions_group_id_b120cbf9_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_group_permissions
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for auth_permission
-- ----------------------------
DROP TABLE IF EXISTS `auth_permission`;
CREATE TABLE `auth_permission` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `content_type_id` int NOT NULL,
  `codename` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `auth_permission_content_type_id_codename_01ab375a_uniq` (`content_type_id`,`codename`) USING BTREE,
  CONSTRAINT `auth_permission_content_type_id_2f476e4b_fk_django_co` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=129 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_permission
-- ----------------------------
BEGIN;
INSERT INTO `auth_permission` VALUES (1, 'Can add log entry', 1, 'add_logentry');
INSERT INTO `auth_permission` VALUES (2, 'Can change log entry', 1, 'change_logentry');
INSERT INTO `auth_permission` VALUES (3, 'Can delete log entry', 1, 'delete_logentry');
INSERT INTO `auth_permission` VALUES (4, 'Can view log entry', 1, 'view_logentry');
INSERT INTO `auth_permission` VALUES (5, 'Can add permission', 2, 'add_permission');
INSERT INTO `auth_permission` VALUES (6, 'Can change permission', 2, 'change_permission');
INSERT INTO `auth_permission` VALUES (7, 'Can delete permission', 2, 'delete_permission');
INSERT INTO `auth_permission` VALUES (8, 'Can view permission', 2, 'view_permission');
INSERT INTO `auth_permission` VALUES (9, 'Can add group', 3, 'add_group');
INSERT INTO `auth_permission` VALUES (10, 'Can change group', 3, 'change_group');
INSERT INTO `auth_permission` VALUES (11, 'Can delete group', 3, 'delete_group');
INSERT INTO `auth_permission` VALUES (12, 'Can view group', 3, 'view_group');
INSERT INTO `auth_permission` VALUES (13, 'Can add user', 4, 'add_user');
INSERT INTO `auth_permission` VALUES (14, 'Can change user', 4, 'change_user');
INSERT INTO `auth_permission` VALUES (15, 'Can delete user', 4, 'delete_user');
INSERT INTO `auth_permission` VALUES (16, 'Can view user', 4, 'view_user');
INSERT INTO `auth_permission` VALUES (17, 'Can add content type', 5, 'add_contenttype');
INSERT INTO `auth_permission` VALUES (18, 'Can change content type', 5, 'change_contenttype');
INSERT INTO `auth_permission` VALUES (19, 'Can delete content type', 5, 'delete_contenttype');
INSERT INTO `auth_permission` VALUES (20, 'Can view content type', 5, 'view_contenttype');
INSERT INTO `auth_permission` VALUES (21, 'Can add session', 6, 'add_session');
INSERT INTO `auth_permission` VALUES (22, 'Can change session', 6, 'change_session');
INSERT INTO `auth_permission` VALUES (23, 'Can delete session', 6, 'delete_session');
INSERT INTO `auth_permission` VALUES (24, 'Can view session', 6, 'view_session');
INSERT INTO `auth_permission` VALUES (25, 'Can add 文章分类', 7, 'add_category');
INSERT INTO `auth_permission` VALUES (26, 'Can change 文章分类', 7, 'change_category');
INSERT INTO `auth_permission` VALUES (27, 'Can delete 文章分类', 7, 'delete_category');
INSERT INTO `auth_permission` VALUES (28, 'Can view 文章分类', 7, 'view_category');
INSERT INTO `auth_permission` VALUES (29, 'Can add 文章标签', 8, 'add_tag');
INSERT INTO `auth_permission` VALUES (30, 'Can change 文章标签', 8, 'change_tag');
INSERT INTO `auth_permission` VALUES (31, 'Can delete 文章标签', 8, 'delete_tag');
INSERT INTO `auth_permission` VALUES (32, 'Can view 文章标签', 8, 'view_tag');
INSERT INTO `auth_permission` VALUES (33, 'Can add 文章内容', 9, 'add_article');
INSERT INTO `auth_permission` VALUES (34, 'Can change 文章内容', 9, 'change_article');
INSERT INTO `auth_permission` VALUES (35, 'Can delete 文章内容', 9, 'delete_article');
INSERT INTO `auth_permission` VALUES (36, 'Can view 文章内容', 9, 'view_article');
INSERT INTO `auth_permission` VALUES (37, 'Can add 用户详细信息', 10, 'add_userinfo');
INSERT INTO `auth_permission` VALUES (38, 'Can change 用户详细信息', 10, 'change_userinfo');
INSERT INTO `auth_permission` VALUES (39, 'Can delete 用户详细信息', 10, 'delete_userinfo');
INSERT INTO `auth_permission` VALUES (40, 'Can view 用户详细信息', 10, 'view_userinfo');
INSERT INTO `auth_permission` VALUES (41, 'Can add 用户浏览记录', 11, 'add_articleviewhistory');
INSERT INTO `auth_permission` VALUES (42, 'Can change 用户浏览记录', 11, 'change_articleviewhistory');
INSERT INTO `auth_permission` VALUES (43, 'Can delete 用户浏览记录', 11, 'delete_articleviewhistory');
INSERT INTO `auth_permission` VALUES (44, 'Can view 用户浏览记录', 11, 'view_articleviewhistory');
INSERT INTO `auth_permission` VALUES (45, 'Can add 留言回复记录', 12, 'add_leavemessage');
INSERT INTO `auth_permission` VALUES (46, 'Can change 留言回复记录', 12, 'change_leavemessage');
INSERT INTO `auth_permission` VALUES (47, 'Can delete 留言回复记录', 12, 'delete_leavemessage');
INSERT INTO `auth_permission` VALUES (48, 'Can view 留言回复记录', 12, 'view_leavemessage');
INSERT INTO `auth_permission` VALUES (49, 'Can add 评论回复记录', 13, 'add_commentmessage');
INSERT INTO `auth_permission` VALUES (50, 'Can change 评论回复记录', 13, 'change_commentmessage');
INSERT INTO `auth_permission` VALUES (51, 'Can delete 评论回复记录', 13, 'delete_commentmessage');
INSERT INTO `auth_permission` VALUES (52, 'Can view 评论回复记录', 13, 'view_commentmessage');
INSERT INTO `auth_permission` VALUES (53, 'Can add 轮播图管理', 14, 'add_carousel');
INSERT INTO `auth_permission` VALUES (54, 'Can change 轮播图管理', 14, 'change_carousel');
INSERT INTO `auth_permission` VALUES (55, 'Can delete 轮播图管理', 14, 'delete_carousel');
INSERT INTO `auth_permission` VALUES (56, 'Can view 轮播图管理', 14, 'view_carousel');
INSERT INTO `auth_permission` VALUES (57, 'Can add 友情链接', 15, 'add_link');
INSERT INTO `auth_permission` VALUES (58, 'Can change 友情链接', 15, 'change_link');
INSERT INTO `auth_permission` VALUES (59, 'Can delete 友情链接', 15, 'delete_link');
INSERT INTO `auth_permission` VALUES (60, 'Can view 友情链接', 15, 'view_link');
INSERT INTO `auth_permission` VALUES (61, 'Can add 关于页面', 16, 'add_about');
INSERT INTO `auth_permission` VALUES (62, 'Can change 关于页面', 16, 'change_about');
INSERT INTO `auth_permission` VALUES (63, 'Can delete 关于页面', 16, 'delete_about');
INSERT INTO `auth_permission` VALUES (64, 'Can view 关于页面', 16, 'view_about');
INSERT INTO `auth_permission` VALUES (65, 'Can add 网站配置', 17, 'add_websiteconfig');
INSERT INTO `auth_permission` VALUES (66, 'Can change 网站配置', 17, 'change_websiteconfig');
INSERT INTO `auth_permission` VALUES (67, 'Can delete 网站配置', 17, 'delete_websiteconfig');
INSERT INTO `auth_permission` VALUES (68, 'Can view 网站配置', 17, 'view_websiteconfig');
INSERT INTO `auth_permission` VALUES (69, 'Can add 图片配置', 18, 'add_imagesconfig');
INSERT INTO `auth_permission` VALUES (70, 'Can change 图片配置', 18, 'change_imagesconfig');
INSERT INTO `auth_permission` VALUES (71, 'Can delete 图片配置', 18, 'delete_imagesconfig');
INSERT INTO `auth_permission` VALUES (72, 'Can view 图片配置', 18, 'view_imagesconfig');
INSERT INTO `auth_permission` VALUES (73, 'Can add 博主信息', 19, 'add_info');
INSERT INTO `auth_permission` VALUES (74, 'Can change 博主信息', 19, 'change_info');
INSERT INTO `auth_permission` VALUES (75, 'Can delete 博主信息', 19, 'delete_info');
INSERT INTO `auth_permission` VALUES (76, 'Can view 博主信息', 19, 'view_info');
INSERT INTO `auth_permission` VALUES (77, 'Can add captcha store', 20, 'add_captchastore');
INSERT INTO `auth_permission` VALUES (78, 'Can change captcha store', 20, 'change_captchastore');
INSERT INTO `auth_permission` VALUES (79, 'Can delete captcha store', 20, 'delete_captchastore');
INSERT INTO `auth_permission` VALUES (80, 'Can view captcha store', 20, 'view_captchastore');
INSERT INTO `auth_permission` VALUES (81, 'Can add 笔记名称', 21, 'add_note');
INSERT INTO `auth_permission` VALUES (82, 'Can change 笔记名称', 21, 'change_note');
INSERT INTO `auth_permission` VALUES (83, 'Can delete 笔记名称', 21, 'delete_note');
INSERT INTO `auth_permission` VALUES (84, 'Can view 笔记名称', 21, 'view_note');
INSERT INTO `auth_permission` VALUES (85, 'Can add 笔记一级目录', 22, 'add_firstcatalogue');
INSERT INTO `auth_permission` VALUES (86, 'Can change 笔记一级目录', 22, 'change_firstcatalogue');
INSERT INTO `auth_permission` VALUES (87, 'Can delete 笔记一级目录', 22, 'delete_firstcatalogue');
INSERT INTO `auth_permission` VALUES (88, 'Can view 笔记一级目录', 22, 'view_firstcatalogue');
INSERT INTO `auth_permission` VALUES (89, 'Can add 笔记二级目录', 23, 'add_secondcatalogue');
INSERT INTO `auth_permission` VALUES (90, 'Can change 笔记二级目录', 23, 'change_secondcatalogue');
INSERT INTO `auth_permission` VALUES (91, 'Can delete 笔记二级目录', 23, 'delete_secondcatalogue');
INSERT INTO `auth_permission` VALUES (92, 'Can view 笔记二级目录', 23, 'view_secondcatalogue');
INSERT INTO `auth_permission` VALUES (93, 'Can add 笔记内容', 24, 'add_section');
INSERT INTO `auth_permission` VALUES (94, 'Can change 笔记内容', 24, 'change_section');
INSERT INTO `auth_permission` VALUES (95, 'Can delete 笔记内容', 24, 'delete_section');
INSERT INTO `auth_permission` VALUES (96, 'Can view 笔记内容', 24, 'view_section');
INSERT INTO `auth_permission` VALUES (97, 'Can add 笔记评论回复记录', 25, 'add_sectionmessage');
INSERT INTO `auth_permission` VALUES (98, 'Can change 笔记评论回复记录', 25, 'change_sectionmessage');
INSERT INTO `auth_permission` VALUES (99, 'Can delete 笔记评论回复记录', 25, 'delete_sectionmessage');
INSERT INTO `auth_permission` VALUES (100, 'Can view 笔记评论回复记录', 25, 'view_sectionmessage');
INSERT INTO `auth_permission` VALUES (101, 'Can add 笔记浏览记录', 26, 'add_sectionviewhistory');
INSERT INTO `auth_permission` VALUES (102, 'Can change 笔记浏览记录', 26, 'change_sectionviewhistory');
INSERT INTO `auth_permission` VALUES (103, 'Can delete 笔记浏览记录', 26, 'delete_sectionviewhistory');
INSERT INTO `auth_permission` VALUES (104, 'Can view 笔记浏览记录', 26, 'view_sectionviewhistory');
INSERT INTO `auth_permission` VALUES (105, 'Can add 笔记目录', 27, 'add_catalogue');
INSERT INTO `auth_permission` VALUES (106, 'Can change 笔记目录', 27, 'change_catalogue');
INSERT INTO `auth_permission` VALUES (107, 'Can delete 笔记目录', 27, 'delete_catalogue');
INSERT INTO `auth_permission` VALUES (108, 'Can view 笔记目录', 27, 'view_catalogue');
INSERT INTO `auth_permission` VALUES (109, 'Can add association', 28, 'add_association');
INSERT INTO `auth_permission` VALUES (110, 'Can change association', 28, 'change_association');
INSERT INTO `auth_permission` VALUES (111, 'Can delete association', 28, 'delete_association');
INSERT INTO `auth_permission` VALUES (112, 'Can view association', 28, 'view_association');
INSERT INTO `auth_permission` VALUES (113, 'Can add code', 29, 'add_code');
INSERT INTO `auth_permission` VALUES (114, 'Can change code', 29, 'change_code');
INSERT INTO `auth_permission` VALUES (115, 'Can delete code', 29, 'delete_code');
INSERT INTO `auth_permission` VALUES (116, 'Can view code', 29, 'view_code');
INSERT INTO `auth_permission` VALUES (117, 'Can add nonce', 30, 'add_nonce');
INSERT INTO `auth_permission` VALUES (118, 'Can change nonce', 30, 'change_nonce');
INSERT INTO `auth_permission` VALUES (119, 'Can delete nonce', 30, 'delete_nonce');
INSERT INTO `auth_permission` VALUES (120, 'Can view nonce', 30, 'view_nonce');
INSERT INTO `auth_permission` VALUES (121, 'Can add user social auth', 31, 'add_usersocialauth');
INSERT INTO `auth_permission` VALUES (122, 'Can change user social auth', 31, 'change_usersocialauth');
INSERT INTO `auth_permission` VALUES (123, 'Can delete user social auth', 31, 'delete_usersocialauth');
INSERT INTO `auth_permission` VALUES (124, 'Can view user social auth', 31, 'view_usersocialauth');
INSERT INTO `auth_permission` VALUES (125, 'Can add partial', 32, 'add_partial');
INSERT INTO `auth_permission` VALUES (126, 'Can change partial', 32, 'change_partial');
INSERT INTO `auth_permission` VALUES (127, 'Can delete partial', 32, 'delete_partial');
INSERT INTO `auth_permission` VALUES (128, 'Can view partial', 32, 'view_partial');
COMMIT;

-- ----------------------------
-- Table structure for auth_user
-- ----------------------------
DROP TABLE IF EXISTS `auth_user`;
CREATE TABLE `auth_user` (
  `id` int NOT NULL AUTO_INCREMENT,
  `password` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `last_login` datetime(6) DEFAULT NULL,
  `is_superuser` tinyint(1) NOT NULL,
  `username` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `first_name` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `last_name` varchar(150) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `email` varchar(254) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `is_staff` tinyint(1) NOT NULL,
  `is_active` tinyint(1) NOT NULL,
  `date_joined` datetime(6) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `username` (`username`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=78 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_user
-- ----------------------------
BEGIN;
INSERT INTO `auth_user` VALUES (1, 'pbkdf2_sha256$216000$wIPVwhfCX6Vd$mX8tcmD/0Jc9RnN4fRGNcA5klfBioErsVb5Zf0EFRKo=', '2021-01-15 17:16:09.217743', 1, 'admin', '', '', 'cuiliang0302@qq.com', 1, 1, '1900-01-20 03:04:31.291410');
INSERT INTO `auth_user` VALUES (45, 'pbkdf2_sha256$216000$oamubMxbPgao$R/jrykFw6Wjd0AGUp5d2aACNYXwKyXRQ9u3dpmIg1KU=', '1900-01-20 14:42:39.821093', 0, 'admin1', '', '', '1554382112311@qq.com', 0, 1, '1900-01-20 11:24:00.000000');
INSERT INTO `auth_user` VALUES (62, '!REY4p8q4n53U8b7o379H6QnbaVJiYktfyBFhJ1bE', '2020-12-26 18:49:49.128819', 0, 'ζั͡ޓއއ陌影', 'ζั͡ޓއއ陌影', '', '', 0, 1, '2020-12-26 18:49:49.106095');
INSERT INTO `auth_user` VALUES (63, '!A5uyigknHEuwaFJYfDKvj9yQ4slEv9v6ndaBdfRy', '2020-12-26 18:52:52.084599', 0, '笙清初艳离', '笙清初艳离', '', '', 0, 1, '2020-12-26 18:52:52.056801');
INSERT INTO `auth_user` VALUES (67, '!HkFdQ56UWZFNvhdzcndrXy4iYyQRJJhgY7ckSsBg', '2020-12-26 21:04:29.405307', 0, 'Starrysky', 'Starry sky', '', '', 0, 1, '2020-12-26 21:04:29.382429');
INSERT INTO `auth_user` VALUES (68, '!7P2zSVlDkcvyAj6hxD3HTj89Uo6uy5tDl9zDVDu3', '2020-12-26 21:08:00.286236', 0, '鑫空物语50880', '鑫空物语50880', '', '', 0, 1, '2020-12-26 21:08:00.265263');
INSERT INTO `auth_user` VALUES (69, '!kGVhDy6wOGjLyMqCyE4M3eV67KLPzFYUVP7BQL3i', '2020-12-26 21:09:54.018038', 0, '~', '~', '', '', 0, 1, '2020-12-26 21:09:53.986020');
INSERT INTO `auth_user` VALUES (70, '!TlsxEhHgaAQW89sIRBJKXUohEZzeDP9tRdnbSi6d', '2020-12-26 21:10:14.126805', 0, 'cj16201603', 'cj16201603', '', '', 0, 1, '2020-12-26 21:10:14.098074');
INSERT INTO `auth_user` VALUES (71, '!0djt2of3kupU77VaBlZABbiGP0YTIzmylfPhUkmB', '2020-12-26 21:12:24.427130', 0, 'cuiliang0302', '', '', '', 0, 1, '2020-12-26 21:12:24.402865');
INSERT INTO `auth_user` VALUES (72, '!vaTEiyOTXfA1mVlr4Cby6gfjKcX3FSroAxN41Adv', '2020-12-27 18:05:31.248925', 0, '✎﹏ℳ๓₯㎕♡柠檬不萌', '✎﹏ℳ๓₯㎕ ♡柠檬不萌', '', '', 0, 1, '2020-12-26 21:44:14.887520');
INSERT INTO `auth_user` VALUES (73, '!qmTN7ynH8pIUtQp36mWL263eciugIsbtANnNajFv', '2020-12-26 21:46:53.085585', 0, '崔亮60857', '崔亮60857', '', '', 0, 1, '2020-12-26 21:46:53.044129');
INSERT INTO `auth_user` VALUES (74, '!QFy40B8CNpUTwyng1mGsG6ahCwhPPrYkDOBex3tk', '2020-12-26 22:21:25.691867', 0, 'SoulSurferdj', 'gitddj', '', '154319309@qq.com', 0, 1, '2020-12-26 22:21:25.665529');
INSERT INTO `auth_user` VALUES (75, '!edA1szUpCQuoCZ8tHnqtVxHZeCR5Ae14vmgqPVZ2', '2020-12-26 22:47:19.979478', 0, 'guancongcong', '', '', '951578673@qq.com', 0, 1, '2020-12-26 22:47:19.959677');
INSERT INTO `auth_user` VALUES (76, '!Kz58z1e4ZvEdApJFn22m7RhlwAdn0doK0PKjUglb', '2020-12-27 20:23:37.240793', 0, '蕉太狼', '蕉太狼', '', '', 0, 1, '2020-12-27 20:23:37.210876');
INSERT INTO `auth_user` VALUES (77, '!bl1dcBRgmkxcoIlEli3NvDGT4svdSD7HXK4h2nVy', '2021-01-04 14:30:10.072417', 0, 'bug', 'bug', '', '', 0, 1, '2021-01-04 14:30:10.042521');
COMMIT;

-- ----------------------------
-- Table structure for auth_user_groups
-- ----------------------------
DROP TABLE IF EXISTS `auth_user_groups`;
CREATE TABLE `auth_user_groups` (
  `id` int NOT NULL AUTO_INCREMENT,
  `user_id` int NOT NULL,
  `group_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `auth_user_groups_user_id_group_id_94350c0c_uniq` (`user_id`,`group_id`) USING BTREE,
  KEY `auth_user_groups_group_id_97559544_fk_auth_group_id` (`group_id`) USING BTREE,
  CONSTRAINT `auth_user_groups_group_id_97559544_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `auth_user_groups_user_id_6a12ed8b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_user_groups
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for auth_user_user_permissions
-- ----------------------------
DROP TABLE IF EXISTS `auth_user_user_permissions`;
CREATE TABLE `auth_user_user_permissions` (
  `id` int NOT NULL AUTO_INCREMENT,
  `user_id` int NOT NULL,
  `permission_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `auth_user_user_permissions_user_id_permission_id_14a6b632_uniq` (`user_id`,`permission_id`) USING BTREE,
  KEY `auth_user_user_permi_permission_id_1fbb5f2c_fk_auth_perm` (`permission_id`) USING BTREE,
  CONSTRAINT `auth_user_user_permi_permission_id_1fbb5f2c_fk_auth_perm` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `auth_user_user_permissions_user_id_a95ead1b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of auth_user_user_permissions
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for blog_article
-- ----------------------------
DROP TABLE IF EXISTS `blog_article`;
CREATE TABLE `blog_article` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `excerpt` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `img` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `body` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `view` int unsigned NOT NULL,
  `like` int unsigned NOT NULL,
  `collection` int unsigned NOT NULL,
  `created_time` datetime(6) NOT NULL,
  `modified_time` datetime(6) NOT NULL,
  `author_id` int NOT NULL,
  `category_id` int DEFAULT NULL,
  `comment` int unsigned NOT NULL,
  `is_release` tinyint(1) NOT NULL,
  `is_recommend` tinyint(1) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  KEY `blog_article_author_id_905add38_fk_auth_user_id` (`author_id`) USING BTREE,
  KEY `blog_article_category_id_7e38f15e_fk_blog_category_id` (`category_id`) USING BTREE,
  CONSTRAINT `blog_article_author_id_905add38_fk_auth_user_id` FOREIGN KEY (`author_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `blog_article_category_id_7e38f15e_fk_blog_category_id` FOREIGN KEY (`category_id`) REFERENCES `blog_category` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=132 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of blog_article
-- ----------------------------
BEGIN;
INSERT INTO `blog_article` VALUES (2, 'k8s基础-概念和术语', '本文从kubernets集群组件、资源抽象、控制器抽象、辅助概念四个方面对kubernets基本概念做简单介绍', 'cover/2020_10_28_23_41_52_169571.jpg', '[TOC]\n		\n		![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603895717319-4987bb90-1d1d-49ba-b97d-de8cd21211f8.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=720&originWidth=594&size=119660&status=done&style=none&width=594)\n		\n		\n		# 一、集群组件\n		\n		1. Master：\n		\n		指的是集群的控制节点，每个 k8s 集群里至少需要一个 Master 节点来负责整个集群的管理和控制，所有控制命令都是发给它，它来负责具体的调度和执行。\n		\n		2. Node：\n		\n		是 k8s 集群中用于运行 Pod 的机器，Node 为整个集群提供可用的集群资源，比如用于保持数据、运行作业、创建网络路由等。如果某个 Node节点宕机，其上的工作负载会被 Master 自动转移到其它节点上去。\n		# 二、资源抽象\n		\n		1. 容器组（Pod）:\n		\n		Kubernetes中最小的资源单位。由位于同一节点上若干容器组成，彼此共享网络命名空间和存储卷（Volume）。Pod是Kubernetes中进行管理的最小资源单位，是最为基础的概念。跟容器类似，Pod是短暂的，随时可变的，通常不带有状态。一般每个Pod中除了应用容器外，还包括一个初始的pause容器，完成网络和存储空间的初始化；\n		\n		2. 服务（Service）：\n		\n		对外提供某个特定功能的一组Pod（可通过标签来选择）和所关联的访问配置。由于Pod的地址是不同的，而且可能改变，直接访问Pod将无法获得稳定的业务。Kubernetes通过服务提供唯一固定的访问地址（如IP地址或者域名），不随后面Pod改变而变化。用户无须关心具体的Pod信息；\n		\n		3. 存储卷（Volume）：\n		\n		存储卷类似Docker中的概念，提供数据的持久化存储（如Pod重启后），并支持更高级的生命周期管理和参数指定功能，支持多种本地和云存储类型；\n		\n		4. 命名空间（Namespace）:\n		\n		Kubernetes通过命名空间来实现虚拟化，将同一组物理资源虚拟为不同的抽象集群，避免不同租户的资源发生命名冲突，另外可以进行资源限额。\n		# 三、控制器抽象\n		\n		1. 副本集（ReplicaSet）：\n		\n		基于Pod的抽象。使用它可以让集群中始终维持某个Pod的指定副本数的健康实例。副本集中的Pod相互并无差异，可以彼此替换。\n		\n		2. 部署（Deployment）：\n		\n		管理Pod或副本集，并且支持升级操作。部署控制器可以提供提供比副本集更方便的操作，推荐使用；\n		\n		3. 状态集（StatefulSet）：\n		\n		管理带有状态的应用。相比部署控制器，状态集可以为Pod分配独一无二的身份，确保在重新调配等操作时也不会相互替换。自1.9版本开始正式支持；\n		\n		4. Daemon集（DaemonSet）：\n		\n		确保节点上肯定运行某个Pod，一般用来采集日志（如logstash）、监控节点（如collectd）或提供存储（如glusterd）使用；\n		\n		5. 任务（Job）：\n		\n		适用于短期处理场景。任务将创建若干Pod，并确保给定数目的Pod最终正常退出（完成指定的处理）；\n		\n		6. 横向Pod扩展器（Horizontal Pod Autoscaler, HPA）：\n		\n		类似云里面的自动扩展组，根据Pod的使用率（典型如CPU）自动调整一个部署里面Pod的个数，保障服务可用性；\n		\n		7. 入口控制器（Ingress Controller）：\n		\n		定义外部访问集群中资源的一组规则，用来提供七层代理和负载均衡服务。\n		# 四、辅助概念\n		\n		1. 标签（Label）：\n		\n		键值对，可以标记到资源对象上，用来对资源进行分类和筛选；\n		\n		2. 选择器（Selector）：\n		\n		基于标签概念的一个正则表达式，可通过标签来筛选出一组资源；\n		\n		3. 注解（Annotation）：\n		\n		键值对，可以存放大量任意数据，一般用来添加对资源对象的细说明，可供其他工具处理。\n		\n		4. 秘密数据（Secret）：\n		\n		存放敏感数据，例如用户认证的口令等；\n		\n		5. 名字（Name）：\n		\n		用户提供给资源的别名，同类资源不能重名；\n		\n		6. 持久化存储（PersistentVolume）：\n		\n		确保数据不会丢失；\n		\n		7. 资源限额（Resource Quotas）：\n		\n		用来限制某个命名空间下对资源的使用，开始逐渐提供多租户支持；\n		\n		8. 安全上下文（Security Context）：\n		\n		应用到容器上的系统安全配置，包括uid、gid、capabilities、SELinux角色等；\n		\n		9. 服务账号（Service Accounts）：\n		\n		操作资源的用户账号。\n		', 247, 21, 23, '2020-07-20 03:07:08.561860', '2021-01-26 05:39:40.156443', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (3, 'k8s基础-抽象对象', 'Kubernetes对集群中的资源进行了不同级别的抽象，每个资源都是一个REST对象，通过API进行操作，通过json或yaml格式的模板文件进行定义。', 'cover/2020_10_28_23_44_50_238342.jpg', '[TOC]\n# 一、抽象资源对象\n## 1.  容器组（Pod）\n在Kubernetes中，并不直接操作容器，最小的管理单位是容器组（Pod）。容器组由一个或多个容器组成，Kubernetes围绕容器组进行创建、调度、停止等生命周期管理。\n同一个容器组中，各个容器共享命名空间（包括网络、IPC、文件系统等容器支持的命名空间）、cgroups限制和存储卷。这意味着同一个容器组中，各个应用可以很方便地相互进行访问，比如通过localhost地址进行网络访问，通过信号量和共享内存进行进程间通信等，类似经典场景中运行在同一个操作系统中的一组进程。可以简单地将一个Pod当作是一个抽象的“虚拟机”，里面运行若干个不同的进程（每个进程实际上就是一个容器）。\n实现上，是先创建一个gcr.io/google_containers/pause容器，创建相关命名空间，然后创建Pod中的其他应用容器，并共享pause容器的命名空间。\n组成容器组的若干容器往往是存在共同的应用目的，彼此关联十分紧密，例如一个Web应用与对应的日志采集应用、状态监控应用。如果单纯把这些相关的应用放一个容器里面，又会造成过度耦合，管理、升级都不方便。\n容器组既保持了容器轻量解耦的特性，又提供了调度操作的便利性，在实践中提供了比单个容器更为灵活和更有意义的抽象。\n## 2. 服务（Service）\n服务（Service）的提出，主要是要解决Pod地址可变的问题。由于Pod随时可能发生故障，并可能在其他节点上被重启，它的地址是不能保持固定的。因此，用一个服务来代表提供某一类功能（可以通过标签来筛选）的一些Pod，并分配不随Pod位置变化而改变的虚拟访问地址（Cluster IP）。\n典型情况是，比如网站的后端服务，可能有多个Pod都运行了后端处理程序，它们可以组成一个服务。前端只需通过服务的唯一虚拟地址来访问即可，而无须关心具体是访问到了哪个Pod。可见，服务跟负载均衡器实现的功能很相似。\n根据访问方式的不同，服务可以分为如下几种类型：\n\n- ClusterIP：提供一个集群内部的地址，该地址只能在集群内解析和访问。ClusterIP是默认的服务类型；\n- NodePort：在每个集群节点上映射服务到一个静态的本地端口（默认范围为30000～32767）。从集群外部可以直接访问，并自动路由到内部自动创建的ClusterIP；\n- LoadBalancer：使用外部的路由服务，自动路由访问到自动创建的NodePort和ClusterIP；\n- ExternalName：将服务映射到externalName域指定的地址，需要1.7以上版本kube-dns的支持。\n\n组成一个服务的Pod可能是属于不同复制控制器的，但服务自身是不知道复制控制器的存在的。\n## 3. 存储卷（Volume）\n即容器挂载的数据卷，跟Pod有一致的生命周期，Pod生存过程（包括重启）中，数据卷跟着存在；Pod退出，则数据卷跟着退出。\n几个比较常见的数据卷类型包括：emptyDir、hostPath、gcePersistentDisk、awsElastic-BlockStore、nfs、gitRepo、secret。\n\n- emptyDir：当Pod创建的时候，在节点上创建一个空的挂载目录，挂载到容器内。当Pod从节点离开（例如删除掉）的时候，自动删除挂载目录内数据。节点上的挂载位置可以为物理硬盘或内存。这一类的挂载适用于非持久化的存储，例如与Pod任务相关的临时数据等。除此之外，其他存储格式大都是持久化的；\n- hostPath：将节点上某已经存在的目录挂载到Pod中，Pod退出后，节点上的数据将保留；\n- gcePersistentDisk：使用GCE的Persistent Disk服务，Pod退出后，会保留数据；\n- awsElasticBlockStore：使用AWS的EBS Volume服务，数据也会持久化保留；\n- nfs：使用NFS协议的网络存储，也是持久化数据存储；\n- gitRepo：挂载一个空目录到Pod，然后clone指定的git仓库代码到里面，适用于直接从仓库中给定版本的代码来部署应用；\n- secret：用来传递敏感信息（如密码等），基于内存的tmpfs，挂载临时秘密文件。\n- 持久化的存储以插件的形式提供为PersistentVolume资源，用户通过请求某个类型的PersistentVolumeClaim资源，来从匹配的持久化存储卷上获取绑定的存储。\n# 二、控制器抽象对象\n> 控制器抽象对象是基于对所操控对象的进一步抽象，附加了各种资源的管理功能，目前主要包括副本集、部署、状态集、Daemon集、任务等。\n\n## 1. 副本集（ReplicaSet）和部署（Deployment）\n在Kubernetes看来，Pod资源是可能随时发生故障的，并不需要保证Pod的运行，而是在发生失败后重新生成。Kubernetes通过复制控制器来实现这一功能。\n用户申请容器组后，复制控制器将负责调度容器组到某个节点上，并保证它的给定份数（replica）正常运行。当实际运行Pod份数超过数目，则终止某些Pod；当不足，则创建新的Pod。一般建议，即使Pod份数是1，也要使用复制控制器来创建，而不是直接创建Pod。\n可以将副本集类比为进程的监管者（supervisor）的角色，只不过它不光能保持Pod的持续运行，还能保持集群中给定类型Pod同时运行的个数为指定值。Pod是临时性的，可以随时由副本集创建或者销毁，这意味着要通过Pod自身的地址访问应用是不能保证一致性的。Kubernetes通过服务的概念来解决这个问题。\n从1.2.0版本开始，Kubernetes将正式引入部署机制来支持更灵活的Pod管理，从而用户无须直接跟复制控制器打交道了。部署代表用户对集群中应用的一次更新操作，在副本集的基础上还支持更新操作。每次滚动升级（rolling-update），会自动将副本集中旧版本的Pod逐渐替换为新的版本。\n另外，副本集也可以支持成为“横向Pod扩展器”的操作对象。\n## 2. 状态集（StatefulSet）\n通常情况下，使用容器的应用都是不带状态的，意味着部署同一个应用的多个Pod彼此可以替换，而且生命周期可以是很短暂的。任何一个Pod退出后，Kubernetes在集群中可以自动创建一个并按照调度策略调度到节点上。无状态的应用时候，关心的主要是副本的个数，而不关心名称、位置等。与此对应，某些应用需要关心Pod的状态（包括各种数据库和配置服务等），挂载独立的存储。一旦当某个Pod故障退出后，Kubernetes会创建同一命名的Pod，并挂载原来的存储，以便Pod中应用继续执行，实现了该应用的高可用性。\n状态集正是针对这种需求而设计的，提供比副本集和部署更稳定可靠的运行支持。\n## 3. Daemon集（DaemonSet）\nDaemon集适合于长期运行在后台的伺服类型应用，例如对节点的日志采集或状态监控等后台支持服务。\nDaemon集的应用会确保在指定类型的每个节点上都运行一个该应用的Pod。可能是集群中所有节点，也可能是指定标签的一类节点。\n## 4. 任务（Job）\n不同于长期运行的应用，任务（Job）代表批处理类型的应用。任务中应用完成某一类的处理即可退出，有头有尾。例如，计算Pi到多少位，可以指定若干个Pod成功完成计算，即算任务成功执行。\n## 5. 横向Pod扩展器（Horizontal Pod Autoscaler, HPA）\n横向Pod扩展器（Horizontal Pod Autoscaler, HPA）解决应用波动的情况。类似云里面的自动扩展组，扩展器根据Pod的使用率（典型如CPU、内存等）自动调整一个部署里面Pod的个数，保障服务在不同压力情况下保证平滑的输出效果。\n控制管理器会定期检查性能指标，在满足条件时候触发横向伸缩。Kubernetes 1.6版本开始支持基于多个指标的伸缩。\n# 三、其他抽象对象\n## 1. 标签（Label）\n标签（Label）是一组键值对，用来标记所绑定对象（典型的就是Pod）的识别属性，进而可以分类。比如name=apache|nginx、type=web|db、release=alpha|beta|stable、tier=frontend|backend等。另外，Label键支持通过/来添加前缀，可以用来标注资源的组织名称等。一般的，前缀不能超过253个字符，键名不能超过63个字符。\n标签所定义的属性是不唯一的，这意味着不同资源可能带有相同的标签键值对。这些属性可以将业务的相关信息绑定到对象上，用来对资源对象进行分类和选择过滤。\n## 2. 注解（Annotation）\n注解（Annotation）跟标签很相似，也是键值对，但并非用来标识对象，同时可以存储更多更复杂的信息。不同的是，注解并不是为了分类资源对象，而是为了给对象增加更丰富的描述信息。这些信息是任意的，数据量可以很大，可以包括各种结构化、非结构化的数据。\n常见的注解包括时间戳、发行信息、开发者信息等，一般是为了方便用户查找相关线索。\n## 3. 选择器（Selector）\n基于资源对象上绑定的标签信息，选择器（Selector）可以通过指定标签键值对来过滤出一组特定的资源对象。\n选择器支持的语法包括基于等式（Equality-based）的，和基于集合（Set-based）的。\n基于等式的选择，即通过指定某个标签是否等于某个值，例如env=production或者tier! =frontend等。多个等式可以通过AND逻辑组合在一起。\n基于集合的选择，即通过指定某个标签的值是否在某个集合内，例如env in (staging, production)。\n## 4. 秘密数据（Secret）\n秘密数据（Secret）资源用来保存一些敏感的数据，这些数据（例如密码）往往不希望别的用户看到，但是在启动某个资源（例如Pod）的时候需要提供。通过把敏感数据放到Secret里面，用户只需要提供Secret的别名即可使用。\n在对应容器（secret-test-pod.test-container）内，通过环境变量$SECRET_USERNAME和$SECRET_PASSWORD即可获取到原始的用户名和密码信息。\n此外，还可以采用数据卷的方式把秘密数据的值以文件形式放到容器内。通常，秘密数据不要超过1 MB。\n在整个过程中，只有秘密数据的所有人和最终运行的容器（准确的说，需要是同一个服务账号下面的）能获取原始敏感数据，只接触到Pod定义模板的人是无法获取到的。\n## 5. UID和名字\nKubernetes用UID和名字（Name）来标识对象。其中，UID是全局唯一的，并且不能复用；而名字则仅仅要求对某种类型的资源（在同一个命名空间内）内是唯一的，并且当某个资源移除后，其名字可以被新的资源复用。\n这意味着，可以创建一个Pod对象，命名为test，同样可以创建一个复制控制器，命名也为test。一般的，名字字符串的长度不要超过253个字符。\n## 6. 命名空间\n命名空间（Namespace）用来隔离不同用户的资源，类似租户或项目的概念。默认情况下，相同命名空间中的对象将具有相同的访问控制策略。\n同一个命名空间内，资源不允许重名，但不同命名空间之间，允许存在重名。用户在创建资源的时候可以通过--namespace=<some_namespace>来指定所属的命名空间。\nKubernetes集群启动后，会保留两个特殊的命名空间：\n\n- default：资源未指定命名空间情况下，默认属于该空间；\n- kube-system：由Kubernetes系统自身创建的资源。\n\n另外，大部分资源都属于某个命名空间，但部分特殊资源，如节点、持久存储等不属于任何命名空间。\n## 7. 污点和容忍\n污点（Taint）和容忍（Toleration）用于辅助管理Pod到工作节点的调度过程。具有某个污点的工作节点，在不容忍的Pod看来，要尽量避免调度到它上面去。\n通常情况下，可以为一个工作节点注明若干污点，只有对这些污点容忍的Pod，才可以被调度到这些具有污点的节点上。\n', 293, 4, 55, '2020-08-18 07:01:19.543029', '2021-01-27 05:50:25.330836', 1, 6, 2, 1, 0);
INSERT INTO `blog_article` VALUES (4, 'k8s基础-阿里云获取镜像文件', '本文介绍了如何通过阿里云快速获取gcr.io上的镜像文件，解决google镜像下载慢的问题', 'cover/2020_10_28_23_45_21_435184.jpg', '[TOC]\r\n# 一、常用国内加速地址\r\n## 1. 使用方法\r\n• 原镜像下载操作\r\n` docker pull quay.io/deis/go-dev:v1.10.0 ` \r\n• 修改国内地址操作\r\n`docker pull quay.azk8s.cn/deis/go-dev:v1.10.0` \r\n• 修改镜像标签\r\n`docker tag quay.azk8s.cn/deis/go-dev:v1.10.0 quay.io/deis/go-dev:v1.10.0` \r\n## 2. 加速地址\r\ngcr.azk8s.cn/google_containers/:\r\ngcr.mirrors.ustc.edu.cn/xxx/yyy:zzz\r\ndockerhub.azk8s.cn/xxx/yyy:zzz\r\n# 二、使用阿里镜像服务加速\r\n\r\n1. github创建仓库，新建Dockerfile文件，内容为FROM 镜像名称\r\n\r\n![1.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897552599-323da0d8-66e1-40f1-bb04-88d72783b405.png#align=left&display=inline&height=366&margin=%5Bobject%20Object%5D&name=1.png&originHeight=366&originWidth=1045&size=32133&status=done&style=none&width=1045)\r\n\r\n2. 登陆阿里云的容器镜像服务然后点击创建镜像仓库\r\n\r\n![2.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897572140-b7e88118-70bb-4b1b-bea6-c35e93e84ba4.png#align=left&display=inline&height=695&margin=%5Bobject%20Object%5D&name=2.png&originHeight=695&originWidth=777&size=25057&status=done&style=none&width=777)\r\n\r\n3. 代码源绑定github账号\r\n\r\n![3.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897600107-67ccca44-50f6-4f47-bcf6-06635625c716.png#align=left&display=inline&height=377&margin=%5Bobject%20Object%5D&name=3.png&originHeight=377&originWidth=1260&size=49582&status=done&style=none&width=1260)\r\n\r\n4. 构建——添加规则\r\n\r\n![4.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897615621-1a52d45d-fc1c-4274-b8de-baadff193d97.png#align=left&display=inline&height=466&margin=%5Bobject%20Object%5D&name=4.png&originHeight=466&originWidth=598&size=15270&status=done&style=none&width=598)\r\n\r\n5. 根据提示使用镜像\r\n\r\n![5.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897638784-ba27762a-6b42-4655-8dd9-374b1e977272.png#align=left&display=inline&height=310&margin=%5Bobject%20Object%5D&name=5.png&originHeight=310&originWidth=768&size=14613&status=done&style=none&width=768)\r\n\r\n6. pull镜像，修改tag\r\n`docker tag registry.cn-hangzhou.aliyuncs.com/cuiliang_images/cuiliang_images:1 k8s.gcr.io/metrics-server-amd64:v0.3.6` \r\n6. 导出镜像\r\n`docker save -o metrics-server.tar k8s.gcr.io/metrics-server-amd64:v0.3.6` \r\n6. 导入镜像\r\n`docker load -i metrics-server.tar`', 105, 2, 1, '2020-08-20 01:05:52.175303', '2021-01-26 05:40:15.248808', 1, 6, 1, 1, 0);
INSERT INTO `blog_article` VALUES (5, 'k8s基础-Kubernetes特性', 'Kubernetes是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。', 'cover/2020_10_29_16_12_11_939561.jpg', '>Kubernetes是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平台。用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。Kubernetes提供了接口和可组合的平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。简单总结起来，它具有以下几个重要特性。\n\n1. 自动装箱\n建构于容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以提升资源利用率。\n1. 自我修复（自愈）\n支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制。\n1. 水平扩展\n支持通过简单命令或UI手动水平扩展，以及基于CPU等资源负载率的自动水平扩展机制。\n1. 服务发现和负载均衡\nKubernetes通过其附加组件之一的KubeDNS（或CoreDNS）为系统内置了服务发现功能，它会为每个Service配置DNS名称，并允许集群内的客户端直接使用此名称发出访问请求，而Service则通过iptables或ipvs内建了负载均衡机制。\n1. 自动发布和回滚\nKubernetes支持“灰度”更新应用程序或其配置信息，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作。\n1. 密钥和配置管理\nKubernetes的ConfigMap实现了配置数据与Docker镜像解耦，需要时，仅对配置做出变更而无须重新构建Docker镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用户名和密码、令牌、密钥等信息，Kubernetes专门提供了Secret对象为其解耦，既便利了应用的快速开发和交付，又提供了一定程度上的安全保障。\n1. 存储编排\nKubernetes支持Pod对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储（如AWS和GCP等），以及网络存储系统（例如，NFS、iSCSI、GlusterFS、Ceph、Cinder和Flocker等）。\n1. 批量处理执行\n除了服务型应用，Kubernetes还支持批处理作业及CI（持续集成），如果需要，一样可以实现容器故障后恢复。', 227, 6, 2, '2020-08-20 02:06:30.886199', '2021-01-26 05:40:24.760996', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (6, 'k8s基础-集群组件', '本文主要从kubernets的结构拓扑、Master组件、集群状态存储、node组件、核心组件五方面对kubernets集群组件做简介', 'cover/2020_10_28_23_44_13_991458.jpg', '[TOC]\n\n# 一、结构拓扑\n![12.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603896164490-77596429-04f6-4278-8fad-8dbf9c1ce799.png#align=left&display=inline&height=573&margin=%5Bobject%20Object%5D&name=12.png&originHeight=573&originWidth=793&size=16777&status=done&style=none&width=793)\n\n1. 一个典型的Kubernetes集群由多个工作节点（worker node）和一个集群控制平面（control plane，即Master），以及一个集群状态存储系统（etcd）组成。其中Master节点负责整个集群的管理工作，为集群提供管理接口，并监控和编排集群中的各个工作节点。\n1. Master节点主要由apiserver、controller-manager和scheduler三个组件，以及一个用于集群状态存储的etcd存储服务组成，而每个Node节点则主要包含kubelet、kube-proxy及容器引擎（Docker是最为常用的实现）等组件。此外，完整的集群服务还依赖于一些附加组件，如KubeDNS等。\n# 二、Master组件\n> Kubernetes的集群控制平面由多个组件组成，这些组件可统一运行于单一Master节点，也可以以多副本的方式同时运行于多个节点，以为Master提供高可用功能，甚至还可以运行于Kubernetes集群自身之上。Master主要包含以下几个组件。\n\n1. API服务器（API Server）\n\nAPI Server负责输出RESTful风格的Kubernetes API，它是发往集群的所有REST操作命令的接入点，并负责接收、校验并响应所有的REST请求，结果状态被持久存储于etcd中。因此，API Server是整个集群的网关。\n\n2. 控制器管理器（Controller Manager）\n\nController-Manager Serve用于执行大部分的集群层次的功能，它既执行生命周期功能(例如：命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集、node垃圾收集)，也执行API业务逻辑（例如：pod的弹性扩容）。控制管理提供自愈能力、扩容、应用生命周期管理、服务发现、路由、服务绑定和提供。\n\n3. 调度器（Scheduler）\n\nKubernetes是用于部署和管理大规模容器应用的平台，根据集群规模的不同，其托管运行的容器很可能会数以千计甚至更多。API Server确认Pod对象的创建请求之后，便需要由Scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策。另外，Kubernetes还支持用户自定义调度器。\n# 三、集群状态存储（ETCD）\n\n1. Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中，etcd是由CoreOS基于Raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障（如数据库主节点选择、分布式锁等）。\n1. etcd是独立的服务组件，并不隶属于Kubernetes集群自身。生产环境中应该以etcd集群的方式运行以确保其服务可用性。\n1. etcd不仅能够提供键值数据存储，而且还为其提供了监听（watch）机制，用于监听和推送变更。Kubernetes集群系统中，etcd中的键值发生变化时会通知到API Server，并由其通过watch API向客户端输出。基于watch机制，Kubernetes集群的各组件实现了高效协同。\n# 四、Node组件\n> Node负责提供运行容器的各种依赖环境，并接受Master的管理。每个Node主要由以下几个组件构成。\n\n1. Node的核心代理程序kubelet\n\nkubelet是运行于工作节点之上的守护进程，它从API Server接收关于Pod对象的配置信息并确保它们处于期望的状态（desired state，后文不加区别地称之为“目标状态”）。kubelet会在API Server上注册当前工作节点，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点的资源占用状况。\n每个Node都要提供一个容器运行时（Container Runtime）环境，它负责下载镜像并运行容器。kubelet并未固定链接至某容器运行时环境，而是以插件的方式载入配置的容器环境。这种方式清晰地定义了各组件的边界。目前，Kubernetes支持的容器运行环境至少包括Docker、RKT、cri-o和Fraki等。\n\n2. kube-proxy\n\n每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod对象。\n# 五、核心附件\n> Kubernetes集群还依赖于一组称为“附件”（add-ons）的组件以提供完整的功能，它们通常是由第三方提供的特定应用程序，且托管运行于Kubernetes集群之上。\n\n1. KubeDNS：\n\n在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他Pod可使用此DNS服务解决主机名。Kubernetes自1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kube-dns项目，而SkyDNS则是更早一代的项目。\n\n2. Kubernetes Dashboard\n\nKubernetes集群的全部功能都要基于Web的UI，来管理集群中的应用甚至是集群自身。\n\n3. Ingress Controller\n\nService是一种工作于传统层的负载均衡器，而Ingress是在应用层实现的HTTP（s）负载均衡机制。不过，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器（Ingress Controller）发挥作用。目前，此类的可用项目有Nginx、Traefik、Envoy及HAProxy等。\n', 239, 23, 45, '2020-07-20 03:07:25.459324', '2021-01-26 05:41:04.935339', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (7, 'Jenkins-Jenkins部署', '本文详细讲述了如何使用yum方式安装Jenkins的详细部署流程', 'cover/2020_10_29_16_14_14_498649.jpg', '[TOC]\n# 一、yum安装部署Jenkins\n## 1. 安装JDK\n`yum install -y java`  \n## 2. 安装jenkins\n\n- 添加Jenkins库到yum库，Jenkins将从这里下载安装。\n```bash\nwget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nrpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nyum install -y jenkins\n```\n\n- 如果不能安装就到官网下载jenkis的rmp包，官网地址（[http://pkg.jenkins-ci.org/redhat-stable/](http://pkg.jenkins-ci.org/redhat-stable/)）\n```bash\nwget http://pkg.jenkins-ci.org/redhat-stable/jenkins-2.7.3-1.1.noarch.rpm\nrpm -ivh jenkins-2.7.3-1.1.noarch.rpm1 wget http://pkg.jenkins-ci.org/redhat-stable/jenkins-2.7.3-1.1.noarch.rpm\nrpm -ivh jenkins-2.7.3-1.1.noarch.rpm\n```\n\n- 配置jenkis的端口 (此端口不冲突可以不修改)\n\n`vi /etc/sysconfig/jenkins` \n\n- 找到修改端口号：\n\n`JENKINS_PORT=\"8080\"`\n## 3. 启动jenkins\n`systemctl start jenkins`\n`systemctl enable jenkins`  \n\n- 安装成功后Jenkins将作为一个守护进程随系统启动\n- 系统会创建一个“jenkins”用户来允许这个服务，如果改变服务所有者，同时需要修改/var/log/jenkins, /var/lib/jenkins, 和/var/cache/jenkins的所有者\n- 启动的时候将从/etc/sysconfig/jenkins获取配置参数\n- 默认情况下，Jenkins运行在8080端口，在浏览器中直接访问该端进行服务配置\n- Jenkins的RPM仓库配置被加到/etc/yum.repos.d/jenkins.repo\n## 4. 打开jenkins \n\n- 在浏览器中访问 \n- 首次进入会要求输入初始密码如下图， \n- 初始密码在：/var/lib/jenkins/secrets/initialAdminPassword \n\n![](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603936617984-ea890ea6-224e-4b6d-95c5-93d4248fb25a.png#align=left&display=inline&height=340&margin=%5Bobject%20Object%5D&originHeight=340&originWidth=558&size=0&status=done&style=none&width=558)\n\n- 选择“Install suggested plugins”安装默认的插件，下面Jenkins就会自己去下载相关的插件进行安装。 \n\n![](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603936617954-72e5367a-e11d-4fda-ac28-5cd7252e23d1.png#align=left&display=inline&height=346&margin=%5Bobject%20Object%5D&originHeight=346&originWidth=558&size=0&status=done&style=none&width=558)\n![](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603936618053-e99b6d46-aa71-475b-a894-77456eeebd53.png#align=left&display=inline&height=347&margin=%5Bobject%20Object%5D&originHeight=347&originWidth=558&size=0&status=done&style=none&width=558)\n\n- 创建超级管理员账号 \n\n![](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603936617935-6bddbae1-21d1-4f6f-b8f2-81821d5d47a6.png#align=left&display=inline&height=344&margin=%5Bobject%20Object%5D&originHeight=344&originWidth=558&size=0&status=done&style=none&width=558)\n![](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603936617890-935f3a9f-b3a3-4a0a-9ef0-3a644bdb8105.png#align=left&display=inline&height=351&margin=%5Bobject%20Object%5D&originHeight=351&originWidth=558&size=0&status=done&style=none&width=558)\n', 262, 22, 69, '2020-08-20 15:20:51.672696', '2021-01-27 14:50:05.747136', 1, 7, 1, 1, 0);
INSERT INTO `blog_article` VALUES (8, 'k8s部署-前期准备', '本文主要从架构概述、基础环境配置、安装前设置三个方面对k8s部署准备工作做一个详细的介绍', 'cover/2020_10_30_13_58_38_698822.jpg', '[TOC]\n# 一、概述\n## 1. kubernetes组件架构图\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603980733741-61d38ca3-b46f-47df-b5fc-e7933d41abb7.png#align=left&display=inline&height=705&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=705&originWidth=931&size=353797&status=done&style=none&width=931)\n## 2. 节点信息\n| 主机名 | IP | 角色 | 组件 | 配置 |\n| --- | --- | --- | --- | --- |\n| master | 192.168.10.100 | 管理节点 | kube-apiserver kube-controller-manager docker | 2C2G |\n| node1 | 192.168.10.101 | 计算节点1 | kube-proxy kube-flannel docker | 2C2G |\n| node2 | 192.168.10.102 | 计算节点2 | kube-porxy kube-flannel docker | 2C2G |\n| harbor | 192.168.10.103 | 私有仓库 | harbor docker | 2C2G |\n\n## 3. kubernetes与docker版本\n\n- kubernets1.18支持最新docker版本为19.03.8\n## 4. 整体步骤\n\n- 基础环境配置\n- kubernetes安装前设置(源、镜像及相关配置)\n- kubeadm部署(master)\n- 启用基于flannel的Pod网络\n- kubeadm加入node节点\n- dashboard组件安装与使用\n- heapster监控组件安装与使用\n- 访问测试\n1. 官方参考文档\n[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n# 二、基础环境配置（所有节点）\n\n\n## 1. 修改主机名与hosts文件\n`vim /etc/hosts ` \n![0e6e3f33849a339ede77a5c556f154cb.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981603961-6ca446b6-ffab-42c1-b8ff-0cce9be381a7.png#align=left&display=inline&height=78&margin=%5Bobject%20Object%5D&name=0e6e3f33849a339ede77a5c556f154cb.png&originHeight=78&originWidth=295&size=3474&status=done&style=none&width=295)\n## 2. 验证mac地址uuid，保证各节点mac和uuid唯一\n`cat /sys/class/net/ens33/address` \n`cat /sys/class/dmi/id/product_uuid` \n## 3. 安装依赖包\n`yum -y install conntrack chrony bash-completion ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git` \n## 4. 时间同步\n\n- master节点设置\n\n\n`[root@master ~]#yum -y install chrony` \n`[root@master ~]#vim /etc/chrony.conf` \n![ef248b652982ded900b7f5f94aafdcaf.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981889979-0ed27d84-b3b6-42a3-b81a-7f9f4056a69f.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=ef248b652982ded900b7f5f94aafdcaf.png&originHeight=144&originWidth=774&size=32894&status=done&style=none&width=774)\n![482275c6cc13a702571c9e5807fed028.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981870828-323a2ec7-37b5-49f5-bb5a-eb18768f450d.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=482275c6cc13a702571c9e5807fed028.png&originHeight=81&originWidth=585&size=5695&status=done&style=none&width=585)  \n`[root@master ~]#systemctl start chronyd ` \n    `[root@master  ~]#systemctl enable chronyd ` \n    `[root@master  ~]#timedatectl set-timezone Asia/Shanghai`  \n    `[root@master  ~]#chronyc sources ` \n![0a8ef4abc1a05c8fbc2e997c1174a028.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982167245-efe52879-5c33-43cc-873f-abf85841f396.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=0a8ef4abc1a05c8fbc2e997c1174a028.png&originHeight=129&originWidth=960&size=9122&status=done&style=none&width=960)\n\n- node节点配置\n`[root@node1  ~]# yum -y install chrony ` \n`[root@node1  ~]# vim /etc/chrony.conf ` \n![5dd88d85cbae94d1aef235d0396d22da.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982277805-b1538afa-3289-40cd-9481-b5341dd5cbc6.png#align=left&display=inline&height=127&margin=%5Bobject%20Object%5D&name=5dd88d85cbae94d1aef235d0396d22da.png&originHeight=127&originWidth=498&size=9109&status=done&style=none&width=498)\n`[root@node1  ~]# systemctl start chronyd ` \n`[root@node1  ~]# systemctl enable chronyd ` \n`[root@node1  ~]# chronyc sources ` \n\n![3e9e432bcf81b01ef223a64c83d6e012.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982328125-f607aaf1-2a57-4c8f-b568-fcb293afb5d9.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=3e9e432bcf81b01ef223a64c83d6e012.png&originHeight=125&originWidth=961&size=8831&status=done&style=none&width=961)\n\n\n## 5. 设置防火墙规则\n`[root@master  ~]# systemctl stop firewalld ` \n`[root@master  ~]# systemctl disable firewalld ` \n`[root@master  ~]# yum -y install iptables-services ` \n`[root@master  ~]# systemctl start iptables ` \n`[root@master  ~]# systemctl enable iptables ` \n`[root@master  ~]# iptables -F ` \n`[root@master  ~]# service iptables save ` \n## 6. 关闭selinux\n`[root@master  ~]# setenforce 0 ` \n`[root@master  ~]# sed -i \'s/^SELINUX=.*/SELINUX=disabled/\' /etc/selinux/config` \n## 7. 关闭swap分区\n`[root@master  ~]# swapoff -a ` \n`[root@master  ~]# sed -i \'/ swap / s/^(.*)$/#1/g\' /etc/fstab ` \n## 8. 修改内核iptables相关参数,启用iptables查看桥接流量\n```bash\n[root@master  ~]# cat <<EOF > /etc/sysctl.d/kubernetes.conf `\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n```\n`[root@master  ~]# sysctl -p /etc/sysctl.d/kubernetes.conf ` \n\n- centos8会有如下报错\n\n![8484ea61fb135eecdd54290e47345121.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982683093-f204be5e-65d7-40d2-a0cf-5a0374a655ec.png#align=left&display=inline&height=126&margin=%5Bobject%20Object%5D&name=8484ea61fb135eecdd54290e47345121.png&originHeight=126&originWidth=1027&size=21806&status=done&style=none&width=1027)\n\n- 临时解决，重启失效\nmodprobe br_netfilter\n- 开机加载上面这个模块\n```bash\ncat > /etc/rc.sysinit << EOF\n#!/bin/bash\nfor file in /etc/sysconfig/modules/*.modules ; do\n[ -x $file ] && $file\ndone\nEOF\ncat > /etc/sysconfig/modules/br_netfilter.modules << EOF\nmodprobe br_netfilter\nEOF\n```\n`[root@master  ~]#chmod 755 /etc/sysconfig/modules/br_netfilter.modules` \n`[root@master  ~]#lsmod |grep br_netfilter` \n\n\n## 9. 升级内核（可选）\n\n\n- 载入公钥\n`[root@master  ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org` \n- 升级安装ELRepo\n\n`[root@master  ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm` \n\n- centos8使用如下命令\n`[root@master  ~]#yum install https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm` \n- 载入elrepo-kernel元数据\n`[root@master  ~]# yum --disablerepo=* --enablerepo=elrepo-kernel repolist` \n- 安装最新版本的kernel\n`[root@master  ~]# yum --disablerepo=* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y` \n- 删除旧版本工具包\n`[root@master  ~]# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y` \n- 安装新版本工具包\n`[root@master  ~]# yum --disablerepo=* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y` \n- 查看内核插入顺序\n`[root@server-1  ~]# awk -F \'\'$1==\"menuentry \" {print i++ \" : \" $2}\'  /etc/grub2.cfg` \n\n\n![aea20dffb5b646d6a0ccc29759e28e2b.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983161220-53c8857e-2143-4c1e-ba67-15a1cd92d97f.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=aea20dffb5b646d6a0ccc29759e28e2b.png&originHeight=249&originWidth=987&size=25959&status=done&style=none&width=987)\n\n- 设置默认启动\n`[root@server-1  ~]# grub2-set-default 0 // 0代表当前第一行，也就是5.3版本 ` \n`[root@server-1  ~]# grub2-editenv list ` \n- 重启验证\n\n\n\n# 二、kubernetes安装前设置（每个节点）\n\n\n## 1. kube-proxy开启ipvs的前置条件（每个节点执行）\n`[root@node1  ~]# yum -y install ipset ipvsadm` \n```bash\n[root@node1  ~]# cat > /etc/sysconfig/modules/ipvs.modules <<EOF \n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n```\n`[root@node2  ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash ` \n`[root@node2  ~]# /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack` \n![d307861136126af61481ba884f655357.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983371668-ff0e07ae-19b2-4653-a724-177b625a7874.png#align=left&display=inline&height=322&margin=%5Bobject%20Object%5D&name=d307861136126af61481ba884f655357.png&originHeight=322&originWidth=1109&size=34102&status=done&style=none&width=1109)\n\n- linux kernel 4.19版本已经将nf_conntrack_ipv4 更新为 nf_conntrack\n\n## 2. docker安装（所有节点）\n\n- 安装前源准备\n`yum install -y yum-utils device-mapper-persistent-data lvm2` \n- 配置yum源\n`yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo` \n- 查看可安装的docker版本\n`yum list docker-ce --showduplicates | sort -r` \n- 安装19.03.8版本docker\n`yum install -y docker-ce-19.03.8*` \n- 使用阿里云做镜像加速\n\n```bash\nmkdir -p /etc/docker\ntee /etc/docker/daemon.json <<-\'EOF\'\n{\n\"registry-mirrors\": [\"https://o2j0mc5x.mirror.aliyuncs.com\"]\n}\nEOF\n```\n`systemctl daemon-reload` \n\n- 启动docker\n`systemctl start docker` \n`systemctl enable docker` \n\n## 3. docker 1.13以上版本默认禁用iptables的forward调用链，因此需要执行开启命令：\n`iptables -P FORWARD ACCEPT` \n## 4. 修改docker cgroup driver为systemd\n\n- 使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定\n\n```bash\n[root@master  ~]#vim /etc/docker/daemon.json \n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n```\n`systemctl daemon-reload` \n       `systemctl restart docker` \n       `docker info | grep Cgroup` \n![c752b0f068f75826340b6d11aea08b14.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983714681-9e9973a4-79e0-4449-bb18-0617507ba2b8.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=c752b0f068f75826340b6d11aea08b14.png&originHeight=49&originWidth=513&size=4262&status=done&style=none&width=513)\n## 5. 配置阿里云yum源\n```bash\n[root@master  ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo \n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttps://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n## 6. 安装kubeadm、kubectl、kubelet\n`[root@master  ~]# yum -y install kubeadm kubectl kubelet` \n`[root@master  ~]# systemctl enable kubelet ` \n', 575, 44, 234, '2020-08-20 15:26:50.260050', '2021-01-26 05:41:56.519597', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (9, 'k8s部署-部署kubernets', '本文主要从kubeadm部署的方式介绍如何部署三个节点的k8s集群操作，以及基于flannel的pod网络介绍，后期work节点加入k8s集群如何操作。', 'cover/2020_10_30_14_01_03_317330.jpg', '[TOC]\n# 一、kubeadm部署(master)\n## 1. 执行安装命令\n`[root@master  ~]#kubeadm init --apiserver-advertise-address=192.168.10.100 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.16.3 --pod-network-cidr=10.244.0.0/16` \n\n- --apiserver-advertise-address\n指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的interface。\n- --pod-network-cidr\n指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对--pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用flannel 网络方案，必须设置成这个 CIDR。\n- --image-repository\nKubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问gcr.io，在1.13版本中我们可以增加–image-repository参数，默认值是k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。\n- --kubernetes-version=v1.16.3\n关闭版本探测，因为它的默认值是stable-1，会导致从[https://dl.k8s.io/release/stable-1.txt](https://dl.k8s.io/release/stable-1.txt)下载最新的版本号\n\n![a5fb6787d8acdc6284242fd1c2f58b9b.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984052634-f103d264-1a32-4447-aec5-79d8c0a7501b.png#align=left&display=inline&height=430&margin=%5Bobject%20Object%5D&name=a5fb6787d8acdc6284242fd1c2f58b9b.png&originHeight=430&originWidth=1040&size=32994&status=done&style=none&width=1040)\n## 2. 根据提示初始化kubectl\n`[root@master  ~]# mkdir -p $HOME/.kube ` \n`[root@master  ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config ` \n`[root@master  ~]# chown $(id -u):$(id -g) $HOME/.kube/config ` \n## 3. 启用 kubectl 命令自动补全功能（注销重新登录生效）\n`[root@master  ~]# yum -y install bash-completion ` \n`[root@master  ~]# echo \"source <(kubectl completion bash)\" >> ~/.bashrc` \n## 4. 测试kubectl\n![b344bf365bb90a67a8703581c77f9f8f.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984121864-bd375b49-f0ba-40c1-b390-88124f8d2226.png#align=left&display=inline&height=79&margin=%5Bobject%20Object%5D&name=b344bf365bb90a67a8703581c77f9f8f.png&originHeight=79&originWidth=516&size=5373&status=done&style=none&width=516)\n# 二、启用基于flannel的Pod网络\n\n\n## 1. 下载配置文件\n`[root@master  ~]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml` \n## 2. 启用flannel\n`[root@master  ~]# kubectl apply -f kube-flannel.yml ` \n## 3. 验证操作\n`[root@master  ~]# kubectl get pods --all-namespaces ` \n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984276434-4fdf16c7-fea2-4025-b899-c4685fa63b59.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=249&originWidth=987&size=353797&status=done&style=none&width=987)6\n\n\n# 三、kubeadm加入node节点\n## 1. 将节点加入到集群\n`[root@node1  docker]# kubeadm join 192.168.10.100:6443 --token twjk7a.wkjilo6g39urowbj  --discovery-token-ca-cert-hash sha256:efeb65c86da8587794ee0503258201d6d15e793f833e3b716dce9f35eb72b83b` \n## 2. 查看集群信息\n`[root@master  ~]# kubectl get nodes` \n![b052f62ac2e1b7357fe1648a328cacd7.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984372036-e3573546-3674-4aea-a5b5-9c9558ec3de8.png#align=left&display=inline&height=128&margin=%5Bobject%20Object%5D&name=b052f62ac2e1b7357fe1648a328cacd7.png&originHeight=128&originWidth=507&size=8785&status=done&style=none&width=507)\n\n## 3. 后期有新节点加入时，使用kubeadm token list可以查询token信息，执行join操作\n', 283, 36, 9, '2020-08-20 15:27:32.995212', '2021-01-26 05:42:25.829695', 1, 6, 1, 1, 0);
INSERT INTO `blog_article` VALUES (10, 'k8s部署-helm安装', '本文主要从如何安装helm和如何添加helm的官方仓库两个方面介绍helm工具的部署流程操作。', 'cover/2020_10_30_14_04_03_844089.jpg', '[TOC]\n\n# 一、安装Helm\n\n\n- 官方参考文档：[https://helm.sh/docs/intro/quickstart/](https://helm.sh/docs/intro/quickstart/)\n- Helm的安装方式有两种：预编译的二进制程序和源码编译安装。\n- Helm项目托管在GitHub之上，项目地址为[https://github.com/helm/helm/releases](https://github.com/helm/helm/releases)。\n- Helm的运行依赖于本地安装并配置完成的kubectl方能与运行于Kubernetes集群之上的Tiller服务器进行通信，因此，运行Helm的节点也应该是可以正常使用kubectl命令的主机，或者至少是有着可用kubeconfig配置文件的主机。\n## 1. 下载合适版本的压缩包并将其展开。\n`wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz` \n`tar -xvf helm-v3.2.0-linux-amd64.tar.gz` \n## 2. 将其二进制程序文件复制或移动到系统PATH环境变量指向的目录中\n`cp linux-amd64/helm /usr/local/bin/` \n## 3. 以添加自动完成的代码：\n`source <(helm completion bash)` \n## 4. Helm客户端安装完成后，进行验证。\n![0f56509ab3c33a7f284b4ceddbda3e1c.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984527925-c322f0ab-67db-479f-9670-e70a7258b463.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=0f56509ab3c33a7f284b4ceddbda3e1c.png&originHeight=44&originWidth=370&size=14541&status=done&style=none&width=370)\n\n\n# 二、添加Helm的官方仓库\n\n\n## 1. 添加官方Charts仓库\n`helm repo add stable https://kubernetes-charts.storage.googleapis.com/` \n## 2. 更新仓库信息\n`helm repo update` \n## 3. 查看官方Charts仓库\n`helm search repo stable` \n', 287, 73, 4, '2020-08-20 15:28:08.544602', '2021-01-26 10:50:02.705879', 1, 6, 1, 1, 0);
INSERT INTO `blog_article` VALUES (11, 'k8s部署-部署ingress控制器', '本文主要介绍如何使用yaml配置文件和使用helm两种方式部署ingress控制器', 'cover/2020_10_30_14_06_45_459385.jpg', '[TOC]\r\n# 一、使用yaml配置文件部署\r\n\r\n\r\n## 1. 参考地址\r\n[github地址](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.2/docs/deploy/index.md)\r\n[ingress-nginx官网](https://kubernetes.github.io/ingress-nginx/)\r\n## 2. 创建ingress基础环境资源\r\n`# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.31.1/deploy/static/provider/aws/deploy-tls-termination.yaml` \r\n\r\n\r\n- 下载慢可以去Github下载\r\n[https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml)\r\n- 创建资源\r\n`# kubectl apply -f mandatory.yaml` \r\n- 查看pod资源信息\r\n`# kubectl get pod -n ingress-nginx` \r\n\r\n\r\n\r\n## 3. 采用nodepod暴露服务\r\n`# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml` \r\n\r\n\r\n## 4. 查看svc资源信息\r\n`kubectl get svc -n ingress-nginx` \r\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603985541050-69cc3be0-848d-481f-ad04-91473d710c25.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=909&size=353797&status=done&style=none&width=909)\r\n# 二、使用helm部署\r\n\r\n\r\n## 1. 创建ingress名称空间\r\n`# kubectl create namespace ingress` \r\n## 2. 添加仓库\r\n`# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx` \r\n## 3. 部署ingress\r\n`# helm install my-release ingress-nginx/ingress-nginx --namespace ingress` \r\n## 4. 查看验证\r\n`# kubectl get pod -n ingress` \r\n`# kubectl get svc -n ingress`', 113, 14, 3, '2020-08-20 16:44:37.001204', '2021-01-26 14:08:19.086989', 1, 6, 2, 1, 0);
INSERT INTO `blog_article` VALUES (12, 'k8s部署-部署calico网络组件', '本文详细介绍了如何安装部署canal网络插件的部署过程', 'cover/2020_10_30_14_10_46_055948.jpg', '[TOC]\n\n# 一、Calico官网：\n\n[https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel](https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel)\n\n# 二、安装部署\n\n## 1. 下载calico的canal插件：\n\n`# curl https://docs.projectcalico.org/manifests/canal.yaml -O` \n\n- 如果使用的是pod cidr 10.244.0.0/16，请跳到下一步。如果您使用的是不同的pod cidr，请使用以下命令来设置包含pod cidr的环境变量pod cidr，并将清单中的10.244.0.0/16替换为pod cidr。\n\nPOD_CIDR=\"<your-pod-cidr>\"\n`sed -i -e \"s?10.244.0.0/16?$POD_CIDR?g\" canal.yaml` \n\n## 2. 部署canal插件：\n\n`kubectl apply -f canal.yaml` \n\n## 3. 使用kubectl get pods -n kube-system中查看安装进程。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986218413-be39a69c-279a-4114-badb-6128ed215913.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=742&size=353797&status=done&style=none&width=742)', 42, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:38:28.850292', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (13, 'k8s部署-部署dashboard', '本文详细介绍了如何自建证书安装dashboard以及获取token信息的流程，以及kubeconfig的安装与使用教程', 'cover/2020_10_30_14_15_37_116603.jpg', '[TOC]\n\n# 一、dashboard组件安装-token\n\n\n- 官方参考文档：\n[https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui)\n- github项目地址：\n[https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)\n- 说明\n1.7及之后的版本默认在部署时仅定义了运行Dashboard所需要的最小权限，仅能够在Master主机上通过“kubectl\nproxy”命令创建代理后于本机进行访问，它默认禁止了来自于其他任何主机的访问请求。\n\n\n\n## 1. 因为自动生成的证书很多浏览器无法使用，所以自己创建证书\n\n\n- 新建证书存放目录\n`mkdir /etc/kubernetes/dashboard-certs` \n`cd /etc/kubernetes/dashboard-certs/` \n- 创建命名空间\n`kubectl create namespace kubernetes-dashboard` \n- 创建key文件\n`openssl genrsa -out dashboard.key 2048` \n- 证书请求\n`openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj \'/CN=dashboard-cert\'` \n- 自签证书\n`openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt` \n- 创建kubernetes-dashboard-certs对象\n`kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard` \n\n## 2. 下载并修改配置文件\n\n\n- 下载配置文件\n`wget kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml` \n- 修改配置文件，增加直接访问端口\n`[root@master  ~]# vim recommended.yaml ` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986440425-d0aa96ba-83dc-49b1-8e18-599b381bfefc.png#align=left&display=inline&height=354&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=354&originWidth=353&size=353797&status=done&style=none&width=353)\n\n- 修改配置文件，注释原kubernetes-dashboard-certs对象声明\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986459006-3930aa82-bee0-49aa-b245-428149b6d2c2.png#align=left&display=inline&height=256&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=256&originWidth=431&size=353797&status=done&style=none&width=431)\n## 3. 运行dashboard\n`[root@master  ~]# kubectl apply -f recommended.yaml ` \n## 4. 更新配置信息\n\n\n- 创建Dashboard管理员账号dashboard-admin.yaml，并apply\n```yaml\n	apiVersion: v1\n	kind: ServiceAccount\n	metadata:\n	  labels:\n	    k8s-app: kubernetes-dashboard\n	  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n\n- 赋权dashboard-admin-bind-cluster-role.yaml，并apply\n```yaml\n	apiVersion: rbac.authorization.k8s.io/v1\n	kind: ClusterRoleBinding\n	metadata:\n	  name: dashboard-admin-bind-cluster-role\n	  labels:\n	    k8s-app: kubernetes-dashboard\n	roleRef:\n	  apiGroup: rbac.authorization.k8s.io\n	  kind: ClusterRole\n	  name: cluster-admin\n	subjects:\n	- kind: ServiceAccount\n	  name: dashboard-admin\n	  namespace: kubernetes-dashboard\n```\n\n\n## 3. 获取token信息\n`kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk \'{print $1}\')` \n## 4. 登录访问[https://192.168.10.100:30001](https://192.168.10.100:30001)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986577911-f4796102-2fff-49df-b021-e5c90dd0ec31.png#align=left&display=inline&height=731&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=731&originWidth=1109&size=353797&status=done&style=none&width=1109)\n\n\n# 二、dashboard组件安装-kubeconfig\n\n1. 设置变量\n\n\n\n- 获取dashboard-admin-token的名称\n`kubectl get secrets --all-namespaces | grep dashboard-admin-token` \n- 获取token\n`DASH_TOCKEN=$(kubectl -n kubernetes-dashboard get secret dashboard-admin-token-9k522 -o jsonpath={.data.token}| base64 -d)`\n\n2. 初始化集群信息，提供API Server的URL，以及验证API Server证书所用到的CA证书等\n\n`kubectl config set-cluster kubernetes --server=192.168.10.100:6443 --kubeconfig=/root/dashbord-admin.conf` \n\n3. 获取dashboard-admin的token，并将其作为认证信息。\n\n`kubectl config set-credentials dashboard-admin --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf` \n\n4. 设置context列表，定义一个名为dashboard-admin的context：\n\n`kubectl config set-context dashboard-admin@kubernetes  --cluster=kubernetes  --user=dashboard-admin --kubeconfig=/root/dashbord-admin.conf` \n\n5.  使用的context为前面定义的名为dashboard-admin的context：\n\n`kubectl config use-context dashboard-admin@kubernetes  --kubeconfig=/root/dashbord-admin.conf` \n\n6.  sz发送kubeconfig，然后使用kubeconfig登录\n6. 切换到admin用户\n\n`kubectl config use-context kubernetes-admin@kubernetes ` \n', 29, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 08:50:20.354993', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (17, 'k8s部署-部署metrics-server监控组件', '本文详细介绍了部署metrics-server的详细过程。', 'cover/2020_10_30_14_20_54_712087.jpg', '[TOC]\n\n# 一、部署metrics-server\n[github地址](https://github.com/kubernetes-sigs/metrics-server)\n\n1. 克隆项目代码的仓库至本地node节点目录以获得其资源配置清单\n\n`# git clone https://github.com/kubernetes-incubator/metrics-server.git` \n\n2. 需要修改metrics-server/deploy/1.8+/metrics-server-deployment.yaml清单文件\n\n![Snipaste_2020-01-29_20-35-06.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986822431-7916f44e-cf76-40b6-a690-806b975707cb.png#align=left&display=inline&height=142&margin=%5Bobject%20Object%5D&name=Snipaste_2020-01-29_20-35-06.png&originHeight=142&originWidth=1107&size=57139&status=done&style=none&width=1107)\n```yaml\nimage: mirrorgooglecontainers/metrics-server-amd64:v0.3.6\nimagePullPolicy: IfNotPresent\ncommand:\n- /metrics-server\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n```\n\n3. 为master节点添加label\n\n`# kubectl label nodes master metrics=yes` \n\n4. 部署metrics-server\n\n`# kubectl create -f metrics-server/deploy/1.8+/` \n\n5. 检验相应的API群组metrics.k8s.io是否出现在Kubernetes集群的API群组列表中\n\n`# kubectl api-versions | grep metrics` \n\n6. 确认相关的Pod对象运行正常\n\n`# kubectl get pods -n kube-system -l k8s-app=metrics-server` \n\n7. 使用kubectl top node查看结果\n', 24, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 11:49:35.857361', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (20, 'k8s部署-部署Prometheus+Grafana', '本文详细介绍了prometheus和grfana监控，以及各个组件的功能，还有如何安装部署Prometheus+Grafana，还有如何配置prometheus监控', 'cover/2020_10_30_14_23_57_647672.jpg', '[TOC]\n\n# 一、Prometheus介绍\n\n\n> 如果已安装metrics-server需要先卸载，否则冲突\n\n\n\n1. Prometheus（普罗米修斯）是一套开源的监控&报警&时间序列数据库的组合，起始是由SoundCloud公司开发的。随着发展，越来越多公司和组织接受采用Prometheus，社会也十分活跃，他们便将它独立成开源项目。现在最常见的Kubernetes容器管理系统中，通常会搭配Prometheus进行监控。\n1. Prometheus基本原理是通过HTTP协议周期性抓取被监控组件的状态，这样做的好处是任意组件只要提供HTTP接口就可以接入监控系统，不需要任何SDK或者其他的集成过程。这样做非常适合虚拟化环境比如VM或者Docker\n   。\n1. 输出被监控组件信息的HTTP接口被叫做exporter\n   。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux\n   系统信息 (包括磁盘、内存、CPU、网络等等)\n1. 与其他监控系统相比，Prometheus的主要特点是：\n\n\n\n- 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸）；\n- 非常高效的存储；\n- 一种灵活的查询语言；\n- 不依赖分布式存储，单个服务器节点；\n- 时间集合通过HTTP上的PULL模型进行；\n- 通过中间网关支持推送时间；\n- 通过服务发现或静态配置发现目标；\n- 多种模式的图形和仪表板支持。\n\n\n\n# 二、Grafana介绍\n\n\n> Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。它主要有以下六大特点：\n\n\n\n- 展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；\n- 数据源：Graphite，InfluxDB，OpenTSDB，Prometheus，Elasticsearch，CloudWatch和KairosDB等；\n- 通知提醒：以可视方式定义最重要指标的警报规则，Grafana将不断计算并发送通知，在数据达到阈值时通过Slack、PagerDuty等获得通知；\n- 混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数据源；\n- 注释：使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据和标记；\n- 过滤器：Ad-hoc过滤器允许动态创建新的键/值过滤器，这些过滤器会自动应用于使用该数据源的所有查询。\n\n\n\n# 三、组件说明\n\n\n1. MetricServer：是kubernetes集群资源使用情况的聚合器，收集数据给kubernetes集群内使用，如kubectl,hpa,scheduler等。\n1. PrometheusOperator：是一个系统监测和警报工具箱，用来存储监控数据。\n1. NodeExporter：用于各node的关键度量指标状态数据。\n1. KubeStateMetrics：收集kubernetes集群内资源对象数据，制定告警规则。\n1. Prometheus：采用pull方式收集apiserver，scheduler，controller-manager，kubelet组件数据，通过http协议传输。\n1. Grafana：是可视化数据统计和监控平台。\n\n\n\n# 四、安装部署\n\n\n## 1. git项目至本地\n\n`# git clone https://github.com/coreos/kube-prometheus.git` \n\n## 2. 修改资源清单文件\n\n\n- 修改 kube-prometheus/manifests/grafana-service.yaml 文件，使用 nodepode 方式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987218245-ea1245d4-ff2b-48c2-b93d-540d6231cd20.png#align=left&display=inline&height=370&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=370&originWidth=308&size=353797&status=done&style=none&width=308)\n\n- 修改修改 kube-prometheus/manifests/prometheus-service.yaml，改为nodepode方式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987249483-629b3c1a-085f-4c6b-b898-300dcc3d99d7.png#align=left&display=inline&height=414&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=414&originWidth=349&size=353797&status=done&style=none&width=349)\n\n- 修改 kube-prometheus/manifests/alertmanager-service.yaml，改为 nodepode\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987271069-764c1ea4-05e7-4e79-a755-fc612d0fe718.png#align=left&display=inline&height=416&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=416&originWidth=354&size=353797&status=done&style=none&width=354)\n\n\n## 3. 创建资源对象\n\n\n- 创建monitoring名称空间\n  `# kubectl create namespace monitoring` \n- 创建crd资源\n  `# kubectl apply -f kube-prometheus/manifests/setup` \n- 创建其他资源\n  `# kubectl apply -f kube-prometheus/manifests/` \n\n\n\n## 4. 验证查看\n\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987440659-7b24fa61-0fd4-4d2a-967d-65f8f07da024.png#align=left&display=inline&height=319&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=319&originWidth=603&size=353797&status=done&style=none&width=603)\n\n- 查看top信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987475444-35f7d2c4-09d0-464e-b17a-4ce0f210fa99.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=181&originWidth=580&size=353797&status=done&style=none&width=580)\n\n\n## 5. web访问验证\n\n\n- 访问prometheus http://192.168.10.100:30200/targets，查看节点状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987514446-de0637dd-cacd-4998-8ffc-4686b59b4a00.png#align=left&display=inline&height=726&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=726&originWidth=952&size=353797&status=done&style=none&width=952)\n\n- 访问 grafana http://192.168.10.100:30100/login，默认用户名和密码是admin/admin\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987570120-647e1df6-8337-49c5-9b93-e2844cb059cd.png#align=left&display=inline&height=598&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=598&originWidth=1020&size=353797&status=done&style=none&width=1020)\n\n- 配置数据源\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987591308-acf44c8c-1d55-47eb-9fdc-45867ae5575b.png#align=left&display=inline&height=499&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=499&originWidth=1146&size=353797&status=done&style=none&width=1146)\n\n- 使用默认配置，点击测试\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987610894-a48d6427-f28b-45b8-a2a1-abd9549cb449.png#align=left&display=inline&height=356&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=356&originWidth=1125&size=353797&status=done&style=none&width=1125)\n\n- 导入默认仪表盘\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987630982-25258bd8-f3c5-4023-8396-70873f2809f9.png#align=left&display=inline&height=327&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=327&originWidth=1246&size=353797&status=done&style=none&width=1246)\n\n- 点击home选择仪表盘查看\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987649716-c9122fc8-fc64-4de7-85ca-9cde5bf85686.png#align=left&display=inline&height=659&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=659&originWidth=1240&size=353797&status=done&style=none&width=1240)\n\n\n# 五、配置prometheus\n\n\n## 1. 两个监控任务没有对应的目标，这和serviceMonitor资源对象有关\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987690255-d187beca-93da-470f-9a9b-c1192bfb4f15.png#align=left&display=inline&height=157&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=157&originWidth=600&size=353797&status=done&style=none&width=600)\n\n\n- 由于prometheus-serviceMonitorKubeScheduler文件中，selector匹配的是service的标签，但是namespace中并没有k8s-app=kube-scheduler的service\n\n\n\n- 新建prometheus-kubeSchedulerService.yaml并apply创建资源\n\n```yaml\n	apiVersion: v1\n	kind: Service\n	metadata:\n	  namespace: kube-system\n	  name: kube-scheduler\n	  labels:\n	    k8s-app: kube-scheduler #与servicemonitor中的selector匹配\n	spec:\n	  selector: \n	    component: kube-scheduler # 与scheduler的pod标签一直\n	  ports:\n	  - name: http-metrics\n	    port: 10251\n	    targetPort: 10251\n	    protocol: TCP\n\n```\n\n- 新建prometheus-kubeControllerManagerService.yaml并apply创建资源\n\n```yaml\n	apiVersion: v1\n	kind: Service\n	metadata:\n	  namespace: kube-system\n	  name: kube-controller-manager\n	  labels:\n	    k8s-app: kube-controller-manager\n	spec:\n	  selector:\n	    component: kube-controller-manager\n	  ports:\n	  - name: http-metrics\n	    port: 10252\n	    targetPort: 10252\n	    protocol: TCP\n\n```\n\n3. 再次查看targets信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988018072-dc53fffd-80ce-403d-9dbd-451a2fe041d8.png#align=left&display=inline&height=527&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=527&originWidth=711&size=353797&status=done&style=none&width=711)', 28, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:43.307761', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (22, 'k8s部署-部署elk日志收集', '本文主要从准备工作、部署Elasticsearch、部署Filebeat、部署Logstash、部署', 'cover/2020_10_30_14_27_16_386941.jpg', '[TOC]\n\n# 一、准备工作\n\n\n- 添加elk仓库\n  `# helm repo add elastic https://helm.elastic.co` \n- 创建elk名称空间\n  `# kubectl create namespace elk` \n\n\n\n# 二、部署Elasticsearch\n\n\n1. 将chart包下载本地\n   `# helm fetch elastic/elasticsearch` \n1. 修改elasticsearch/values.yaml配置文件（降低配置要求）\n\n- master节点数改为1\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988104443-96e8f591-55d0-4546-8cc2-6bf630067240.png#align=left&display=inline&height=23&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=23&originWidth=238&size=353797&status=done&style=none&width=238)\n\n- 不使用持久卷\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988121224-df3248d2-2773-488d-9f58-cbda613e98cd.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=229&size=353797&status=done&style=none&width=229)\n\n3. helm安装chart包\n   `helm install elasticsearch -f values.yaml elastic/elasticsearch --namespace elk` \n3. 验证结果\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988170952-93a09828-203d-4852-a624-b8ced3feb626.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=843&size=353797&status=done&style=none&width=843)\n\n\n# 三、部署Filebeat\n\n\n1. 将chart包下载本地\n\n`# helm fetch elastic/filebeat` \n\n2. 安装chart包\n   `# helm install filebeat elastic/filebeat --namespace elk` \n2. 验证结果\n\n- 查看pod状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988224463-d69aea9a-ed5e-42be-a4ed-19d00048e37a.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=805&size=353797&status=done&style=none&width=805)\n\n# 四、部署Logstash\n\n\n1. 将chart包下载本地\n   `# helm fetch elastic/logstash` \n1. 修改logstash/values.yaml配置文件\n\n- 不使用持久卷\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988254598-7efac7c6-4c7c-4736-b209-78e45adb4f3f.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=226&size=353797&status=done&style=none&width=226)\n\n3. helm安装chart包\n   `# helm install logstash -f values.yaml elastic/logstash --namespace elk` \n3. 验证结果\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988289176-966e6a1d-f874-4dae-9813-042e6f893b31.png#align=left&display=inline&height=69&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=69&originWidth=705&size=353797&status=done&style=none&width=705)\n\n\n# 五、部署 kibana\n\n\n1. 将chart包下载至本地\n   `# helm fetch elastic/kibana` \n1. 编辑kibana/values.yaml配置文件\n\n- 服务类型改为nodeport\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988322205-6e247a9e-ac99-4118-8426-704e3fe0b05f.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=247&size=353797&status=done&style=none&width=247)\n\n3. helm安装chart包\n   `# helm install kibana -f values.yaml elastic/kibana --namespace elk` \n3. 结果验证\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988354212-f94d6364-6125-4de4-9e36-4bf6c95d5664.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=718&size=353797&status=done&style=none&width=718)\n\n\n# 六、web访问测试\n\n![bdf9ab3eedce76bccd980f4710f2a803.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988405166-430b6b6a-de6f-4c2a-a34c-4b2dec59ed36.png#align=left&display=inline&height=640&margin=%5Bobject%20Object%5D&name=bdf9ab3eedce76bccd980f4710f2a803.png&originHeight=640&originWidth=1244&size=244291&status=done&style=none&width=1244)\n![aaba2f13bc7bf131c6571b4707c0b163.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988414154-8ce60590-b0d8-4a96-b811-839723df0084.png#align=left&display=inline&height=934&margin=%5Bobject%20Object%5D&name=aaba2f13bc7bf131c6571b4707c0b163.png&originHeight=934&originWidth=1245&size=420555&status=done&style=none&width=1245)', 25, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:45.577742', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (23, 'k8s部署-部署Harbor私有镜像仓库', '本文主要从Harbor私有镜像仓库、docker授权访问harbor仓库、kubernets访问harbor仓库这三个模块详细介绍如何部署和使用harbor私有镜像仓库', 'cover/2020_10_30_14_29_51_248572.jpg', '[TOC]\n\n# 一、Harbor私有镜像仓库\n\n\n1. 安装docker\n1. 安装docker-compose\n1. 下载harbor离线安装包\n   [参考链接](https://github.com/goharbor/harbor/releases)\n   `[root@harbor  ~] wget https://github.com/vmware/harbor/releases/download/v1.8.6/harbor-offline-installer-v1.8.6.tgz` \n   `[root@harbor  ~]# tar -xvf harbor-offline-installer-v1.8.6.tgz ` \n1. 修改harbor.yml配置文件\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988544911-e6a87aea-59f4-4c87-a388-865be20e9fa4.png#align=left&display=inline&height=127&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=127&originWidth=362&size=353797&status=done&style=none&width=362)\n\n- 注释https相关配置\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988572290-24b5f355-9396-409b-9901-d58307a2832e.png#align=left&display=inline&height=155&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=155&originWidth=590&size=353797&status=done&style=none&width=590)\n\n5. 运行install.sh脚本\n   [root[@harbor ](/harbor ) harbor]# ./install.sh \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988591722-52fc5191-6221-497d-b50f-838f395648e8.png#align=left&display=inline&height=105&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=105&originWidth=894&size=353797&status=done&style=none&width=894)\n\n6. 访问Harbor并登陆。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988612916-ff68ff22-8de8-4cd0-9a06-e61f16a24983.png#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=629&originWidth=669&size=353797&status=done&style=none&width=669)\n\n\n- 初始用户名admin\n- 初始密码Harbor12345\n\n\n\n7. 创建systemd服务管理脚本\n\n```bash\n	[Unit]\n	Description=Harbor\n	After=docker.service systemd-networkd.service systemd-resolved.service\n	Requires=docker.service\n	Documentation=http://github.com/vmware/harbor\n	\n	[Service]\n	Type=simple\n	Restart=on-failure\n	RestartSec=5\n	ExecStart=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml up\n	ExecReload=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml restart\n	ExecStop=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml down\n	\n	[Install]\n	WantedBy=multi-user.target\n\n```\n\n\n# 二、docker授权访问harbor仓库（所有安装docker的节点）\n\n\n1. docker配置文件私有仓库设置\n   `[root@master  ~]# vim /etc/docker/daemon.json ` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988680971-a7382b3b-9276-40ff-8820-157818f39e64.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=804&size=353797&status=done&style=none&width=804)\n\n2. 重启docker\n   `systemctl daemon-reload` \n   `systemctl restart docker` \n2. master节点登陆测试\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988703787-3dfcd92f-1287-4bc4-ba33-1d90d54cc0ef.png#align=left&display=inline&height=197&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=197&originWidth=948&size=353797&status=done&style=none&width=948)\n\n4. 推送镜像测试\n\n\n![4b3c38a2ca924a141215c8050cca5f3c.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988723097-0794693f-ba4f-4332-bfab-d01d4530b421.png#align=left&display=inline&height=229&margin=%5Bobject%20Object%5D&name=4b3c38a2ca924a141215c8050cca5f3c.png&originHeight=229&originWidth=513&size=15430&status=done&style=none&width=513)\n\n\n# 三、kubernets访问harbor仓库\n\n\n- 由于harbor采用了用户名密码认证，所以在镜像下载时需要配置sercet\n\n\n\n1. 创建认证secret\n   `kubectl create secret docker-registry registry-secret --namespace=default --docker-server=192.168.10.103 --docker-username=admin --docker-password=Harbor12345` \n1. 查看secret\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988773016-f09118ac-6a2b-4a40-b80f-02cc37e9a45f.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=787&size=353797&status=done&style=none&width=787)\n\n\n3. 使用相应的私有registry中镜像的Pod资源的定义，即可通过imagePullSecrets字段使用此Secret对象\n\n```yaml\n	apiVersion: v1\n	kind: Pod \n	metadata:\n	  name: secret-imagepull-demo\n	  namespace: default\n	spec:\n	  imagePullSecrets:\n	  - name: registry-secret\n	  containers:\n	  - image: harbor.cy.bj/k8s/nginx\n	    name: myapp\n```\n\n4. 删除\n   `kubectl delete secret registry-secret` ', 25, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:48.518468', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (24, 'kubectl命令-命令格式', '本文从命令格式', 'cover/2020_11_01_16_32_13_822845.jpg', '[TOC]\n\n# 一、命令格式\n\n\n> kubectl [command] [TYPE] [NAME] [flags]\n\n\n\n# 二、command\n\n\n1. 指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048264044-a061f530-f07e-4b0e-9d8b-5b78ce9ef139.png#align=left&display=inline&height=825&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=825&originWidth=533&size=178039&status=done&style=none&width=533)\n# 三、TYPE\n\n\n1. 指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:\n\n	kubectl get pod pod1\n	kubectl get pods pod1\n	kubectl get po pod1\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048361139-80f4120c-f561-43e3-9196-a5544e38bc78.png#align=left&display=inline&height=823&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=823&originWidth=533&size=178039&status=done&style=none&width=533)\n# 四、NAME\n\n\n1. 指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息。 `kubectl get pods` \n1. 在对多个资源执行操作时，可以按类型和名称指定每个资源，或指定一个或多个文件\n1. 要对所有类型相同的资源进行分组，执行以下操作：TYPE1 name1 name2  name。 `kubectl get pod example-pod1 example-pod2` \n1. 分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3。 `kubectl get pod/example-pod1 replicationcontroller/example-rc1` \n1. 用一个或多个文件指定资源：-f file1 -f file2 -f file 。 `kubectl get pod -f ./pod.yaml` \n# 五、flags\n\n1. 指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API服务器的地址和端口。\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048939406-551d39d8-f417-46f1-a6d2-cfc3fc0afb9d.png#align=left&display=inline&height=586&margin=%5Bobject%20Object%5D&name=image.png&originHeight=586&originWidth=533&size=178039&status=done&style=none&width=533)\n# 六、输出格式\n\n1. kubectl命令还包含了多种不同的输出格式（如表3-2所示），它们为用户提供了非常灵活的自定义输出机制，如输出为YAML或JSON格式等。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048960887-c5457ab4-2497-495e-a1c5-1fa388f7616d.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=527&size=178039&status=done&style=none&width=527)\n# 七、帮助命令\n\n1. 可以通过kubectl help [subcommand]命令查看命令格式和支持的子命令信息。\n', 22, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:51.150986', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (25, 'kubectl命令-node操作常用命令', '本文介绍了对于kubernets的node节点常用的操作命令。', 'cover/2020_11_01_16_35_47_423868.jpg', '1. 显示Node的详细信息\n`$ kubectl describe nodes <node-name>` \n2. 驱逐node节点的pod\n`$ kubectl drain node2 --delete-local-data` \n3. 删除node节点\n`$ kubectl delete node nodename` \n4. 为node节点设置“disktype=ssd”标签以标识其拥有SSD设备：\n`$ kubectl label nodes node2 disktype=ssd` \n5. 查看具有键名SSD的标签的Node资源：\n`$ kubectl get nodes -l \'disktype\' -L disktype` \n', 20, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:54.924802', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (26, 'kubectl命令-pod常用命令', '本文从如下方面介绍了pod操作的常用命令一、创建资源对象', 'cover/2020_11_01_16_38_12_166703.jpg', '[TOC]\n# 一、创建资源对象\n\n\n1. 根据yaml配置文件一次性创建service和rc\n`$ kubectl create -f my-service.yaml -f my-rc.yaml` \n1. 根据<directory>目录下所有.yaml、.yml、.json文件的定义进行创建操作\n`$ kubectl create -f <directory>` \n\n\n\n# 二、查看pod对象\n\n\n1. 查看所有Pod列表\n`$ kubectl get pods` \n1. 显示Pod的更多信息\n`kubectl get pod <pod-name> -o wide` \n1. 以yaml格式显示Pod的详细信息\n`kubectl get pod <pod-name> -o yaml` \n1. 查看命名空间\n`kubectl get namespaces` \n1. 查看所有命令空间pod\n`kubectl get pod --all-namespaces`  或者 `kubectl get pod -A`\n1. 查看指定命名空间pod信息\n`kubectl get pod -n kube` \n7. 查看所有pod标签信息\n`kubectl get pods --show-labels` \n7. 查看指定标签的pod\n`kubectl get pods -l app=rs-demo` \n7. 格式化输出自定义列信息\n\n```bash\n[root@k8s-1 ~]# kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image\npod_name                                     pod_image\nmytomcat-84884dbbfc-22csl                    jy-k8s-registry.jiayuan.idc/project:v5\nmytomcat-84884dbbfc-cl2m7                    jy-k8s-registry.jiayuan.idc/project:v5\nmytomcat-84884dbbfc-ngpmw                    jy-k8s-registry.jiayuan.idc/project:v5\nonline-message-deployment-59c75ddc8d-pfk4s   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\nonline-message-deployment-59c75ddc8d-s2b77   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\nonline-message-deployment-59c75ddc8d-s5wv5   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\n```\n\n\n# 三、查看pod对象的详细信息\n\n\n1. 显示Pod的详细信息\n`$ kubectl describe pods/<pod-name>` \n`$ kubectl describe pods <pod-name>` \n\n\n\n# 四、查看容器中的日志信息\n\n\n1. 查看容器的日志\n`kubectl logs <pod-name>` \n1. 实时查看日志\n`kubectl logs -f <pod-name>` \n\n\n\n# 五、在pod中执行命令\n\n\n1. 执行Pod的data命令，默认是用Pod中的第一个容器执行\n`$ kubectl exec <pod-name> data` \n1. 指定Pod中某个容器执行data命令\n`$ kubectl exec <pod-name> -c <container-name> data` \n1. 通过bash获得Pod中某个容器的TTY，相当于登录容器\n`$ kubectl exec -it <pod-name> -c <container-name> bash` \n\n\n\n# 六．删除pod对象\n\n\n1. 基于Pod.yaml定义的名称删除Pod\n`$ kubectl delete -f pod.yaml` \n1. 删除所有包含某个label的Pod\n`$ kubectl delete pods -l name=<label-name>` \n1. 删除所有Pod\n`$ kubectl delete pods --all` \n\n\n\n# 七、其他相关命令\n\n\n1. 编辑名为 docker-registry 的 pod\n`$ kubectl edit pod docker-registry ` \n2. 获取相关的使用帮助\n`$ kubectl explain pods` \n`$ kubectl explain pods.spec` \n2. 给pod资源添加lables标签\n`kubectl label pods/pod-with-labels version=v1` \n2. 修改已有pod资源标签\n`kubectl label pods/pod-with-labels version=v2 --overwrite` ', 25, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-26 05:39:57.592806', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (27, 'kubectl命令-控制器常用命令', '本文主要从以下几个方介绍了kubectl控制器操作的常用命令一、ReplicaSet', 'cover/2020_11_01_16_40_14_424834.jpg', '[TOC]\n# 一、ReplicaSet\n\n\n1. 查看replicaset信息\n`$ kubectl get replicaset` \n`$ kubectl get replicaset rs-example -o wide` \n1. 查看replicasets控制器下某资源详细信息\n`$ kubectl describe replicasets/rs-example` \n1. 显示由RC管理的Pod的信息\n`$ kubectl describe pods <rc-name>` \n1. Pod的扩容与缩容\n`$ kubectl scale rc redis --replicas=3` \n- 当我们需要进行永久性扩容时，不要忘记修改rc配置文件中的replicas数量。\n`$ kubectl patch statefulset myapp -p \'{\"spec\":{\"replicas\":3}}\'` \n5. 更新镜像\n`$ kubectl set image deployments myapp-deploy  myapp=192.168.10.110/k8s/myapp:v2` \n\n# 二、Deployment\n\n\n1. 查看版本信息\n`$ kubectl rollout history deployment myapp-deploy` \n1. 查看更新状态：\n`$ kubectl rollout status deployment nginx` \n1. 终止升级\n`$ kubectl rollout pause deployment/nginx` \n1. 继续升级\n`$ kubectl rollout resume deployment/nginx` \n1. 回滚到上一个版本\n`$ kubectl rollout undo deployments myapp-deploy` \n1. 回滚到指定版本\n`$ kubectl rollout undo deployments myapp-deploy --to-revision=1`\n\n\n\n# 三、DaemonSet控制器\n\n\n1. 更新镜像\n`$ kubectl set image daemonsets filebeat-ds filebeat=ikubernetes/filebeat:5.6.6`\n\n# 四、Job控制器\n\n\n1. 以Job控制器名称为标签进行匹配：\n`$ kubectl get pods -l job-name=job-example` \n1. job扩容\n`$ kubectl scale jobs job-multi --replicas=2` \n\n\n\n# 五、CronJob控制器\n\n\n1. 以CronJob控制器名称为标签进行匹配：\n`$ kubectl get jobs -l app=mycronjob-jobs` \n', 26, 0, 0, '2020-09-21 22:17:51.727670', '2021-01-27 06:19:58.774710', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (28, 'kubectl命令-service常用命令', '本文主要从一、service', 'cover/2020_11_01_16_43_50_649669.jpg', '[TOC]\n# 一、service\n1. 查看svc信息\n`$ kubectl get svc` \n\n# 二、ingress\n\n\n1. 查看ingress-nginx信息\n`$ kubectl get svc -n ingress-nginx` \n1. 查看ingress规则\n`$ kubectl get ingress` ', 305, 29, 0, '2020-09-23 11:45:07.083242', '2021-01-26 06:55:48.082983', 1, 6, 10, 1, 0);
INSERT INTO `blog_article` VALUES (44, 'kubectl命令-存储常用命令', '本文主要从一、ConfigMap', 'cover/2020_11_01_16_50_57_982762.jpg', '[TOC]\n# 一、ConfigMap\n\n\n1. 在命令行直接给出键值对来创建ConfigMap对象\n   `$ kubectl create configmap configmap_name --from-literal=key-name-1=value-1` \n1. 查看ConfigMap对象special-config的相关信息\n   `$ kubectl get configmaps special-config -o yaml` \n   `$ kubectl describe configmaps special-config` \n1. 基于文件创建ConfigMap对象\n   `# kubectl create configmap resolv.conf --from-file=/etc/resolv.conf` \n1. 基于目录创建ConfigMap对象\n   `# kubectl create configmap docker-config-files --from-file=/etc/docker/` \n\n\n\n# 二、secret\n\n\n1. 创建了一个名为mysql-auth的Secret对象，用户名为root，密码为123.com\n   `# kubectl create secret generic mysql-auth --from-literal=username=root --from-literal=password=123.com` \n1. 将ssh密钥认证文件创建secret对象\n   `# kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-file=ssh-publickey=/root/.ssh/id_rsa.pub` \n1. 查看secret资源详细信息\n   `# kubectl get secrets ssh-key-secret -o yaml` ', 25, 0, 0, '2020-10-11 16:39:33.374090', '2021-01-26 05:14:31.277984', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (45, 'kubectl命令-日常命令总结', '文章目录：一、查看命令', 'cover/2020_11_01_16_54_01_662875.jpg', '[TOC]\n# 一、查看命令\n\n\n1. 查看所有namespace的pods运行情况\n   `kubectl get pods --all-namespaces` \n1. 查看具体pods，记得后边跟namespace名字哦\n   `kubectl get pods kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system` \n1. 查看pods具体信息\n   `kubectl get pods -o wide kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system` \n1. 获取所有deployment\n   `kubectl get deployment --all-namespaces` \n1. 查看kube-system namespace下面的pod/svc/deployment 等等（-o wide选项可以查看存在哪个对应的节点）\n   `kubectl get pod /svc/deployment -n kube-system` \n1. 列出该 namespace 中的所有 pod 包括未初始化的\n   `kubectl get pods --include-uninitialized` \n1. 查看deployment\n   `kubectl get deployment nginx-app` \n1. 查看rc和servers\n   `kubectl get rc,services` \n1. 查看pods结构信息（重点，通过这个看日志分析错误）对控制器和服务，node同样有效\n   `kubectl describe pods xxxxpodsname --namespace=xxxnamespace` \n\n- 其他控制器类似，就是kubectl get 控制器 控制器具体名称\n\n10. 查看pod日志\n    `kubectl logs $POD_NAME` \n10. 查看pod变量\n    `kubectl exec my-nginx-5j8ok -- printenv | grep SERVICE` \n\n\n\n# 二、集群查看\n\n\n1. 查看集群健康情况\n   `kubectl get cs` \n1. 集群核心组件运行情况\n   `kubectl cluster-info` \n1. 查看表空间名\n   `kubectl get namespaces` \n1. 查看版本\n   `kubectl version` \n1. 查看API\n   `kubectl api-versions` \n1. 查看事件\n   `kubectl get events` \n1. 获取全部节点\n   `kubectl get nodes` \n1. 删除节点\n   `kubectl delete node k8s2` \n\n\n\n# 三、创建资源\n\n\n1. 创建资源\n   `kubectl create -f ./nginx.yaml` \n1. 创建+更新，可以重复使用\n   `kubectl apply -f xxx.yaml` \n1. 创建当前目录下的所有yaml资源\n   `kubectl create -f .` \n1. 使用多个文件创建资源\n   `kubectl create -f ./nginx1.yaml -f ./mysql2.yaml` \n1. 使用目录下的所有清单文件来创建资源\n   `kubectl create -f ./dir` \n1. 使用 url 来创建资源\n   `kubectl create -f https://git.io/vPieo` \n1. 创建带有终端的pod\n   `kubectl run -i --tty busybox --image=busybox` \n1. 启动一个 nginx 实例\n   `kubectl run nginx --image=nginx` \n1. 启动多个pod\n   `kubectl run mybusybox --image=busybox --replicas=5` \n1. 获取 pod 和 svc 的文档\n   `kubectl explain pods,svc` \n\n\n\n# 四、更新\n\n\n1. 滚动更新 pod frontend-v1\n   `kubectl rolling-update python-v1 -f python-v2.json` \n1. 更新资源名称并更新镜像\n   `kubectl rolling-update python-v1 python-v2 --image=image:v2` \n1. 更新 frontend pod 中的镜像\n   `kubectl rolling-update python --image=image:v2` \n1. 退出已存在的进行中的滚动更新\n   `kubectl rolling-update python-v1 python-v2 --rollback` \n1. 基于 stdin 输入的 JSON 替换 pod\n   `cat pod.json | kubectl replace -f -` \n1. 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口\n   `kubectl expose rc nginx --port=80 --target-port=8000` \n1. 更新单容器 pod 的镜像版本（tag）到 v4\n   `kubectl get pod nginx-pod -o yaml | sed \'s/(image:myimage):.*$/1:v4/\' | kubectl replace -f –` \n1. 添加标签\n   `kubectl label pods nginx-pod new-label=awesome` \n1. 添加注解\n   `kubectl annotate pods nginx-pod icon-url=http://goo.gl/XXBTWq` \n1. 自动扩展 deployment\n   `kubectl autoscale deployment foo --min=2 --max=10` \n\n\n\n# 五、编辑资源\n\n\n1. 编辑名为 docker-registry 的 service\n   `kubectl edit svc/docker-registry` \n1. 修改启动参数\n   `vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf` \n\n\n\n# 六、动态伸缩pod\n\n\n1. 将foo副本集变成3个\n   `kubectl scale --replicas=3 rs/foo` \n1. 缩放“foo”中指定的资源。\n   `kubectl scale --replicas=3 -f foo.yaml` \n1. 将deployment/mysql从2个变成3个\n   `kubectl scale --current-replicas=2 --replicas=3 deployment/mysql` \n1. 变更多个控制器的数量\n   `kubectl scale --replicas=5 rc/foo rc/bar rc/baz` \n1. 查看变更进度\n   `kubectl rollout status deploy deployment/mysql` \n\n\n\n# 七、label 操作\n\n\n1. 增加节点lable值\n   `kubectl label nodes node1 zone=north` \n1. 增加lable值 [key]=[value]\n   `kubectl label pod redis-master-1033017107-q47hh role=master` \n1. 删除lable值\n   `kubectl label pod redis-master-1033017107-q47hh role-` \n1. 修改lable值\n   `kubectl label pod redis-master-1033017107-q47hh role=backend --overwrite` \n\n\n\n# 八、滚动升级\n\n\n1. 配置文件滚动升级\n   `kubectl rolling-update redis-master -f redis-master-controller-v2.yaml` \n1. 命令升级\n   `kubectl rolling-update redis-master --image=redis-master:2.0` \n1. pod版本回滚\n   `kubectl rolling-update redis-master --image=redis-master:1.0 --rollback` \n\n\n\n# 九、etcdctl 常用操作\n\n\n1. 检查网络集群健康状态\n   `etcdctl cluster-health` \n1. 带有安全认证检查网络集群健康状态\n   `etcdctl --endpoints=https://192.168.71.221:2379 cluster-health` \n1. 查看集群成员\n   `etcdctl member list` \n1. 设置网络配置\n   `etcdctl set /k8s/network/config ‘{ “Network”: “10.1.0.0/16” }’` \n1. 获取网络配置\n   `etcdctl get /k8s/network/config` \n\n\n\n# 十、删除资源\n\n\n1. 根据label删除：\n\n`kubectl delete pod -l app=flannel -n kube-system`   \n\n2. 删除 pod.json 文件中定义的类型和名称的 pod\n   `kubectl delete -f ./pod.json` \n2. 删除名为“baz”的 pod 和名为“foo”的 service\n   `kubectl delete pod,service baz foo` \n2. 删除具有 name=myLabel 标签的 pod 和 serivce\n   `kubectl delete pods,services -l name=myLabel` \n2. 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的\n   `kubectl delete pods,services -l name=myLabel --include-uninitialized` \n2. 删除 my-ns namespace下的所有 pod 和 serivce，包括尚未初始化的\n   `kubectl -n my-ns delete po,svc --all` \n2. 强制删除\n   `kubectl delete pods prometheus-7fcfcb9f89-qkkf7 --grace-period=0 --force` \n2. 删除指定deployment\n   `kubectl delete deployment kubernetes-dashboard --namespace=kube-system` \n2. 删除指定svc\n   `kubectl delete svc kubernetes-dashboard --namespace=kube-system` \n2. 根据资源清单文件删除\n   `kubectl delete -f kubernetes-dashboard.yaml` \n2. 强制替换，删除后重新创建资源。会导致服务中断。\n   `kubectl replace --force -f ./pod.json` \n\n# 十一、交互\n\n\n1. dump 输出 pod 的日志（stdout）\n   `kubectl logs nginx-pod` \n1. dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）\n   `kubectl logs nginx-pod -c my-container` \n1. 流式输出 pod 的日志（stdout）\n   `kubectl logs -f nginx-pod` \n1. 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）\n   `kubectl logs -f nginx-pod -c my-container` \n1. 交互式 shell 的方式运行 pod\n   `kubectl run -i --tty busybox --image=busybox -- sh` \n1. 连接到运行中的容器\n   `kubectl attach nginx-pod -i` \n1. 转发 pod 中的 6000 端口到本地的 5000 端口\n   `kubectl port-forward nginx-pod 5000:6000` \n1. 在已存在的容器中执行命令（只有一个容器的情况下）\n   `kubectl exec nginx-pod -- ls /` \n1. 在已存在的容器中执行命令（pod 中有多个容器的情况下）\n   `kubectl exec nginx-pod -c my-container -- ls /` \n1. 显示指定 pod和容器的指标度量\n   `kubectl top pod POD_NAME --containers` \n1. 进入pod\n   `kubectl exec -ti podName /bin/bash` \n\n\n\n# 十二、调度配置\n\n\n1. 标记 my-node 不可调度\n   `kubectl cordon k8s-node` \n1. 清空 my-node 以待维护\n   `kubectl drain k8s-node` \n1. 标记 my-node 可调度\n   `kubectl uncordon k8s-node` \n1. 显示 my-node 的指标度量\n   `kubectl top node k8s-node` \n1. 将当前集群状态输出到 stdout\n   `kubectl cluster-info dump` \n1. 将当前集群状态输出到 /path/to/cluster-state\n   `kubectl cluster-info dump --output-directory=/path/to/cluster-state`\n\n- 如果该键和影响的污点（taint）已存在，则使用指定的值替换\n\n7. 查看kubelet进程启动参数\n   `kubectl taint nodes foo dedicated=special-user:NoSchedule` \n7. 查看日志\n   `journalctl -u kubelet -f` \n\n# 十三、导出配置文件\n\n\n1. 导出proxy\n   `kubectl get ds -n kube-system -l k8s-app=kube-proxy -o yaml>kube-proxy-ds.yaml` \n1. 导出kube-dns\n   `kubectl get deployment -n kube-system -l k8s-app=kube-dns -o yaml >kube-dns-dp.yaml` \n   `kubectl get services -n kube-system -l k8s-app=kube-dns -o yaml >kube-dns-services.yaml` \n1. 导出所有 configmap\n   `kubectl get configmap -n kube-system -o wide -o yaml > configmap.yaml` \n\n\n\n# 十四、复杂操作命令\n\n\n1. 删除kube-system 下Evicted状态的所有pod\n   `kubectl get pods -n kube-system |grep Evicted| awk ‘{print $1}’|xargs` ', 26, 0, 0, '2020-10-11 16:40:20.742422', '2021-01-26 11:44:41.579116', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (51, '资源对象-K8S中的资源对象', 'Kubernetes的API对象大体可分为工作负载（Workload）、发现和负载均衡（Discovery&', 'cover/2020_11_01_16_57_37_183206.jpg', '[TOC]\n> Kubernetes的API对象大体可分为工作负载（Workload）、发现和负载均衡（Discovery& LB）、配置和存储（Config &Storage）、集群（Cluster）以及元数据（Metadata）五个类别。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604066905178-6f18d9f5-6dfe-4325-a4d1-5374cc83c0e6.png#align=left&display=inline&height=559&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=559&originWidth=1102&size=353797&status=done&style=none&width=1102)\n\n# 一、工作负载型资源\n\n\n> Pod是工作负载型资源中的基础资源，它负责运行容器，并为其解决环境性的依赖。但Pod可能会因为资源超限或节点故障等原因而终止，这些非正常终止的Pod资源需要被重建，不过，这类工作将由工作负载型的控制器来完成，它们通常也称为pod控制器。\n\n\n\n1. ReplicationController：用于确保每个Pod副本在任一时刻均能满足目标数量，换言之，它用于保证每个容器或容器组总是运行并且可访问；它是上一代的无状态Pod应用控制器，现已被Deployment和ReplicaSet取代。\n1. ReplicaSet：新一代ReplicationController，它与ReplicationController的唯一不同之处仅在于支持的标签选择器不同，ReplicationController只支持等值选择器，而ReplicaSet还额外支持基于集合的选择器。\n1. Deployment：用于管理无状态的持久化应用，例如HTTP服务器；它用于为Pod和ReplicaSet提供声明式更新，是建构在ReplicaSet之上的更为高级的控制器。\n1. StatefulSet：用于管理有状态的持久化应用，如database服务程序；其与Deployment的不同之处在于StatefulSet会为每个Pod创建一个独有的持久性标识符，并会确保各Pod之间的顺序性。\n1. DaemonSet：用于确保每个节点都运行了某Pod的一个副本，新增的节点一样会被添加此类Pod；在节点移除时，此类Pod会被回收；DaemonSet常用于运行集群存储守护进程——如glusterd和ceph，还有日志收集进程——如fluentd和logstash，以及监控进程——如Prometheus的Node Exporter、collectd、Datadog agent和Ganglia的gmond等。\n1. Job：用于管理运行完成后即可终止的应用，例如批处理作业任务；换句话讲，Job创建一个或多个Pod，并确保其符合目标数量，直到Pod正常结束而终止。\n1. CronJob：用于管理Job控制器资源的运行时间。Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及重复运行的方式\n\n# 二、发现和负载均衡\n\n\n> Pod资源可能会因为任何意外故障而被重建，于是它需要固定的可被“发现”的方式。另外，Pod资源仅在集群内可见，它的客户端也可能是集群内的其他Pod资源，若要开放给外部网络中的用户访问，则需要事先将其暴露到集群外部，并且要为同一种工作负载的访问流量进行负载均衡。\n\n\n\n1. Service：基于标签选择器将一组pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的对象上。并对客户端隐藏了真实的处理用户请求的pod资源。Service资源会通过API Service持续监视着标签选择器匹配到的后端pod对象，并实时跟踪个对象的变动；\n1. Endpoint：存储在etcd中，用来记录一个service对应的所有pod的访问地址，创建Service资源对象时，其关联的Endpoint对象会自动创建。\n1. Ingress：利用nginx，haproxy，envoy,traefik等负载均衡器来暴露集群内部服务，利用Ingress可以解决内部资源访问外部资源的方式，和四层调度替换为七层调度的问题。\n\n# 三、配置与存储\n\n\n> Docker容器分层联合挂载的方式决定了不宜在容器内部存储需要持久化的数据，于是它通过引入挂载外部存储卷的方式来解决此类问题\n\n\n\n1. Volume(存储卷)：本质上，Kubernetes Volume 是一个目录，当 Volume 被 mount 到Pod，Pod 中的所有容器都可以访问这个Volume。Kubernetes支持众多类型的存储设备或存储系统，如GlusterFS、CEPH、RBD和Flocker等。\n1. CSI：容器存储接口,可以扩展各种各样的第三方存储卷)特殊类型的存储卷\n1. ConfigMap：用于为容器中的应用提供配置数据以定制程序的行为\n1. Secret：保存敏感数据，如敏感的配置信息，例如密钥、证书等\n1. DownwardAPI：把外部环境中的信息输出给容器\n\n# 四、集群级资源\n\n\n> 用于定义集群自身配置信息的对象，它们仅应该由集群管理员进行操作\n\n1. Namespace：资源对象名称的作用范围，绝大多数对象都隶属于某个名称空间，默认时隶属于“default”。\n1. Node:Kubernetes集群的工作节点，其标识符在当前集群中必须是唯一的。\n1. Role：名称空间级别的由规则组成的权限集合，可被RoleBinding引用。\n1. ClusterRole:Cluster级别的由规则组成的权限集合，可被RoleBinding和ClusterRoleBinding引用。\n1. RoleBinding：将Role中的许可权限绑定在一个或一组用户之上，它隶属于且仅能作用于一个名称空间；绑定时，可以引用同一名称空间中的Role，也可以引用全局名称空间中的ClusterRole。\n1. ClusterRoleBinding：将ClusterRole中定义的许可权限绑定在一个或一组用户之上；它能够引用全局名称空间中的ClusterRole，并能通过Subject添加相关信息。\n\n# 五、元数据型资源\n\n\n> 用于为集群内部的其他资源配置其行为或特性\n\n\n\n1. HPA：自动伸缩工作负载类型的资源对象的规模\n1. PodTemplate：为pod资源的创建预制模板\n1. LimitRange：为名称空间的资源设置其CPU和内存等系统级资源的数量限制等。', 29, 0, 0, '2020-10-15 09:13:27.576631', '2021-01-26 05:40:42.426042', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (52, '资源对象-yuml文件', '本文详细介绍了yaml配置文件基础语法，以及k8s各种资源如何使用yaml语法定义', 'cover/2020_11_01_17_00_05_420901.jpg', '[TOC]\n\n# 一、yuml文档格式\n\n\n## 1. 注释\n\n使用#作为注释，YAML中只有行注释。\n\n## 2. 基本格式要求\n\n- YAML大小写敏感；\n- 使用缩进代表层级关系；\n- 缩进只能使用空格，不能使用TAB，不要求空格个数，只需要相同层级左对齐（一般2个或4个空格）\n\n## 3. YAML支持的数据结构\n\n- 对象：键值对集合，又称为映射、哈希、字典\n- 数组：一组按照次序排列的值，又称为序列、列表\n- 纯量：单个、不可再分的值\n\n## 4. k8s yaml 文件中字段类型\n\n- <Object> 对象类型\n\n```yaml\nmetadata：\n    name：\n  	namespace:\n```\n\n- <[]Object> 对象列表类型\n\n```yaml\n	containers:\n	-  name: name1\n	   images:\n	-  name: name2\n   	 images:\n```\n\n- <string> 字符串类型\n  namespace: default\n- <integer> 整型\n  replica: 3\n- <boolean> 布尔类型 true or false\n  hostIPC: false\n- <map[string]string>字符串嵌套\n  nodeSelector:\n      label: lablename\n- <[]> 列表类型\n\n```yaml\n写法1：\ncommand:\n	- \"string1\"\n	- \"string2\"\n写法二：\ncommand: [\"string1\",\"string2\",\"string3\"]\n```\n\n# 二、必要字段\n\n\n## 1. 模板\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n	name: dev\nspec:\n  finalizers:\n  - kubernetes\n```\n\n## 2. 字段说明\n\n| 字段       | 类型   | 说明                                                         |\n| ---------- | ------ | ------------------------------------------------------------ |\n| apiVersion | string | k8s API版本，可以使用kubectl api-versions查询                |\n| kind       | string | yaml文件定义的资源类型和角色，有Pod、Deployment、Endpoints、Service |\n| metadata   | object | 元数据对象                                                   |\n| spec       | object | 详细定义对象，用来描述所期望的对象应该具有的状态             |\n\n\n\n# 三、metadata字段\n\n| 字段      | 类型   | 说明                                                         |\n| --------- | ------ | ------------------------------------------------------------ |\n| namespace | string | 指定当前对象隶属的名称空间，默认值为default。                |\n| name：    | string | 设定当前对象的名称，在其所属的名称空间的同一类型中必须唯一。 |\n| labels：  | string | 用于标识当前对象的标签，键值数据，常被用作挑选条件。         |\n\n\n\n# 四、spec字段\n\n\n## 1. spec.containers[]：spec对象的容器列表定义\n\n| spec.containers[].name  | string | 定义容器的名字     |\n| ----------------------- | ------ | ------------------ |\n| spec.containers[].image | string | 定义用到的镜像名称 |\n\n| spec.containers[].imagePullPolicy | string | 定义镜像拉取策略 always：默认，每次都hub拉取 \nnever：仅使用本机镜像\n ifnotprestnt：本机没有就hub拉取 |\n| spec.containers[].command[] | list | 指定一个或多个容器启动命令 |\n| spec.containers[].args[] | list | 指定容器启动命令参数，Dockerfile中CMD参数 |\n| spec.containers[].workingDir | string | 指定容器工作目录 |\n\n\n\n## 2. spec.containers[].volumeMounts[]：指定容器内部的存储卷配置\n\n| spec.containers[].volumeMounts[].name      | string | 容器挂载的存储卷名称                         |\n| ------------------------------------------ | ------ | -------------------------------------------- |\n| spec.containers[].volumeMounts[].mountPath | string | 容器挂载的存储卷路径                         |\n| spec.containers[].volumeMounts[].readOnly  | string | 存储卷读写模式，ture或false， 默认为读写模式 |\n\n\n\n## 3. spec.containers[].ports[]：指定容器端口列表\n\n| spec.containers[].ports[].name          | string | 指定端口名称                                                 |\n| --------------------------------------- | ------ | ------------------------------------------------------------ |\n| spec.containers[].ports[].containerPort | string | 指定容器监听的端口号                                         |\n| spec.containers[].ports[].hostPort      | string | 指定容器所在主机监听的端口号，设置了hostPort后同一台主机无法启动相同副本 |\n| spec.containers[].ports[].protocol      | string | 指定端口协议，默认为TCP                                      |\n\n\n\n## 4. spec.containers[].env[]：指定容器运行前环境变量\n\n| spec.containers[].env[].name  | string | 指定环境变量名称 |\n| ----------------------------- | ------ | ---------------- |\n| spec.containers[].env[].value | string | 指定环境变量值   |\n\n\n\n## 5. spec.containers[].resources.limits：指定容器运行时资源的上限\n\n| spec.containers[].resources.limits.cpu    | string | 指定cpu限制，单位为core数    |\n| ----------------------------------------- | ------ | ---------------------------- |\n| spec.containers[].resources.limits.memory | string | 指定内存限制，单位为MIB，GIB |\n\n\n\n## 6. spec.containers[].resources.requests：指定容器启动和调度时资源的上限\n\n| spec.containers[].resources.requests.cpu    | string | 指定cpu限制，单位为core数    |\n| ------------------------------------------- | ------ | ---------------------------- |\n| spec.containers[].resources.requests.memory | string | 指定内存限制，单位为MIB，GIB |\n\n\n\n## 7. 探针\n\n| spec.containers[].livenessProbe                | object | 存活检测                                                     |\n| ---------------------------------------------- | ------ | ------------------------------------------------------------ |\n| spec.containers[].ReadinessProbe               | object | 就绪检测                                                     |\n| spec.containers[].检测方式.initialDelaySeconds | Int    | 初始化延迟的时间，监测从多久之后开始运行，单位是秒           |\n| spec.containers[].检测方式.timeoutSeconds      | Int    | 监测的超时时间，如果超过这个时长后，则认为监测失败           |\n| spec.containers[].检测方式.periodSeconds       | int    | 存活性探测的频度，显示为period属性，默认为10s，最小值为1     |\n| spec.containers[].检测方式.successThreshold    | int    | 处于失败状态时，探测操作至少连续多少次的成功才被认为是通过检测，显示为#success属性，默认值为1，最小值也为1。 |\n| spec.containers[].检测方式.failureThreshold：  | int    | 处于成功状态时，探测操作至少连续多少次的失败才被视为是检测不通过，显示为#failure属性，默认值为3，最小值为1。 |\n\n\n\n## 8. exec检测\n\n| spec.containers[].检测方式.exec         | Object | exec方式执行命令检测 |\n| --------------------------------------- | ------ | -------------------- |\n| spec.containers[].检测方式.exec.command | List   | 执行的检测命令依据   |\n\n\n\n## 9. httpGet检测\n\n| spec.containers[].检测方式.httpGet             | Object | http探针，依据状态码                                |\n| ---------------------------------------------- | ------ | --------------------------------------------------- |\n| spec.containers[].检测方式.httpGet.host        | string | 请求的主机地址，默认为Pod IP；                      |\n| spec.containers[].检测方式.httpGet.port        | string | 请求的端口，必选字段。                              |\n| spec.containers[].检测方式.httpGet.httpHeaders | Object | 自定义的请求报文首部                                |\n| spec.containers[].检测方式.httpGet.path        | string | 请求的HTTP资源路径，即URL                           |\n| spec.containers[].检测方式.httpGet.scheme      | string | 建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。 |\n\n\n\n## 10. tcpSocket检测\n\n| spec.containers[].检测方式.tcpSocket      | Object | tcpSocket检测，依据端口是否开放      |\n| ----------------------------------------- | ------ | ------------------------------------ |\n| spec.containers[].检测方式.tcpSocket.host | string | 请求连接的目标IP地址，默认为Pod IP。 |\n| spec.containers[].检测方式.tcpSocket.port | string | 请求连接的目标端口，必选字段。       |\n\n\n\n## 11. 其他spec\n\n| spec.restartPolicy    | string  | 定义Pod重启策略 always：pod一旦终止就重启 onfailure：pod只有以非零退出码终止时（正常结束退出码为0），才会重启 never：pod终止后，不会重启 |\n| --------------------- | ------- | ------------------------------------------------------------ |\n| spec.nodeSelector     | object  | 定义node的label过滤标签，以key:value指定                     |\n| spec.imagePullSecrets | object  | 定义pull镜像时，使用secret名称，以key:value指定              |\n| spec.hostNetwork      | Boolean | 定义是否使用主机网络模式，默认使用docker网桥，如果设置true无法启动相同副本 |\n\n\n\n# 五、控制器字段\n\n\n## 1. ReplicaSet\n\n| spec.replicas                  | integer | 期望的Pod对象副本数                    |\n| ------------------------------ | ------- | -------------------------------------- |\n| spec.selector                  | Object  | 当前控制器匹配Pod对象副本的标签选择器  |\n| spec.selector.matchLabels      | string  | matchLabels标签选择器                  |\n| spec.selector.matchExpressions | string  | matchExpressions标签选择器             |\n| spec.template                  | Object  | 用于补足Pod副本数量时使用的Pod模板资源 |\n\n\n\n## 2. Deployment\n\n| spec.strategy.rollingUpdate. maxSurge      | integer/percent | 升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是一个期望值的百分比 |\n| ------------------------------------------ | --------------- | ------------------------------------------------------------ |\n| spec.strategy.rollingUpdate.maxUnavailable | integer/percent | 升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比 |\n| spec.revisionHistoryLimit                  | integer         | 控制器可保存的历史版本数量                                   |\n| spec.minReadySeconds                       | integer         | 新建的Pod对象，在启动后的多长时间内如果其容器未发生崩溃等异常情况即被视为“就绪”；默认为0秒，表示一旦就绪性探测成功，即被视作可用 |\n\n\n\n## 3. DaemonSet控制器\n\n- .spec.selector 是必填字段，且指定该字段时，必须与.spec.template.metata.labels 字段匹配（不匹配的情况下创建 DaemonSet将失败）。DaemonSet 创建以后，.spec.selector字段就不可再修改。如果修改，可能导致不可预见的结果。\n\n## 4. Job控制器\n\n- Pod模板中的spec.restartPolicy默认为“Always”，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为“Never”或“OnFailure”。\n\n| .spec.parallelism | integer | 能够定义作业执行的并行度，将其设置为2或者以上的值即可实现并行多队列作业同时运行 |\n| ----------------- | ------- | ------------------------------------------------------------ |\n| spec.completions  | integer | 使用的是默认值1，则表示并行度即作业总数                      |\n\n\n\n- 将.spec.completions属性值设置为大于.spec.parallelism的属性值，则表示使用多队列串行任务作业模式\n\n| .spec.activeDeadlineSeconds | integer | Job的deadline，用于为其指定最大活动时间长度，超出此时长的作业将被终止。 |\n| --------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.backoffLimit          | integer | 将作业标记为失败状态之前的重试次数，默认值为6。              |\n\n\n\n## 5. CronJob控制器\n\n| jobTemplate                      | Object               | Job控制器模板，用于为CronJob控制器生成Job对象；必选字段      |\n| -------------------------------- | -------------------- | ------------------------------------------------------------ |\n| schedule                         | string               | Cron格式的作业调度运行时间点；必选字段。                     |\n| concurrencyPolicy                | string               | 并发执行策略，可用值有“Allow”（允许）、“Forbid”（禁止）和“Replace”（替换），用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 |\n| failedJobHistoryLimit            | integer              | 为失败的任务执行保留的历史记录数，默认为1。                  |\n| successfulJobsHistoryLimit       | integer              | 为成功的任务执行保留的历史记录数，默认为3                    |\n| startingDeadlineSeconds          | integer              | 因各种原因缺乏执行作业的时间点所导致的启动作业错误的超时时长，会被记入错误历史记录 |\n| suspend                          | boolean              | 是否挂起后续的任务执行，默认为false，对运行中的作业不会产生影响。 |\n| .spec.successfulJobsHistoryLimit | integer              | 保留多少完成的 Job。默认没有限制，所有成功和失败的 Job 都会被保留。 当运行一个 Cron Job 时，Job 可以很快就堆积很多，推荐设置这两个字段的值。设置限制的值为 0，相关类型的 Job 完成后将不会被保留。 |\n| .spec.failedJobsHistoryLimit     | integer              | 保留多少失败的 Job                                           |\n| .spec.concurrencyPolicy          | Allow/Forbid/Replace | 属性控制作业并存的机制，其默认值为“Allow”，即允许前后Job，甚至属于同一个CronJob的更多Job同时运行。 “Forbid”用于禁止前后两个Job同时运行，如果前一个尚未结束，后一个则不予启动（跳过）， “Replace”用于让后一个Job取代前一个，即终止前一个并启动后一个。 |\n\n\n\n## 6. PDB\n\n| .spec.selector       | Object | 当前PDB对象使用的标签选择器，一般是与相关的Pod控制器使用同一个选择器。 |\n| -------------------- | ------ | ------------------------------------------------------------ |\n| .spec.minAvailable   | string | Pod自愿中断的场景中，至少要保证可用的Pod对象数量或比例，要阻止任何Pod对象发生自愿中断，可将其设置为100% |\n| .spec.maxUnavailable | string | Pod自愿中断的场景中，最多可转换为不可用状态的Pod对象数量或比例，0值意味着不允许Pod对象进行自愿中断；此字段与minAvailable互斥 |\n\n\n\n# 六、service\n\n| .spec.selector:        | Object  | 当前svc使用的标签选择器，用来管理pod中一样的标签资源。 |\n| ---------------------- | ------- | ------------------------------------------------------ |\n| .spec.type             | string  | service类型                                            |\n| .spec.clusterIP        | string  | 虚拟服务IP地址                                         |\n| .spec.ExternalName     | string  | ExternalName模式域名                                   |\n| .spec.sessionAffinity  | string  | 是否支持session会话绑定                                |\n| .spec.ports.name       | string  | 端口名称                                               |\n| .spec.ports.protocol   | string  | 端口协议，默认tcp                                      |\n| .spec.ports.port       | integer | 服务监听端口号                                         |\n| .spec.ports.targetPort | integer | 转发到后端的服务端口号                                 |\n| .spec.ports.nodePort   | integer | nodeport模式绑定物理主机端口                           |\n\n\n\n# 七、ingress\n\n| .spec.rules               | Object  | 用于定义当前Ingress资源的转发规则列表                        |\n| ------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.rules.host          | string  | 指定访问地址，留空表示通配所有的主机名                       |\n| .spec.backend             | Object  | 为负载均衡器指定一个全局默认的后端                           |\n| .spec.backend.serviceName | string  | 流量转发的后端目标Service资源的名称                          |\n| .spec.backend.servicePort | integer | 流量转发的后端目标Service资源的端口                          |\n| .spec.tls                 | Object  | TLS配置，目前仅支持通过默认端口443提供服务                   |\n| .spec.tls.hosts           | string  | 使用的TLS证书之内的主机名称，此处使用的主机名必须匹配tlsSecret中的名称。 |\n| .spec.tls.secretName      | string  | SSL会话的secret对象名称，在基于SNI实现多主机路由的场景中，此字段为可选。 |\n\n\n\n# 八、存储\n\n\n## 1. configmap\n\n| .spec.volumes.configMap.items.key  | string  | 要引用的键名称，必选字段                                     |\n| ---------------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.volumes.configMap.items.path | string  | 对应的键于挂载点目录中生成的文件的相对路径，可以不同于键名称，必选字段 |\n| .spec.volumes.configMap.items.mode | integer | 文件的权限模型，可用范围为0到0777。                          |\n\n', 28, 0, 0, '2020-10-15 10:32:07.797427', '2021-01-26 11:44:55.636504', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (53, '资源对象-k8s', '总结k8s所有yaml字段的含义以及如何使用', 'cover/2020_11_01_17_04_48_459054.jpg', '\n![yaml字段大全.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604221358213-2399b1c5-f46c-4515-9ad0-6d1e4bb764d5.png#align=left&display=inline&height=8801&margin=%5Bobject%20Object%5D&name=yaml%E5%AD%97%E6%AE%B5%E5%A4%A7%E5%85%A8.png&originHeight=8801&originWidth=2232&size=2245006&status=done&style=none&width=2232)\n\n', 30, 0, 0, '2020-10-15 10:35:38.219229', '2021-01-26 11:45:26.544341', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (54, '资源对象-管理Namespace资源', '名称空间（Namespace）是Kubernetes集群级别的资源，用于将集群分隔为多个隔离的逻辑分区以配置给不同的用户、租户、环境或项目使用，例如，可以为development、qa和production应用环境分别创建各自的名称空间', 'cover/2020_11_01_17_06_55_602093.jpg', '[TOC]\n# 一、简介\n\n\n1. 名称空间（Namespace）是Kubernetes集群级别的资源，用于将集群分隔为多个隔离的逻辑分区以配置给不同的用户、租户、环境或项目使用，例如，可以为development、qa和production应用环境分别创建各自的名称空间。\n1. Kubernetes的绝大多数资源都隶属于名称空间级别（另一个是全局级别或集群级别），名称空间资源为这类的资源名称提供了隔离的作用域，同一名称空间内的同一类型资源名必须是唯一的，但跨名称空间时并无此限制。不过，Kubernetes还是有一些资源隶属于集群级别的，如Node、Namespace和PersistentVolume等资源，它们不属于任何名称空间，因此资源对象的名称必须全局唯一\n\n\n\n# 二、Namespaces 的常用操作\n\n\n1. 查看命名空间\n   `# kubectl get namespaces `\n\n2. Kubernetes默认有三个命名空间\n\n- default:默认的命名空间\n- kube-system:由Kubernetes系统对象组成的命名空间\n- kube-public:该空间由系统自动创建并且对所有用户可读性，做为集群公用资源的保留命名空间\n\n3. 查看特定名称空间的详细信息\n   `# kubectl describe namespaces default ` \n3. 创建命名空间\n   `# kubectl create namespace test-cluster ` \n3. 查询命名空间中的资源\n   `# kubectl get all --namespace=test-cluster ` \n   `# kubectl get all -n test-clutser ` \n   `# kubectl get nodes ` \n   `# kubectl get pods -n kube-system ` \n3. 修改默认的namespace配置\n   `# kubectl config view` \n\n- 先查看是否设置了current-context\n  `# kubectl config set-context default --namespace=bs-test` \n- 设置default配置的namespace参数\n  `# kubectl config set current-context default`  //设置当前环境变量为 default\n- 通过这段代码设置默认的命名空间后，就不用每次在输入命令的时候带上--namespace参数了。\n\n\n\n# 三、其他操作\n\n\n1. 查看命名空间中的资源。\n   `# kubectl api-resources --namespaced=true ` \n1. 查看不在命名空间中的资源\n   `# kubectl api-resources --namespaced=false ` ', 25, 0, 0, '2020-10-15 10:37:56.799222', '2021-01-26 05:40:55.705051', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (55, '资源对象-标签与标签选择器', '标签就是“键值”类型的数据，它们可于资源创建时直接指定，也可随时按需添加于活动对象中，而后即可由标签选择器进行匹配度检查从而完成资源挑选。一个对象可拥有不止一个标签，而同一个标签也可被添加至多个资源之上。', 'cover/2020_11_01_17_09_25_710291.jpg', '[TOC]\n\n# 一、标签概述\n\n1. 标签就是“键值”类型的数据，它们可于资源创建时直接指定，也可随时按需添加于活动对象中，而后即可由标签选择器进行匹配度检查从而完成资源挑选。一个对象可拥有不止一个标签，而同一个标签也可被添加至多个资源之上。\n1. 为资源附加多个不同纬度的标签以实现灵活的资源分组管理功能，例如，版本标签、环境标签、分层架构标签等，用于交叉标识同一个资源所属的不同版本、环境及架构层级等\n\n\n\n# 二、管理标签资源\n\n\n1. 创建资源时，可直接在其metadata中嵌套使用“labels”字段以定义要附加的标签项。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126204022-ef2634c8-2778-44a2-b01f-612421a11e08.png#align=left&display=inline&height=262&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=262&originWidth=304&size=353797&status=done&style=none&width=304)\n\n2. 在“kubectl get pods”命令中使用“--show-labels”选项，以额外显示对象的标签信息：\n   $ kubectl get pods --show-labels\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126243588-ce05cf44-b574-475d-9cd9-4297f2734d7a.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=907&size=353797&status=done&style=none&width=907)\n\n3. 按需进行添加标签操作。\n   $ kubectl label pods/pod-with-labels version=v1\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126267070-315fe539-5f71-41be-875b-17665cd15740.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=1051&size=353797&status=done&style=none&width=1051)\n\n4. 对于已经附带了指定键名的标签，使用“--overwrite”命令以强制覆盖原有的键值\n   $ kubectl label pods/pod-with-labels version=v2 --overwrite\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126304496-c1da9364-ba38-4fbd-9304-7d0534a0a15e.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1050&size=353797&status=done&style=none&width=1050)\n\n5. 删除指定键名的标签，使用“标签名-”，即可删除\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126350783-a6297040-652a-4d19-bb52-27847a7cd3a5.png#align=left&display=inline&height=274&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=274&originWidth=930&size=353797&status=done&style=none&width=930)\n\n\n# 三、标签选择器\n\n\n1. Kubernetes API目前支持两个选择器：基于等值关系（equality-based）以及基于集合关系（set-based）。\n1. 基于等值关系的标签选择器的可用操作符有“=”“==”和“!=”三种，其中前两个意义相同，都表示“等值”关系，最后一个表示“不等”关系\n1. “kubectl get”命令的“-l”选项能够指定使用标签选择器，例如，显示键名env的值不为qa的所有Pod对象：\n   $ kubectl get pods -l \"env! =qa\" -L env\n1. 基于集合关系的标签选择器，它们的使用格式及意义具体如下。\n\n- KEY in (VALUE1, VALUE2, …)：指定的键名的值存在于给定的列表中即满足条件。\n- KEY notin (VALUE1, VALUE2, …)：指定的键名的值不存在于给定的列表中即满足条件。\n- KEY：所有存在此键名标签的资源。\n- ! KEY：所有不存在此键名标签的资源。\n  例如，显示标签键名env的值为production或dev的所有Pod对象：\n  `$ kubectl get pods -l \"env in (production, dev)\" -L env` \n\n5. Kubernetes的诸多资源对象必须以标签选择器的方式关联到Pod资源对象，例如Service、Deployment和ReplicaSet类型的资源等，它们在spec字段中嵌套使用嵌套的“selector”字段，通过“matchLabels”来指定标签选择器，有的甚至还支持使用“matchExpressions”构造复杂的标签选择机制。\n\n- matchLabels：通过直接给定键值对来指定标签选择器。\n- matchExpressions：基于表达式指定的标签选择器列表，每个选择器都形如“{key:KEY_NAME,operator: OPERATOR, values: [VALUE1, VALUE2,…]}”，选择器列表间为“逻辑与”关系；使用In或NotIn操作符时，其values不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空。\n\n\n\n# 四、Pod节点选择器nodeSelector\n\n\n1. Pod节点选择器是标签及标签选择器的一种应用，它能够让Pod对象基于集群中工作节点的标签来挑选倾向运行的目标节点。\n1. Pod对象的spec.nodeSelector可用于定义节点标签选择器，用户事先为特定部分的Node资源对象设定好标签，而后配置Pod对象通过节点标签选择器进行匹配检测，从而完成节点亲和性调度。\n1. 为Node资源对象附加标签的方法同Pod资源，使用“kubectl label nodes/NODE”命令即可。\n\n- 例如，可为node2节点设置“disktype=ssd”标签以标识其拥有SSD设备：\n  $ kubectl label nodes node2 disktype=ssd\n- 查看具有键名SSD的标签的Node资源：\n  $ kubectl get nodes -l \'disktype\' -L disktype\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127096983-ebf1cade-2be2-4b56-96ff-c70e5f5cb0f4.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=693&size=353797&status=done&style=none&width=693)\n\n4. 如果某Pod资源需要调度至这些具有SSD设备的节点之上，那么只需要为其使用spec.nodeSelector标签选择器即可，例如下面的资源清单文件pod-with-nodeselector.yaml示例：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n	name: pod-with-nodeselector\n	labels:\n		env: testing\nspec:\n	containers:\n	- name: myapp\n		image: busybox\n	nodeSelector:\n		disktype: ssd\n```\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127214235-d7685357-a292-43ec-9842-df7717ecf03a.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=831&size=353797&status=done&style=none&width=831)', 26, 0, 1, '2020-10-15 10:39:52.136688', '2021-01-26 11:45:49.797032', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (60, '资源对象-Pod资源对象', '在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。而Kubernetes项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”', 'cover/2020_11_01_17_14_48_571802.jpg', '[TOC]\n\n# 一、Pod概念的产生\n\n\n1. 在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。\n1. 而Kubernetes项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。\n1. 分别运行于各自容器的进程之间无法实现基于IPC的通信机制，此时，容器间的隔离机制对于依赖于此类通信方式的进程来说却又成了阻碍。Pod资源抽象正是用来解决此类问题\n\n# 二、Pod特点\n\n\n1. 在 Kubernetes项目里， Pod的实现需要使用一个中间容器，这个容器叫作Infra容器。在这个 Pod中，Infra容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace的方式，与 Infra容器关联在一起。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127738905-9d9b0827-feae-4f96-9ece-cca30a0315bd.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=321&originWidth=533&size=353797&status=done&style=none&width=533)\n\n2. 对于 Pod 里的容器 A 和容器 B 来说：\n\n- 它们可以直接使用 localhost 进行通信；\n- 它们看到的网络设备跟 Infra 容器看到的完全一样；\n- 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP地址；\n- 其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；\n- Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。\n\n3. 把整个虚拟机想象成为一个\n   Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。\n\n\n\n# 三、Pod模型\n\n\n1. Sidecar\n   pattern（边车模型或跨斗模型）：即为Pod的主应用容器提供协同的辅助应用容器，每个应用独立运行\n\n\n\n- 最为典型的代表是将主应用容器中的日志使用agent收集至日志服务器中时，可以将agent运行为辅助应用容器，即sidecar。另一个典型的应用是为主应用容器中的database server启用本地缓存。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128061676-353480da-9726-418a-98c7-d6c208cfaed8.png#align=left&display=inline&height=225&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=225&originWidth=526&size=353797&status=done&style=none&width=526)\n\n\n2. Ambassador\n   pattern（大使模型）：即为远程服务创建一个本地代理，代理应用运行于容器中，主容器中的应用通过代理容器访问远程服务\n\n\n\n- 一个典型的使用示例是主应用容器中的进程访问“一主多从”模型的远程Redis应用时，可在当前Pod容器中为Redis服务创建一个Ambassador container，主应用容器中的进程直接通过localhost接口访问Ambassador container即可。即便是Redis主从集群架构发生变动时，也仅需要将Ambassador container加以修改即可，主应用容器无须对此做出任何反应。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128174063-7f3ab819-1d02-4d4f-bb7f-6dbebe1eef8d.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=531&size=353797&status=done&style=none&width=531)\n\n\n3. Adapter\n   pattern（适配器模型）：此种模型一般用于将主应用容器中的内容进行标准化输出，\n\n\n\n- 例如，日志数据或指标数据的输出，这有助于调用者统一接收数据的接口，另外，某应用滚动升级后的版本不兼容旧的版本时，其报告信息的格式也存在不兼容的可能性，使用Adapter\n  container有助于避免那些调用此报告数据的应用发生错误。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128214702-12cfa4b4-29ac-4bff-b4cc-04251107cddf.png#align=left&display=inline&height=187&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=187&originWidth=462&size=353797&status=done&style=none&width=462)\n\n\n4. Kubernetes系统的Pod资源对象用于运行单个容器化应用，此应用称为Pod对象的主容器（main container），同时Pod也能容纳多个容器，不过额外的容器一般工作为Sidecar模型，用于辅助主容器完成工作职能。\n\n\n\n# 四、yaml文件Pod与Container对象\n\n\n1. 基本原则\n\n- 凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的，凡是跟容器的Linux Namespace 相关的属性，也一定是 Pod 级别的，凡是 Pod中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义\n\n2. 应用举例\n\n- NodeSelector：供用户将 Pod 与 Node 进行绑定\n- HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容\n- shareProcessNamespace=true：这个 Pod 里的容器要共享 PID Namespace', 26, 0, 0, '2020-10-15 10:45:02.875083', '2021-01-26 05:15:10.356908', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (63, '资源对象-Pod生命周期', 'Pod', 'cover/2020_11_01_17_14_26_565630.jpg', '[TOC]\n\n# 一、Pod相位\n\n\n1. Pod 生命周期的变化，主要体现在 Pod API 对象的 Status 部分，这是它除了Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod的当前状态，它有如下几种可能的情况：\n\n- Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。\n- Running。这个状态下，Pod已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。\n- Succeeded。这个状态意味着，Pod里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。\n- Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0的返回码）退出。这个状态的出现，意味着你得想办法 Debug这个容器的应用，比如查看 Pod 的 Events 和日志。\n- Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给kube-apiserver，这很有可能是主从节点（Master 和Kubelet）间的通信出现了问题。\n\n# 二、Pod创建过程\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128919581-117903ea-eb30-41e5-bdee-7547aa1dadc6.png#align=left&display=inline&height=524&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=524&originWidth=790&size=353797&status=done&style=none&width=790)\n\n1. 用户通过kubectl或其他API客户端提交Pod Spec给API Server。\n1. API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。\n1. API Server开始反映etcd中的状态变化。\n1. 所有的Kubernetes组件均使用“watch”机制来跟踪检查API Server上的相关的变动。\n1. kube-scheduler（调度器）通过其“watcher”觉察到API Server创建了新的Pod对象但尚未绑定至任何工作节点。\n1. kube-scheduler为Pod对象挑选一个工作节点并将结果信息更新至API Server。\n1. 调度结果信息由API Server更新至etcd存储系统，而且API Server也开始反映此Pod对象的调度结果。\n1. Pod被调度到的目标工作节点上的kubelet尝试在当前节点上调用Docker启动容器，并将容器的结果状态回送至API Server。\n1. API Server将Pod状态信息存入etcd系统中。\n1. 在etcd确认写入操作成功完成后，API Server将确认信息\n\n\n\n# 三、Pod生命周期中行为\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129048585-fe284a47-712e-4210-99b0-9ced83bbd745.png#align=left&display=inline&height=537&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=537&originWidth=1052&size=353797&status=done&style=none&width=1052)\n\n\n# 四、初始化容器\n\n\n1. 初始化容器（init container）即应用程序的主容器启动之前要运行的容器，常用于为主容器执行一些预置操作，它们具有两种典型特征。\n\n- 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么Kubernetes需要重启它直到成功完成。\n- 每个init容器都必须在下一个init容器启动之前成功完成\n\n2. 初始化容器作用\n\n- 包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的\n- 包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要FROM另一个镜像，只需要在安装过程中使用类似sed、awk、python或dig这样的工具。\n- 应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。\n- Init容器使用LinuxNamespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问Secret的权限，而应用程序容器则不能。\n- 它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以Init容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。\n\n3. 特殊说明\n\n- 在Pod启动过程中，Init容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出\n- 如果由于运行时或失败退出，将导致容器启动失败，它会根据Pod的restartPolicy指定的策略进行重试。然而，如果Pod的restartPolicy设置为Always，Init容器失败时会使用RestartPolicy策略\n- 在所有的Init容器没有成功之前，Pod将不会变成Ready状态。Init容器的端口将不会在Service中进行聚集。正在初始化中的Pod处于Pending状态，但应该会将Initializing状态设置为true\n- 如果Pod重启，所有Init容器必须重新执行\n- 对Init容器spec的修改被限制在容器image字段，修改其他字段都不会生效。更改Init容器的image字段，等价于重启该Pod\n- Init容器具有应用容器的所有字段。除了readinessProbe，因为Init容器无法定义不同于完成（completion）的就绪（readiness）之外的其他状态。这会在验证过程中强制执行\n- 在Pod中的每个app和Init容器的名称必须唯一；与任何其它容器共享同一个名称，会在验证时抛出错误\n\n\n\n# 五、生命周期钩子函数\n\n\n1. 容器生命周期钩子使它能够感知其自身生命周期管理中的事件，并在相应的时刻到来时运行由用户指定的处理程序代码。Kubernetes为容器提供了两种生命周期钩子。\n\n- postStart：于容器创建完成之后立即运行的钩子处理器（handler）\n- preStop：于容器终止操作之前立即运行的钩子处理器，它以同步的方式调用，因此在其完成之前会阻塞删除容器的操作的调用。\n\n2. 钩子处理器的实现方式有“Exec”和“HTTP”两种，\n\n- exec：执行一段命令\n- HTTP：发送HTTP请求\n\n\n\n# 六、容器探测\n\n\n1. kubelet对容器周期性执行的健康状态诊断，诊断操作由容器的处理器（handler）进行定义。Kubernetes支持三种处理器用于Pod探测。\n\n- ExecAction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。\n- TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。\n- HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起HTTP GET请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。\n\n2. 每次探测都将获得以下三种结果之一：\n\n- 成功：容器通过了诊断。\n- 失败：容器未通过诊断。\n- 未知：诊断失败，因此不会采取任何行动\n\n\n\n# 七、容器的重启策略\n\n\n1. 容器程序发生崩溃或容器申请超出限制的资源等原因都可能会导致Pod对象的终止，此时是否应该重建该Pod对象则取决于其重启策略（restartPolicy）属性的定义。\n\n- Always：但凡Pod对象终止就将其重启，此为默认设定。\n- OnFailure：仅在Pod对象出现错误时方才将其重启。\n- Never：从不重启。\n\n2. 需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一节点上重新启动Pod对象的相关容器。首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么终止，直到节点发生故障或被删除。\n\n\n\n# 八、存活性检测\n\n\n1. 用于判定容器是否处于“运行”（Running）状态；一旦此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为“Success”。\n1. 设置exec探针：通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，若命令状态返回值为0则表示“成功”通过检测，其值均为“失败”状态。\n\n- 示例：使用busybox镜像创建/tem/headthy文件，1分钟后删除。如果文件存在，检测通过，否则为失败\n- 定义liveness-exec.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129450300-7de6b935-0f7d-4304-9695-68ca88d9536f.png#align=left&display=inline&height=303&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=303&originWidth=922&size=353797&status=done&style=none&width=922)\n\n- 在60秒之内使用“kubectl describe pods/liveness-exec”查看其详细信息，其存活性探测不会出现错误。而超过60秒之后，再次运行“kubectl describe pods/liveness-exec”查看其详细信息可以发现，存活性探测出现了故障\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129556467-7e1ed46a-62a7-43bf-b07a-9b56ebf0fd63.png#align=left&display=inline&height=257&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=257&originWidth=1252&size=353797&status=done&style=none&width=1252)\n\n- 待容器重启完成后再次查看，容器已经处于正常运行状态，直到文件再次被删除，存活性探测失败而重启。从下面的命令显示可以看出，liveness-exec已然重启\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129581406-ffa73e9c-0e70-421e-befa-0ee84a11f274.png#align=left&display=inline&height=76&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=76&originWidth=563&size=353797&status=done&style=none&width=563)\n\n3. 设置HTTP探针\n   基于HTTP的探测（HTTPGetAction）向目标容器发起一个HTTP请求，根据其响应码进行结果判定。“spec.containers.livenessProbe.httpGet”字段用于定义此类检测，它的可用配置字段包括如下几个。\n\n- host <string>：请求的主机地址，默认为Pod IP\n- port <string>：请求的端口，必选字段。\n- httpHeaders <[]Object>：自定义的请求报文首部。\n- path <string>：请求的HTTP资源路径，即URL path。\n- scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。\n- 示例：通过nginx生成一个index.html页面文件，请求的资源路径为“/index.html”，地址默认为Pod\n  IP，端口使用了容器中定义的端口名称HTTP，然后删除文件，再次查看pod信息\n- 定义liveness-http.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129720758-fa6f5e9c-a6f6-4159-9ab5-b4838a6c27d7.png#align=left&display=inline&height=350&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=350&originWidth=391&size=353797&status=done&style=none&width=391)\n\n- 而后查看其健康状态检测相关的信息，健康状态检测正常时，容器也将正常运行：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129771625-6781a43c-b156-4d15-960f-2b0ce778fbb8.png#align=left&display=inline&height=187&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=187&originWidth=1241&size=353797&status=done&style=none&width=1241)\n\n- 接下来借助于“kubectl exec”命令删除index.html测试页面：\n  `$ kubectl exec liveness-httpget rm /usr/share/nginx/html/index.html` \n- 而后再次使用“kubectl describe pod/liveness-httpget”查看其详细的状态信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129802054-f380b57f-9fb4-45e9-9f86-e1a6d31b7d1e.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=1252&size=353797&status=done&style=none&width=1252)\n\n4. 设置TCP探针\n   向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。“spec.containers.livenessProbe.tcpSocket”字段用于定义此类检测，它主要包含以下两个可用的属性。\n\n- host <string>：请求连接的目标IP地址，默认为Pod IP。\n- port <string>：请求连接的目标端口，必选字段。\n- 下面是一个定义在资源清单文件liveness-tcp.yaml中的示例，它向Pod IP的80/tcp端口发起连接请求，并根据连接建立的状态判定测试结果：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129833989-50bb841c-6c68-47eb-b5e1-7d0fadefce20.png#align=left&display=inline&height=328&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=328&originWidth=352&size=353797&status=done&style=none&width=352)\n\n\n# 九、就绪性检测\n\n\n1. 用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着其尚未准备就绪，端点控制器（如Service对象）会将其IP从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中。\n1. 与存活性探测机制相同，就绪性探测也支持Exec、HTTP GET和TCP Socket三种探测方式。\n1. 与存活性探测触发的操作不同的是，探测失败时，就绪性探测不会杀死或重启容器以保证其健康性，而是通知其尚未就绪，并触发依赖于其就绪状态的操作（例如，从Service对象中移除此Pod对象）以确保不会有客户端请求接入此Pod对象。', 37, 0, 0, '2020-10-15 10:52:29.557234', '2021-01-26 05:41:06.974271', 1, 6, 0, 1, 1);
INSERT INTO `blog_article` VALUES (64, '资源对象-资源需求与限制', 'kubernets的资源主要有cpu和内存，策略分别为requests和limits', 'cover/2020_11_01_17_17_07_861245.jpg', '[TOC]\n\n# 一、资源\n\n\n1. CPU：1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1core）相当于1000个微核心（millicores），因此500m相当于是0.5个核心，即二分之一个核心。\n1. 内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K作为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。\n\n# 二、资源需求\n\n\n1. “requests”属性定义其请求的确保可用值，即容器运行可能用不到这些额度的资源，但用到时必须要确保有如此多的资源可用\n1. 示例\n\n- 定义一个Pod中stress容器，确保128Mi的内存及五分之一个CPU核心（200m）资源可用，\n- 它运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时它也会尽可能多地占用CPU资源\n- 另外再启动一个专用的CPU压力测试进程（-c 1）。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129999463-c42e210a-efd4-489c-961a-4fa735cf04c2.png#align=left&display=inline&height=309&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=309&originWidth=825&size=353797&status=done&style=none&width=825)\n\n- 查看资源使用情况\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130015373-6098717c-cb94-4248-9acc-a868a327098a.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=894&size=353797&status=done&style=none&width=894)\n\n- 每个测试进程CPU占用率为25%，内存占用为262m，此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用的范围内尽量多地占用相关的资源\n\n\n\n# 三、资源限制\n\n\n1. ”limits”属性则用于限制资源可用的最大值，即硬限制\n1. 示例：一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而导致“OOMKillled”为止：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130039468-61e8c5a9-6fe0-41d6-9bd2-5d493eca5695.png#align=left&display=inline&height=328&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=328&originWidth=373&size=353797&status=done&style=none&width=373)\n\n\n- Pod资源的默认重启策略为Always，于是在memleak因内存资源达到硬限制而被终止后会立即重启。不过，多次重复地因为内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制，即每次重启的时间间隔会不断地拉长。于是，用户看到的Pod资源的相关状态通常为“CrashLoopBackOff”：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130056913-1d317d32-9210-4d62-bb94-392dc6a42c41.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=623&size=353797&status=done&style=none&width=623)', 32, 0, 0, '2020-10-19 21:35:40.472204', '2021-01-26 11:46:08.434786', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (65, '资源对象-Pod服务质量（优先级）', 'Kubernetes允许节点资源对limits的过载使用，当节点无法同时满足其上的所有Pod对象以资源满载的方式运行。在内存资源紧缺时，通过Pod对象的优先级完成先后终止哪些Pod对象判定。根据Pod对象的requests和limits属性，Kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality', 'cover/2020_11_01_17_17_45_478132.jpg', '1. Kubernetes允许节点资源对limits的过载使用，当节点无法同时满足其上的所有Pod对象以资源满载的方式运行。\n1. 在内存资源紧缺时，通过Pod对象的优先级完成先后终止哪些Pod对象判定。根据Pod对象的requests和limits属性，Kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality  of Service, QoS）类别下，具体说明如下。\n1. Guaranteed：每个容器都为CPU资源设置了具有相同值的requests和limits属性，以及每个容器都为内存资源设置了具有相同值的requests和limits属性的Pod资源会自动归属于此类别，这类Pod资源具有最高优先级。\n1. Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别要求的Pod资源将自动归属于此类别，它们具有中等优先级。\n1. BestEffort：未为任何一个容器设置requests或limits属性的Pod资源将自动归属于此类别，它们的优先级为最低级别。\n1. 内存资源紧缺时，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够在可用时做到尽可能多地占用资源。若已然不存任何BestEffort类别的容器，则接下来是有着中等优先级的Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod资源存在。', 29, 0, 0, '2020-10-21 22:14:42.713099', '2021-01-27 07:59:23.179104', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (66, '控制器-Pod控制器', 'Master的各组件中，API', 'cover/2020_11_01_17_20_46_893129.jpg', '[TOC]\n# 一、Pod控制器概述\n\n\n1. Master的各组件中，API Server仅负责将资源存储于etcd中，并将其变动通知给各相关的客户端程序，如kubelet、kube-scheduler、kube-proxy和kube-controller-manager等，kube-scheduler监控到处于未绑定状态的Pod对象出现时遂启动调度器为其挑选适配的工作节点。\n1. Kubernetes的核心功能之一还在于要确保各资源对象的当前状态（status）以匹配用户期望的状态（spec），使当前状态不断地向期望状态“和解”（reconciliation）来完成容器应用管理，而这些则是kube-controller-manager的任务。\n1. 创建为具体的控制器对象之后，每个控制器均通过API Server提供的接口持续监控相关资源对象的当前状态，并在因故障、更新或其他原因导致系统状态发生变化时，尝试让资源的当前状态向期望状态迁移和逼近。\n\n\n\n# 二、控制器与Pod对象\n\n\n1. Pod控制器资源通过持续性地监控集群中运行着的Pod资源对象来确保受其管控的资源严格符合用户期望的状态，例如资源副本的数量要精确符合期望等。\n1. 一个Pod控制器资源至少应该包含三个基本的组成部分。\n\n\n\n- 标签选择器：匹配并关联Pod资源对象，并据此完成受其管控的Pod资源计数。\n- 期望的副本数：期望在集群中精确运行着的Pod资源的对象数量。\n- Pod模板：用于新建Pod资源对象的Pod模板资源。\n\n\n\n# 三、Pod资源模板\n\n\n1. Pod模板的配置信息中不需要apiVersion和kind字段，但除此之外的其他内容与定义自主式Pod对象所支持的字段几乎完全相同，这包括metadata和spec及其内嵌的其他各个字段。\n1. Pod控制器类资源的spec字段通常都要内嵌replicas、selector和template字段，其中template即为Pod模板的定义。\n1. 下面是一个定义在Deployment资源中的模板资源示例：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604149660295-808544dd-968b-44c6-a2c7-d83a6599d800.png#align=left&display=inline&height=786&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=786&originWidth=953&size=353797&status=done&style=none&width=953)', 31, 0, 0, '2020-10-22 13:30:24.800692', '2021-01-26 05:41:15.993666', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (67, '控制器-ReplicaSet控制器', '用于确保由其管控的Pod对象副本数在任一时刻都能精确满足期望的数量。ReplicaSet控制器资源启动后会查找集群中匹配其标签选择器的Pod资源对象，当前活动对象的数量与期望的数量不吻合时，多则删除，少则通过Pod模板创建以补足。', 'cover/2020_11_01_17_22_17_967895.jpg', '[TOC]\n\n# 一、概述\n\n\n1. 作用：用于确保由其管控的Pod对象副本数在任一时刻都能精确满足期望的数量。ReplicaSet控制器资源启动后会查找集群中匹配其标签选择器的Pod资源对象，当前活动对象的数量与期望的数量不吻合时，多则删除，少则通过Pod模板创建以补足。\n1. 功能：\n\n- 确保Pod资源对象的数量：ReplicaSet需要确保由其控制运行的Pod副本数量精确吻合配置中定义的期望值，否则就会自动补足所缺或终止所余。\n- 确保Pod健康运行：探测到由其管控的Pod对象因其所在的工作节点故障而不可用时，自动请求由调度器于其他工作节点创建缺失的Pod副本。\n- 弹性伸缩：业务规模因各种原因时常存在明显波动，在波峰或波谷期间，可以通过ReplicaSet控制器动态调整相关Pod资源对象的数量。\n- 通过HPA（HroizontalPodAutoscaler）控制器实现Pod资源规模的自动伸缩。\n\n\n\n# 二、创建ReplicaSet\n\n\n1. 可以使用YAML或JSON格式的清单文件定义其配置它的spec字段一般嵌套使用以下几个属性字段。\n\n| replicas        | integer | 期望的Pod对象副本数                                          |\n| --------------- | ------- | ------------------------------------------------------------ |\n| selector        | Object  | 当前控制器匹配Pod对象副本的标签选择器，支持matchLabels和matchExpressions两种匹配机制 |\n| template        | Object  | 用于补足Pod副本数量时使用的Pod模板资源                       |\n| minReadySeconds | integer | 新建的Pod对象，在启动后的多长时间内如果其容器未发生崩溃等异常情况即被视为“就绪”；默认为0秒，表示一旦就绪性探测成功，即被视作可用 |\n\n\n\n2. 示例：\n\n\n\n- 创建rs-example.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150057289-9c0e5277-dadb-413c-92e9-bf95932faf7d.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=471&originWidth=355&size=353797&status=done&style=none&width=355)\n\n- 创建rs资源：\n  `$ kubectl apply -f rs-example.yaml` \n- 查看名称为\"rs-demo\"的pod资源\n  `$ kubectl get pods -l app=rs-demo` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150119870-500b735e-d7fc-4d37-9656-ca7e59f05869.png#align=left&display=inline&height=90&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=90&originWidth=573&size=353797&status=done&style=none&width=573)\n\n- 查看replicaset控制器资源状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150153396-7114ff2a-e5fd-4fa9-aaf5-055d722a2164.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=966&size=353797&status=done&style=none&width=966)\n\n\n# 三、 ReplicaSet管控Pod对象\n\n\n1. 缺少Pod副本\n\n> 任何原因导致的相关Pod对象丢失，都会由ReplicaSet控制器自动补足。\n\n- 手动删除上面列出的一个Pod对象\n  `$ kubectl delete pods rs-example-5ncrr` \n- 再次列出相关Pod对象的信息，可以看到rs-example-5ncrr被删除，而新的Pod对象rs-example-jfp4k被rs-example控制器创建：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150220585-57639cbe-c988-411d-b3e5-6b02d0e7ad08.png#align=left&display=inline&height=233&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=233&originWidth=604&size=353797&status=done&style=none&width=604)\n\n- 强行修改隶属于控制器rs-example的Pod资源标签，会导致它不再被控制器作为副本计数，这也将触发控制器的Pod对象副本缺失补足机制。\n  例如，将rs-example-26fnb的标签app的值改为rs：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150258258-7632160f-0e20-4519-a94d-c3898b9eeb4e.png#align=left&display=inline&height=251&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=251&originWidth=818&size=353797&status=done&style=none&width=818)\n\n\n2. 多出pod副本\n\n> 一旦被标签选择器匹配到的Pod资源数量因任何原因超出期望值，多余的部分都将被控制器自动删除。\n\n\n\n- 例如，为pod-example手动为其添加“app: rs-demo”标签：\n  `$ kubectl label pods rs-example-26fnb app=rs-demp --overwrite` \n- 再次列出相关的Pod资源，可以看到rs-example控制器启动了删除多余Pod的操作，pod-example正处于终止过程中：\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150440661-0a3cae78-9e65-47f1-a519-9bc724d88e49.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=869&size=353797&status=done&style=none&width=869)\n  这就意味着，任何自主式的或本隶属于其他控制器的Pod资源其标签变动的结果一旦匹配到了其他的副本数足额的控制器，就会导致这类Pod资源被删除。\n\n\n\n# 三、查看replicaset资源信息\n\n\n1. 查看所有replicaset（子资源）信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150483115-c206c0ca-55a1-432d-a673-ce20af5202fd.png#align=left&display=inline&height=175&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=175&originWidth=729&size=353797&status=done&style=none&width=729)\n\n2. 查看所有replicaset（子资源）详细信息\n   `$ kubectl describe replicasets` \n   `$ kubectl describe replicasets/rs-example` \n\n\n\n# 四、更新ReplicaSet控制器\n\n\n1. 更改Pod模板：升级应用\n   ReplicaSet控制器的Pod模板可随时按需修改，但它仅影响这之后由其新建的Pod对象，对已有的副本不会产生作用。但在用户逐个手动关闭其旧版本的Pod资源后就能以新代旧，实现控制器下应用版本的滚动升级。\n\n\n\n- 修改原ReplicaSet.yaml文件镜像\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150699261-68a5e8ef-c8f1-462e-b37f-312974f4b97d.png#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=465&originWidth=349&size=353797&status=done&style=none&width=349) \n\n- apply文件，查看image信息\n  `kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152520384-d67836b0-b1a6-4f75-a93c-61cdf6edf676.png#align=left&display=inline&height=66&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=66&originWidth=340&size=353797&status=done&style=none&width=340)\n  rs-example管控的现存Pod对象使用的仍然是原来版本中定义的镜像\n- 删除pod后自动生成新版pod\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152554444-293ba76c-c8a1-4109-a4f9-0732607aa9c7.png#align=left&display=inline&height=156&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=156&originWidth=591&size=353797&status=done&style=none&width=591)\n\n\n2. 扩容与缩容\n   改动ReplicaSet控制器对象配置中期望的Pod副本数量（replicas字段）会由控制器实时做出响应，从而实现应用规模的水平伸缩。\n   kubectl还提供了一个专用的子命令scale用于实现应用规模的伸缩，它支持从资源清单文件中获取新的目标副本数量，也可以直接在命令行通过“--replicas”选项进行读取，\n\n- 将rs-example控制器的Pod副本数量提升至5个：\n  `$ kubectl scale replicasets rs-example --replicas=5` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152704085-4a04a44f-3c87-46de-8e07-6c1e664efef8.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=751&size=353797&status=done&style=none&width=751)\n- 由下面显示的rs-example资源的状态可以看出，将其Pod资源副本数量扩展至5个的操作已经成功完成：\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152735637-f7e6e92c-8fd1-4d01-b6a3-8fdfb7befb3d.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=751&size=353797&status=done&style=none&width=751)\n- 收缩规模的方式与扩展相同，只需要明确指定目标副本数量即可。\n\n\n\n# 五、删除ReplicaSet控制器资源\n\n\n1. 使用kubectl delete命令删除ReplicaSet对象时默认会一并删除其管控的各Pod对象。\n   `$ kubectl delete replicasets rs-example` \n\n\n\n# 六、故障转移\n\n\n1. 目前有3个副本分别运行在node1和node2上。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152785665-9435a84e-0b0c-481a-883c-bfba05ac4192.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=963&size=353797&status=done&style=none&width=963)\n\n2. 现在模拟 k8s-node2 故障，关闭该节点。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152808782-980c4975-72b6-4e70-af69-d009d3bf013b.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=963&size=353797&status=done&style=none&width=963)\n\n3. 等待一段时间，Kubernetes 会检查到 k8s-node2 不可用，将 k8s-node2 上的 Pod 标记为 terminating 状态，并在 k8s-node1 上新创建两个 Pod，维持总副本数为 3。\n\n![clip_image005.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604152843254-c1dae6ef-9b20-4191-8af8-7b2d9d1e34e9.jpeg#align=left&display=inline&height=145&margin=%5Bobject%20Object%5D&name=clip_image005.jpg&originHeight=145&originWidth=554&size=18296&status=done&style=none&width=554)\n\n4. 当 k8s-node2 恢复后，terminating的 Pod 会被删除，不过已经运行的 Pod不会重新调度回 k8s-node2。\n\n\n![clip_image006.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152887640-accca54f-ac4c-49e7-b132-1a766f7b3be2.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=clip_image006.png&originHeight=129&originWidth=514&size=9344&status=done&style=none&width=514)\n![clip_image008.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604152893646-ab1ac390-f5a8-4534-baf2-21a747c4d4df.jpeg#align=left&display=inline&height=102&margin=%5Bobject%20Object%5D&name=clip_image008.jpg&originHeight=102&originWidth=554&size=12510&status=done&style=none&width=554)', 38, 0, 0, '2020-11-01 17:22:46.479331', '2021-01-26 11:49:52.137249', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (68, '控制器-Deployment控制器', 'Deployment', 'cover/2020_11_01_17_24_11_604275.jpg', '[TOC]\n\n# 一、Deployment\n\n\n1. Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet的属性。而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153315362-8e0e0d83-bebf-4fb2-a40f-a1ec140a1ee9.png#align=left&display=inline&height=341&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=341&originWidth=790&size=114249&status=done&style=none&width=790)\n\n2. Deployment具备ReplicaSet的全部功能，同时还增添了部分特性。\n\n- 事件和状态查看：必要时可以查看Deployment对象升级的详细进度和状态。\n- 回滚：升级操作完成后发现问题时，支持使用回滚机制将应用返回到前一个或由用户指定的历史记录中的版本上。\n- 版本记录：对Deployment对象的每一次操作都予以保存，以供后续可能执行的回滚操作使用。\n- 暂停和启动：对于每一次升级，都能够随时暂停和启动。\n- 多种自动更新方案：一是Recreate，即重建更新机制，全面停止、删除旧有的Pod后用新版本替代；另一个是RollingUpdate，即滚动升级机制，逐步替换旧有的Pod至新的版本。\n\n\n\n# 二、创建Deployment\n\n\n1. 除了控制器类型和名称之外，它与前面ReplicaSet控制器示例中的内容几乎没有什么不同。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153398464-73c3b45e-3a30-4d10-9172-4e0e440867b4.png#align=left&display=inline&height=464&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=464&originWidth=514&size=114249&status=done&style=none&width=514)\n\n2. 创建资源对象\n   `kubectl apply -f Deployment.yaml` \n2. 查看Deployment资源\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153432874-ee4ee178-774b-4e14-be71-dbac7888e084.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=572&size=114249&status=done&style=none&width=572)\n\n4. 查看ReplicaSets资源\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153451104-e1492757-4c1e-4236-ac4c-a4d7bcb92183.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=656&size=114249&status=done&style=none&width=656)\n\n5. 查看Pod资源\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153467807-65815c87-a3a4-4d4b-ac7d-0df5d2fcac7f.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=732&size=114249&status=done&style=none&width=732)\n\n6. 由此印证了Deployment借助于ReplicaSet管理Pod资源的机制，于是可以得知，其大部分管理操作与ReplicaSet相同\n\n# 三、更新\n\n\n1. 更新策略\n\n> Deployment控制器支持两种更新策略：滚动更新和重新创建。\n\n- 滚动更新是默认的更新策略，它在删除一部分旧版本Pod资源的同时，补充创建一部分新版本的Pod对象进行应用升级。\n- 重新创建首先删除现有的Pod对象，而后由控制器基于新模板重新创建出新版本资源对象。\n\n2. 滚动更新时，应用升级期间还要确保可用的Pod对象数量不低于某阈值以确保可以持续处理客户端的服务请求，变动的方式和Pod对象的数量范围将通过spec.strategy.rollingUpdate.maxSurge和spec.strategy.rollingUpdate.maxUnavailable两个属性协同进行定义，\n\n- maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；例如，如果期望值为3，当前的属性值为1，则表示Pod对象的总数不能超过4个。\n- maxUnavailable：升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；默认值为1，该值意味着如果期望值是3，则升级期间至少要有两个Pod对象处于正常提供服务的状态\n\n3. 为了保存版本升级的历史，需要在创建Deployment对象时于命令中使用“--record”选项。\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153634802-1f3bb9df-0a02-4446-aee2-16837ec514ea.png#align=left&display=inline&height=278&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=278&originWidth=722&size=132639&status=done&style=none&width=722)\n\n4. 使用命令临时更新镜像\n\n- 使用192.168.10.110/k8s/myapp:v2镜像文件修改Pod模板中的myapp容器，启动Deployment控制器的滚动更新\n  `$ kubectl set image deployments myapp-deploy myapp=192.168.10.110/k8s/myapp:v2` \n- 访问验证\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153671032-f135ccfe-6076-41be-b887-6653feb726d0.png#align=left&display=inline&height=254&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=254&originWidth=892&size=132639&status=done&style=none&width=892)\n\n5. 灰度发布（金色雀发布）\n\n- 待第一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一小部分新版本的应用，主体部分还是旧的版本。然后，再根据用户特征精心筛选出小部分用户的请求路由至新版本的Pod应用，并持续观察其是否能稳定地按期望的方式运行。\n- 采用首批添加1个Pod资源的方式。将Deployment控制器的maxSurge属性的值设置为1，并将maxUnavailable属性的值设置为0：\n  `$ kubectl patch deployments myapp-deploy -p \'{\"spec\":{\"strategy\":{\"rollingUpdate\": {\"maxSurge\": 1, \"maxUnavailable\":0}}}}\'` \n\n6. rollout pause和resume\n   kubectl rollout pause会用来停止触发下一次rollout，正在执行的滚动历程是不会停下来的，而是会继续正常的进行滚动，直到完成。等下一次，用户再次触发rollout时，Deployment就不会真的去启动执行滚动更新了，而是等待用户执行了kubectl rollout resume，流程才会真正启动执行。\n6. 灰度发布、滚动发布和蓝绿发布\n\n**假设replicaSet=10 maxSurge &maxUnavailable不能同时为0 **\n\n| 类型     | 描述                                               | maxSurge                 | maxUnavailable       |\n| -------- | -------------------------------------------------- | ------------------------ | -------------------- |\n| 灰度发布 | 又名金丝雀发布。先极个别更新，通过后再一次全部更新 | 1或10%                   | 视对服务可用度的需求 |\n| 滚动发布 | （部分更新，投入使用）* 直到全部更新完成           | 1<x<（具体看更新的粒度） | 视对服务可用度的需求 |\n| 蓝绿发布 | 新旧版共存，靠切换流量完成更新                     | 10                       | 0                    |\n\n# 四、回滚\n\n\n1. 将myapp-deploy回滚至此前的版本：\n   `$ kubectl rollout undo deployments myapp-deploy` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153850895-6ba381ca-45c9-4574-861d-749aa775727e.png#align=left&display=inline&height=271&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=271&originWidth=1217&size=132639&status=done&style=none&width=1217)\n\n2. 若要回滚到号码为1的revision记录，则使用如下命令即可完成：\n   `$ kubectl rollout undo deployments myapp-deploy --to-revision=1` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153877271-28d59a3b-630c-49d9-bd88-0f585f681252.png#align=left&display=inline&height=164&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=164&originWidth=1222&size=132639&status=done&style=none&width=1222)\n\n- 回滚操作中，其revision记录中的信息会发生变动，回滚操作会被当作一次滚动更新追加进历史记录中，而被回滚的条目则会被删除。\n- 如果此前的滚动更新过程处于“暂停”状态，那么回滚操作就需要先将Pod模板的版本改回到之前的版本，然后“继续”更新，否则，其将一直处于暂停状态而无法回滚。', 35, 0, 0, '2020-11-01 17:24:16.140511', '2021-01-26 11:46:28.481965', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (69, '控制器-DaemonSet控制器', '用于在集群中的全部节点上同时运行一份指定的Pod资源副本，后续新加入集群的工作节点也会自动创建一个相关的Pod对象，当从集群移除节点时，此类Pod对象也将被自动回收而无须重建。管理员也可以使用节点选择器及节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。', 'cover/2020_11_01_17_25_56_231388.jpg', '[TOC]\n\n# 一、简介\n\n\n1. 用于在集群中的全部节点上同时运行一份指定的Pod资源副本，后续新加入集群的工作节点也会自动创建一个相关的Pod对象，当从集群移除节点时，此类Pod对象也将被自动回收而无须重建。管理员也可以使用节点选择器及节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。\n1. DaemonSet是一种特殊的控制器，它有特定的应用场景，通常运行那些执行系统级操作任务的应用，其应用场景具体如下。\n\n- 运行集群存储的守护进程，如在各个节点上运行glusterd或ceph。\n- 在各个节点上运行日志收集守护进程，如fluentd和logstash。\n- 在各个节点上运行监控系统的代理守护进程，如Prometheus Node Exporter、collectd、Datadog agent、New Relic agent或Ganglia gmond等。\n\n# 二、创建DaemonSet资源对象\n\n\n1. 下面的资源清单，定义一个DaemonSet资源：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154069406-b2317718-9b12-4d64-b139-60b2bc376974.png#align=left&display=inline&height=507&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=507&originWidth=605&size=132639&status=done&style=none&width=605)\n\n- DaemonSet必须使用selector来匹配Pod模板中指定的标签，而且它也支持matchLabels和matchExpressions两种标签选择器。\n\n1. 创建资源对象\n   `$ kubectl apply -f filebeat-ds.yaml` \n1. 查看资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154128565-006da16b-8510-4365-a64f-ead2672e71c3.png#align=left&display=inline&height=185&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=185&originWidth=813&size=132639&status=done&style=none&width=813)\n\n\n# 三、更新DaemonSet对象\n\n\n> DaemonSet支持更新机制，相关配置定义在spec.update-Strategy嵌套字段中。目前，它支持RollingUpdate（滚动更新）和OnDelete（删除时更新）两种更新策略，滚动更新为默认的更新策略，工作逻辑类似于Deployment控制，不过仅支持使用maxUnavailabe属性定义最大不可用Pod资源副本数（默认值为1），而删除时更新的方式则是在删除相应节点的Pod资源后重建并更新为新版本。\n\n\n\n1. 将此前创建的filebeat-ds中Pod模板中的容器镜像升级\n   `$ kubectl set image daemonsets filebeat-daemonset filebeat=192.168.10.110/k8s/filebeat:5.6.6` \n1. 查看镜像信息\n   `$ kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154226654-76542800-4f32-40cd-b245-9f84c1e6ef4a.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=676&size=132639&status=done&style=none&width=676)', 23, 0, 0, '2020-11-01 17:25:58.883476', '2021-01-26 05:41:26.774045', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (70, '控制器-Job控制器', 'Job控制器用于调配Pod对象运行一次性任务，容器中的进程在正常运行结束后不会对其进行重启，而是将Pod对象置于“Completed”（完成）状态。若容器中的进程因错误而终止，则需要依配置确定重启与否，未运行完成的Pod对象因其所在的节点故障而意外终止后会被重新调度。', 'cover/2020_11_10_15_42_11_076672.jpg', '[TOC]\n\n# 一、简介\n\n\n1. Job控制器用于调配Pod对象运行一次性任务，容器中的进程在正常运行结束后不会对其进行重启，而是将Pod对象置于“Completed”（完成）状态。若容器中的进程因错误而终止，则需要依配置确定重启与否，未运行完成的Pod对象因其所在的节点故障而意外终止后会被重新调度。\n1. 有的作业任务可能需要运行不止一次，用户可以配置它们以串行或并行的方式运行。\n\n- 单工作队列（work queue）的串行式Job：即以多个一次性的作业方式串行执行多次作业，直至满足期望的次数；这次Job也可以理解为并行度为1的作业执行方式，在某个时刻仅存在一个Pod资源对象。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154406275-f1e9a768-9ef6-4fce-9ba7-0f39e8f752f5.png#align=left&display=inline&height=131&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=131&originWidth=528&size=132639&status=done&style=none&width=528)\n\n- 多工作队列的并行式Job：这种方式可以设置工作队列数，即作业数，每个队列仅负责运行一个作业；也可以用有限的工作队列运行较多的作业，即工作队列数少于总作业数，相当于运行多个串行作业队列。工作队列数即为同时可运行的Pod资源数。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154520208-c4617373-9f89-46de-ba37-51605af44dba.png#align=left&display=inline&height=263&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=263&originWidth=529&size=132639&status=done&style=none&width=529)\n\n\n# 二、创建Job对象\n\n\n1. Job控制器的spec字段内嵌的必要字段仅为template，它的使用方式与Deployment等控制器并无不同。Job会为其Pod对象自动添加“job-name=JOB_NAME”和“controller-uid=UID”标签，并使用标签选择器完成对controller-uid标签的关联。需要注意的是，Job位于API群组“batch/v1”之内。\n1. 定义了一个Job控制器：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154553161-b877a31a-6d56-45c6-9a0f-422441da367b.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=571&size=132639&status=done&style=none&width=571)\n\n- Pod模板中的spec.restartPolicy默认为“Always”，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为“Never”或“OnFailure”\n\n3. 创建资源\n   `$ kubectl apply -f job-example.yaml` \n3. 查看资源信息\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154632504-816d524f-03e8-4ebd-9624-5eb9fc5b5fa2.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=531&size=132639&status=done&style=none&width=531)\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154619453-46dc5077-807b-45be-a969-6296a8eeb289.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=611&size=132639&status=done&style=none&width=611)\n\n- 两分钟后，待sleep命令执行完成并成功退出后，Pod资源即转换为Completed状态，completions也从0/1变为1/1\n\n# 三、并行式Job\n\n\n1. spec.parallelism的值设置为1，并设置总任务数．spec.completion属性便能够让Job控制器以串行方式运行多任务。\n1. 串行运行5次任务的Job控制器示例：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154660694-4c82614b-2fd9-4ca7-bee0-ab81f15beb96.png#align=left&display=inline&height=305&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=305&originWidth=575&size=132639&status=done&style=none&width=575)\n\n3. 使用kubectl get pod -w监控其变动\n\n# 四、Job扩容\n\n\n1. Job控制器的.spec.parallelism定义的并行度表示同时运行的Pod对象数，此属性值支持运行时调整从而改变其队列总数，实现扩容和缩容。\n1. 例如在其运行过程中（未完成之前）将job-multi的并行度扩展为三路：\n   `$ kubectl scale jobs job-multi --replicas=3` \n1. 执行命令后可以看到，其同时运行的Pod对象副本数量立即扩展到了三个：\n   `$ kubectl get pods -l job-name=job-multi` \n\n# 五、删除Job\n\n1. Job控制器待其Pod资源运行完成后，将不再占用系统资源。用户可按需保留或使用资源删除命令将其删除。不过，如果某Job控制器的容器应用总是无法正常结束运行，而其restartPolicy又定为了重启，则它可能会一直处于不停地重启和错误的循环当中。所幸的是，Job控制器提供了两个属性用于抑制这种情况的发生，具体如下。\n\n- spec.activeDeadlineSeconds <integer>:Job的deadline，用于为其指定最大活动时间长度，超出此时长的作业将被终止。\n- spec.backoffLimit <integer>：将作业标记为失败状态之前的重试次数，默认值为6。\n\n2. 例如，下面的配置片断表示其失败重试的次数为5，并且如果超出100秒的时间仍未运行完成，那么其将被终止：\n\n```yaml\nspec:\n	backoffLimit: 5\n	activeDeadlineSeconds: 100\n```\n\n', 21, 0, 0, '2020-11-01 17:27:29.529927', '2021-01-26 05:41:29.859758', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (71, '控制器-CronJob控制器', 'CronJob控制器用于管理Job控制器资源的运行时间。Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及重复运行的方式', 'cover/2020_11_01_17_28_58_430716.jpg', '[TOC]\n\n# 一、简介\n\n\n1. CronJob控制器用于管理Job控制器资源的运行时间。Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及重复运行的方式\n1. 具体如下。\n\n- 在未来某时间点运行作业一次。\n- 在指定的时间点重复运行作业。\n\n3. CronJob对象支持使用的时间格式类似于Crontab，略有不同的是，CronJob控制器在指定的时间点时，“?”和“*”的意义相同，都表示任何可用的有效值。\n\n# 二、创建CronJob对象\n\n\n1. CronJob控制器的spec字段可嵌套使用以下字段。\n\n| jobTemplate                | Object  | Job控制器模板，用于为CronJob控制器生成Job对象；必选字段      |\n| -------------------------- | ------- | ------------------------------------------------------------ |\n| schedule                   | string  | Cron格式的作业调度运行时间点；必选字段。                     |\n| concurrencyPolicy          | string  | 并发执行策略，可用值有“Allow”（允许）、“Forbid”（禁止）和“Replace”（替换），用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 |\n| failedJobHistoryLimit      | integer | 为失败的任务执行保留的历史记录数，默认为1。                  |\n| successfulJobsHistoryLimit | integer | 为成功的任务执行保留的历史记录数，默认为3                    |\n| startingDeadlineSeconds    | integer | 因各种原因缺乏执行作业的时间点所导致的启动作业错误的超时时长，会被记入错误历史记录 |\n| suspend                    | boolean | 是否挂起后续的任务执行，默认为false，对运行中的作业不会产生影响。 |\n\n\n\n2. 下面是一个定义在资源清单文件中的CronJob资源对象示例，它每隔2分钟运行一次由jobTemplate定义的简单任务：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154905848-2962f9ab-09ef-40fe-b2f1-a49d7da0535d.png#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=555&originWidth=797&size=132639&status=done&style=none&width=797)\n\n3. 查看资源信息，命令结果中的SCHEDULE是指其调度时间点，SUSPEND表示后续任务是否处于挂起状态，即暂停任务的调度和运行，ACTIVE表示活动状态的Job对象的数量，而LAST SCHEDULE则表示上次调度运行至此刻的时长：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154926608-f72b89ca-04e5-464d-9282-0c1a1df12b04.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=789&size=132639&status=done&style=none&width=789)\n\n\n# 三、CronJob的控制机制\n\n\n1. CronJob控制器是一个更高级别的资源，它以Job控制器资源为其管控对象，并借助它管理Pod资源对象。\n1. 使用类似如下命令来查看某CronJob控制器创建的Job资源对象，其中的标签“mycronjob-jobs”是在创建cronjob-example时为其指定。不过，只有相关的Job对象被调度执行时，此命令才能将其正常列出。可列出的Job对象的数量取决于CronJob资源的．spec.successfulJobsHistoryLimit的属性值，默认为3。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154953836-48d6af5d-faca-4924-8050-43fb0500d2b7.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=666&size=132639&status=done&style=none&width=666)\n\n3. 如果作业重复执行时指定的时间点较近，而作业执行时长（普遍或偶尔）跨过了其两次执行的时间长度，则会出现两个Job对象同时存在的情形。有些Job对象可能会存在无法或不能同时运行的情况，这个时候就要通过．spec.concurrencyPolicy属性控制作业并存的机制，其默认值为“Allow”，即允许前后Job，甚至属于同一个CronJob的更多Job同时运行。\n3. 其他两个可用值中，“Forbid”用于禁止前后两个Job同时运行，如果前一个尚未结束，后一个则不予启动（跳过），“Replace”用于让后一个Job取代前一个，即终止前一个并启动后一个。', 29, 0, 0, '2020-11-01 17:29:04.270767', '2021-01-26 17:57:29.240313', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (72, '控制器-StatefulSet控制器', 'StatefulSet是Kubernetes提供的管理有状态应用的负载管理控制器API。用于部署和扩展有状态应用的Pod资源，确保它们的运行顺序及每个Pod资源的唯一性。其与ReplicaSet控制器不同的是，虽然所有的Pod对象都基于同一个spec配置所创建，但StatefulSet需要为每个Pod维持一个唯一且固定的标识符，必要时还要为其创建专有的存储卷', 'cover/2020_11_01_17_31_07_794708.jpg', '[TOC]\n\n# 一、介绍\n\n\n1. StatefulSet是Kubernetes提供的管理有状态应用的负载管理控制器API。用于部署和扩展有状态应用的Pod资源，确保它们的运行顺序及每个Pod资源的唯一性。其与ReplicaSet控制器不同的是，虽然所有的Pod对象都基于同一个spec配置所创建，但StatefulSet需要为每个Pod维持一个唯一且固定的标识符，必要时还要为其创建专有的存储卷\n1. StatefulSet适用于具有以下特点的应用：\n\n- 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于 PVC来实现。\n- 稳定的网络标识符，即 Pod 重新调度后其 PodName 和 HostName 不变。\n- 有序部署，有序扩展，基于 init containers 来实现。\n- 有序收缩。\n\n3. Statefulset的启停顺序：\n\n- 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。\n- 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。\n- 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。\n\n4. 一个完整的StatefulSet控制器需要由一个Headless Service、一个StatefulSet和一个volumeClaimTemplate组成。\n\n- HeadlessService用于为Pod资源标识符生成可解析的DNS资源记录，\n- StatefulSet用于管控Pod资源\n- volumeClaimTemplate则基于静态或动态的PV供给方式为Pod资源提供专有且固定的存储\n\n5. 为什么要有headless？？\n   在deployment中，每一个pod是没有名称，是随机字符串，是无序的。而statefulset中是要求有序的，每一个pod的名称必须是固定的。当节点挂了，重建之后的标识符是不变的，每一个节点的节点名称是不能改变的。pod名称是作为pod识别的唯一标识符，必须保证其标识符的稳定并且唯一。为了实现标识符的稳定，这时候就需要一个headless service 解析直达到pod，还需要给pod配置一个唯一的名称。\n5. 为什么要有volumeClainTemplate？？\n   大部分有状态副本集都会用到持久存储，比如分布式系统来说，由于数据是不一样的，每个节点都需要自己专用的存储节点。而在deployment中pod模板中创建的存储卷是一个共享的存储卷，多个pod使用同一个存储卷，而statefulset定义中的每一个pod都不能使用同一个存储卷，由此基于pod模板创建pod是不适应的，这就需要引入volumeClainTemplate，当在使用statefulset创建pod时，会自动生成一个PVC，从而请求绑定一个PV，从而有自己专用的存储卷。\n\n# 二、创建StatefulSet对象\n\n\n1. 先定义了一个名为myapp-svc的Headless Service资源，用于为关联到的每个Pod资源创建DNS资源记录。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155203726-2e0bc9c2-2211-464f-ae29-2199f39acded.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=266&size=132639&status=done&style=none&width=266)\n\n2. 定义多个使用NFS存储后端的PV，空间大小为2GB，仅支持单路的读写操作。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155234926-9a27af5d-d5d7-49fb-b5bd-e436beca4a48.png#align=left&display=inline&height=337&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=337&originWidth=493&size=132639&status=done&style=none&width=493)\n\n3. 定义了一个名为myapp的StatefulSet资源，它通过Pod模板创建了两个Pod资源副本，并基于volumeClaimTemplates（存储卷申请模板）向nfs存储类请求动态供给PV，从而为每个Pod资源提供大小为1GB的专用存储卷。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155259323-30bd5f94-9a87-4e8c-a633-a10ae7316816.png#align=left&display=inline&height=772&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=772&originWidth=524&size=132639&status=done&style=none&width=524)\n\n\n- headless保证它的网络，statefulset存储模版来保证每个pod存储的唯一性，这样才解决了有状态应用的两大痛点\n\n\n\n4. 查看资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155284426-81ddce5e-b176-4d25-b848-764b0b8c1417.png#align=left&display=inline&height=186&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=186&originWidth=474&size=132639&status=done&style=none&width=474)\n\n\n# 三、Pod资源标识符及存储卷\n\n\n> 由StatefulSet控制器创建的Pod资源拥有固定、唯一的标识和专用存储卷，即便重新调度或终止后重建，其名称也依然保持不变，且此前的存储卷及其数据不会丢失。\n\n\n\n1. Pod资源的固定标识符\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155383969-f69a81d1-6828-4a15-a29c-14049f3823fe.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=900&size=132639&status=done&style=none&width=900)\n\n\n- 这些名称标识会由StatefulSet资源相关的Headless\n  Service资源创建为DNS资源记录。在Pod资源创建后，与其相关的DNS资源记录格式为“$(pod_name).$(service_name).$(namespace).svc.cluster.local”\n- 使用coredns解析测试\n\n`# dig -t A myapp-0.myapp-svc.default.svc.cluster.local@10.244.2.7`\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155432312-0f86dbc3-a4f2-4c7a-b9a8-2b48b11bcf45.png#align=left&display=inline&height=52&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=52&originWidth=691&size=132639&status=done&style=none&width=691)\n\n- 终端中删除Pod资源myapp-1，删除完成后控制器将随之开始重建Pod资源，其名称标识符的确未发生改变：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155449464-973cc6e4-fc82-4f95-a402-a24a747cf36f.png#align=left&display=inline&height=419&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=419&originWidth=891&size=132639&status=done&style=none&width=891)\n\n\n2. Pod资源的专有存储卷\n   控制器通过volumeClaimTemplates为每个Pod副本自动创建并关联一个PVC对象，它们分别绑定了一个动态供给的PV对象：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155475814-dc6293b5-559b-41b9-bc95-bbf762a1c426.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=843&size=132639&status=done&style=none&width=843)\n\n- 重新调度或终止后重建，此前的存储卷及其数据不会丢失。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155491334-774ec032-62ec-4e64-a3a3-1839dce06c2c.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=865&size=132639&status=done&style=none&width=865)\n\n\n# 四、StatefulSet资源扩缩容\n\n\n> 通过修改资源的副本数来改动其目标Pod资源数量。对StatefulSet资源来说，kubectl scale和kubectl patch命令均可实现此功能，也可以使用kubectl edit命令直接修改其副本数，或者在修改配置文件之后，由kubectl apply命令重新声明。\n\n\n\n1. 将myapp中的Pod副本数量扩展至5个：\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155539864-0389cb14-9c34-40e2-ad0c-eb9f8966c05d.png#align=left&display=inline&height=206&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=206&originWidth=677&size=132639&status=done&style=none&width=677)\n\n- StatefulSet资源的扩展过程与创建过程的Pod资源生成策略相同，默认为顺次进行，而且其名称中的序号也将以现有Pod资源的最后一个序号向后进行\n\n2. 执行缩容操作只需要将其副本数量调低即可，例如，这里可使用kubectl patch命令将StatefulSet资源myapp的副本数量修补为3个：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155636738-209e1c54-172d-460f-88f0-bc9b47116650.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=848&size=132639&status=done&style=none&width=848)\n\n\n- 终止Pod资源后，其存储卷并不会被删除，因此缩减规模后若再将其扩展回来，那么此前的数据依然可用，且Pod资源名称保持不变\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155653645-b31fca57-f650-41be-aed4-6f8d81593356.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=842&size=132639&status=done&style=none&width=842)\n\n\n# 五、StatefulSet资源升级\n\n\n1. 滚动更新\n   滚动更新StatefulSet控制器的Pod资源以逆序的形式从其最大索引编号的Pod资源逐一进行。对于主从复制类的集群应用来说，这样也能保证起主节点作用的Pod资源最后进行更新，确保兼容性。\n\n- 更新Pod中的容器镜像可以使用“kubectl set image”命令进行，例如下面的命令可将myapp控制器下的Pod资源镜像版本升级为“myapp:v2”：\n  `# kubectl set image statefulset myapp nginx=192.168.10.110/k8s/myapp:v2` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155742907-0d4cc8fa-9343-4dbf-8cb4-7596ecb6eaeb.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=117&originWidth=1197&size=132639&status=done&style=none&width=1197)\n\n2. 暂存更新\n   当用户需要设定一个更新操作，但又不希望它立即执行时，可将更新操作予以“暂存”，待条件满足后再手动触发其执行更新。将．spec.update-Strategy.rollingUpdate.partition字段的值设置为Pod资源的副本数量，即比Pod资源的最大索引号大1，这就意味着，所有的Pod资源都不会处于可直接更新的分区之内，那么于其后设定的更新操作也就不会真正执行，直到用户降低分区编号至现有Pod资源索引号范围之内\n\n- 首先将StatefulSet资源myapp的滚动更新分区值设定为3：\n  `# kubectl patch statefulset myapp -p \'{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":3}}}}\'` \n- 而后，将myapp控制器的Pod资源镜像版本更新为“myapp:v3”\n  `# kubectl set image statefulset myapp nginx=192.168.10.110/k8s/myapp:v3` \n- 接着检测各Pod资源的镜像文件版本信息，可以发现其版本并未发生改变：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155807850-acac1dc8-3e80-483f-8820-56355730d2e6.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1192&size=132639&status=done&style=none&width=1192)\n\n- 即便删除某Pod资源，它依然会基于旧的版本镜像进行重建。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155836218-e6808a92-3773-4a9d-92b7-ee5758a73805.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=1196&size=132639&status=done&style=none&width=1196)\n\n\n3. 灰度部署\n   将处于暂存状态的更新操作的partition定位于Pod资源的最大索引号，即可放出一只金丝雀，由其测试第一轮的更新操作，在确认无误后通过修改partition属性的值更新其他的Pod对象是一种更为稳妥的更新操作。\n\n- 将暂停的更新StatefulSet控制器myapp资源的分区号设置为Pod资源的最大索引号2，将会触发myapp-2的更新操作：\n  `# kubectl patch statefulset myapp -p \'{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":2}}}}\'` \n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155904099-a4eee6db-3995-4855-ba67-e7b3f905e9b7.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1192&size=132639&status=done&style=none&width=1192)\n\n- 将副本数目修改大于3后，也会触发更新操作\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155922305-42d24c7d-3f0b-487a-8996-3fb6f6fc3546.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=184&originWidth=1197&size=132639&status=done&style=none&width=1197)', 26, 0, 0, '2020-11-01 17:31:12.091321', '2021-01-26 05:41:34.841856', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (73, '控制器-PDB中断预算', '控制器无法保证在某一时刻一定会存在指定数量或比例的Pod对象，然而这种需求在某些强调服务可用性的场景中却是必备的。Pod中断预算（PodDisruptionBudget，简称PDB）类型的资源，用于为那些自愿的（Voluntary）中断做好预算方案（Budget），限制可自愿中断的最大Pod副本数或确保最少可用的Pod副本数，以确保服务的高可用性。', 'cover/2020_11_01_17_32_30_102695.jpg', '\n# 一、简介\n\n1. 控制器无法保证在某一时刻一定会存在指定数量或比例的Pod对象，然而这种需求在某些强调服务可用性的场景中却是必备的。Pod中断预算（PodDisruptionBudget，简称PDB）类型的资源，用于为那些自愿的（Voluntary）中断做好预算方案（Budget），限制可自愿中断的最大Pod副本数或确保最少可用的Pod副本数，以确保服务的高可用性。\n1. 部署在Kubernetes的每个应用程序都可以创建一个对应的PDB对象以限制自愿中断时最大可以中断的副本数或者最少应该保持可用的副本数，从而保证应用自身的高可用性。\n\n- 非自愿中断是指那些由不可控外界因素导致的Pod中断退出操作，例如，硬件或系统内核故障、网络故障以及节点资源不足导致Pod对象被驱逐等；\n- 由用户特地执行的管理操作导致的Pod中断则称为“自愿中断”，例如排空节点、人为删除Pod对象、由更新操作触发的Pod对象重建等。\n\n3. 最常见的要保护的对象是是以下kubernetes内置的controller创建的应用对象之一:\n\n- Deployment\n- ReplicaSet\n- ReplicaSet\n- StatefulSet\n\n4. 定义PDB资源时，其spec字段主要嵌套使用以下三个字段。\n\n| .spec.selector       | Object | 当前PDB对象使用的标签选择器，一般是与相关的Pod控制器使用同一个选择器。 |\n| -------------------- | ------ | ------------------------------------------------------------ |\n| .spec.minAvailable   | string | Pod自愿中断的场景中，至少要保证可用的Pod对象数量或比例，要阻止任何Pod对象发生自愿中断，可将其设置为100% |\n| .spec.maxUnavailable | string | Pod自愿中断的场景中，最多可转换为不可用状态的Pod对象数量或比例，0值意味着不允许Pod对象进行自愿中断；此字段与minAvailable互斥 |\n\n\n\n2. 应用场景\n\n| 场景             | 关注点                       | 解决方案                                                     |\n| ---------------- | ---------------------------- | ------------------------------------------------------------ |\n| 无状态的前端     | 服务能力不能减少超过10%      | 使用一个包含minAvailable 90%值的PDB                          |\n| 单实例有状态应用 | 不要在不知情情况下中断       | 1:不使用PDB,容易偶尔的宕机 2:使用PDB,设置maxUnavailable=0.当集群管理员想要终止pod的时候,他需要联系你,然后删除掉PDB以准备应对中断,然后重新创建.(如果maxUnavailable=0则不能进行自愿中断操作) |\n| 多实例有状态应用 | 运行的实例数不能低于法定数量 | 1:把maxUnavailable to 1(根据不同集群要求不同,可以设置为不同的值) 2:把minAvailable设置为法定数量. |\n\n\n\n# 二、示例\n\n\n1. 由Deployment控制器myapp-deploy创建的Pod对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156057563-1fda37f9-d9dd-4156-a997-141a7f1ed45a.png#align=left&display=inline&height=473&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=473&originWidth=520&size=132639&status=done&style=none&width=520)\n\n2. 设置Pod中断预算，要求其最少可用的Pod对象数量为2个\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156073960-c9034938-fa57-42ad-98a6-1fa6514857e6.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=218&originWidth=338&size=132639&status=done&style=none&width=338)\n\n- PDB的app与pod中的app信息一致\n\n3. 创建PDB对象\n   `$ kubectl apply -f pdb.yaml` \n3. 查看PDB的状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156111406-b14602ea-f3fe-45df-8e91-e4afedec9afe.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=218&originWidth=338&size=132639&status=done&style=none&width=338)', 23, 0, 0, '2020-11-01 17:32:32.956533', '2021-01-26 11:38:52.993220', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (74, 'Service和Ingress-Service资源及模型', 'Service资源基于标签选择器将一组Pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的Pod对象之上，它向客户端隐藏了真实的、处理用户请求的Pod资源，使得客户端的请求看上去就像是由Service直接处理并进行响应的一样。', 'cover/2020_11_01_17_33_00_544844.jpg', '[TOC]\n\n# 一、service资源概述\n\n\n1. Service资源基于标签选择器将一组Pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的Pod对象之上，它向客户端隐藏了真实的、处理用户请求的Pod资源，使得客户端的请求看上去就像是由Service直接处理并进行响应的一样。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156865423-4f504971-71fe-43f5-bcd3-a855d7d6650d.png#align=left&display=inline&height=503&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=503&originWidth=924&size=132639&status=done&style=none&width=924)\n\n2. Service对象的IP地址位于为Kubernetes集群配置指定专用IP地址的范围之内，而且是一种虚拟IP地址，它在Service对象创建后即保持不变，并且能够被同一集群中的Pod资源所访问。Service端口用于接收客户端请求并将其转发至其后端的Pod中应用的相应端口之上。\n2. 通过其标签选择器匹配到的后端Pod资源不止一个时，Service资源能够以负载均衡的方式进行流量调度，实现了请求流量的分发机制。Service与Pod对象之间的关联关系通过标签选择器以松耦合的方式建立，它可以先于Pod对象创建而不会发生错误。\n2. Service并不直接链接至Pod对象，它们之间还有一个中间层——Endpoints资源对象，它是一个由IP地址和端口组成的列表，这些IP地址和端口则来自于由Service的标签选择器匹配到的Pod资源。默认情况下，创建Service资源对象时，其关联的Endpoints对象会自动创建。\n\n\n\n# 二、VIP和service代理\n\n\n1. 一个Service对象就是工作节点上的一些iptables或ipvs规则，用于将到达Service对象IP地址的流量调度转发至相应的Endpoints对象指向的IP地址和端口之上。工作于每个工作节点的kube-proxy组件通过API Server持续监控着各Service及与其关联的Pod对象，并将其创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156949122-c04e92d4-5141-467f-a708-10f181051132.png#align=left&display=inline&height=524&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=524&originWidth=886&size=182488&status=done&style=none&width=886)\n\n2. Service\n   IP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现Kubernetes集群网络的内部通信，并且仅能够将规则中定义的转发服务的请求作为目标地址予以响应，这也是它被称为虚拟IP的原因之一。kube-proxy将请求代理至相应端点的方式有三种：userspace（用户空间）、iptables和ipvs。\n\n\n\n# 三、代理模式\n\n\n1. userspace代理模型\n\n- kube-proxy负责跟踪API\n  Server上Service和Endpoints对象的变动，并据此调整Service资源的定义。对于每个Service对象，它会随机打开一个本地端口（运行于用户空间的kube-proxy进程负责监听），任何到达此代理端口的连接请求都将被代理至当前Service资源后端的各Pod对象上，至于会挑中哪个Pod对象则取决于当前Service资源的调度方式，默认的调度算法是轮询（round-robin）\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157032077-690cbd7d-c176-452e-b9b1-871e769de63f.png#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=683&originWidth=1016&size=318322&status=done&style=none&width=1016)\n\n- 这种代理模型中，请求流量到达内核空间后经由套接字送往用户空间的kube-proxy，而后再由它送回内核空间，并调度至后端Pod。\n\n2. iptables代理模型\n\n- kube-proxy负责跟踪API\n  Server上Service和Endpoints对象的变动（创建或移除），并据此做出Service资源定义的变动。同时，对于每个Service，它都会创建iptables规则直接捕获到达ClusterIP和Port的流量，并将其重定向至当前Service的后端。对于每个Endpoints对象，Service资源会为其创建iptables规则并关联至挑选的后端Pod资源，默认算法是随机调度\n  ![](media/7100bed3fc22161c86e8117142a82db3.png#alt=Node%20X%20%20user%5C%2A%27ace%20%20Server%20%20Client%20%20Pod%201%20%20Pod%20%20kernel%20space%20%20kubeapiserver%20%20Service%20I%20P%20%20%28iptables%29%20%20Node%20Y%20%20Pod%202%20%20user%C2%BBace%20%20Pod%203%20%20kernel%20space)\n- 在创建Service资源时，集群中每个节点上的kube-proxy都会收到通知并将其定义为当前节点上的iptables规则，用于转发工作接口接收到的与此Service资源的ClusterIP和端口的相关流量。客户端发来的请求被相关的iptables规则进行调度和目标地址转换（DNAT）后再转发至集群内的Pod对象之上。\n  相对于用户空间模型来说，iptables模型无须将流量在用户空间和内核空间来回切换，因而更加高效和可靠。不过，其缺点是iptables代理模型不会在被挑中的后端Pod资源无响应时自动进行重定向，而userspace模型则可以。\n\n\n\n1. ipvs代理模型\n\n\n\n- kube-proxy跟踪API\n  Server上Service和Endpoints对象的变动，据此来调用netlink接口创建ipvs规则，并确保与API\n  Server中的变动保持同步。它与iptables规则的不同之处仅在于其请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。\n  ![](media/db76f927fe8770c7421fb3a4b8e602d8.png#alt=dl%20%20aoeds%20%20POCI%20%20a.%27rdsnsn%20%20A%20OPON%20%20aoeds%20%20pod%20%201%20POCI%20%20aot%E2%80%A2dsnsn%20%20X%20OPON)\n- 类似于iptables模型，ipvs构建于netfilter的钩子函数之上，但它使用hash表作为底层数据结构并工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性。\n- ipvs支持众多调度算法，有rr：轮询调度lc：最小连接数dh：目标哈希sh：源哈希sed：最短期望延迟nq：不排队调度\n\n\n\n# 四、会话粘性\n\n\n1. 当客户端访问Pod中的应用程序时，如果有基于客户端身份保存某些私有信息，并基于这些私有信息追踪用户的活动等一类的需求时，那么应该启用session\n   affinity机制。\n1. Session\n   affinity的效果仅会在一定时间期限内生效，默认值为10800秒，超出此时长之后，客户端的再次访问会被调度算法重新调度。另外，Service资源的Session\n   affinity机制仅能基于客户端IP地址识别客户端身份，它会把经由同一个NAT服务器进行源地址转换的所有客户端识别为同一个客户端，调度粒度粗糙且效果不佳，因此，实践中并不推荐使用此种方法实现粘性会话。\n1. Service资源通过．spec.sessionAffinity和．spec.sessionAffinityConfig两个字段配置粘性会话。spec.sessionAffinity字段用于定义要使用的粘性会话的类型，它仅支持使用“None”和“ClientIP”两种属性值。\n\n\n\n- None：不使用sessionAffinity，默认值。\n- ClientIP：基于客户端IP地址识别客户端身份，把来自同一个源IP地址的请求始终调度至同一个Pod对象。\n\n\n\n1. 在启用粘性会话机制时，.spec.sessionAffinityConfig用于配置其会话保持的时长，它是一个嵌套字段，使用格式如下所示，其可用的时长范围为“1～86400”，默认为10800秒：\n\n\n\nspec:\n\n\nsessionAffinity: ClientIP\n\n\nsessionAffinityConfig:\n\n\nclientIP:\n\n\ntimeoutSeconds: <integer>', 24, 0, 0, '2020-11-01 17:34:34.744487', '2021-01-26 11:46:45.054743', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (75, 'Service和Ingress-服务发现', '服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。', 'cover/2020_11_01_17_35_05_391361.jpg', '[TOC]\n\n# 一、概述\n\n\n1. 服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。\n1. 根据服务发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。\n\n- 客户端发现：由客户端到服务注册中心发现其依赖到的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。\n- 服务端发现：这种方式需要额外用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。\n\n3. Kubernetes自1.3版本开始，其用于服务发现的DNS更新为了kubeDNS，自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。不过，Kubernetes依然支持使用环境变量进行服务发现。\n\n# 二、服务发现方式：环境变量\n\n> 创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。\n\n1. Kubernetes Service环境变量\n   Kubernetes为每个Service资源生成包括以下形式的环境变量在内的一系列环境变量，在同一名称空间中创建的Pod对象都会自动拥有这些变量。\n\n- {SVCNAME}_SERVICE_HOST\n- {SVCNAME}_SERVICE_PORT\n\n2. Docker Link形式的环境变量\n   Docker使用--link选项实现容器连接时所设置的环境变量形式。在创建Pod对象时，Kubernetes也会将与此形式兼容的一系列环境变量注入Pod对象中。\n\n- 在Service资源myapp-svc创建后创建的Pod对象中查看可用的环境变量，其中以KUBERNETES_SERVICE开头的表示Kubernetes Service环境变量，名称中不包含“SERVICE”字符串的环境变量为Docker Link形式的环境变量：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157621104-88986080-a2b6-495a-b300-d7abc5381e1d.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=210&originWidth=715&size=393875&status=done&style=none&width=715)\n\n3. 基于环境变量的服务发现其功能简单、易用，但存在一定的局限，例如，仅有那些与创建的Pod对象在同一名称空间中且事先存在的Service对象的信息才会以环境变量的形式注入，那些处于非同一名称空间，或者是在Pod资源创建之后才创建的Service对象的相关环境变量则不会被添加。\n\n\n\n# 三、ClusterDNS和服务发现\n\n\n1. 集群中创建的每个Service对象，都会由其自动生成相关的资源记录。默认情况下，集群内各Pod资源会自动配置其作为名称解析服务器，并在其DNS搜索列表中包含它所属名称空间的域名后缀。\n1. 无论是使用kubeDNS还是CoreDNS，它们提供的基于DNS的服务发现解决方案都会负责解析以下资源记录（Resource Record）类型以实现服务发现。\n1. 拥有ClusterIP的Service资源，需要具有以下类型的资源记录。\n\n- A记录：<service>.<ns>.svc.<zone>. <ttl> IN A <cluster-ip>\n- SRV记录：_<port>._<proto>.<service>.<ns>.svc.<zone>. <ttl> IN SRV <weight> <priority><port-number> <service>.<ns>.svc.<zone>\n- PTR记录：<d>.<c>.<b>.<a>.in-addr.arpa. <ttl> IN PTR <service>.<ns>.svc.<zone>\n\n4. Headless类型的Service资源，需要具有以下类型的资源记录。\n\n- A记录：<service>.<ns>.svc.<zone>. <ttl> IN A <endpoint-ip>\n- SRV记录：_<port>._<proto>.<service>.<ns>.svc.<zone>. <ttl> IN SRV <weight> <priority><port-number> <hostname>.<service>.<ns>.svc.<zone>\n- PTR记录：<d>.<c>.<b>.<a>.in-addr.arpa. <ttl> IN PTR <hostname>.<service>.<ns>.svc.<zone>\n\n5. ExternalName类型的Service资源，需要具有CNAME类型的资源记录。\n\n- CNAME记录：<service>.<ns>.svc.<zone>. <ttl> IN CNAME <extname>\n\n\n\n# 四、服务发现方式：DNS\n\n\n1. 创建Service资源对象时，ClusterDNS会为它自动创建资源记录用于名称解析和服务注册，于是，Pod资源可直接使用标准的DNS名称来访问这些Service资源。每个Service对象相关的DNS记录包含如下两个。\n\n- {SVCNAME}.{NAMESPACE}.{CLUSTER_DOMAIN}\n- {SVCNAME}.{NAMESPACE}.svc.{CLUSTER_DOMAIN}\n\n2. 系统初始化时默认会将“cluster.local.”和主机所在的域“ilinux.io.”作为DNS的本地域使用，这些信息会在Pod创建时以DNS配置的相关信息注入它的/etc/resolv.conf配置文件中。例如，在此前创建的用于交互式Pod资源的客户端中查看其配置，命令如下：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157821727-9263b27f-00f6-4b83-8432-741cfa186a7d.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=848&size=393875&status=done&style=none&width=848)\n\n3. 上述search参数中指定的DNS各搜索域，是以次序指定的几个域名后缀，具体如下所示。\n\n- {NAMESPACE}.svc.{CLUSTER_DOMAIN}：如default.svc.cluster.local。\n- svc.{CLUSTER_DOMAIN}：如svc.cluster.local。\n- {CLUSTER_DOMAIN}：如cluster.local。\n- {WORK_NODE_DOMAIN}：如ilinux.io。', 28, 0, 0, '2020-11-01 17:36:24.561419', '2021-01-26 11:39:15.841397', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (76, 'Service和Ingress-Service类型', '本文主要介绍了k8s常见的四种service类型一、ClusterIP', 'cover/2020_11_01_17_37_02_102628.jpg', '[TOC]\n\n# 一、ClusterIP\n\n1. 通过集群内部IP地址暴露服务，此地址仅在集群内部可达，而无法被集群外部的客户端访问\n1. clusterIP 主要在每个 node 节点使用 iptables，将发向 clusterIP对应端口的数据，转发到 kube-proxy 中。然后 kube-proxy自己内部实现有负载均衡的方法，并可以查询到这个 service 下对应 pod的地址和端口，进而把数据转发给对应的 pod 的地址和端口\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158749855-8f640730-a749-45ac-a9a0-ec0aa71af7f2.png#align=left&display=inline&height=445&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=445&originWidth=766&size=393875&status=done&style=none&width=766)\n\n3. 实现过程\n\n- apiserver\n  用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求后将数据存储到etcd中\n- kube-proxy\n  kubernetes的每个节点中都有一个叫做kube-porxy的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables规则中\n- iptables 使用NAT等技术将virtualIP的流量转至endpoint中\n\n4. 示例\n\n- 创建myapp-deployment.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158809520-ea795138-e9b1-446c-a917-d5f681544165.png#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=465&originWidth=514&size=393875&status=done&style=none&width=514)\n\n- 创建ClusterIP service\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158847124-b75de6f7-713c-4993-9511-c430cc25796e.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=264&size=393875&status=done&style=none&width=264)\n\n- 查看创建的svc信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158862063-2c4b9711-3dd6-42c0-b4e9-c7e65e74a5a5.png#align=left&display=inline&height=100&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=100&originWidth=769&size=393875&status=done&style=none&width=769)\n\n- 访问验证\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158901956-aaa0fc00-c604-459d-87b1-6e1ca3f52b2f.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=410&size=393875&status=done&style=none&width=410)\n\n- 查看ipvs规则\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158914101-5ced9d92-d0f7-4b69-9543-8dfa075cc4d5.png#align=left&display=inline&height=371&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=371&originWidth=763&size=393875&status=done&style=none&width=763)\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158938473-732c1404-334a-48cb-a2ca-46b096437a68.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=901&size=393875&status=done&style=none&width=901)\n\n# 二、NodePort\n\n\n1. 这种类型建立在ClusterIP类型之上，其在每个node节点的IP地址的某静态端口（NodePort）暴露服务，因此，它依然会为Service分配集群IP地址，并将此作为NodePort的路由目标。\n1. NodePort类型就是在工作节点的IP地址上选择一个端口用于将集群外部的用户请求转发至目标Service的ClusterIP和Port，因此，这种类型的Service既可如ClusterIP一样受到集群内部客户端Pod的访问，也会受到集群外部客户端通过套接字<NodeIP>:<NodePort>进行的请求。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159004760-2ac7a32a-0ddb-4e45-af51-653641da9fb5.png#align=left&display=inline&height=397&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=397&originWidth=536&size=393875&status=done&style=none&width=536)\n\n3. 示例\n\n- 创建myapp-service.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159046138-f8311f23-cb36-4061-a0ea-3545a1787d7a.png#align=left&display=inline&height=307&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=307&originWidth=270&size=393875&status=done&style=none&width=270)\n\n- 查看service信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159091442-ed04a51c-bfcf-4847-a6eb-050a305a01ab.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=803&size=393875&status=done&style=none&width=803)\n\n- 访问验证（nodeIP:port）,任意节点都可以\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159071112-51550bc2-67c0-4753-bd4a-31684ae4d188.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=491&size=393875&status=done&style=none&width=491)\n\n- 查看ipvs规则\n  master\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159125280-b35416ab-c557-4d7a-b403-3d1487ddc392.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=321&size=393875&status=done&style=none&width=321)\n  node1\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159138957-88281852-a397-4220-b817-8d029cac0417.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=320&size=393875&status=done&style=none&width=320)\n  node2\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159153620-9452453c-631e-4707-a871-80b9fa21b979.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=317&size=393875&status=done&style=none&width=317)\n\n# 三、LoadBalancer\n\n\n1. 这种类型建构在NodePort类型之上，创建一个具有公网IP地址的云供应商提供的负载均衡器，由它接入外部客户端的请求并调度至集群节点相应的NodePort之上。因此LoadBalancer一样具有NodePort和ClusterIP。\n1. 简而言之，一个LoadBalancer类型的Service会指向关联至Kubernetes集群外部的、切实存在的某个负载均衡设备，该设备通过工作节点之上的NodePort向集群内部发送请求流量。\n1. 例如Amazon云计算环境中的ELB实例即为此类的负载均衡设备。此类型的优势在于，它能够把来自于集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而不是依赖于客户端自行决定连接至哪个节点，从而避免了因客户端指定的节点故障而导致的服务不可用。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159236311-acb3dc53-e8cd-44c3-b96e-fa69f586b59d.png#align=left&display=inline&height=382&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=382&originWidth=532&size=393875&status=done&style=none&width=532)\n\n4. 示例\n\n- 创建LoadBalancer.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159253194-4c07583e-61f9-402d-939c-2e3f91a42fd4.png#align=left&display=inline&height=306&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=306&originWidth=277&size=393875&status=done&style=none&width=277)\n\n\n# 四、ExternalName\n\n\n1. 通过将Service映射至由externalName字段的内容指定的主机名来暴露服务，此主机名需要被DNS服务解析至CNAME类型的记录。\n1. 此种类型并非定义由Kubernetes集群提供的服务，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部的Service的一种实现方式。\n1. 这种类型的Service没有ClusterIP和NodePort，也没有标签选择器用于选择Pod资源，因此也不会有Endpoints存在。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159278073-c8ca1c94-b6e5-4f39-8415-29cd7070cffc.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=490&size=393875&status=done&style=none&width=490)\n\n4. 示例\n\n- 创建资源清单ExternalName.yaml\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159301221-20b1ae87-dbf1-4181-9d9a-563d03af833b.png#align=left&display=inline&height=170&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=170&originWidth=419&size=393875&status=done&style=none&width=419)\n\n- 查看svc资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159341968-6c860be2-2b75-40d3-b239-47a795f07495.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=94&originWidth=675&size=393875&status=done&style=none&width=675)\n\n- 解析验证\n\n`# dig -t A myapp-service.default.svc.cluster.local. @10.244.2.189`\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159363802-6fa98cf0-d06f-454c-932d-559a39359f3f.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=774&size=393875&status=done&style=none&width=774)', 25, 0, 0, '2020-11-01 17:39:12.082586', '2021-01-26 05:15:18.789998', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (77, 'Service和Ingress-Service资源及模型', '当客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由。', 'cover/2020_11_01_17_39_42_227951.jpg', '[TOC]\n\n# 一、概述\n\n1. 当客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由。\n1. 如何为此类Service资源配置IP地址，则取决于它的标签选择器的定义。\n\n- 具有标签选择器：端点控制器（Endpoints Controller）会在API中为其创建Endpoints记录，并将ClusterDNS服务中的A记录直接解析到此Service后端的各Pod对象的IP地址上。\n- 没有标签选择器：端点控制器（Endpoints Controller）不会在API中为其创建Endpoints记录，ClusterDNS的配置分为两种情形，对ExternalName类型的服务创建CNAME记录，对其他三种类型来说，为那些与当前Service共享名称的所有Endpoints对象创建一条记录。\n\n3. 根据Headless Service的工作特性可知，它记录于ClusterDNS的A记录的相关解析结果是后端Pod资源的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源\n\n# 二、实例\n\n\n1. 创建yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159521030-3e093ac5-b820-4a56-8f43-f01a654b8c82.png#align=left&display=inline&height=263&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=263&originWidth=301&size=393875&status=done&style=none&width=301)\n\n2. 查看svc信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159539059-81c05a3d-3bb7-440e-9b52-c0015f9a7fc7.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=786&size=393875&status=done&style=none&width=786)\n\n3. 查看ClusterDNS地址信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159550860-c8e88b33-2487-46ef-8025-fd599500062a.png#align=left&display=inline&height=145&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=145&originWidth=1018&size=393875&status=done&style=none&width=1018)\n\n4. dns命令解析资源\n\n`# dig -t A myapp-headless.default.svc.cluster.local. @10.244.2.189`\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159565071-509c5c61-1709-4338-89b9-95b6b673dd52.png#align=left&display=inline&height=99&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=99&originWidth=694&size=393875&status=done&style=none&width=694)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159584863-5547af4d-5530-4eae-a81a-a91b392693a1.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=896&size=393875&status=done&style=none&width=896)\n\n- 客户端向此Service对象发起的请求将直接接入到Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源则是由DNS服务器接收到查询请求时以轮询（roundrobin）的方式返回的IP地址。', 29, 0, 0, '2020-11-01 17:40:52.255542', '2021-01-26 11:47:21.333172', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (78, 'Service和Ingress-Headless', '当客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由', 'cover/2020_11_01_17_50_29_136087.jpg', '[TOC]\n\n# 一、概述\n\n1. 当客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由。\n1. 如何为此类Service资源配置IP地址，则取决于它的标签选择器的定义。\n\n- 具有标签选择器：端点控制器（Endpoints Controller）会在API中为其创建Endpoints记录，并将ClusterDNS服务中的A记录直接解析到此Service后端的各Pod对象的IP地址上。\n- 没有标签选择器：端点控制器（Endpoints Controller）不会在API中为其创建Endpoints记录，ClusterDNS的配置分为两种情形，对ExternalName类型的服务创建CNAME记录，对其他三种类型来说，为那些与当前Service共享名称的所有Endpoints对象创建一条记录。\n\n3. 根据Headless Service的工作特性可知，它记录于ClusterDNS的A记录的相关解析结果是后端Pod资源的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源\n\n# 二、实例\n\n\n1. 创建yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159521030-3e093ac5-b820-4a56-8f43-f01a654b8c82.png#align=left&display=inline&height=263&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=263&originWidth=301&size=393875&status=done&style=none&width=301)\n\n2. 查看svc信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159539059-81c05a3d-3bb7-440e-9b52-c0015f9a7fc7.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=786&size=393875&status=done&style=none&width=786)\n\n3. 查看ClusterDNS地址信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159550860-c8e88b33-2487-46ef-8025-fd599500062a.png#align=left&display=inline&height=145&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=145&originWidth=1018&size=393875&status=done&style=none&width=1018)\n\n4. dns命令解析资源\n\n`# dig -t A myapp-headless.default.svc.cluster.local. @10.244.2.189`\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159565071-509c5c61-1709-4338-89b9-95b6b673dd52.png#align=left&display=inline&height=99&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=99&originWidth=694&size=393875&status=done&style=none&width=694)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159584863-5547af4d-5530-4eae-a81a-a91b392693a1.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=896&size=393875&status=done&style=none&width=896)\n\n- 客户端向此Service对象发起的请求将直接接入到Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源则是由DNS服务器接收到查询请求时以轮询（roundrobin）的方式返回的IP地址。', 24, 0, 0, '2020-11-01 17:51:04.597039', '2021-01-26 11:39:58.375707', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (79, 'Service和Ingress-Ingress资源', 'Ingress就是一组基于DNS名称（host）或URL路径把请求转发至指定的Service资源的规则，用于将集群外部的请求流量转发至集群内部完成服务发布。然而，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则要想真正发挥作用还需要其他功能的辅助，如监听某套接字，然后根据这些规则的匹配机制路由请求流量。这种能够为Ingress资源监听套接字并转发流量的组件称为Ingress控制器（Ingress', 'cover/2020_11_01_17_51_54_634001.jpg', '[TOC]\n\n# 一、Ingress和Ingress Controller\n\n\n1. Ingress就是一组基于DNS名称（host）或URL路径把请求转发至指定的Service资源的规则，用于将集群外部的请求流量转发至集群内部完成服务发布。然而，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则要想真正发挥作用还需要其他功能的辅助，如监听某套接字，然后根据这些规则的匹配机制路由请求流量。这种能够为Ingress资源监听套接字并转发流量的组件称为Ingress控制器（Ingress Controller）。\n1. Ingress控制器并不直接运行为kube-controller-manager的一部分，它是Kubernetes集群的一个重要附件，类似于CoreDNS，需要在集群上单独部署。\n1. Ingress控制器可以由任何具有反向代理（HTTP/HTTPS）功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器自身也是运行于集群中的Pod资源对象，它与被代理的运行为Pod资源的应用运行于同一网络中\n1. 使用Ingress资源进行流量分发时，Ingress控制器可基于某Ingress资源定义的规则将客户端的请求流量直接转发至与Service对应的后端Pod资源之上，这种转发机制会绕过Service资源，从而省去了由kube-proxy实现的端口代理开销。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159965183-66a3167e-cb45-40c0-b443-57fd4ff239a6.png#align=left&display=inline&height=375&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=375&originWidth=835&size=393875&status=done&style=none&width=835)\n\n\n# 二、部署Ingress控制器（Nginx）\n\n\n1. Ingress控制器自身是运行于Pod中的容器应用，一般是Nginx或Envoy一类的具有代理及负载均衡功能的守护进程，它监视着来自于API\n   Server的Ingress对象状态，并以其规则生成相应的应用程序专有格式的配置文件并通过重载或重启守护进程而使新配置生效。\n1. 对于Nginx来说，Ingress规则需要转换为Nginx的配置信息。简单来说，Ingress控制器其实就是托管于Kubernetes系统之上的用于实现在应用层发布服务的Pod资源，它将跟踪Ingress资源并实时生成配置规则。\n1. 参考地址\n   github地址\n   ingress-nginx官网\n1. 部署ingress-nginx\n   • 创建ingress基础环境资源\n\n`kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml` \n\n- 下载慢可以去Github下载\n\n[https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml)\n\n- 创建资源\n\n`kubectl apply -f mandatory.yaml` \n\n- 查看pod资源信息\n\n`kubectl get pod -n ingress-nginx` \n\n- 采用nodepod暴露服务\n\n`kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml` \n\n- 查看svc资源信息\n\n`kubectl get svc -n ingress-nginx` \n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604160889107-c3e1612a-6c4f-42b5-afcf-07f4f6102778.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=909&size=393875&status=done&style=none&width=909)\n\n# 三、Ingress资源类型\n\n\n1. 单Service资源型Ingress\n   使用Ingress来暴露服务，此时只需要为Ingress指定“default backend”即可\n\n- 例如下面的示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n	name: my-ingress\nspec:\n	backend:\n		serviceName: my-svc\n		servicePort: 80\n```\n\n- Ingress控制器会为其分配一个IP地址接入请求流量，并将它们转至示例中的my-svc后端。\n\n2. 基于URL路径进行流量分发\n   垂直拆分或微服务架构中，每个小的应用都有其专用的Service资源暴露服务，但在对外开放的站点上，可通过主域名的URL路径（path）分别接入。\n\n- 例如，对www.ilinux.io/api的请求统统转发至API Service资源，将对www.ilinux.io/wap的请求转发至WAP Service资源\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n	name: test\n	annotations:\n		ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: www.ilinux.io\n    http:\n      paths:\n      - path: /wap\n        backend:\n        	serviceName: wap\n        	servicePort: 80\n      - path: /api\n      	backend:\n      		serviceName: api\n      		servicePort: 80\n```\n\n\n3. 基于主机名称的虚拟主机\n   将每个应用分别以独立的FQDN主机名进行输出，如wap.ik8s.io和api.ik8s.io，这两个主机名解析到external LB（如图6-12所示）的IP地址之上，分别用于发布集群内部的WAP和API这两个Service资源。这种实现方案其实就是Web站点部署中的“基于主机名的虚拟主机”，将多个FQDN解析至同一个IP地址，然后根据“主机头”进行转发。\n\n- 以独立FQDN主机形式发布服务的Ingress资源示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: api.ik8s.io\n    http:\n      paths:\n      - backend:\n          serviceName: api\n          servicePort: 80\n  - host: wap.ik8s.io\n    http:\n      paths:\n      - backend:\n          serviceName: wap\n          servicePort: 80\n```\n\n\n4. TLS类型的Ingress资源\n   用于以HTTPS发布Service资源，基于一个含有私钥和证书的Secret对象即可配置TLS协议的Ingress资源，目前来说，Ingress资源仅支持单TLS端口，并且还会卸载TLS会话。在Ingress资源中引用此Secret即可让Ingress控制器加载并配置为HTTPS服务。\n\n- 下面是一个简单的TLS类型的Ingress资源示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: no-rules-map\nspec:\n  tls:\n  - secretName: ikubernetesSecret\n  backend:\n    serviceName: homesite\n    servicePort: 80\n```\n\n\n\n\n', 23, 0, 0, '2020-11-01 17:53:00.819300', '2021-01-27 04:40:15.116458', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (80, 'Service和Ingress-Ingress案例', '-', 'cover/2020_11_01_17_53_34_943811.jpg', '[TOC]\n\n> 注意要点：\n>\n> - ingress中的serviceName要与service中的metadata-name保持一致\n> - service中的selector-app要与deployment中的selector-app保持一致\n> - deployment中的matchlables-app要与template中的matadata-lables-app保持一致\n\n\n\n# 一、部署myapp1实例\n\n\n1. 使用Deployment控制器部署myapp1相关的Pod对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161078673-8008e035-9397-4cbd-9417-3f9a0ac2c938.png#align=left&display=inline&height=486&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=486&originWidth=518&size=393875&status=done&style=none&width=518)\n\n- 查看deployment状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161105531-f0d96d61-1e0e-415b-b00f-927f4b9d3f16.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=726&size=393875&status=done&style=none&width=726)\n\n2. 使用ClusterIP控制器部署svc1相关的对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161136924-e382777c-8169-4cb5-b90d-857ba5ec6d91.png#align=left&display=inline&height=281&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=281&originWidth=262&size=393875&status=done&style=none&width=262)\n\n- 查看svc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161222118-85b4411f-1f5b-4be4-a18c-1c575e7bd277.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=766&size=393875&status=done&style=none&width=766)\n\n\n# 二、部署myapp2实例\n\n\n1. 使用Deployment控制器部署myapp2相关的Pod对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161337774-1af7c1e6-885d-4cf5-8a92-48d18e85cda0.png#align=left&display=inline&height=468&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=468&originWidth=515&size=393875&status=done&style=none&width=515)\n\n\n- 查看deployment状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161360694-7b7a345f-1284-4a74-bf14-29ba844add46.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=725&size=393875&status=done&style=none&width=725)\n\n2. 使用ClusterIP控制器部署svc2相关的对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161389154-aa28fb9e-a29e-4a49-b66f-4b12d4302ea3.png#align=left&display=inline&height=281&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=281&originWidth=256&size=393875&status=done&style=none&width=256)\n\n\n- 查看svc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190423156-2fd9afa2-9888-4476-b31b-380b86a66fc4.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=771&size=393875&status=done&style=none&width=771)\n\n\n# 三、创建Ingress实例\n\n\n1. 编写ingress使访问myapp1.cuiliang.com跳转至myapp1，访问myapp2.cuiliang.com跳转至myapp2\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190463056-17d83770-7537-4377-842b-262123804060.png#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=627&originWidth=442&size=393875&status=done&style=none&width=442)\n\n2. 查看svc服务信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190514114-a0414d50-c6b0-4d45-9e96-5dd805c51b1a.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=912&size=393875&status=done&style=none&width=912)\n\n3. 查看ingress规则\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190541804-f9278af0-de86-4374-842d-0b1ad7c96d6b.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=732&size=393875&status=done&style=none&width=732)\n\n4. 查看ingress-nginx配置文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190575330-f4d45647-a004-4dbb-9b6a-e4cbf4eb53a2.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=1098&size=393875&status=done&style=none&width=1098)\n\n5. 修改host文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190609730-834d5652-0391-44e1-b185-bd16c5e17266.png#align=left&display=inline&height=39&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=39&originWidth=281&size=393875&status=done&style=none&width=281)\n\n6. 访问测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190624017-a76acd75-7698-410b-9e2f-f358e85e82dc.png#align=left&display=inline&height=109&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=109&originWidth=395&size=393875&status=done&style=none&width=395)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190638087-37168b87-a052-45d3-870b-43985e215323.png#align=left&display=inline&height=103&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=103&originWidth=399&size=393875&status=done&style=none&width=399)\n\n\n# 四、Ingress https代理\n\n\n1. 创建证书，以及 cert 存储\n   `# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=nginxsvc/O=nginxsvc\"` \n   `# kubectl create secret tls tls-secret --key tls.key --cert tls.crt` \n1. 使用Deployment控制器部署myapp3相关的Pod对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190674237-a5e7f8be-f1ac-4368-8d50-5bde14004d4c.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=517&size=393875&status=done&style=none&width=517)\n\n\n- 查看deployment状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190714394-a2c8fb61-6176-4cf6-9a41-3c11daac1170.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=723&size=393875&status=done&style=none&width=723)\n\n\n3. 使用ClusterIP控制器部署svc3相关的对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190733320-ddec8958-ae10-4957-9c67-337883920bbc.png#align=left&display=inline&height=282&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=282&originWidth=266&size=393875&status=done&style=none&width=266)\n\n\n- 查看svc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190751272-8106e34f-26ba-4ccd-a1d3-64831724b2c7.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=757&size=393875&status=done&style=none&width=757)\n\n\n4. 创建Ingress实例\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190776734-07edae7e-4d51-43a7-bf72-c307d9fb0834.png#align=left&display=inline&height=399&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=399&originWidth=437&size=393875&status=done&style=none&width=437)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190792336-dfd622ec-cb5a-45dc-80a7-55f1bdeed01f.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=384&size=393875&status=done&style=none&width=384)\n\n\n- 查看svc-ingress信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190808435-d18b6ea9-5c6e-4b38-b149-0c6bcac9b4ca.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=909&size=393875&status=done&style=none&width=909)\n\n\n5. 修改host文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190839193-c09e4d82-4ad6-4e90-815b-d166ad10851f.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=291&size=393875&status=done&style=none&width=291)\n\n6. 访问测试\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190855301-14c7cc0d-76c6-4148-bad4-9d8dca6ad206.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=394&size=393875&status=done&style=none&width=394)\n\n\n# 五、BasicAuth用户认证\n\n\n1. 创建证书，以及 cert 存储\n   `#yum -y install httpd` \n   `#htpasswd -c auth foo` \n   `#kubectl create secret generic basic-auth --from-file=auth` \n1. 使用Deployment控制器部署myapp4相关的Pod对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190903397-0e217b91-39fa-4d08-8b14-8ec145db1995.png#align=left&display=inline&height=462&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=462&originWidth=517&size=393875&status=done&style=none&width=517)\n\n- 查看deployment状态\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190923687-eba4cd71-0816-45e9-9e7b-3ae490db26cc.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=723&size=393875&status=done&style=none&width=723)\n\n3. 使用ClusterIP控制器部署svc4相关的对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191407683-77b76b2f-fc37-4b6f-aeb3-e0a84dee3ee4.png#align=left&display=inline&height=284&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=284&originWidth=258&size=393875&status=done&style=none&width=258)\n\n- 查看svc\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191431934-e4b20de3-7e39-45a7-868f-7d7ea63de0dd.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=753&size=393875&status=done&style=none&width=753)\n\n4. 创建Ingress实例\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191453753-bed7e60a-64ed-453f-b337-6332e04a2624.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=874&size=393875&status=done&style=none&width=874)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191472913-f40f56ad-a02b-4706-bc23-9b2641422430.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=50&originWidth=316&size=393875&status=done&style=none&width=316)\n\n5. 查看svc-ingress信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191484691-b571c89d-6be7-4f92-b566-73275831966e.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=685&size=393875&status=done&style=none&width=685)\n\n6. 修改host文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191500579-19b7894e-f1e8-4865-9355-ac29b9f70a03.png#align=left&display=inline&height=85&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=85&originWidth=284&size=393875&status=done&style=none&width=284)\n\n7. 访问测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191522260-a2bb4287-174c-4aad-9114-f2ca4709f5c4.png#align=left&display=inline&height=320&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=320&originWidth=625&size=393875&status=done&style=none&width=625)\n\n\n# 六、nginx重写\n\n\n- 当用户访问myapp5.cuiliang.com时跳转到myapp3.cuiliang.com\n\n\n\n1. 创建Ingress实例\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191542667-c01ca764-53de-4661-a262-f5753723e075.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=214&originWidth=944&size=393875&status=done&style=none&width=944)\n\n2. 查看svc-ingress信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191562237-e4d0e9f2-43c2-457d-ac15-daebf049efcd.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=161&originWidth=762&size=393875&status=done&style=none&width=762)\n\n3. 修改host文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191576591-03079a9d-a253-413a-a95f-6a239b5c2d85.png#align=left&display=inline&height=106&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=106&originWidth=287&size=393875&status=done&style=none&width=287)\n\n4. 访问测试\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191595792-a13bdd4b-2d6e-447b-965c-a77501731a91.png#align=left&display=inline&height=202&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=202&originWidth=468&size=393875&status=done&style=none&width=468)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191614503-5dffb7cd-62a7-4ab4-a707-81a580799584.png#align=left&display=inline&height=87&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=87&originWidth=401&size=393875&status=done&style=none&width=401)', 40, 0, 0, '2020-11-01 17:54:57.415180', '2021-01-26 11:47:33.469613', 1, 6, 0, 1, 1);
INSERT INTO `blog_article` VALUES (81, '存储-配置集合ConfigMap', 'ConfigMap对象将配置数据以键值对的形式进行存储，这些数据可以在Pod对象中使用或者为系统组件提供配置。无论应用程序如何使用ConfigMap对象中的数据，用户都完全可以通过在不同的环境中创建名称相同但内容不同的ConfigMap对象，从而为不同环境中同一功能的Pod资源提供不同的配置信息。', 'cover/2020_11_01_18_15_15_621919.jpg', '[TOC]\n\n# 一、简介\n\n\n1. 一个ConfigMap对象就是一系列配置数据的集合，这些数据可“注入”到Pod对象中，并为容器应用所使用，注入方式有挂载为存储卷和传递为环境变量两种。\n1. ConfigMap对象将配置数据以键值对的形式进行存储，这些数据可以在Pod对象中使用或者为系统组件提供配置。无论应用程序如何使用ConfigMap对象中的数据，用户都完全可以通过在不同的环境中创建名称相同但内容不同的ConfigMap对象，从而为不同环境中同一功能的Pod资源提供不同的配置信息。\n\n\n\n# 二、创建ConfigMap对象\n\n\n1. 用户可以根据目录、文件或直接值创建ConfigMap对象。命令的语法格式为：\n\n\n\n> kubectl create configmap <map-name> <data-source>\n\n\n\n- <map-name>即为ConfigMap对象的名称，而<data-source>是数据源，它可以通过直接值、文件或目录来获取。无论是哪一种数据源供给方式，它都要转换为ConfigMap对象中的Key-Value数据，其中Key由用户在命令行给出或是文件数据源的文件名，它仅能由字母、数字、连接号和点号组成，而Value则是直接值或文件数据源的内容。\n\n\n\n2. 利用值直接创建\n\n\n\n- 使用“--from-literal”选项可在命令行直接给出键值对来创建ConfigMap对象，重复使用此选项则可以传递多个键值对\n  `# kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm` \n\n\n\n- 查看ConfigMap对象special-config的相关信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188818032-e6abcf3c-7584-4c93-85e0-d07d6a28dff6.png#align=left&display=inline&height=302&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=302&originWidth=712&size=393875&status=done&style=none&width=712)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188935512-412a37f2-f393-4cea-af64-db5cab05c8be.png#align=left&display=inline&height=346&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=346&originWidth=679&size=393875&status=done&style=none&width=679)\n\n4. 基于文件创建\n\n- 使用“--from-file”选项即可基于文件内容来创建ConfigMap对象，可以重复多次使用“--from-file”选项以传递多个文件内容：\n  `# kubectl create configmap resolv.conf --from-file=/etc/resolv.conf` \n- 查看ConfigMap对象，其数据存储的键为文件名，值为文件内容\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188962605-677a5bc9-ea87-4736-b838-51c76867e72d.png#align=left&display=inline&height=351&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=351&originWidth=683&size=393875&status=done&style=none&width=683)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189011074-546cadac-f68f-450f-92ce-cf4a062cbd62.png#align=left&display=inline&height=349&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=349&originWidth=650&size=393875&status=done&style=none&width=650)\n\n\n5. 基于目录创建\n\n- 将“--from-file”选项后面所跟的路径指向一个目录路径就能将目录下的所有文件一同创建于同一ConfigMap资源中，将/etc/docker/目录下的所有文件都保存于docker-config-files对象中：\n  `# kubectl create configmap docker-config-files --from-file=/etc/docker/` \n- 此目录中包含daemon.json\n  key.json两个配置文件。创建ConfigMap资源时，它们会被分别存储为两个键值数据\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189079700-558b4e5a-1f13-4d3e-b965-4cfa56422d2b.png#align=left&display=inline&height=459&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=459&originWidth=813&size=393875&status=done&style=none&width=813)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189100514-3c3639d1-d011-49ff-9997-65e9a13180da.png#align=left&display=inline&height=510&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=510&originWidth=770&size=393875&status=done&style=none&width=770)\n\n6. 使用清单创建\n\n- 基于配置文件创建ConfigMap资源时，它所使用的字段包括通常的apiVersion、kind和metadata字段，以及用于存储数据的关键字段“data”\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189145232-017a469f-3278-489d-8dee-8dfb0664b962.png#align=left&display=inline&height=171&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=171&originWidth=373&size=393875&status=done&style=none&width=373)\n\n- 查看资源信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189166519-c38d7d80-649f-426d-aed5-6db10fa6b88a.png#align=left&display=inline&height=393&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=393&originWidth=737&size=393875&status=done&style=none&width=737)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189194626-11411c8c-7032-48ba-8441-c2e7d8799e1a.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=707&size=393875&status=done&style=none&width=707)\n\n# 三、向Pod环境变量传递ConfigMap对象键值数据\n\n\n1. 使用 ConfigMap 来导入环境变量\n\n- 创建configmap资源清单\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189231795-79a058e1-e9a3-41ef-99ec-93fdc512035a.png#align=left&display=inline&height=329&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=329&originWidth=294&size=393875&status=done&style=none&width=294)\n\n- 查看资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189374366-05a440f6-8764-4264-82dd-4dd9fdfa909e.png#align=left&display=inline&height=237&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=237&originWidth=840&size=393875&status=done&style=none&width=840)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189349188-858235b3-2af5-4292-a5eb-70c6f0610ef3.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=161&originWidth=788&size=393875&status=done&style=none&width=788)\n\n- 创建pod资源，设置env信息\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189419345-66e6d93b-1c1f-4d1f-9bcd-326ce239f298.png#align=left&display=inline&height=559&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=559&originWidth=498&size=393875&status=done&style=none&width=498)\n\n> 字段name的值为要引用的ConfigMap对象的名称，字段key可用于指定要引用ConfigMap对象中某键的键名在容器中使用envFrom字段直接将ConfigMap资源中的所有键值一次性地完成导入\n\n- 查看pod日志环境变量信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189530036-db8ebbe9-6189-4e65-af42-6bd4672282ee.png#align=left&display=inline&height=141&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=141&originWidth=800&size=393875&status=done&style=none&width=800)\n\n\n2. 用 ConfigMap 设置命令行参数\n\n- 创建pod资源，echo变量信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189621739-91b331c2-eba5-40b7-84ac-e9d54dd4feb9.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=922&size=393875&status=done&style=none&width=922)\n\n- 查看pod输出信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189658188-38063454-4bd5-48bd-b01b-e3b4d73e8743.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=50&originWidth=525&size=393875&status=done&style=none&width=525)\n\n\n3. 通过数据卷使用整个ConfigMap\n\n- 创建pod资源，将configmap挂载到/etc/config\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189724438-db5db602-290a-4c45-b930-010f89d99935.png#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=405&originWidth=539&size=393875&status=done&style=none&width=539)\n\n- 进入pod容器查看文件内容\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189837412-e1ad1758-2d42-4a2b-acf2-bc7838de34b4.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=886&size=393875&status=done&style=none&width=886)\n\n\n4. 通过数据卷使用部分ConfigMap值\n\n\n\n- 创建pod资源，将configmap挂载到/etc/config\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189972621-d6ba1569-2a73-48ba-ad58-153ffd60210d.png#align=left&display=inline&height=491&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=491&originWidth=535&size=393875&status=done&style=none&width=535)\n\n- 进入pod查看文件内容\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190237357-872a821e-bc75-4465-9e26-5543ebec112a.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=94&originWidth=876&size=393875&status=done&style=none&width=876)\n\n\n# 四、configmap热更新\n\n\n1. 创建资源清单，变量log_level: INFO，数据卷挂载到deployment中\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190268910-9567ca1d-1f13-4df1-abab-22572576903e.png#align=left&display=inline&height=791&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=791&originWidth=516&size=393875&status=done&style=none&width=516)\n\n\n- 进入pod容器查看文件内容\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190309173-04a66fc7-1883-4ebe-abb5-4fd58f23cbcc.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=985&size=393875&status=done&style=none&width=985)\n\n\n2. 修改configmap\n   `# kubectl edit configmap log-config` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190329882-6e897bbf-890c-40ea-8832-32b018c15199.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=214&originWidth=639&size=393875&status=done&style=none&width=639)\n\n- 进入pod查看文件内容\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190355134-71479c41-aafc-4a6f-9238-b700a8090786.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=989&size=393875&status=done&style=none&width=989)\n\n\n# 五、注意事项\n\n\n1. ConfigMap是名称空间级的资源，因此，引用它的Pod必须处于同一名称空间中。\n1. ConfigMap 更新后滚动更新 Pod更新 ConfigMap 目前并不会触发相关 Pod\n   的滚动更新，可以通过修改 pod annotations 的方式强制触发滚动更新\n1. 在.spec.template.metadata.annotations中添加version/config，每次通过修改version/config来触发滚动更新\n1. 更新 ConfigMap 后：使用该 ConfigMap 挂载的 Env 不会同步更新使用该 ConfigMap\n   挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新', 32, 0, 0, '2020-11-01 18:15:29.538569', '2021-01-26 16:38:45.469492', 1, 1, 0, 1, 0);
INSERT INTO `blog_article` VALUES (82, '存储-敏感信息Secret', 'Secret对象主要有两种用途，一是作为存储卷注入到Pod上由容器应用程序所使用，二是用于kubelet为Pod里的容器拉取镜像时向私有仓库提供认证信息。使用ServiceAccount资源自建的Secret对象是一种更具安全性的方式。', 'cover/2020_11_01_18_15_59_818977.jpg', '[TOC]\n# 一、secret概述\n\n\n1. Secret对象存储数据以键值方式存储数据，在Pod资源中通过环境变量或存储卷进行数据访问。Secret对象仅会被分发至调用了此对象的Pod资源所在的工作节点，且只能由节点将其存储于内存中。Secret对象的数据的存储及打印格式为Base64编码的字符串，因此用户在创建Secret对象时也要提供此种编码格式的数据。\n1. Secret对象主要有两种用途，一是作为存储卷注入到Pod上由容器应用程序所使用，二是用于kubelet为Pod里的容器拉取镜像时向私有仓库提供认证信息。使用ServiceAccount资源自建的Secret对象是一种更具安全性的方式。\n1. Secret资源主要由四种类型组成\n\n- Opaque：自定义数据内容；base64编码，用来存储密码、密钥、信息、证书等数据，类型标识符为generic。\n- kubernetes.io/service-account-token:Service Account的认证信息，可在创建Service Accout时由Kubernetes自动创建。\n- kubernetes.io/dockerconfigjson：用来存储Docker镜像仓库的认证信息，类型标识为docker-registry。\n- kubernetes.io/tls：用于为SSL通信模式存储证书和私钥文件，命令式创建时类型标识为tls。\n\n# 二、Opaque\n\n\n1. 通过kubectl create命令创建资源\n\n> 使用 `“kubectl createsecret generic <SECRET_NAME>--from-literal=key=value”` 命令直接进行创建\n\n- 创建了一个名为mysql-auth的Secret对象，用户名为root，密码为123.com\n  `# kubectl create secret generic mysql-auth --from-literal=username=root --from-literal=password=123.com` \n- 查看资源详细信息\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198075538-c8f3cac0-9423-46a7-ba5b-c723c46df897.png#align=left&display=inline&height=324&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=324&originWidth=661&size=393875&status=done&style=none&width=661)\n\n> 使用“ `kubectl create secret generic <SECRET_NAME> --from-file[=KEY1]=/PATH/TO/FILE` ”命令加载认证文件内容并生成为Secret对象\n\n- 将ssh密钥认证文件创建secret对象\n  `# kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-file=ssh-publickey=/root/.ssh/id_rsa.pub` \n- 查看资源详细信息\n  `# kubectl get secrets ssh-key-secret -o yaml` \n\n2. 使用Secret清单创建\n\n- Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198184047-2c609a1a-7b48-407e-9481-99b1af4967f4.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=480&size=393875&status=done&style=none&width=480)\n\n- 创建资源清单文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198226076-438789a7-c299-4112-81d0-bd4557e4b1c1.png#align=left&display=inline&height=194&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=194&originWidth=320&size=393875&status=done&style=none&width=320)\n\n3. 将secret挂载到volume中\n\n- 创建pod资源清单\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198261324-da05873a-e527-4f35-8683-1799f284def4.png#align=left&display=inline&height=424&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=424&originWidth=469&size=393875&status=done&style=none&width=469)\n\n- 查看pod信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198285307-9d55a2aa-e4ea-4284-a5b2-f063e4522a21.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=824&size=393875&status=done&style=none&width=824)\n\n4. 将secret导入到环境变量中\n\n- 创建pod资源清单\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198311560-5d51f6b2-d5c6-45d5-bb12-8e43a17c69e0.png#align=left&display=inline&height=492&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=492&originWidth=471&size=393875&status=done&style=none&width=471)\n\n- 查看pod信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198327864-5551fd7c-acb9-4e17-9594-52a0b3692c5a.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=763&size=393875&status=done&style=none&width=763)\n\n\n# 三、Service Account\n\n\n1. Service Account 用来访问 Kubernetes API，由 Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中\n1. 验证\n\n- 创建nginx资源\n  `# kubectl run nginx --image nginx` \n- 查看资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199092161-4259f67d-89e1-46d6-9b4d-283071aa20cb.png#align=left&display=inline&height=90&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=90&originWidth=1120&size=393875&status=done&style=none&width=1120)\n\n\n# 四、docker config json\n\n\n1. 使用 Kuberctl 创建 docker registry 认证的 secret\n   `kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL` \n1. 在创建 Pod 的时候，通过imagePullSecrets来引用刚创建的 `myregistrykey`\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199137237-f24dc658-8d31-4bcc-9703-f4d3ac6b147d.png#align=left&display=inline&height=237&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=237&originWidth=516&size=393875&status=done&style=none&width=516)\n\n\n# 五、TLS\n\n\n1. 使用“kubectl create secret tls <SECRET_NAME> --cert=--key=”命令加载TLS文件内容并生成secret对象\n\n- 将TLS密钥认证文件创建secret对象\n  `# kubectl create secret tls nginx-ssl --key=nginx.key --cert=nginx.crt` ', 24, 0, 0, '2020-11-01 18:17:00.256110', '2021-01-26 05:42:04.681197', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (83, '存储-临时存储emptyDir', 'emptyDir存储卷是Pod对象生命周期中的一个临时目录，当', 'cover/2020_11_01_18_17_24_690041.jpg', '[TOC]\n# 一、概述\n\n\n1. emptyDir存储卷是Pod对象生命周期中的一个临时目录，当 Pod被分配给节点时，首先创建emptyDir卷，并且只要该 Pod在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除Pod 时，emptyDir中的数据将被永久删除\n1. emptyDir的用法有：\n\n- 同一Pod内的多个容器间文件的共享\n- 作为容器数据的临时存储目录用于数据缓存系统等\n\n3. emptyDir存储卷则定义于．spec.volumes.emptyDir嵌套字段中，可用字段主要包含两个。\n\n- medium：此目录所在的存储介质的类型，可取值为“default”或“Memory”，默认为default，表示使用节点的默认存储介质；“Memory”表示使用基于RAM的临时文件系统tmpfs，空间受限于内存，但性能非常好，通常用于为容器中的应用提供缓存空间。\n- sizeLimit：当前存储卷的空间限额，默认值为null，表示不限制；不过，在medium字段值为“Memory”时建议务必定义此限额。\n\n# 二、实例\n\n\n> 存储卷名称为html，挂载于容器nginx的/usr/share/nginx/html目录，以及容器page的/html目录。容器page每隔10秒向存储卷上的index.html文件中追加一行信息，而容器nginx中的nginx进程则以其为站点主页。\n\n\n\n1. 创建pod资源清单\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199304031-2145f16a-8b81-4c30-9411-e3d80cac0370.png#align=left&display=inline&height=576&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=576&originWidth=623&size=393875&status=done&style=none&width=623)\n\n2. 访问测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199332803-315500c9-ba4a-498f-ba29-0817a6b2ba73.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=494&size=393875&status=done&style=none&width=494)', 22, 0, 0, '2020-11-01 18:18:30.750490', '2021-01-26 11:47:39.823848', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (84, '存储-节点存储hostPath', 'hostPath类型的存储卷是指将工作节点上某文件系统的目录或文件挂载于Pod中的一种存储卷，它可独立于Pod资源的生命周期，因而具有持久性。但它是工作节点本地的存储空间，仅适用于特定情况下的存储卷使用需求。', 'cover/2020_11_01_18_19_58_292673.jpg', '[TOC]\n\n# 一、概述\n\n\n1. hostPath类型的存储卷是指将工作节点上某文件系统的目录或文件挂载于Pod中的一种存储卷，它可独立于Pod资源的生命周期，因而具有持久性。但它是工作节点本地的存储空间，仅适用于特定情况下的存储卷使用需求。\n1. hostPath的用途：\n\n- 运行需要访问 Docker 内部的容器\n- 使用/var/lib/docker的hostPath在容器中运行 cAdvisor\n- 使用/dev/cgroups的hostPath允许 pod 指定给定的 hostPath 是否应该在 pod运行之前存在，是否应该创建，以及它应该以什么形式存在\n\n3. 配置hostPath存储卷的嵌套字段共有两个：一个是用于指定工作节点上的目录路径的必选字段path；另一个是指定存储卷类型的type，它支持使用的卷类型包含如下几种。\n\n- DirectoryOrCreate：指定的路径不存时自动将其创建为权限是0755的空目录，属主属组均为kubelet。\n- Directory：必须存在的目录路径。\n- FileOrCreate：指定的路径不存时自动将其创建为权限是0644的空文件，属主和属组同是kubelet。\n- File：必须存在的文件路径。\n- Socket：必须存在的Socket文件路径。\n- CharDevice：必须存在的字符设备文件路径。\n- BlockDevice：必须存在的块设备文件路径。\n\n4. 使用这种卷类型的注意：\n\n- 由于每个节点上的文件都不同，具有相同配置（例如从 podTemplate 创建的）的 pod在不同节点上的行为可能会有所不同\n- 当 Kubernetes 按照计划添加资源感知调度时，将无法考虑hostPath使用的资源\n- 在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root身份运行进程，或修改主机上的文件权限以便写入hostPath卷\n\n# 二、实例\n\n\n> 存储卷名称为html，将本地/html下的文件挂载于容器nginx的/usr/share/nginx/html目录下，向index.html文件中追加时间信息，而容器nginx中的nginx进程则以其为站点主页。\n\n\n\n1. 提前在每个节点创建/html目录，并生成index.html文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199463708-c7f624e5-94b6-4bf8-b289-aa8efd964b4a.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=437&size=393875&status=done&style=none&width=437)\n\n2. 创建pod资源清单\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199480199-0f654744-8c85-4f7e-ac2f-4599a2eea1c4.png#align=left&display=inline&height=364&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=364&originWidth=460&size=393875&status=done&style=none&width=460)\n\n3. 访问验证\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199645221-9f4abee8-1045-46c4-aa90-bbbb31496407.png#align=left&display=inline&height=43&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=43&originWidth=416&size=393875&status=done&style=none&width=416)\n\n4. 进入容器修改index.html文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199669305-886fe79e-bb47-4052-98eb-856c3d653e8b.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=460&size=393875&status=done&style=none&width=460)\n\n5. 宿主机查看文件是否更改\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199692831-1caf46fd-d2b7-4128-aede-e1c2a2c17014.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=395&size=393875&status=done&style=none&width=395)', 25, 0, 0, '2020-11-01 18:22:01.584043', '2021-01-26 05:42:09.377567', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (85, '存储-网络存储卷', 'Kubernetes拥有众多类型的用于适配专用存储系统的网络存储卷。这类存储卷包括传统的NAS或SAN设备（如NFS、iSCSI、fc）、分布式存储（如GlusterFS、RBD）、云端存储（如gcePersistentDisk、azureDisk、cinder和awsElasticBlockStore）以及建构在各类存储系统之上的抽象管理层（如flocker、portworxVolume和vsphereVolume）等。', 'cover/2020_11_01_18_23_03_733347.jpg', '[TOC]\n\n> Kubernetes拥有众多类型的用于适配专用存储系统的网络存储卷。这类存储卷包括传统的NAS或SAN设备（如NFS、iSCSI、fc）、分布式存储（如GlusterFS、RBD）、云端存储（如gcePersistentDisk、azureDisk、cinder和awsElasticBlockStore）以及建构在各类存储系统之上的抽象管理层（如flocker、portworxVolume和vsphereVolume）等。\n\n\n\n# 一、NFS存储卷\n\n\n1. Kubernetes的NFS存储卷用于将某事先存在的NFS服务器上导出（export）的存储空间挂载到Pod中以供容器使用。与emptyDir不同的是，NFS存储卷在Pod对象终止后仅是被卸载而非删除。另外，NFS是文件系统级共享服务，它支持同时存在的多路挂载请求。\n1. 定义NFS存储卷时，常用到以下字段。\n\n- server <string>:NFS服务器的IP地址或主机名，必选字段。\n- path <string>:NFS服务器导出（共享）的文件系统路径，必选字段。\n- readOnly <boolean>：是否以只读方式挂载，默认为false。\n\n3. 示例\n\n- Pod资源拥有一个关联至NFS服务器192.168.10.110的存储卷，Redis容器将其挂载于/data目录上，它是运行于容器中的redis-server数据的持久保存位置。\n- 创建pod资源清单\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204302441-cd531776-3b59-4b69-bf69-67bbcdb4d597.png#align=left&display=inline&height=512&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=512&originWidth=389&size=393875&status=done&style=none&width=389)\n\n- 通过其命令客户端redis-cli创建测试数据\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204321708-c395f70e-983f-4170-9dd4-58c9b554318c.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=184&originWidth=660&size=393875&status=done&style=none&width=660)\n\n- 删除pod，重新创建，查看redis数据\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204337393-4c9e2084-5cc0-40a3-a0fa-357a3f0e1b5c.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=657&size=393875&status=done&style=none&width=657)\n\n- 这表明在删除Pod资源时，其关联的外部存储卷并不会被一同删除。\n\n# 二、RBD存储卷\n\n1. Ceph是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和REST三种存储接口。它是一个高度可配置的系统，并提供了一个命令行界面用于监视和控制其存储集群。\n1. 要配置Pod资源使用RBD存储卷，需要事先满足如下几个前提条件。\n\n- 存在某可用的Ceph RBD存储集群，否则就需要创建一个。\n- 在Ceph RBD集群中创建一个能满足Pod资源数据存储需要的存储映像（image）。\n- 在Kubernetes集群内的各节点上安装Ceph客户端程序包（ceph-common）。\n\n3. 在配置RBD类型的存储卷时，需要指定要连接的目标服务器和认证信息等，这一点通常使用以下嵌套字段进行定义。\n\n- monitors <[]string>:Ceph存储监视器，逗号分隔的字符串列表；必选字段。\n- image <string>:rados image的名称，必选字段。\n- pool <string>:rados存储池名称，默认为RBD。\n- user <string>:rados用户名，默认为admin。\n- keyring <string>:RBD用户认证时使用的keyring文件路径，默认为/etc/ceph/keyring。\n- secretRef <Object>:RBD用户认证时使用的保存有相应认证信息的Secret对象，会覆盖由keyring字段提供的密钥信息。\n- readOnly <boolean>：是否以只读的方式进行访问。\n- fsType：要挂载的存储卷的文件系统类型，至少应该是节点操作系统支持的文件系统，如ext4、xfs、ntfs等，默认为ext4。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vol-rbd-pod\nspec:\n  containers:\n  - name: redis\n    image: redis:4-alpine\n    ports:\n    - containerPort: 6379\n      name: redisport\n    volumeMounts:\n    - mountPath: /data\n      name: redis-rbd-vol\n  volumes:\n    - name: redis-rbd-vol\n      rbd:\n        monitors:\n        - \'172.16.0.56:6789\'\n        - \'172.16.0.57:6789\'\n        - \'172.16.0.58:6789\'\n        pool: kube\n        image: redis\n        fsType: ext4\n        readOnly: false\n        user: admin\n        secretRef:\n            name: ceph-secret\n```', 30, 0, 0, '2020-11-01 18:23:44.730627', '2021-01-26 11:48:01.498055', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (86, '存储-持久存储卷', '本文主要从以下几个方面介绍k8s中的持久存储卷的使用：一、概念', 'cover/2020_11_01_18_26_12_367027.jpg', '[TOC]\n\n# 一、概念\n\n\n1. PersistentVolume（PV）\n   是由管理员设置的存储，它是群集的一部分。它是对底层共享存储的抽象，将共享存储作为一种可由用户申请使用的资源。PV 是Volume 之类的卷插件，但具有独立于使用 PV 的 Pod的生命周期。PV是集群级别的资源，不属于任何名称空间\n1. pv与volume区别\n\n- PV只能是网络存储（区别于上述的hostPath本地存储），不属于任何Node，但可以在每个Node上访问。\n- PV并不是定义在Pod上的，而是独立于Pod之外定义。\n- PV的生命周期与Pod是独立的。\n\n3. PersistentVolumeClaim（PVC）\n   是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或只读多次模式挂载）\n3. 静态 pv\n   集群管理员创建一些 PV。它们带有可供群集用户使用的实际存储的细节。它们存在于Kubernetes API 中，可用于消费\n3. 动态pv\n   当管理员创建的静态 PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试动态地为 PVC创建卷。此配置基于StorageClasses：PVC 必须请求\n   [存储类]，并且管理员必须创建并配置该类才能进行动态创建。声明该类为\"\"可以有效地禁用其动态配置要启用基于存储级别的动态存储配置，集群管理员需要启用 API server上的DefaultStorageClass[准入控制器]。例如，通过确保DefaultStorageClass位于API server组件的--admission-control标志，使用逗号分隔的有序值列表中，可以完成此操作\n3. pv与pvc绑定\n   master 中的控制环路监视新的 PVC，寻找匹配的PV（如果可能），并将它们绑定在一起。如果为新的 PVC 动态调配PV，则该环路将始终将该 PV 绑定到PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦 PV和 PVC 绑定后，PersistentVolumeClaim绑定是排他性的，不管它们是如何绑定的。PVC 跟PV 绑定是一对一的映射\n3. 持久化卷声明的保护\n   PVC 保护的目的是确保由 pod 正在使用的 PVC不会从系统中移除，因为如果被移除的话可能会导致数据丢失当启用PVC 保护 alpha功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC的删除将被推迟，直到 PVC 不再被任何 pod 使用\n3. PVC、PV、StorageClass关系\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204659479-e39d12d7-3b95-4a93-b62d-291bf8ec5ced.png#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=679&originWidth=967&size=393875&status=done&style=none&width=967)\n\n- PVC：Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。\n- PV ：具体的 Volume 的属性，比如 Volume的类型、挂载目录、远程存储服务器地址等。\n- StorageClass：充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和PVC，才可以绑定在一起。当然，StorageClass 的另一个重要作用，是指定 PV 的Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning的话，Kubernetes 就可以自动为你创建 PV 了。\n\n# 二、创建PV\n\n1. PV容量(capacity)：设定当前PV的容量，单位为Gi、Mi\n1. 访问模式：PersistentVolume可以以资源提供者支持的任何方式挂载到主机上。每个PV的访问模式都将被设置为该卷支持的特定模式。\n\n| Volume Plugin        | ReadWriteOnce         | ReadOnlyMany          | ReadWriteMany                      |\n| -------------------- | --------------------- | --------------------- | ---------------------------------- |\n| AWSElasticBlockStore | ✓                     | -                     | -                                  |\n| AzureFile            | ✓                     | ✓                     | ✓                                  |\n| AzureDisk            | ✓                     | -                     | -                                  |\n| CephFS               | ✓                     | ✓                     | ✓                                  |\n| Cinder               | ✓                     | -                     | -                                  |\n| CSI                  | depends on the driver | depends on the driver | depends on the driver              |\n| FC                   | ✓                     | ✓                     | -                                  |\n| FlexVolume           | ✓                     | ✓                     | depends on the driver              |\n| Flocker              | ✓                     | -                     | -                                  |\n| GCEPersistentDisk    | ✓                     | ✓                     | -                                  |\n| Glusterfs            | ✓                     | ✓                     | ✓                                  |\n| HostPath             | ✓                     | -                     | -                                  |\n| iSCSI                | ✓                     | ✓                     | -                                  |\n| Quobyte              | ✓                     | ✓                     | ✓                                  |\n| NFS                  | ✓                     | ✓                     | ✓                                  |\n| RBD                  | ✓                     | ✓                     | -                                  |\n| VsphereVolume        | ✓                     | -                     | - (works when Pods are collocated) |\n| PortworxVolume       | ✓                     | -                     | ✓                                  |\n| ScaleIO              | ✓                     | ✓                     | -                                  |\n| StorageOS            | ✓                     | -                     | -                                  |\n\n\n\n- ReadWriteOnce——该卷可以被单个节点以读/写模式挂载\n- ReadOnlyMany——该卷可以被多个节点以只读模式挂载\n- ReadWriteMany——该卷可以被多个节点以读/写模式挂载\n\n3. 在命令行中，访问模式缩写为：\n\n- RWO - ReadWriteOnce\n- ROX - ReadOnlyMany\n- RWX - ReadWriteMany\n\n4. persistentVolumeReclaimPolicy（回收策略）\n\n- Retain（保留）——管理员手动回收\n- Recycle（回收）——基本擦除（rm -rf /thevolume/*），目前仅NFS和hostPath支持此操作。\n- Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和OpenStack Cinder 卷）将被删除\n\n5. StorageClass的名称(storageClassName)：当前PV所属的StorageClass的名称；pvc与pv绑定时根据此name值匹配\n5. 卷可以处于以下的某种状态：\n\n- Available（可用）——一块空闲资源还没有被任何声明绑定\n- Bound（已绑定）——卷已经被声明绑定\n- Released（已释放）——声明被删除，但是资源还未被集群重新声明\n- Failed（失败）——该卷的自动回收失败命令行会显示绑定到 PV 的 PVC 的名称\n\n7. 示例\n\n- 定义了一个使用NFS存储后端的PV，空间大小为10GB，支持多路的读写操作。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204799340-b2f90ca3-8702-4b19-a7ec-826825ba637c.png#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=378&originWidth=495&size=393875&status=done&style=none&width=495)\n\n- 查看创建的pv信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204827887-5693ba9c-1700-4f2d-a839-cb7bc3108cdb.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=928&size=393875&status=done&style=none&width=928)\n\n# 三、创建pvc\n\n1. PersistentVolumeClaim是存储卷类型的资源，它通过申请占用某个PersistentVolume而创建，它与PV是一对一的关系，用户无须关心其底层实现细节。申请时，用户只需要指定目标空间的大小、访问模式、PV标签选择器和StorageClass等相关信息即可。\n1. PVC的Spec字段的可嵌套字段具体如下。\n\n- accessMode：当前PVC的访问模式，其可用模式与PV相同。\n- resources：当前PVC存储卷需要占用的资源量最小值\n- selector：绑定时对PV应用的标签选择器（matchLabels）或匹配条件表达式（matchEx-pressions），用于挑选要绑定的PV；如果同时指定了两种挑选机制，则必须同时满足两种选择机制的PV才能被选出。\n- storageClassName：所依赖的存储类的名称。\n- volumeName：用于直接指定要绑定的PV的卷名。\n\n3. 示例\n\n- 安装nfs服务器\n\n```bash\nyum install -y nfs-common nfs-utils rpcbind\nmkdir /nfsdata\nchmod 666 /nfsdata\nchown nfsnobody /nfsdata\ncat /etc/exports\n/nfsdata *(rw,no_root_squash,no_all_squash,sync)\nsystemctl start rpcbind\nsystemctl enable rpcbind\nsystemctl start nfs\nsystemctl enable nfs\n```\n\n- 客户端测试\n  `showmount -e 192.168.10.110` \n  `mount -t nfs 192.168.10.110:/nfsdata /mnt` \n- 创建pvc资源清单，绑定release为stable且storageClassName为slow的pv\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204913606-3af22987-0ce9-4b94-9c5a-9baa256a8209.png#align=left&display=inline&height=351&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=351&originWidth=357&size=393875&status=done&style=none&width=357)\n\n- 查看pvc详细信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204942724-69897a11-3e03-4399-96ba-30c0beb7d224.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=806&size=393875&status=done&style=none&width=806)\n\n# 四、pod中使用pvc\n\n1. 在Pod资源中调用PVC资源，只需要在定义volumes时使用persistentVolumeClaims字段嵌套指定两个字段即可，具体如下。\n\n- claimName：要调用的PVC存储卷的名称，PVC卷要与Pod在同一名称空间中。\n- readOnly：是否将存储卷强制挂载为只读模式，默认为false。\n\n2. 示例\n\n- 清单定义了一个Pod资源，调用前面刚刚创建的名为pv-nfs的PVC资源：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204982524-a526ad83-f400-4529-8dd8-a8bacfc1e96d.png#align=left&display=inline&height=421&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=421&originWidth=365&size=393875&status=done&style=none&width=365)\n\n- 进入pod添加测试数据\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205004635-b5f9b30e-8dfb-4cb1-ad71-c4556daf2b44.png#align=left&display=inline&height=232&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=232&originWidth=652&size=393875&status=done&style=none&width=652)\n\n- 删除pod，重新创建，查看数据\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205019145-5b5110d9-013a-4366-ac12-2f036c807fbe.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=657&size=393875&status=done&style=none&width=657)\n\n# 五、PV和PVC的生命周期\n\n\n1. 存储供给\n   存储供给（Provisioning）是指为PVC准备可用PV的机制。Kubernetes支持静态供给和动态供给。\n\n- 静态供给\n  静态供给是指由集群管理员手动创建一定数量的PV的资源供应方式。这些PV负责处理存储系统的细节，并将其抽象成易用的存储资源供用户使用。\n- 动态供给\n  不存在某静态的PV匹配到用户的PVC申请时，Kubernetes集群会尝试为PVC动态创建符合需求的PV，此即为动态供给。这种方式依赖于存储类的辅助，PVC必须向一个事先存在的存储类发起动态分配PV的请求，没有指定存储类的PVC请求会被禁止使用动态创建PV的方式。另外，为了支持使用动态供给机制，集群管理员需要为准入控制器（admission controller）启用“DefaultStorageClass”选项\n\n2. 存储绑定\n   用户基于一系列存储需求和访问模式定义好PVC后，Kubernetes系统的控制器即会为其查找匹配的PV，并于找到之后在此二者之间建立起关联关系，而后它们二者之间的状态即转为“绑定”（Binding）。若PV是为PVC而动态创建的，则该PV专用于其PVC。\n   若是无法为PVC找到可匹配的PV，则PVC将一直处于未绑定（unbound）状态，直到有符合条件的PV出现并完成绑定方才可用。\n\n- 存储使用（Using）\n  Pod资源基于persistenVolumeClaim卷类型的定义，将选定的PVC关联为存储卷，而后即可为内部的容器所使用。对于支持多种访问模式的存储卷来说，用户需要额外指定要使用的模式。一旦完成将存储卷挂载至Pod对象内的容器中，其应用即可使用关联的PV提供的存储空间。\n- PVC保护（Protection）\n  为了避免使用中的存储卷被移除而导致数据丢失，Kubernetes自1.9版本起引入了“PVC保护机制”。启用了此特性后，万一有用户删除了仍处于某Pod资源使用中的PVC时，Kubernetes不会立即予以移除，而是推迟到不再被任何Pod资源使用后方才执行删除操作。处于此种阶段的PVC资源的status字段为“Termination”，并且其Finalizers字段中包含“kubernetes.io/pvc-protection”。\n\n3. 存储回收（Reclaiming）\n   完成存储卷的使用目标之后，即可删除PVC对象以便进行资源回收。不过，至于如何操作则取决于PV的回收策略。目前，可用的回收策略有三种：Retained、Recycled和Deleted。\n\n- 留存（Retain）\n  留存策略意味着在删除PVC之后，Kubernetes系统不会自动删除PV，而仅仅是将它置于“释放”（released）状态。不过，此种状态的PV尚且不能被其他PVC申请所绑定，因为此前的申请生成的数据仍然存在，需要由管理员手动决定其后续处理方案。这就意味着，如果想要再次使用此类的PV资源，则需要由管理员按下面的步骤手动执行删除操作。\n  1）删除PV，这之后，此PV的数据依然留存于外部的存储之上。\n  2）手工清理存储系统上依然留存的数据。\n  3）手工删除存储系统级的存储卷（例如，RBD存储系统上的image）以释放空间，以便再次创建，或者直接将其重新创建为PV。\n- 回收（Recycle）\n  如果可被底层存储插件支持，资源回收策略会在存储卷上执行数据删除操作并让PV资源再次变为可被Claim。另外，管理员也可以配置一个自定义的回收器Pod模板，以便执行自定义的回收操作。不过，此种回收策略行将废弃。\n- 删除（Delete）\n  对于支持Deleted回收策略的存储插件来说，在PVC被删除后会直接移除PV对象，同时移除的还有PV相关的外部存储系统上的存储资产（asset）。支持这种操作的存储系统有AWS\n  EBS、GCE PD、Azure\n  Disk或Cinder。动态创建的PV资源的回收策略取决于相关存储类上的定义，存储类上相关的默认策略为Delete，大多数情况下，管理员都需要按用户期望的处理机制修改此默认策略，以免导致数据非计划内的误删除。\n\n4. 扩展PVC\n   Kubernetes自1.8版本起增加了扩展PV空间的特性，截至目前，它所支持的扩展PVC机制的存储卷共有以下几种。\n\n- gcePersistentDisk\n- awsElasticBlockStore\n- Cinder\n- glusterfs\n- rbd\n  “PersistentVolumeClaimResize”准入插件负责对支持空间大小变动的存储卷执行更多的验证操作，管理员需要事先启用此插件才能使用PVC扩展机制，那些将“allowVolume\n  Expansion”字段的值设置为“true”的存储类即可动态扩展存储卷空间。随后，用户改动Claim请求更大的空间即能触发底层PV空间扩展从而带来PVC存储卷的扩展。\n  对于包含文件系统的存储卷来说，只有在有新的Pod资源基于读写模式开始使用PVC时才会执行文件系统的大小调整操作。换句话说，如果某被扩展的存储卷已经由Pod资源所使用，则需要重建此Pod对象才能触发文件系统大小的调整操作。支持空间调整的文件系统仅有XFS和EXT3/EXT4。', 81, 0, 0, '2020-11-01 18:27:13.327388', '2021-01-26 14:08:19.103849', 1, 6, 0, 1, 1);
INSERT INTO `blog_article` VALUES (87, '存储-downwardAPI存储卷', '运行于Kubernetes的Pod对象中的容器化应用偶尔也需要获取其所属Pod对象的IP、主机名、标签、注解、UID、请求的CPU及内存资源量及其限额，甚至是Pod所在的节点名称等，容器可以通过环境变量或downwardAPI存储卷访问此类信息，不过，标签和注解仅支持通过存储卷暴露给容器。', 'cover/2020_11_01_18_28_35_903635.jpg', '\n> 运行于Kubernetes的Pod对象中的容器化应用偶尔也需要获取其所属Pod对象的IP、主机名、标签、注解、UID、请求的CPU及内存资源量及其限额，甚至是Pod所在的节点名称等，容器可以通过环境变量或downwardAPI存储卷访问此类信息，不过，标签和注解仅支持通过存储卷暴露给容器。\n\n\n\n# 一、环境变量式元数据注入\n\n1. 因为在进程启动完成后将无法再向其告知变量值的变动，于是，环境变量也就不支持中途的更新操作。\n1. 可通过fieldRef字段引用的信息。\n\n- spec.nodeName：节点名称。\n- status.hostIP：节点IP地址。\n- metadata.name:Pod对象的名称。\n- metadata.namespace:Pod对象隶属的名称空间。\n- status.podIP:Pod对象的IP地址。\n- spec.serviceAccountName:Pod对象使用的ServiceAccount资源的名称。\n- metadata.uid:Pod对象的UID。\n- metadata.labels[\'<KEY>\']:Pod对象标签中的指定键的值\n- metadata.annotations[\'<KEY>\']:Pod对象注解信息中的指定键的值\n  通过resourceFieldRef字段引用的信息是指当前容器的资源请求及资源限额的定义，包括requests.cpu、limits.cpu、requests.memory和limits.memory四项。\n\n3. 示例\n   资源配置清单中定义的Pod对象通过环境变量向容器env-test-container中注入了Pod对象的名称、隶属的名称空间、标签app的值以及容器自身的CPU资源限额和内存资源请求等信息：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205214421-989c895f-25c5-419a-a9fc-bacde06932dd.png#align=left&display=inline&height=850&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=850&originWidth=479&size=393875&status=done&style=none&width=479)\n\n- 查看环境变量信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205235343-edfe6078-e574-4ee7-aedf-608cbe24b4a5.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=664&size=393875&status=done&style=none&width=664)\n\n\n# 二、存储卷式元数据注入\n\n\n1. 在downwardAPI存储卷中使用fieldRef引用如下两个数据源。\n\n- metadata.labels:Pod对象的所有标签信息。\n- metadata.annotations:Pod对象的所有注解信息。\n\n2. 示例\n   清单中定义的Pod对象通过downwardAPI存储卷向容器volume-test-container中注入了Pod对象隶属的名称空间、标签、注解以及容器自身的CPU资源限额和内存资源请求等信息。存储卷在容器中的挂载点为/etc/podinfo目录\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205259501-2659279b-d1e8-4092-8bec-661a77dc438e.png#align=left&display=inline&height=791&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=791&originWidth=432&size=393875&status=done&style=none&width=432)\n\n\n- 测试访问上述的映射文件，例如，查看Pod对象的标签列表：\n\n```bash\nkubectl exec dapi-vol-pod -- cat /etc/podinfo/pod_labels\napp=\"dapi-vol-pod\"\nrack=\"rack-101\"\nzone=\"east-china\"\n```\n\n', 24, 0, 0, '2020-11-01 18:28:41.175333', '2021-01-26 11:40:31.572630', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (88, '网络-网络概述', 'Kubernetes的网络通信主要分为以下几种情况：', 'cover/2020_11_01_18_36_04_180209.jpg', '[TOC]\n\n# 一、概述\n\n1. k8s网络特征\n\n- 每个POD 具有集群内唯一IP\n- 所有POD 通过IP 直接访问其他POD，不管POD 是否在同一台物理机上\n- POD 内的所有容器共享一个网络堆栈, POD 内的容器, 都可以使用localhost 来访问pod 内的其他容器。\n\n2. Kubernetes的网络通信主要分为以下几种情况：\n\n- Pod内容器之间的通信\n- Pod到Pod之间的通信\n- Pod到Service之间的通信\n- 集群外部与内部组件之间的通信\n\n# 二、KubeProxy的实现模式\n\n1. 当service account创建的时候，Service Controller和EndPoints Controller就会被触发更新一些资源，例如基于Service中配置的Pod的selector给每一个Pod创建一个EndPoint资源并存入etcd，kube-proxy还会更新iptables的chain规则生成基于Service的Cluster IP链路到对应Pod的链路规则。\n1. 接下来集群内的一个pod想访问某个服务，直接cluster ip:port即可基于iptables的链路将请求发送到对应的Pod，这一层有两种挑选pod的算法，轮询(Round Robin)和亲和度匹配(Session Affinity)。当然，除了这种iptabels的模式，还有一种比较原始的方式就是用户态的转发。\n1. Kube-Proxy 会为每个 Service 随机监听一个端口 (Proxy Port)，并增加一条IPtables 规则。从客户端到 ClusterIP:Port 的报文都会被重定向到 Proxy Port，Kube-Proxy 收到报文后，通过 Round Robin (轮询) 或者 Session Affinity（会话亲和力，即同一 Client IP 都走同一链路给同一 Pod 服务）分发给对应的 Pod。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205669464-b8838320-edc4-4936-8e45-b642e51dd99a.png#align=left&display=inline&height=564&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=564&originWidth=723&size=393875&status=done&style=none&width=723)\n\n\n# 三、CNI插件\n\n1. Kubernetes设计了网络模型，但将其实现交给了网络插件。于是，各种解决方案不断涌现。为了规范及兼容各种解决方案，CoreOS和Google联合制定了CNI（Container Network Interface）标准，旨在定义容器网络模型规范。\n1. 它连接了两个组件：容器管理系统和网络插件。它们之间通过JSON格式的文件进行通信，以实现容器的网络功能。具体的工作均由插件来实现，包括创建容器netns、关联网络接口到对应的netns以及为网络接口分配IP等。\n1. CNI的基本思想是：容器运行时环境在创建容器时，先创建好网络名称空间（netns），然后调用CNI插件为这个netns配置网络，而后再启动容器内的进程。\n1. 常用插件\n\n- Flannel（简单、使用居多）：基于Vxlan技术（叠加网络+二层隧道），不支持网络策略\n- Calico（较复杂，使用率少于Flannel）：也可以支持隧道网络，但是是三层隧道（IPIP），支持网络策略\n- Calico项目既能够独立地为Kubernetes集群提供网络解决方案和网络策略，也能与flannel结合在一起，由flannel提供网络解决方案，而Calico此时仅用于提供网络策略，这时我们也可以将Calico称为Canal。', 31, 0, 0, '2020-11-01 18:37:11.169952', '2021-01-26 05:42:19.811992', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (89, '网络-网络类型', '本文主要从一、Pod内容器之间的通信：localhost', 'cover/2020_11_01_20_52_34_096267.jpg', '[TOC]\n\n# 一、Pod内容器之间的通信：localhost\n\n\n1. 在 Kubernetes 的世界里，IP 是以 Pod 为单位进行分配的。一个 Pod内部的所有容器共享一个网络堆栈（实际上就是一个网络命名空间，包括它们的 IP\n   地址、网络设备、配置等都是共享的）。\n1. 在同一个pod内由pause镜像启动的容器。所有运行于同一个Pod内的容器与同一主机上的多个进程类似，彼此之间可通过lo接口完成交互。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205916825-3c46387d-c2b9-45da-b985-d5fb87aa223b.png#align=left&display=inline&height=785&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=785&originWidth=624&size=393875&status=done&style=none&width=624)\n\n# 二、同一node上的Pod之间的通信：overlay network\n\n\n1. 同一个Node内的不同Pod之间可以直接采用对方Pod的IP地址通信，而且不需要使用其他发现机制，例如DNS、Consul或者etcd。\n1. Pod1和Pod2都是通信veth\n   pair连接到同一个docker0网桥上，它们的IP地址IP1、IP2都是从docker0网段上动态获取的，它们和网桥本身的IP3是同一个网段的。由于Pod1和Pod2处于同一局域网内，它们之间可以通过docker0作为路由量进行通信。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205963977-0e136342-5cbd-483e-a8d4-a8ceb3e8bae1.png#align=left&display=inline&height=455&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=455&originWidth=678&size=393875&status=done&style=none&width=678)\n\n\n# 三、不同node上的Pod之间的通信：iptables规则\n\n\n1. 在Kubernetes的网络世界中，Pod之间假设是通过访问对方的Pod IP进行通信的，而不同Node之间的通信只能通过Node的物理网卡进行，Pod的IP地址是由各Node上的docker0网桥动态分配的。我们想要实现跨Node的Pod之间的通信，至少需要满足下面三个条件：\n\n- 知道Pod IP 和Node IP之间的映射关系，通过Node IP转发到Pod IP；\n- 在整个Kubernetes集群中对Pod的IP分配不能出现冲突；\n- 从Pod中发出的数据包不应该进行NAT地址转换。\n\n2. Kubernetes会记录所有正在运行的Pod的IP分配信息，并将这些信息保存到etcd中（作为Service的Endpoint），这样我们就可以知道Pod\n   IP和Node IP之间的映射关系。\n2. 以Flannel为例，Flannel实现的容器的跨主机通信通过如下过程实现：\n\n- 每个主机上安装并运行etcd和flannel；\n- 在etcd中规划配置所有主机的docker0子网范围；\n- 每个主机上的flanneld根据etcd中的配置，为本主机的docker0分配子网，保证所有主机上的docker0网段不重复，并将结果（即本主机上的docker0子网信息和本主机IP的对应关系）存入etcd库中，这样etcd库中就保存了所有主机上的docker子网信息和本主机IP的对应关系；\n- 当需要与其他主机上的容器进行通信时，查找etcd数据库，找到目的容器的子网所对应的outip（目的宿主机的IP）；\n- 将原始数据包封装在VXLAN或UDP数据包中，IP层以outip为目的IP进行封装；\n- 由于目的IP是宿主机IP，因此路由是可达的；\n- VXLAN或UDP数据包到达目的宿主机解封装，解出原始数据包，最终到达目的容器。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206071139-bb287a80-c537-4d85-8f27-8877679a031a.png#align=left&display=inline&height=776&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=776&originWidth=1278&size=468500&status=done&style=none&width=1278)\n\n\n# 四、Service与Pod间的通信：iptables规则\n\n\n1. 集群网络需要在启动kube-apiserver时经由“--service-cluster-ip-range”选项进行指定，如10.96.0.0/12，而每个Service对象在此网络中均拥一个称为Cluster-IP的固定地址。\n1. 管理员或用户对Service对象的创建或更改操作由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将其定义为相应节点上的iptables规则或ipvs规则，借此完成从Service的Cluster-IP与Pod-IP之间的报文转发\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206146014-8923ae48-0c8c-4974-b572-2562fb1891a6.png#align=left&display=inline&height=215&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=215&originWidth=553&size=468500&status=done&style=none&width=553)\n\n\n# 五、集群外部到Pod对象之间的通信\n\n\n1. 将集群外部的流量引入到Pod对象的方式有受限于Pod所在的工作节点范围的节点端口（nodePort）和主机网络（hostNetwork）两种，以及工作于集群级别的NodePort或LoadBalancer类型的Service对象。\n1. 即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。', 71, 2, 0, '2020-11-01 18:39:00.200769', '2021-01-26 19:52:32.755037', 1, 1, 0, 1, 1);
INSERT INTO `blog_article` VALUES (90, '网络-flannel网络插件', '为了解决docker主机默认使用同一个子网造成容器ip地址相同问题，预留使用一个网络，如10.244.0.0/16，而后自动为每个节点的Docker容器引擎分配一个子网，如10.244.1.0/24和10.244.2.0/24，并将其分配信息保存于etcd持久存储。', 'cover/2020_11_01_18_41_19_353345.jpg', '[TOC]\n\n# 一、原理\n\n1. 为了解决docker主机默认使用同一个子网造成容器ip地址相同问题，预留使用一个网络，如10.244.0.0/16，而后自动为每个节点的Docker容器引擎分配一个子网，如10.244.1.0/24和10.244.2.0/24，并将其分配信息保存于etcd持久存储。\n1. 为了解决因为在网络中缺乏路由信息而无法准确送达。flannel有着多种不同的处理方法，每一种处理方法也可以称为一种网络模型，或者称为flannel使用的后端。\n\n- VxLAN：虚拟可扩展局域网，采用的是MAC in UDP封装方式，将虚拟网络的数据帧添加到VxLAN首部后，封装在物理网络的UDP报文中，然后以传统网络的通信方式传送该UDP报文，待其到达目的主机后，去掉物理网络报文的头部信息以及VxLAN首部，然后将报文交付给目的终端，\n- Host\n  GateWay：通过在节点上创建到达目标容器地址的路由直接完成报文转发，因此这种方式要求各节点本身必须在同一个二层网络中，故该方式不太适用于较大的网络规模（大二层网络除外）。host-gw有着较好的转发性能，且易于设定，推荐对报文转发性能要求较高的场景使用。\n- UDP：使用普通UDP报文封装完成隧道转发，其性能较前两种方式要低很多，仅应该在不支持前两种方式的环境中使用。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206265281-757ac079-145a-4ae9-bcb9-99d786613138.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=285&originWidth=917&size=468500&status=done&style=none&width=917)\n\n# 二、配置清单\n\n1. 参考地址：[https://github.com/coreos/flannel/blob/master/Documentation/configuration.md](https://github.com/coreos/flannel/blob/master/Documentation/configuration.md)\n1. flannel的网络地址是10.244.0.0/16,默认每个子网的掩码长度为24\n1. K8s节点之间（Node）通过Vxlan技术进行通信，根据node情况，会把flannel的16位网络地址拆分成多个24位网络地址，供各Node进行分配\n1. 每个Node节点按序占用一个C类地址，对应节点上面的Podip是在该C类地址中按规则分配的。\n\n# 三、配置参数\n\n> flannel的配置信息保存于etcd的键名/coreos.com/network/config之下，可以使用etcd服务的客户端工具来设定或修改其可用的相关配置。\n\n1. Network：flannel于全局使用的CIDR格式的IPv4网络，字符串格式，此为必选键，余下的均为可选。\n1. SubnetLen：将Network属性指定的IPv4网络基于指定位的掩码切割为供各节点使用的子网，此网络的掩码小于24时（如16），其切割子网时使用的掩码默认为24位。\n1. SubnetMin：可用作分配给节点使用的起始子网，默认为切分完成后的第一个子网；字符串格式。\n1. SubnetMax：可用作分配给节点使用的最大子网，默认为切分完成后最大的一个子网；字符串格式。\n1. Backend：flannel要使用的后端类型，以及后端的相关配置，字典格式；VxLAN、host-gw和UDP后端各有其相关的参数。\n\n# 四、配置实例\n\n1. 全局网络为“10.244.0.0/16”，切分子网时用到的掩码长度为24，将相应的子网10.244.0.0/24-10.244.255.0/24分别分配给每一个工作节点使用，选择VxLAN作为使用的后端类型，并监听于8472端口\n\n```json\n{\n    \"Network\": \"10.244.0.0/16\",\n    \"SubnetLen\": 24,\n    \"SubnetMin\": \"10.244.0.0\",\n    \"SubnetMax\": \"10.244.255.0\",\n    \"Backend\": {\n        \"Type\": \"VxLAN\",\n        \"Port\": 8472\n    }\n}\n```', 34, 0, 0, '2020-11-01 18:41:22.698706', '2021-01-26 21:25:32.706928', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (91, '网络-网络策略', '用于控制分组的Pod资源彼此之间如何进行通信，以及分组的Pod资源如何与其他网络端点进行通信的规范。它用于为Kubernetes实现更为精细的流量控制，实现租户隔离机制。Kubernetes使用标准的资源对象“NetworkPolicy”供管理员按需定义网络访问控制策略', 'cover/2020_11_01_18_42_46_413954.jpg', '[TOC]\n# 一、概述\n\n\n1. 用于控制分组的Pod资源彼此之间如何进行通信，以及分组的Pod资源如何与其他网络端点进行通信的规范。它用于为Kubernetes实现更为精细的流量控制，实现租户隔离机制。Kubernetes使用标准的资源对象“NetworkPolicy”供管理员按需定义网络访问控制策略\n1. 核心概念\n\n![networkpolicy.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206460233-2a4e20f6-9577-4c40-9d91-1f7d3bbb5b36.png#align=left&display=inline&height=562&margin=%5Bobject%20Object%5D&name=networkpolicy.png&originHeight=562&originWidth=970&size=65787&status=done&style=none&width=970)\n\n- Ingress（类似安全组的内外网入）：能接受哪些客户端的访问（-from，ports：自己端口）\n- egress（类似安全组的内外网出）：能访问哪些目标（-to，ports：目标端口）\n- 每种方向的控制策略则包含“允许”和“禁止”两种。默认情况下，Pod处于非隔离状态，它们的流量可以自由来去。\n- 当对应区域有多条规则时，满足其一即可。\n\n# 二、配置网络策略\n\n\n1. 管理过程\n\n- 由namespaceSelector选定名称空间后，NetworkPolicy对象使用标签选择器由ipBlock或podSelector选择出一组Pod资源作为控制对象。\n- 一旦名称空间中有任何NetworkPolicy对象匹配了某特定的Pod对象，则该Pod将拒绝Network-Policy所不允许的一切连接请求，而那些未被任何NetworkPolicy对象匹配到的其他Pod对象仍可接受所有流量。\n- 一旦在spec.policyTypes中指定了生效的规则类型，却在networkpolicy.spec字段中嵌套定义了没有任何规则的Ingress或Egress字段时，则表示拒绝相关方向上的一切流量。\n\n2. 定义网络策略时常用到的术语及说明具体。\n\n- Pod组：由网络策略通过Pod选择器选定的一组Pod的集合，它们是规则生效的目标Pod；可由NetworkPolicy对象通过macthLabel或matchExpression选定。\n- Egress：出站流量，即由特定的Pod组发往其他网络端点的流量，通常由流量的目标网络端点（to）和端口（ports）来进行定义。\n- Ingress：入站流量，即由其他网络端点发往特定Pod组的流量，通常由流量发出的源站点（from）和流量的目标端口所定义。\n- 端口（ports）:TCP或UDP的端口号。\n- 端点（to,\n  from）：流量目标和流量源相关的组件，它可以是CIDR格式的IP地址块（ipBlock）、网络名称空间选择器（namespaceSelector）匹配的名称空间，或Pod选择器（podSelector）匹配的Pod组。\n  无论是Ingress还是Egress流量，与选定的某Pod组通信的另一方都可使用“网络端点”予以描述，它通常是某名称空间中的一个或一组Pod资源。\n\n3. 管控入站流量规则\n   在Ingress规则中嵌套from和ports字段即可匹配特定的入站流量，from字段的值是一个对象列表，它可嵌套使用ipBlock、namespaceSelector和podSelector字段来定义流量来源。\n\n| ipBlock           | Object | 根据IP地址或网络地址块选择流量源端点                         |\n| ----------------- | ------ | ------------------------------------------------------------ |\n| namespaceSelector | Object | 基于集群级别的标签挑选名称空间，它将匹配由此标签选择器选出的所有名称空间内的所有Pod对象；空值表示所有的名称空间，即源站点为所有名称空间内的所有Pod对象 |\n| podSelector       | Object | 于NetworkPolicy所在的当前名称空间内基于标签选择器挑选Pod资源，赋予字段以空值来表示挑选当前名称空间内的所有Pod对象 |\n| ports             | Object | 它嵌套port和protocol来定义流量的目标端口，即由NetworkPolicy匹配到的当前名称空间内的所有Pod资源上的端口 |\n| port              | string | 端口号或在Container上定义的端口名称，未定义时匹配所有端口    |\n| protocol          | string | 传输层协议的名称，TCP或UDP，默认为TCP                        |\n\n4. 管控出站流量规则\n   networkpolicy.spec中嵌套的Egress字段用于定义入站流量规则，就特定的Pod集合来说，出站流量一样默认处于放行状态，除非在所有入站策略中至少有一条规则能够明确匹配到它。Egress字段的值是一个字段列表，它主要由以下两个字段组成。\n\n| to    | Object | 由当前策略匹配到的Pod资源发起的出站流量的目标地址列表，多个项目之间为“或”（OR）关系；若未定义或字段值为空则意味着应用于所有目标地址（默认为不限制）；若明确给出了主机地址列表，则只有目标地址匹配列表中的主机地址的出站流量被放行 |\n| ----- | ------ | ------------------------------------------------------------ |\n| ports | Object | 出站流量的目标端口列表，多个端口之间为“或”（OR）关系；若未定义或字段值为空则意味着应用于所有端口（默认为不限制）；若明确给出了端口列表，则只有目标端口匹配列表中的端口的出站流量被放行 |\n\n\n\n# 三、Calico安装\n\n\n1. Calico官网：\n   [https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel](https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel)\n1. 安装calico的canal插件：\n   curl [https://docs.projectcalico.org/v3.10/manifests/canal.yaml](https://docs.projectcalico.org/v3.10/manifests/canal.yaml) -O\n\n- 如果使用的是pod cidr 10.244.0.0/16，请跳到下一步。如果您使用的是不同的pod cidr，请使用以下命令来设置包含pod cidr的环境变量pod\n  cidr，并将清单中的10.244.0.0/16替换为pod cidr。\n\nPOD_CIDR=\"<your-pod-cidr>\"\nsed -i -e \"s?10.244.0.0/16?$POD_CIDR?g\" canal.yaml\n\n3. 部署canal插件：\n   `kubectl apply -f canal.yaml` \n3. 使用kubectl get pods -n kube-system中查看安装进程。\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206605663-080f060c-6f71-4e6a-b409-c142f52ee640.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=742&size=468500&status=done&style=none&width=742)\n\n\n# 四、示例\n\n1. 管控入站流量-设置默认的Ingress策略\n\n- 通过policyTypes字段指明要生效Ingress类型的规则，但未定义任何Ingress字段，因此不能匹配到任一源端点，从而拒绝所有入站\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-ingress\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n```\n\n2. 管控入站流量-放行特定的入站流量\n\n- 将default名称空间中拥有标签“app=myapp”的Pod资源的80/TCP端口开放给10.244.0.0/16网络内除10.244.3.0/24子网中的所有源端点，以及当前名称空间中拥有标签“app=myapp”的所有Pod资源访问，其他未匹配到的源端点的流量默认为允许访问。\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-myapp-ingress\nnamespace: default\nspec:\npodSelector:\n  matchLabels:\n    app: myapp\npolicyTypes: [\"Ingress\"]\ningress:\n- from:\n  - ipBlock:\n      cidr: 10.244.0.0/16\n      except:\n      -10.244.3.0/24\n  - podSelector:\n      matchLabels:\n        app: myapp\n  ports:\n  - protocol: TCP\n    port: 80\n```\n\n3. 管控出站流量-设置默认的Egress策略\n\n- 通过policyTypes字段指明要生效Egress类型的规则，但未定义任何Egress字段，因此不能匹配到任何目标端点，从而拒绝所有的入站流量\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-egress\nspec:\n  podSelector: {}\n  policyTypes: [\"Egress\"]\n```\n\n4. 管控出站流量-管控出站流量-设置默认的Egress策略\n\n- 对来自拥有“app=tomcat”的Pod对象的，到达标签为“app=nginx”的Pod对象的80端口，以及到达标签为“app=mysql”的Pod对象的3306端口的流量给予放行\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-tomcat-egress\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: tomcat\n  policyTypes: [\"Egress\"]\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: nginx\n    ports:\n    - protocol: TCP\n      port: 80\n  - to:\n    - podSelector:\n        matchLabels:\n          app: mysql\n    ports:\n    - protocol: TCP\n      port: 3306\n```\n\n5. 隔离名称空间\n\n- 为default名称空间定义了相关的规则，在出站和入站流量默认均为拒绝的情况下，它用于放行名称空间内部的各Pod对象之间的通信，以及与kube-system名称空间内各Pod间的通信\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-deny-all\n  namespace: default\nspec:\n  policyTypes: [\"Ingress\", \"Egress\"]\n  podSelector: {}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-\n  namespace: default\nspec:\n  policyTypes: [\"Ingress\", \"Egress\"]\n  podSelector: {}\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchExpressions:\n        - key: name\n          operator: In\n          values: [\"default\", \"kube-system\"]\n  egress:\n  - to:\n    - namespaceSelector:\n        matchExpressions:\n        - key: name\n          operator: In\n          values: [\"default\", \"kube-system\"]\n```', 26, 0, 0, '2020-11-01 18:43:33.611813', '2021-01-26 05:42:30.528816', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (92, '网络-网络与策略实例', '假设有名为testing的名称空间内运行着一组nginx', 'cover/2020_11_01_18_45_20_180757.jpg', '[TOC]\n# 一、实现目标\n\n\n> 假设有名为testing的名称空间内运行着一组nginx Pod和一组myappPod。要求实现如下目标。\n\n1. myapp Pod仅允许来自nginx Pod的流量访问其80/TCP端口，但可以向nginx Pod的所有端口发出出站流量。\n1. nginx Pod允许任何源端点对其80/TCP端口的访问，并能够向任意端点发出出站流量。\n1. myapp Pod和nginx Pod都可与kube-system名称空间的任意Pod进行任何类型的通信，以便于可以使用由kube-dns提供的名称解析服务等。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206958257-269bd384-9409-4802-8f04-e890578c93fb.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=562&size=468500&status=done&style=none&width=562)\n\n# 二、创建基本环境\n\n\n1. 创建testing名称空间，为其添加标签“ns=kube-system”\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206983917-5a1fdfde-693d-4241-97f8-ceae3bb057e0.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=268&size=468500&status=done&style=none&width=268)\n\n2. 使用deployment控制器在testing空间创建nginx pod\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207001046-3b98e111-2a85-40f8-89d3-f25a5eacbc81.png#align=left&display=inline&height=494&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=494&originWidth=356&size=468500&status=done&style=none&width=356)\n\n3. 使用deployment控制器在testing空间创建myapp pod\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207019847-f800c0a9-1c74-48e2-a805-cb39906c379c.png#align=left&display=inline&height=491&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=491&originWidth=441&size=468500&status=done&style=none&width=441)\n\n4. 启动一个终端，在default名称空间中创建一个用于测试的临时交互式客户端：\n\n# kubectl run cirros-$RANDOM --namespace=default --rm -it --image=cirros -- sh\n\n- 分别测试访问nginx和myapp的服务，名称空间的默认网络策略为准许访问，接下来确认其访问请求可正常通过：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207041995-469fdb8e-be8d-4f2c-8055-3df92c32c98c.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=577&size=468500&status=done&style=none&width=577)\n\n# 三、定义默认网络策略并测试\n\n\n1. 将testing名称空间的入站及出站的默认策略修改为拒绝访问\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207099586-3bfd1f4a-a3b1-4ddd-a42b-6736b432270d.png#align=left&display=inline&height=242&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=242&originWidth=417&size=468500&status=done&style=none&width=417)\n\n2. 查看对应的netpol生成情况\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207118851-bcc044bf-7fb9-4bff-83fa-f81c5c751ff9.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=536&size=468500&status=done&style=none&width=536)\n\n3. 再一次进行访问测试：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207133888-53d875d9-090f-48ba-9b87-2b9a7ea8be18.png#align=left&display=inline&height=88&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=88&originWidth=218&size=468500&status=done&style=none&width=218)\n\n\n# 四、定义流量放行规则\n\n\n1. 配置清单，允许任何源端点对其80/TCP端口的访问\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207149721-0a9c05c5-8003-4089-a267-124da41be849.png#align=left&display=inline&height=486&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=486&originWidth=402&size=468500&status=done&style=none&width=402)\n\n- 测试客户端发起访问请求进行测试，nginx已经能够正常访问。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207166918-2b88010f-8ecd-412a-be75-a7be0a2cf9e4.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=364&size=468500&status=done&style=none&width=364)\n\n2. 放行testing名称空间中来自nginx Pod的发往myapp Pod的80/TCP的访问流量，以及myapp Pod发往nginx Pod的所有流量。允许myapp Pod与kube-system名称空间的任何Pod进行交互的所有流量：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207193630-415a5c1a-bedc-47d5-a849-cc62da82dc12.png#align=left&display=inline&height=729&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=729&originWidth=396&size=468500&status=done&style=none&width=396)\n\n- 测试客户端发起访问请求进行测试，myapp拒绝访问。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207211548-74a65f6b-01fa-4d5f-92c9-e6468c782635.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=217&size=468500&status=done&style=none&width=217)\n\n- 进入nginx空间访问myapp测试，允许访问。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207228730-3cea9023-3f87-4f38-b699-1002d3b3cfad.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=882&size=468500&status=done&style=none&width=882)', 24, 0, 0, '2020-11-01 18:44:55.614394', '2021-01-26 11:41:00.172013', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (93, '安全-访问控制', 'Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。API', 'cover/2020_11_01_18_47_00_771237.jpg', '[TOC]\n\n# 一、机制说明\n\n\n1. Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。API Server是集群内部各个组件通信的中介，也是外部控制的入口。所以 Kubernetes的安全机制基本就是围绕保护 API Server 来设计的。Kubernetes使用了认证（Authentication）、鉴权（Authorization）、准入控制（AdmissionControl）三步来保证API Server的安全\n1. API Server处理请求的过程中，认证插件负责鉴定用户身份，授权插件用于操作权限许可鉴别，而准入控制则用于在资源对象的创建、删除、更新或连接（proxy）操作时实现更精细的许可检查。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207401772-5ae6ca0a-ff22-42b8-b294-0775db03523b.png#align=left&display=inline&height=353&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=353&originWidth=811&size=468500&status=done&style=none&width=811)\n\n\n# 二、k8s中的用户概念\n\n\n> 客户端访问API服务的途径通常有三种：kubectl、客户端库或者直接使用REST接口进行请求，而可以执行此类请求的主体也被Kubernetes分为两类：现实中的“人”和Pod对象，它们的用户身份分别对应于常规用户（User Account）和服务账号（Service Account）。\n\n1. User Account（用户账号）：一般是指由独立于Kubernetes之外的其他服务管理的用户账号，例如由管理员分发的密钥、Keystone一类的用户存储（账号库）、甚至是包含有用户名和密码列表的文件等。Kubernetes中不存在表示此类用户账号的对象，因此不能被直接添加进Kubernetes系统中。\n1. Service Account（服务账号）：是指由Kubernetes API管理的账号，用于为Pod之中的服务进程在访问Kubernetes API时提供身份标识（identity）。Service Account通常要绑定于特定的名称空间，它们由API Server创建，或者通过API调用手动创建，附带着一组存储为Secret的用于访问API Server的凭据。\n\n# 三、Service Account\n\n1. Pod中的容器访问API Server。因为Pod的创建、销毁是动态的，所以要为它手动生成证书就不可行了。Kubenetes使用了Service Account解决Pod 访问API Server的认证问题\n1. ServiceAccount 中用到包含三个部分：Token、ca.crt、namespace\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207600564-97813084-0e3d-4f28-b7cf-c123de3bd43b.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=993&size=468500&status=done&style=none&width=993)\n\n- token：保存了Service Account的认证token，容器中的进程使用它向API\n  Server发起连接请求，进而由认证插件完成用户认证并将其用户名传递给授权插件。\n- ca.crt：根证书。用于Client端验证API Server发送的证书\n- namespace：标识这个service-account-token的作用域名空间\n\n3. 默认情况下，每个 namespace 都会有一个 ServiceAccount，如果 Pod 在创建时没有指定 ServiceAccount，就会使用 Pod 所属的 namespace 的ServiceAccount', 25, 0, 0, '2020-11-01 18:47:05.108200', '2021-01-26 11:41:19.342324', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (94, '安全-认证', '-', 'cover/2020_11_01_18_48_53_585630.jpg', '[TOC]\n\n# 一、认证\n\n1. 认证方式\n\n- HTTP Token 认证：用一个很长的特殊编码方式的并且难以被模仿的字符串 - Token来表达客户的一种方式。Token 是一个很长的很复杂的字符串，每一个 Token对应一个用户名存储在 API Server 能访问的文件中。当客户端发起 API调用请求时，需要在 HTTP Header 里放入 Token\n- HTTP Base 认证：用户名+密码用 BASE64 算法进行编码后的字符串放在 HTTP Request中的 HeatherAuthorization域里发送给服务端，服务端收到后进行编码，获取用户名及密码\n- HTTPS 证书认证：基于 CA 根证书签名的客户端身份认证方式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207985541-ea6e9dfc-cd71-4844-9b36-9f8ae890437d.png#align=left&display=inline&height=435&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=435&originWidth=702&size=468500&status=done&style=none&width=702)\n\n2. 安全性说明\n\n- Controller Manager、Scheduler 与 API Server 在同一台机器，所以直接使用 API Server 的非安全端口访问，--insecure-bind-address=127.0.0.1\n- kubectl、kubelet、kube-proxy 访问 API Server 就都需要证书进行 HTTPS 双向认证\n\n# 二、HTTPS认证\n\n\n1. 证书颁发\n\n- 手动签发：通过 k8s 集群的跟 ca 进行签发 HTTPS 证书\n- 自动签发：kubelet 首次访问 API Server 时，使用 token做认证，通过后，Controller Manager 会为kubelet生成一个证书，以后的访问都是用证书做认证了\n\n# 三、kubeconfig\n\n\n1. kubeconfig 文件包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context 信息（集群名称、用户名）。Kubenetes 组件通过启动时指定不同的 kubeconfig文件可以切换到不同的集群。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209072283-49a2e61e-7719-45e9-81c7-774fc3fbadea.png#align=left&display=inline&height=152&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=152&originWidth=493&size=468500&status=done&style=none&width=493)\n\n2. 查看kubeconfig文件内容\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209088933-ae7c8b7c-d82b-4999-aa09-e36f6bd876c8.png#align=left&display=inline&height=464&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=464&originWidth=498&size=468500&status=done&style=none&width=498)\n\n- clusters：集群列表，包含访问API Server的URL和所属集群的名称等。\n- users：用户列表，包含访问API Server时的用户名和认证信息。\n- contexts：kubelet的可用上下文列表，由用户列表中的某特定用户名称和集群列表中的某特定集群名称组合而成。\n- current-context：kubelet当前使用的上下文名称，即上下文列表中的某个特定项。\n\n3. kubectlconfig命令的常用操作\n\n- kubectl config view：打印kubeconfig文件内容。\n- kubectl config set-cluster：设置kubeconfig的clusters配置段。\n- kubectl config set-credentials：设置kubeconfig的users配置段。\n- kubectl config set-context：设置kubeconfig的contexts配置段。\n- kubectl config use-context：设置kubeconfig的current-context配置段。\n\n# 四、TLS bootstrapping机制\n\n\n1. 新的工作节点接入集群时，由kubelet自行生成私钥和证书签署请求，而后发送给集群上的证书签署进程（CA），由管理员验证请求后予以签署或直接控制器进程自动统一签署。这种方式即为Kubelet TLS Bootstrapping机制。\n\n# 五、总结\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209142277-0143f8fd-29b7-423f-9e4e-eadcd39465e1.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=181&originWidth=774&size=468500&status=done&style=none&width=774)', 21, 0, 0, '2020-11-01 18:48:55.822832', '2021-01-26 05:05:55.205152', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (95, '安全-鉴权', 'RBAC的基于“角色”（role）这一核心组件实现了权限指派，它为账号赋予一到多个角色从而让其具有角色之上的权限，其中的账号可以是用户账号、用户组、服务账号及其相关的组等，而同时关联至多个角色的账号所拥有的权限是多个角色之上的权限集合。', 'cover/2020_11_01_18_49_20_384178.jpg', '[TOC]\n\n# 一、RBAC 授权模式\n\n\n1. RBAC的基于“角色”（role）这一核心组件实现了权限指派，它为账号赋予一到多个角色从而让其具有角色之上的权限，其中的账号可以是用户账号、用户组、服务账号及其相关的组等，而同时关联至多个角色的账号所拥有的权限是多个角色之上的权限集合。\n1. RBAC具有如下优势\n\n- 对集群中的资源和非资源型URL的权限实现了完整覆盖。\n- 整个RBAC完全由少数几个API对象实现，而且同其他API对象一样可以用kubectl或API调用进行操作。\n- 支持权限的运行时调整，无须重新启动API Server。\n\n3. RBAC基本概念\n\n- Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。\n- Subject：被作用者，既可以是“人”，也可以是“机器”，也可以使你在 Kubernetes 里定义的“用户”。\n- RoleBinding：定义了“被作用者”和“角色”的绑定关系。\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209404236-d77d934e-9e7c-4ead-a78c-6a1fc35eff68.png#align=left&display=inline&height=461&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=461&originWidth=845&size=468500&status=done&style=none&width=845)\n\n\n4. RBAC 的 API 资源对象\n   RBAC 引入了 4个新的顶级资源对象：Role、ClusterRole、RoleBinding、ClusterRoleBinding，4种对象类型均可以通过 kubectl 与 API 操作\n\n- Role：作用于名称空间级别，用于定义名称空间内的资源权限集合\n- ClusterRole：用于集群级别的资源权限集合。常用于控制Role无法生效的资源类型，这包括集群级别的资源（如Nodes）、非资源类型的端点（如/healthz）和作用于所有名称空间的资源（例如，跨名称空间获取任何资源的权限）。\n- RoleBinding用于将Role上的许可权限绑定到一个或一组用户之上，它隶属于且仅能作用于一个名称空间。绑定时，可以引用同一名称中的Role，也可以引用集群级别的ClusterRole。\n- ClusterRoleBinding把ClusterRole中定义的许可权限绑定在一个或一组用户之上，它仅可以引用集群级别的ClusterRole。\n\n5. 支持的动作\n   create delete deletecollection get list patch update watch，bind等\n5. 支持的资源\n   “services”, “endpoints”, “pods“，\"deployments“ **“**jobs”，“configmaps”，“nodes”，“rolebindings”，“clusterroles”\n\n# 二、Role and ClusterRole\n\n1. Role表示一组规则权限，权限只会增加(累加权限)，不存在一个资源一开始就有很多权限而通过RBAC对其进行减少的操作\n1. Role 可以定义在一个 namespace 中，如果想要跨 namespace则可以创建ClusterRole，ClusterRole 具有与 Role相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的，ClusterRole可以用于:\n\n- 集群级别的资源控制( 例如 node 访问权限 )\n- 非资源型 endpoints( 例如/healthz访问 )\n- 所有命名空间资源控制(例如 pods )\n\n3. Role对象中的rules也称为PolicyRule，用于定义策略规则，不过它不包含规则应用的目标，其可以内嵌的字段包含如下几个。\n\n| apiGroups       | string | 包含了资源的API组的名称，支持列表格式指定的多个组，空串（\"\"）表示核心组。 |\n| --------------- | ------ | ------------------------------------------------------------ |\n| resourceNames   | string | 规则应用的目标资源名称列表，可选，缺省时意味着指定资源类型下的所有资源。 |\n| resources       | string | 规则应用的目标资源类型组成的列表，例如pods、deployments、daemonsets、roles等，ResourceAll表示所有资源 |\n| verbs           | string | 可应用至此规则匹配到的所有资源类型的操作列表，可用选项有get、list、create、update、patch、watch、proxy、redirect、delete和deletecollection；此为必选字段 |\n| nonResourceURLs | string | 用于定义用户应该有权限访问的网址列表，它并非名称空间级别的资源，因此只能应用于ClusterRole和ClusterRoleBinding，在Role中提供此字段的目的仅为与ClusterRole在格式上兼容。 |\n\n\n\n2. RoleBinding的配置中主要包含两个嵌套的字段subjects和roleRef，其中，subjects的值是一个对象列表，用于给出要绑定的主体，而roleRef的值是单个对象，用于指定要绑定的Role或ClusterRole资源。\n\n- subjects字段的可嵌套字段具体如下。\n\n| apiGroup  | string | 要引用的主体所属的API群组，对于ServiceAccount类的主体来说默认为\"\"，而User和Group类主体的默认值为\"rbac.authorization.k8s.io\" |\n| --------- | ------ | ------------------------------------------------------------ |\n| kind      | string | 要引用的资源对象（主体）所属的类别，可用值为\"User\" \"Group\"和\"ServiceAccount\"三个，必选字段 |\n| name      | string | 引用的主体的名称，必选字段                                   |\n| namespace | string | 引用的主体所属的名称空间，对于非名称空间类型的主体，如\"User\"和\"Group\"，其值必须为空，否则授权插件将返回错误信息 |\n\n\n\n- roleRef的可嵌套字段具体如下。\n\n| apiGroup | string | 引用的资源（Role或ClusterRole）所属的API群组，必选字段    |\n| -------- | ------ | --------------------------------------------------------- |\n| kind     | string | 引用的资源所属的类别，可用值为Role或ClusterRole，必选字段 |\n| name     | string | 引用的资源的名称                                          |\n\n\n\n3. Resources\n\n- Kubernetes 集群内一些资源一般以其名称字符串来表示，这些字符串一般会在 API 的\n  URL 地址中出现；同时某些资源也会包含子资源，例如 logs 资源就属于 pods\n  的子资源，它们的URL格式通常形如如下表示：GET\n  /api/v1/namespaces/{namespace}/pods/{name}/log\n- 在RBAC角色定义中，如果要引用这种类型的子资源，则需要使用“resource/subre-source”的格式\n\n# 三、RoleBinding and ClusterRoleBinding\n\n1. RoloBinding 可以将角色中定义的权限授予用户或用户组，RoleBinding包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users,groups, or service accounts)；\n1. RoloBinding 同样包含对被Bind 的 Role 引用；RoleBinding适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权\n1. RoleBinding 同样可以引用 ClusterRole 来对当前 namespace 内用户、用户组或ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的ClusterRole，然后在不同的 namespace 中使用RoleBinding 来引用\n1. to Subjects\n\n- RoleBinding 和 ClusterRoleBinding 可以将 Role 绑定到 Subjects；Subjects可以是 groups、users 或者service accounts\n- Subjects 中 Users 使用字符串表示，它可以是一个普通的名字字符串，如“alice”；也可以是 email 格式的邮箱地址，如“wangyanglinux@163.com”；甚至是一组字符串形式的数字 ID 。但是 Users 的前缀，system: 是系统保留的，集群管理员应该确保普通用户不会使用这个前缀格式\n- Groups 书写格式与 Users 相同，都为一个字符串，并且没有特定的格式要求；同样\n  system: 前缀为系统保留\n\n# 四、常用策略\n\n\n1. Role+RoleBinding\n   正常使用：某个名称空间级别的权限授权\n1. ClusterRole+ClusterRoleBinding\n   正常使用：k8s集群级别的权限授权\n1. ClusterRole+RoleBinding\n   交叉使用：其中ClusterRole策略会降级成rolebinding名称空间的策略，效果和role+rolebinding一样，好处就是在名称空间很多的时候，重复权限的配置文件会少一半，而且更灵活', 27, 0, 0, '2020-11-01 18:50:22.361911', '2021-01-26 02:41:17.768542', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (96, '安全-准入控制', '在经由认证插件和授权插件分别完成身份认证和权限检查之后，准入控制器将拦截那些创建、更新和删除相关的操作请求以强制实现控制器中定义的功能，包括执行对象的语义验证、设置缺失字段的默认值、限制所有容器使用的镜像文件必须来自某个特定的Registry、检查Pod对象的资源需求是否超出了指定的限制范围等。', 'cover/2020_11_01_18_50_50_537817.jpg', '[TOC]\n\n# 一、简介\n\n\n1. 在经由认证插件和授权插件分别完成身份认证和权限检查之后，准入控制器将拦截那些创建、更新和删除相关的操作请求以强制实现控制器中定义的功能，包括执行对象的语义验证、设置缺失字段的默认值、限制所有容器使用的镜像文件必须来自某个特定的Registry、检查Pod对象的资源需求是否超出了指定的限制范围等。\n1. 准入控制是API Server的插件集合，通过添加不同的插件，实现额外的准入控制规则。甚至于API Server的一些主要的功能都需要通过 Admission Controllers 实现，比如ServiceAccount\n1. 在具体运行时，准入控制可分为两个阶段，第一个阶段变更准入控制，用来修改请求的对象，第二个阶段验证准入控制，用于验证请求的对象，在此过程中，一旦任一阶段中的任何控制器拒绝请求，则立即拒绝整个请求，并向用户返回错误。\n\n# 二、常用控制器\n\n- AlwaysPullImages：总是拉取远端镜像；好处：可以避免本地镜像被恶意入侵而篡改\n- LimitRanger：此准入控制器将确保所有资源请求不会超过namespace的LimitRange（定义Pod级别的资源限额，如cpu、mem）\n- ResourceQuota：此准入控制器将观察传入请求并确保它不违反命名空间的ResourceQuota对象中列举的任何约束（定义名称空间级别的配额，如pod数量）\n- PodSecurityPolicy：此准入控制器用于创建和修改pod，并根据请求的安全上下文和可用的Pod安全策略确定是否应该允许它。\n- ServiceAccount：实现了自动化添加 ServiceAccount。\n\n# 三、LimitRange资源与LimitRanger准入控制器\n\n\n1. 使用LimitRange资源在每个名称空间中为每个容器指定最小及最大计算资源用量，甚至是设置默认的计算资源需求和计算资源限制。在名称空间上定义了LimitRange对象之后，客户端提交创建或修改的资源对象将受到LimitRanger控制器的检查，任何违反LimitRange对象定义的资源最大用量的请求将被直接拒绝。\n1. LimitRange资源支持限制容器、Pod、PersistentVolumeClaim的系统资源用量，其中Pod和容器主要用于定义可用的CPU和内存资源范围，而PersistentVolume-Claim则主要定义存储空间的限制范围。\n1. 以容器的CPU资源为例，default用于定义默认的资源限制，defaultRequest定义默认的资源需求，min定义最小的资源用量，而最大的资源用量既可以使用max给出固定值，也可以使用maxLimitRequestRatio设定为最小用量的指定倍数：\n1. 示例\n\n- 创建资源清单，设置资源限制\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209793377-511fe316-991c-4bd0-848e-bf8c9735ca06.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=733&size=468500&status=done&style=none&width=733)\n\n- 创建测试正常pod\n  `kubectl run limit-pod1 --image=ikubernetes/myapp:v1 --restart=Never` \n- 查看pod默认信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209819115-a65c1bc5-9338-4838-b801-cd36dea1b01a.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=196&size=468500&status=done&style=none&width=196)\n\n- 创建pod对象设定的系统资源需求量大于LimitRange中的最大用量限制\n  `kubectl run limit-pod2 --image=ikubernetes/myapp:v1 --restart=Never --limits=\'cpu=3000m\'` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209839362-064cc9c3-16a2-4f7a-9386-8eecefa2ac52.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=44&originWidth=1254&size=468500&status=done&style=none&width=1254)\n\n\n# 四、ResourceQuota资源与准入控制器\n\n\n1. ResourceQuota资源用于定义名称空间的对象数量或系统资源配额，它支持限制每种资源类型的对象总数，以及所有对象所能消耗的计算资源及存储资源总量等。\n1. 管理员可为每个名称空间分别创建一个ResourceQuota对象，随后，用户在名称空间中创建资源对象，ResourceQuota准入控制器将跟踪使用情况以确保它不超过相应ResourceQuota对象中定义的系统资源限制。\n1. ResourceQuota对象可限制指定名称空间中非终止状态的所有Pod对象的计算资源需求及计算资源限制总量\n1. 示例\n\n- 编辑配置文件resoucequota-demo.yaml，并apply\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209860164-665f60d2-0729-44d8-b5ad-366f411b0c48.png#align=left&display=inline&height=331&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=331&originWidth=467&size=468500&status=done&style=none&width=467)\n\n- describe命令打印其限额的生效情况\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209883654-d3590e5e-e83f-48d6-85bc-003b74dd594a.png#align=left&display=inline&height=301&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=301&originWidth=561&size=468500&status=done&style=none&width=561)\n\n- 创建一个pod后查看信息\n  `kubectl run limit-pod1 --image=ikubernetes/myapp:v1 --restart=Never`\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209917755-f03e0aad-27e8-4b54-a5b8-b43579ff5452.png#align=left&display=inline&height=301&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=301&originWidth=618&size=468500&status=done&style=none&width=618) ', 27, 0, 0, '2020-11-01 18:51:48.906435', '2021-01-26 05:42:37.897199', 1, 1, 0, 1, 0);
INSERT INTO `blog_article` VALUES (97, '安全-示例', '本文从以下几个方面介绍安全规则在k8s中如何使用一、创建一个User', 'cover/2020_11_01_19_02_21_235428.jpg', '[TOC]\n\n# 一、创建一个User Account\n\n\n1. 为billy用户创建私钥及证书文件\n\n- 生成私钥文件，并将文件放置于/etc/kubernetes/pki/专用目录中：\n  `[root@master  ~]# cd /etc/kubernetes/pki/ ` \n  `[root@master  pki]#(umask 077;openssl genrsa -out billy.key 2048) ` \n- 创建证书签署请求，-subj选项中CN的值将被kubeconfig作为用户名使用，O的值将被识别为用户组：\n  `[root@master  pki]# openssl req -new -key kube-dba.key -out kube-dba.csr  -subj \"/CN=billy O=kubeusers\"` \n- 基于kubeadm安装Kubernetes集群时生成的CA签署证书，这里设置其有效时长为3650天：\n  `[root@master  pki]# openssl x509 -req -in kube-dba.csr -CA ca.crt -CAkey  ca.key -CAcreateserial -out kube-dba.crt -days 3650` \n\n2. 创建配置文件\n\n- 设置新集群（k8s），指明apiserver地址，k8s证书路径和隐藏证书，并保存在（/root/billy.conf）\n  `[root@master  pki]# kubectl config set-cluster k8s  --server=https://192.168.10.100:6443 --certificate-authority=ca.crt--embed-certs=true --kubeconfig=/root/billy.conf` \n- 配置客户端证书及密钥，用户名信息会通过命令从证书Subject的CN值中自动提取，而组名则来自于“O=kubeusers”的定义：\n  `[root@master  pki]#kubectl config set-credentials billy  --client-certificate=billy.crt --client-key=billy.key --embed-certs=true --kubeconfig=/root/billy.conf` \n- 配置context，用来组合cluster和credentials，即访问的集群的上下文。\n  `[root@master  pki]# kubectl config set-context billy@k8s  --cluster=k8s  --user=billy --kubeconfig=/root/billy.conf` \n\n3. 创建系统用户及k8s验证文件\n\n- 创建系统用户并拷贝配置文件\n\n```bash\n[root@master  ~]# useradd billy \n[root@master  ~]# mkdir /home/billy/.kube \n[root@master  ~]# cp billy.conf /home/billy/.kube/config \n[root@master  ~]# chown billy.billy -R /home/billy/.kube/ \n[root@master  ~]# su - billy\n```\n\n- 测试访问集群资源，不过在启用RBAC的集群上执行命令时，billy并未获得集群资源的访问权限，因此会出现错误提示：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210160504-0f4f35c6-d983-46d7-8e7a-52f3d2e5ce8c.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=1211&size=468500&status=done&style=none&width=1211)\n\n- 切换至admin用户\n  `[root@master  pki]# kubectl config use-context kubernetes-admin@kubernetes ` \n\n\n\n# 二、授权billy访问default命名空间资源\n\n\n1. 创建Role\n\n- 设定读取、列出及监视Pod资源的许可权限：\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210196176-6af23f42-a585-47b5-889b-ff457e9d2b4b.png#align=left&display=inline&height=194&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=194&originWidth=491&size=468500&status=done&style=none&width=491)\n\n2. 创建Rolebinding\n\n- 用户billy和role pods-reader的绑定\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210217798-fc26a9df-b115-41c6-8723-17e0cbb41aab.png#align=left&display=inline&height=284&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=284&originWidth=496&size=468500&status=done&style=none&width=496)\n\n3. 访问验证\n\n- 访问default空间的pod资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210244466-f5d324bc-3cfd-45bf-a2d2-67180394720e.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=453&size=468500&status=done&style=none&width=453)\n\n- 访问所有名称空间的pod资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210390404-c91717b3-09f3-428e-a3a3-2a7a033439eb.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=766&size=468500&status=done&style=none&width=766)\n\n- 访问default空间的svc资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210402647-e6978d67-cd3e-450e-9062-84091e508e02.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=873&size=468500&status=done&style=none&width=873)\n\n\n# 三、授权billy访问所有命名空间资源\n\n\n1. 创建ClusterRole\n\n- 设定读取、列出及监视Pod资源的许可权限\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210425289-189f8812-06f9-4841-9edf-4526f7c95644.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=491&size=468500&status=done&style=none&width=491)\n\n2. 创建Rolebinding\n\n- 用户billy和clusterrole cluster-reader的绑定\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210445481-40f806a7-cd91-4722-a0fb-eafebee310ff.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=285&originWidth=494&size=468500&status=done&style=none&width=494)\n\n3. 访问验证\n\n- 访问所有名称空间的pod资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210471318-fa53ef6a-e408-4bcd-8070-576cd6a2dbc5.png#align=left&display=inline&height=208&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=208&originWidth=699&size=468500&status=done&style=none&width=699)\n\n# 四、授权devuser用户只能访问database名称空间\n\n1. 清除前面授予的权限信息\n\n```bash\n[root@master  ~]# kubectl delete -f pod-reader.yaml \n[root@master  ~]# kubectl delete -f pod-reader-cluster.yaml \n[root@master  ~]# kubectl delete -f resources-reader.yaml \n[root@master  ~]# kubectl delete -f resources-reader-cluster.yaml\n```\n\n2. 创建database名称空间\n   `[root@master  pki]# kubectl create namespace database ` \n2. 创建Role，设定只能读取、列出及监视database下的Pod资源的许可权限\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210665966-fa721b47-84be-4421-b0cb-797237275c37.png#align=left&display=inline&height=216&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=216&originWidth=498&size=468500&status=done&style=none&width=498)\n\n4. 创建Rolebinding\n\n- 用户billy和role pods-reader的绑定\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210685707-552d5912-75bc-4843-ba1d-b8a9a7617ddc.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=494&size=468500&status=done&style=none&width=494)\n\n5. 访问验证\n\n- 访问database下的pod资源信息正常\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210700845-35829c73-448b-437a-bed4-3d970f744e72.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=507&size=468500&status=done&style=none&width=507)\n\n- 访问default下的pod资源信息报错\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210716481-5dd0731e-c9e8-4ebe-820f-8e67f70c2064.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=758&size=468500&status=done&style=none&width=758)\n\n\n# 五、ServiceAccount授权\n\n1. 创建SA\n   `kubectl create sa billy-sa` \n1. 创建Role\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210742150-3b398cb0-2016-4348-8dd9-adf4c1402e00.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=501&size=468500&status=done&style=none&width=501)\n\n3. 创建Rolebinding，将billy-sa和billy-sa-role的绑定\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210765114-e58266d2-7f4d-4e78-93b5-dbc7a3303d42.png#align=left&display=inline&height=288&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=288&originWidth=496&size=468500&status=done&style=none&width=496)\n\n4. 验证结果\n   创建完SA之后系统会自动创建一个secret，我们可以获取这个secret里面的token去登录dashboard，就可以看到相应有权限的资源。\n   `kubectl get secret billy-sa-token-9rc55 -o jsonpath={.data.token} |base64 -d` \n\n- 还可以在创建pod时在pod的spec里指定serviceAccountName，那么这个pod就拥有了对应的权限。', 34, 0, 0, '2020-11-01 18:53:27.794916', '2021-01-26 11:41:42.172720', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (98, 'pod调度-调度器概述', '目标：基于资源可用性将各Pod资源公平地分布于集群节点之上。', 'cover/2020_11_01_18_54_24_607171.jpg', '[TOC]\n\n# 一、概述\n\n\n1. 目标：基于资源可用性将各Pod资源公平地分布于集群节点之上。\n1. 若预选后不存在任何一个满足条件的节点，则Pod被置于Pending状态，直到至少有一个节点可用为止\n1. 除了 kubernetes自带的调度器，你也可以编写自己的调度器。通过spec:schedulername参数指定调度器的名字，可以为pod 选择某个调度器进行调度。\n\n\n\n# 二、调度过程\n\n\n1. 调度步骤：节点预选（Predicate）、节点优先级排序（Priority） 、节点择优\n\n- 节点预选：基于一系列预选规则对每个节点进行检查，将那些不符合条件的节点过滤掉从而完成节点预选。\n- 节点优先排序：对预选出的节点进行优先级排序，以便选出最适合运行Pod对象的节点\n- 节点择优：从优先级排序结果中挑出优先级最高的节点运行Pod对象，当此类节点多于一个时，则从中随机选择一个。\n\n2. 节点预选有一系列的算法可以使用：\n\n- PodFitsResources：节点上剩余的资源是否大于 pod 请求的资源\n- PodFitsHost：如果 pod 指定了 NodeName，检查节点名称是否和 NodeName匹配PodFitsHost\n- Ports：节点上已经使用的 port 是否和 pod 申请的 port 冲突\n- PodSelectorMatches：过滤掉和 pod 指定的 label 不匹配的节点\n- NoDiskConflict：已经 mount 的 volume 和 pod 指定的 volume不冲突，除非它们都是只读\n\n3. priorities\n   过程：按照优先级大小对节点排序优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重（该项的重要性）。这些优先级选项包括：\n\n- LeastRequestedPriority：通过计算 CPU 和 Memory的使用率来决定权重，使用率越低权重越高。换句话说，这个优先级指标倾向于资源使用比例更低的节点\n- BalancedResourceAllocation：节点上 CPU 和 Memory使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用\n- ImageLocalityPriority：倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高通过算法对所有的优先级项目和权重进行计算，得出最终的结果', 29, 0, 0, '2020-11-01 18:55:15.066979', '2021-01-26 11:48:13.030576', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (99, 'pod调度-node亲和调度', '定义节点亲和性规则时有两种类型的节点亲和性规则：硬亲和性（required）和软亲和性（preferred）。', 'cover/2020_11_01_18_57_29_321221.jpg', '[TOC]\n\n# 一、概述\n\n\n1. 基于节点上的自定义标签和Pod对象上指定的标签选择器将pod调度至node节点\n1. 定义节点亲和性规则时有两种类型的节点亲和性规则：硬亲和性（required）和软亲和性（preferred）。\n\n- 硬亲和性是强制性规则，它是Pod调度时必须要满足的规则，而在不存在满足规则的节点时，Pod对象会被置为Pending状态。\n- 软亲和性是一种柔性调度限制，它倾向于将Pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但在无法满足调度需求时它将退而求其次地选择一个不匹配规则的节点。\n\n3. 定义节点亲和规则的关键点有两个，一是为node配置合乎需求的标签，另一个是为Pod对象定义合理的标签选择器，从而能够基于标签选择出符合期望的目标节点。\n3. 在Pod资源基于节点亲和性规则调度至某节点之后，节点标签发生了改变而不再符合此节点亲和性规则时，它仅对新建的Pod对象生效。调度器不会将Pod对象从此节点上移出。\n3. 键值运算关系\n\n| In           | label 的值在某个列表中   |\n| ------------ | ------------------------ |\n| NotIn        | label 的值不在某个列表中 |\n| Gt           | label 的值大于某个值     |\n| Lt           | label 的值小于某个值     |\n| Exists       | 某个 label 存在          |\n| DoesNotExist | 某个 label 不存在        |\n\n6. requiredDuringSchedulingIgnoredDuringExecution字段用于定义节点硬亲和性，由一到多个nodeSelectorTerm定义的对象组成，彼此间为“逻辑或”的关系。\n6. nodeSelectorTerm用于定义节点选择器条目，由一个或多个matchExpressions对象定义的匹配规则组成，多个规则彼此之间为“逻辑与”的关系。\n\n# 二、示例\n\n\n1. 使用节点硬亲和规则定义，将当前Pod对象调度至拥有zone标签且其值为foo的节点之上。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211230253-8f32c9c8-fbe3-4b60-80a2-fc02ffb775a8.png#align=left&display=inline&height=401&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=401&originWidth=638&size=29881&status=done&style=none&width=638)\n\n- 处于Pending阶段，这是由于强制型的节点亲和限制场景中不存在能够满足匹配条件的节点所致\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211288005-f03874d2-67ad-4f3f-95ae-ea4405901b56.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=680&size=29881&status=done&style=none&width=680)\n\n- 为node2节点设置标签zone=foo，使其满足条件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211304203-172588db-4439-46e7-b8bc-e27edfa4dd41.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=631&size=29881&status=done&style=none&width=631)\n\n- pod节点成功调度至node2\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211318820-2df35eac-7cb6-4f94-a6ea-8a0c82fbb81d.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=934&size=29881&status=done&style=none&width=934)\n\n\n2. 定义了调度拥有两个标签选择器的节点挑选条目，zone=foo且设置ssd标签，两个标签选择器彼此之间为“逻辑与”的关系。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211341047-4f170a36-adfd-4fda-a738-fc1950c28617.png#align=left&display=inline&height=355&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=355&originWidth=723&size=34191&status=done&style=none&width=723)\n\n- 处于pending状态，由于当前node只有zone=foo标签，还未设置ssd标签\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211356882-eb8db408-32ad-406b-90dc-eccfeeb785b2.png#align=left&display=inline&height=69&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=69&originWidth=649&size=34191&status=done&style=none&width=649)\n\n- 给node2设置ssd标签后，成功调度至node2\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211371468-2c5eeae2-955b-49f2-b763-45f94cfe381b.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=210&originWidth=952&size=34191&status=done&style=none&width=952)\n\n\n3. 定义节点软亲和性以选择运行在拥有zone=foo和ssd标签（无论其值为何）的节点之上，其中zone=foo是更为重要的倾向性规则，它的权重为60，ssd标签就没有那么关键，它的权重为30。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211428583-8af05bf3-6ebf-433b-b382-2ee56d1b7f45.png#align=left&display=inline&height=681&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=681&originWidth=699&size=48453&status=done&style=none&width=699)\n\n- 将node2原来的zone=foo标签修改为aaa\n  `# kubectl label nodes node2 zone=\'aaa\' --overwrite` \n- apply配置清单后查看pod信息。即便没有zone=foo的node，依然能成功调度\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211464186-32310c72-a31a-4a20-adc5-bdeae1087988.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=744&size=48453&status=done&style=none&width=744)', 32, 0, 0, '2020-11-01 18:57:37.272002', '2021-01-27 03:30:01.359624', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (100, 'pod调度-pod亲和调度', '把一些Pod对象组织在相近的位置，如某业务的前端Pod和后端Pod等，这些Pod对象间的关系为亲和性。出于安全或分布式等原因也有可能需要将一些Pod对象在其运行的位置上隔离开来，这些Pod对象间的关系为反亲和性。', 'cover/2020_11_01_19_02_41_442538.jpg', '[TOC]\n\n# 一、概述\n\n\n1. 把一些Pod对象组织在相近的位置，如某业务的前端Pod和后端Pod等，这些Pod对象间的关系为亲和性。出于安全或分布式等原因也有可能需要将一些Pod对象在其运行的位置上隔离开来，这些Pod对象间的关系为反亲和性。\n1. 常用实现方式是允许调度器把第一个Pod放置于任何位置，而后与其有亲和或反亲和关系的Pod据此动态完成位置编排。\n1. Pod的亲和性定义也存在“硬”（required）亲和性和“软”（preferred）亲和性的区别。\n1. 亲和性/反亲和性调度策略比较如下：\n\n| 调度策略        | 匹配标签 | 操作符                                 | 拓扑域支持 | 调度目标                   |\n| --------------- | -------- | -------------------------------------- | ---------- | -------------------------- |\n| nodeAffinity    | 主机     | In, NotIn, Exists,DoesNotExist, Gt, Lt | 否         | 指定主机                   |\n| podAffinity     | POD      | In, NotIn, Exists,DoesNotExist         | 是         | POD与指定POD同一拓扑域     |\n| podAnitAffinity | POD      | In, NotIn, Exists,DoesNotExist         | 是         | POD与指定POD不在同一拓扑域 |\n\n\n\n# 二、示例\n\n\n1. pod硬亲和调度\n\n- 创建带有标签“app=tomcat”的Deployment资源\n  `# kubectl run tomcat -l app=tomcat --image tomcat:alpine` \n- 定义Pod对象，通过labelSelector定义的标签选择器，使其调度到和app=tomcat的资源到同一个主机名的节点上。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211540002-48c8bd23-a8b9-47c2-b32b-df0fdf27f300.png#align=left&display=inline&height=410&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=410&originWidth=615&size=68601&status=done&style=none&width=615)\n\n- 查看pod信息，与“app=tomcat”的Deployment资源在同一node上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211554677-40f7c10b-3a85-4f6e-9096-5d4d80818ad9.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=856&size=68601&status=done&style=none&width=856)\n\n2. pod软亲和调度\n\n- 定义Pod对象，通过labelSelector定义的标签选择器，尽可能使其调度到和app=apache的资源到同一个主机名的节点上。（当前环境不存在app=apache的资源）\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211579185-8dad2d5b-a0c3-48ee-bd19-3c81c9288142.png#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=627&originWidth=757&size=68601&status=done&style=none&width=757)\n\n- myapp被调度到了node1和node2节点上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211595690-c3b9b6ab-41a0-4596-a2b9-93750e92fd95.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=1185&size=68601&status=done&style=none&width=1185)\n\n\n3. pod反亲和调度\n\n- 定义Pod对象，通过labelSelector定义的标签选择器，使其不要调度到和app=apache的资源在同一个主机名的节点上。（当前环境不存在app=apache的资源）\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211627407-b2c6f291-0e8a-4bd4-9941-b7db581dbad4.png#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=697&originWidth=691&size=68601&status=done&style=none&width=691)\n\n- myapp被调度到了node1节点上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211642793-5e43bfee-8eff-4b11-b9e4-9e9aa3200b9a.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=249&originWidth=1098&size=68601&status=done&style=none&width=1098)', 140, 0, 0, '2020-11-01 18:58:51.855812', '2021-01-26 05:37:52.468531', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (101, 'pod调度-污点和容忍度', '污点（taints）是定义在node之上的键值型属性数据，用于让节点拒绝将Pod调度运行于其上，除非该Pod对象具有接纳节点污点的容忍度。', 'cover/2020_11_01_19_00_14_480059.jpg', '[TOC]\n\n# 一、简介\n\n\n1. 污点（taints）是定义在node之上的键值型属性数据，用于让节点拒绝将Pod调度运行于其上，除非该Pod对象具有接纳节点污点的容忍度。\n1. 容忍度（tolerations）是定义在Pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将Pod对象调度至其能够容忍该节点污点的节点之上，\n1. 节点亲和性使得Pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定Pod对象的能力\n\n# 二、定义污点和容忍度\n\n\n1. 污点定义在节点的nodeSpec中，而容忍度则定义在Pod的podSpec中，它们都是键值型数据，但又都额外支持一个效果（effect）标记，语法格式为“key=value:effect”，其中key和value的用法及格式与资源注解信息相似，effect用于定义对Pod对象的排斥等级，它主要包含以下三种类型。\n\n- NoSchedule： k8s 将不会将 Pod 调度到具有该污点的 Node 上。\n- PreferNoSchedule: k8s 将尽量避免将 Pod 调度到具有该污点的 Node 上。\n- NoExecute：k8s 将不会将 Pod 调度到具有该污点的 Node 上，同时会将 Node上已经存在的 Pod 驱逐出去\n\n2. 在Pod对象上定义容忍度时，它支持两种操作符：\n\n- 一种是等值比较（Equal），表示容忍度与污点必须在key、value和effect三者之上完全匹配\n- 另一种是存在性判断（Exists），表示二者的key和effect必须完全匹配，而容忍度中的value字段要使用空值。\n\n3. 一个节点可以配置使用多个污点，一个Pod对象也可以有多个容忍度，不过二者在进行匹配检查时应遵循如下逻辑。\n\n- 首先处理每个有着与之匹配的容忍度的污点。\n- 不能匹配到的污点上，如果存在一个污点使用了NoSchedule效用标识，则拒绝调度Pod对象至此节点。\n- 不能匹配到的污点上，若没有任何一个使用了NoSchedule效用标识，但至少有一个使用了PreferNoScheduler，则应尽量避免将Pod对象调度至此节点。\n- 如果至少有一个不匹配的污点使用了NoExecute效用标识，则节点将立即驱逐Pod对象，或者不予调度至给定节点；另外，即便容忍度可以匹配到使用了NoExecute效用标识的污点，若在定义容忍度时还同时使用tolerationSeconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n4. pod.spec.tolerations字段设置\n\n- key, vaule, effect 要与 Node 上设置的 taint 保持一致\n- operator 的值为 Exists 将会忽略 value 值\n- tolerationSeconds 用于描述当 Pod 需要被驱逐时可以在 Pod 上继续保留运行的时间\n- 不指定 key 值时，表示容忍所有的污点 key\n\ntolerations:\n\n   - operator: \"Exists\"\n\n- 不指定 effect 值时，表示容忍所有的污点作用\n  tolerations:\n- key: \"key\"\n  operator: \"Exists\"\n- 有多个 Master 存在时，防止资源浪费，可以如下设置\n  kubectl taint nodes Node-Name\n  node-role.kubernetes.io/master=:PreferNoSchedule\n\n# 三、污点的设置、查看和去除\n\n\n1. 设置污点\n   `kubectl taint nodes node1 key1=value1:NoSchedule` \n1. 节点说明中，查找 Taints 字段\n   `kubectl describe node node-name` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211757921-1d314136-e2bc-40c1-8b8d-60af2bf99f7b.png#align=left&display=inline&height=23&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=23&originWidth=681&size=68601&status=done&style=none&width=681)\n\n3. 去除污点\n   kubectl taint nodes node1 key1:NoSchedule-\n\n# 四、示例\n\n1. 使用NoExecute：将node1已经存在的Pod对象驱逐。\n\n- 创建deployment资源，系统自动调度至node1和node2上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211785718-b4b29820-b4ca-469e-96f8-adc989e2bb52.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=953&size=68601&status=done&style=none&width=953)\n\n- 给node1设置NoExecute污点\n  `# kubectl taint node node1 status=check:NoExecute` \n- 查看pod信息，已全部调度至node2上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211804410-56de9439-00b2-49ea-bb7c-d7a6185ea554.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=977&size=68601&status=done&style=none&width=977)\n\n2. 设置容忍，容忍status=check:NoExecute的污点存在，60秒后失效\n\n- 配置deployment清单文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211826882-69407c74-e9ce-461b-9518-dabfdb0cec29.png#align=left&display=inline&height=603&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=603&originWidth=445&size=68601&status=done&style=none&width=445)\n\n- 查看pod信息，此时node1和node2上均有pod\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211842180-78ce7443-fc92-49e7-b12f-e902fcaff350.png#align=left&display=inline&height=134&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=134&originWidth=958&size=68601&status=done&style=none&width=958)\n\n- 60秒后查看pod信息，pod已全部调度至node2上\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211865247-c1afa4db-2fa6-48a8-96da-0768bc6e67b2.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=977&size=68601&status=done&style=none&width=977)', 141, 0, 0, '2020-11-01 19:00:18.503318', '2021-01-26 05:37:55.382515', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (102, 'pod调度-固定节点调度', 'Pod.spec.nodeName', 'cover/2020_11_01_19_01_43_452257.jpg', '\n> Pod.spec.nodeName 将 Pod 直接调度到指定的 Node 节点上，会跳过 Scheduler的调度策略，该匹配规则是强制匹配\n\n1. 编写资源清单，指定deployment调度至node1节点\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211983459-55825f2c-0ef4-462c-a933-275bd4179a01.png#align=left&display=inline&height=480&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=480&originWidth=428&size=68601&status=done&style=none&width=428)\n\n2. 查看pod信息，全部调度至node1节点\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211999715-2510aea1-0e57-46e0-8c92-84819e12448a.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=944&size=68601&status=done&style=none&width=944)\n\n3. Pod.spec.nodeSelector：通过 kubernetes 的 label-selector机制选择节点，由调度器调度策略匹配 label，而后调度 Pod到目标节点，该匹配规则属于强制约束\n\n- 编写资源清单，指定deployment调度至disk=ssd标签的节点\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212022814-82ce9889-e7bf-450b-b617-0df55200debb.png#align=left&display=inline&height=508&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=508&originWidth=434&size=68601&status=done&style=none&width=434)\n\n- 给node2设置disk=ssd标签\n  `# kubectl label nodes node2 disk=ssd` \n- 查看pod调度状态信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212058298-a71f34bd-3f61-4141-959a-53434dfab3a8.png#align=left&display=inline&height=130&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=130&originWidth=935&size=68601&status=done&style=none&width=935)', 148, 0, 0, '2020-11-01 19:01:46.648005', '2021-01-26 11:42:32.719078', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (103, '系统扩展-自定义资源类型（CRD）', 'API的常用方式有三种：使用CRD（CustomResourceDefinitions）自定义资源类型、开发自定义的API', 'cover/2020_11_01_19_04_10_452573.jpg', '[TOC]\n\n# 一、简介\n\n\n1. 扩展Kubernetes\n   API的常用方式有三种：使用CRD（CustomResourceDefinitions）自定义资源类型、开发自定义的API\n   Server并聚合至主API Server，以及定制扩展Kubernetes源码。\n1. CRD无须修改Kubernetes源代码就能扩展它支持使用API资源类型。CRD本身也是一种资源类型，隶属于集群级别，实例化出特定的对象之后，它会在API上注册生成GVR类型URL端点，并能够作为一种资源类型被使用并实例化相应的对象。自定义资源类型之前，选定其使用的API群组名称、版本及新建的资源类型名称，根据这些信息即可创建自定义资源类型，并创建自定义类型的资源对象。\n\n\n\n# 二、创建CRD对象\n\n\n1. 定义了一个名为users.auth.ilinux.io的CRD资源对象，它的群组名称为auth.ilinux.io，仅支持一个版本级别v1beta1，复数形式为users，隶属于名称空间级别。\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212244105-4c35b1a9-cac4-43e9-bfe6-1bbac7ffed88.png#align=left&display=inline&height=512&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=512&originWidth=637&size=71092&status=done&style=none&width=637)\n\n2. 列出集群上的CRD对象\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212259550-dde07a1a-1d09-4c71-beb1-f14a670e39ca.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=74&originWidth=614&size=71092&status=done&style=none&width=614)\n\n3. users已经是一个名称空间级别的可用资源类型，用户可按需创建出任意数量的users类型的对象。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212274526-59606950-1466-4305-9e9b-1a3221c15588.png#align=left&display=inline&height=195&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=195&originWidth=430&size=71092&status=done&style=none&width=430)\n\n4. 获取user类型资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212290647-b0d6ba03-14a6-4aa6-8e65-9cc1fe2be9e3.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=397&size=71092&status=done&style=none&width=397)\n\n5. 删除自定义的students对象object-student\n   `# kubectl delete users admin` \n\n\n\n# 三、自定义资源格式验证\n\n\n1. 对象配置的变动在存入etcd之前还需要经由准入控制器的核验，尤其是验证型（validation）控制器使用OpenAPI模式声明验证规则，检查传入的对象格式是否符合有效格式。\n1. 为CRD对象users.auth.ilinux.io，在spec字段定义了userID、groups、email和password字段的数据类型，并指定了userID字段的取值范围，以及password字段的数据格式，而且指定userID和groups是必选字段\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212312408-5309852f-7444-4c5e-b6dd-402ca8bc43a4.png#align=left&display=inline&height=468&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=468&originWidth=485&size=71092&status=done&style=none&width=485)\n\n3. 使用原admin资源清单时报错\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212326632-46db8d5e-539d-4702-bb34-0e8718a54de3.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=512&size=71092&status=done&style=none&width=512)', 137, 0, 0, '2020-11-01 19:04:13.943350', '2021-01-26 05:16:40.111395', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (104, '系统扩展-自定义控制器', '控制器负责持续监视资源变动，根据资源的spec及status中的信息执行某些操作逻辑，并将执行结果更新至资源的status中。自定义控制器同样担负着类似的职责，只不过一个特定的控制器通常仅负责管理一部分特定的资源类型，并执行专有管理逻辑。', 'cover/2020_11_01_19_05_37_023184.jpg', '\n> 控制器负责持续监视资源变动，根据资源的spec及status中的信息执行某些操作逻辑，并将执行结果更新至资源的status中。自定义控制器同样担负着类似的职责，只不过一个特定的控制器通常仅负责管理一部分特定的资源类型，并执行专有管理逻辑。\n\n1. 控制器包含两个重要组件：Informer和Workqueue，前者负责监视资源对象当前状态的更改，并将事件发送至后者，而后由处理函数进行处理\n1. Controller可以有一个或多个informer来跟踪某一个resource。Informter跟API\n   server保持通讯获取资源的最新状态并更新到本地的cache中，一旦跟踪的资源有变化，informer就会调用callback。把关心的变更的Object放到workqueue里面。然后woker执行真正的业务逻辑，计算和比较workerqueue里items的当前状态和期望状态的差别，然后通过client-go向API\n   server发送请求，直到驱动这个集群向用户要求的状态演化。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212407211-56c1e184-dc6d-44cf-826b-f800290fe3a6.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=512&size=71092&status=done&style=none&width=512)\n\n3. 现在已经有了几类更加成熟、更易上手的工具可用，它们甚至已经可以被视作开发CRD和控制器的SDK或框架，其中，主流的项目主要有Kubebuilder、Operator SDK和Metacontroller三个。', 138, 0, 0, '2020-11-01 19:05:49.387634', '2021-01-26 23:01:19.843262', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (105, '资源指标与HPA-资源监控及资源指标', '在集群监控层面，目标是监控整个Kubernetes集群的健康状况，包括集群中的所有工作节点是否运行正常、系统资源容量大小、每个工作节点上运行的容器化应用的数量以及整个集群的资源利用率等，它们通常可分为如下一些可衡量的指标。', 'cover/2020_11_01_19_06_25_889704.jpg', '[TOC]\n\n# 一、资源监控内容\n\n\n1. 在集群监控层面，目标是监控整个Kubernetes集群的健康状况，包括集群中的所有工作节点是否运行正常、系统资源容量大小、每个工作节点上运行的容器化应用的数量以及整个集群的资源利用率等，它们通常可分为如下一些可衡量的指标。\n\n- 节点资源状态：主要有网络带宽、磁盘空间、CPU和内存的利用率。\n- 节点数量：集群中的可用节点数量。\n- 运行的Pod对象：正在运行的Pod对象数量。\n\n2. Pod资源对象的监控需求大体上可以分为三类。\n\n- Kubernetes指标：用于监视特定应用程序相关的Pod对象的部署过程、当前副本数量、期望的副本数量、部署过程进展状态、健康状态监测及网络服务器的可用性等，这些指标数据需要经由Kubernetes系统接口获取。\n- 容器指标：容器的资源需求、资源限制以及CPU、内存、磁盘空间、网络带宽等资源的实际占用状况等。\n- 应用程序指标：应用程序自身内建的指标，通常与其所处理的业务规则相关，例如，关系型数据库应用程序可能会内建用于暴露索引状态有关的指标，以及表和关系的统计信息等。\n\n# 二、资源监控工具\n\n1. kubelet程序中集成相关的工具程序cAdvisor，仅能收集单个节点及其相关Pod资源的相关指标数据，用于对节点上的资源及容器进行实时监控及指标数据采集，支持的相关指标包括CPU、内存使用情况、网络吞吐量及文件系统使用情况等，并可通过TCP的4194端口提供一个Web UI。\n1. Heapster是集群级别的监视和事件数据聚合工具。Heapster本身可作为集群中的一个Pod对象运行，它通过发现集群中的所有节点实现从每个节点kubelet内建的cAdvisor获取性能和指标数据，并通过标签将Pod对象及其相关的监控数据进行分级、聚合后推送到可配置的后端存储系统进行存储和可视化。\n1. 功能完备的Heapster监控系统流行的解决方案是由InfluxDB作为存储后端，Grafana为可视化接口，而Heapster从各节点的cAdvisor采集数据并存储于InfluxDB中，由Grafana进行展示。托管于Kubernetes集群中的InfluxDB、Grafana和Heapster运行为常规的Pod资源对象，它们彼此之间通过环境变量及服务发现功能自动协同。\n\n\n\n# 三、新一代监控架构\n\n\n1. Kubernetes具有灵活的可扩展性，它通过API聚合器为开发人员提供了轻松扩展API资源的能力，可以自定义指标API和资源指标API。新一代的Kubernetes监控系统架构主要由核心指标流水线和监控指标流水线协同组成。\n\n- 核心指标流水线\n  由kubelet、metrics-server以及由API server提供的api组成；CPU累积使用率、内存的实时使用率、pod的资源占用率及容器的磁盘占用率。\n- 监控指标流水线\n  用于从系统收集各种指标数据并提供给终端用户、存储系统以及HPA，包含核心指标以及其他许多非核心指标。非核心指标本身不能被K8S所解析。所以需要k8s-prometheus-adapter将prometheus采集到的数据转化为k8s能理解的格式，为k8s所使用。\n\n2. HPAv2：能同时使用资源指标API和自定义指标API的组件，它实现了基于观察到的指标自动缩放Deployment或ReplicaSet类型控制器管控下的Pod副本数量。\n2. 目前，资源指标API的实现较主流是的metrics-server，而自定义指标API则以建构在监控系统Prometheus之上的k8s-prometheus-adapter最广为接受。', 97, 0, 0, '2020-11-01 19:09:36.068836', '2021-01-26 11:48:30.484138', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (106, '资源指标与HPA-监控组件安装', '本文主要介绍如何部署metrics-server以及查看集群资源状态', 'cover/2020_11_01_19_11_13_047140.jpg', '[TOC]\n\n# 一、部署metrics-server\n\n\n- [github地址](https://github.com/kubernetes-sigs/metrics-server)\n\n1. 克隆项目代码的仓库至本地node节点目录以获得其资源配置清单\n   `# git clone https://github.com/kubernetes-incubator/metrics-server.git` \n1. 需要修改metrics-server/deploy/1.8+/metrics-server-deployment.yaml清单文件，spec.containers下添加：\n\n```yaml\ncommand:\n    - /metrics-server\n    - --kubelet-insecure-tls\n    - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n```\n\n3. 为master节点添加label\n   `# kubectl label nodes master metrics=yes` \n3. 部署metrics-server\n   `# kubectl create -f metrics-server/deploy/1.8+/` \n3. 检验相应的API群组metrics.k8s.io是否出现在Kubernetes集群的API群组列表中\n   `# kubectl api-versions | grep metrics` \n3. 确认相关的Pod对象运行正常\n   `# kubectl get pods -n kube-system -l k8s-app=metrics-server` \n3. 使用kubectl top node查看结果\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212866497-9a5765fd-3b28-409f-9c60-0114495a1d3b.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=579&size=71092&status=done&style=none&width=579)', 96, 0, 0, '2020-11-01 19:11:15.466874', '2021-01-26 05:38:03.295370', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (107, '资源指标与HPA-资源指标及其应用', 'Metrics', 'cover/2020_11_01_19_14_37_948091.jpg', '[TOC]\n\n# 一、metrics-server\n\n\n1. 通过API Server的URL路径/apis/metrics.k8s.io/进行存取，并提供同样级别的安全性、稳定性及可靠性保证。\n1. Metrics Server通过Kubernetes聚合器（kube-aggregator）注册到主API Server之上，而后基于kubelet的Summary API收集每个节点上的指标数据，并将它们存储于内存中然后以指标API格式提供，\n1. Metrics Server基于内存存储，重启后数据将全部丢失，而且它仅能留存最近收集到的指标数据，因此，如果用户期望访问历史数据，就不得不借助于第三方的监控系统（如Prometheus等），或者自行开发以实现其功能。\n\n\n\n# 二、kubectl top命令\n\n\n1. kubectl top命令可显示节点和Pod对象的资源使用信息，它依赖于集群中的资源指标API来收集各项指标数据。它包含有node和pod两个子命令，可分别用于显示Node对象和Pod对象的相关资源占用率。\n1. 列出Node资源占用率命令的语法格式为“kubectl top node [-l label | NAME]”，例如下面显示所有节点的资源占用状况的结果中显示了各节点累计CPU资源占用时长及百分比，以及内容空间占用量及占用比例，可以使用标签选择器进行节点过滤\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212944422-c572e3c3-6cb8-46d2-8b55-a1f741902750.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=579&size=71092&status=done&style=none&width=579)\n\n3. 名称空间级别的Pod对象资源占用率，一般应该限定名称空间及使用标签选择器过滤出目标Pod对象。命令的语法格式为“kubectl top pod [NAME | -l label] [--all-namespaces] [--containers=false|true]”，例如，下面显示kube-system名称空间中标签为“k8s-app=kube-dns”的所有Pod资源及其容器的资源占用状态：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212987133-cf0d61fa-5347-4356-9288-9a4844262104.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=979&size=71092&status=done&style=none&width=979)', 106, 0, 0, '2020-11-01 19:12:57.718997', '2021-01-26 05:38:12.745445', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (108, '资源指标与HPA-自动弹性缩放', 'HPA自身是一个控制循环（control', 'cover/2020_11_01_19_14_01_689029.jpg', '[TOC]\n\n# 一、HPA\n\n\n1. HPA作为Kubernetes\n   API资源和控制器实现，它基于采集到的资源指标数据来调整控制器的行为，控制器会定期调整ReplicaSets或Deployment控制器对象中的副本数，以使得观察到的平均CPU利用率与用户指定的目标相匹配。\n1. HPA自身是一个控制循环（control loop）的实现，其周期由controller-manager的--horizontal-pod-autoscaler-sync-period选项来定义，默认为30秒。在每个周期内，controller-manager将根据每个HPA定义中指定的指标查询相应的资源利用率。controller-manager从资源指标API（针对每个Pod资源指标）或自定义指标API（针对所有其他指标）中获取指标数据。\n1. HPA控制器可以通过两种不同的方式获取指标：Heapster和REST客户端接口。使用直接Heapster获取指标数据时，HPA直接通过API服务器的服务代理子资源向Heapster发起查询请求，因此，Heapster需要事先部署在群集上并在kube-system名称空间中运行。使用REST客户端接口获取指标时，需要事先部署好资源指标API及其API Server，必要时，还应该部署好自定义指标API及其相关的API Server。\n1. HPA是Kubernetes\n   autoscalingAPI群组中的API资源，当前的稳定版本仅支持CPU自动缩放，它位于autoscaling/v1群组中。而测试版本包含对内存和自定义指标的扩展支持，测试版本位于API群组autoscaling/v2beta1之中。\n\n\n\n# 二、HPA（v1）控制器\n\n\n1. HPA也是标准的Kubernetes\n   API资源，其基于资源配置清单的管理方式同其他资源相同。它还有一个特别的“kubectl autoscale”命令用于快速创建HPA控制器\n1. 示例\n\n- 创建一个名为myapp-deploy的Deployment控制器\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213049314-e9c774dd-c3f1-4ed8-b978-de3b9363c4b2.png#align=left&display=inline&height=632&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=632&originWidth=443&size=71092&status=done&style=none&width=443)\n\n- 创建一个同名的HPA控制器自动管控其Pod副本规模：\n  `# kubectl autoscale deployment myapp-deploy --min=2 --max=5 --cpu-percent=60` \n- 查看HPA控制器信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213071074-03a818aa-e880-4c8f-8a4a-013ce1f1051b.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=878&size=71092&status=done&style=none&width=878)\n\n- 向myapp-svc的NodePort发起持续性的压力测试式访问请求，各Pod对象的CPU利用率将持续上升，直到超过目标利用率边界的60%，而后触发增加Pod对象副本数量。\n\n\n\n# 三、HPA（v2）控制器\n\n\n1. HPA（v2）控制器支持基于核心指标CPU和内存资源以及基于任意自定义指标资源占用状态实现应用规模的自动弹性伸缩，它从metrics-server中请求查看核心指标，从k8s-prometheus-adapter一类的自定义API中获取自定义指标数据。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213087084-5aa77bab-e024-45e0-8992-47a2eac4ae4d.png#align=left&display=inline&height=482&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=482&originWidth=463&size=73462&status=done&style=none&width=463)\n\n2. spec字段主要嵌套使用如下几个字段。\n\n| minReplicas    | integer | 自动伸缩可缩减至的Pod副本数下限                              |\n| -------------- | ------- | ------------------------------------------------------------ |\n| maxReplicas    | integer | 自动伸缩可扩展至的Pod副本数上限，其值不能低于min-Replicas属性值 |\n| scaleTargetRef | object  | 要缩放的目标资源，以及应该收集指标数据并更改其副本数量的Pod对象，引用目标对象时，主要会用到三个嵌套属性——apiVersion、kind和name |\n| metrics        | object  | 用于计算所需Pod副本数量的指标列表，每个指标单独计算其所需的副本数，将所有指标计算结果中的最大值作为最终采用的副本数量。 |\n\n\n\n3. metrics字段值是对象列表，它由要引用的各指标的数据源及其类型构成的对象组成。\n\n| external | 用于引用非附属于任何对象的全局指标，甚至可以基于集群之外的组件的指标数据，如消息队列的长度等 |\n| -------- | ------------------------------------------------------------ |\n| object   | 引用描述集群中某单一对象的特定指标，如Ingress对象上的hits-per-second等 |\n| pods     | 引用当前被弹性伸缩的Pod对象的特定指标，如transactions-processed-per-second等 |\n| resource | 引用资源指标，即当前被弹性伸缩的Pod对象中容器的requests和limits中定义的指标（CPU或内存资源） |\n| type     | 表示指标源的类型，其值可为Objects、Pods或Resource            |\n\n', 63, 0, 0, '2020-11-01 19:14:14.584000', '2021-01-26 05:38:14.462381', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (109, 'helm-helm基础', 'Helm就是Kubernetes的应用程序包管理器，类似于Linux系统之上的yum或apt-get等，可用于实现帮助用户查找、分享及使用Kubernetes应用程序。它的核心打包功能组件称为chart，可以帮助用户创建、安装及升级复杂应用。Helm将Kubernetes的资源（如Deployments、Services或ConfigMap等）打包到一个Charts中，制作并测试完成的各个Charts将保存到Charts仓库进行存储和分发。另外，Helm实现了可配置的发布，它支持应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除和更新等操作。', 'cover/2020_11_01_19_17_55_303647.jpg', '[TOC]\n\n# 一、helm功能\n\n\n1. Helm就是Kubernetes的应用程序包管理器，类似于Linux系统之上的yum或apt-get等，可用于实现帮助用户查找、分享及使用Kubernetes应用程序。它的核心打包功能组件称为chart，可以帮助用户创建、安装及升级复杂应用。Helm将Kubernetes的资源（如Deployments、Services或ConfigMap等）打包到一个Charts中，制作并测试完成的各个Charts将保存到Charts仓库进行存储和分发。另外，Helm实现了可配置的发布，它支持应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除和更新等操作。\n1. Helm是一个基于Kubernetes的程序包（资源包）管理器，它将一个应用的相关资源组织成为Charts，并通过Charts管理程序包，其使用优势可简单总结为如下几个方面：\n\n- 管理复杂应用：Charts能够描述哪怕是最复杂的程序结构，其提供了可重复使用的应用安装的定义。\n- 易于升级：使用就地升级和自定义钩子来解决更新的难题。\n- 简单分享：Charts易于通过公共或私有服务完成版本化、分享及主机构建。\n- 回滚：可使用“helm rollback”命令轻松实现快速回滚。\n\n\n\n# 二、helm核心术语\n\n\n1. Helm将Kubernetes应用的相关配置组织为Charts，并通过它完成应用的常规管理操作。使用Charts管理应用的流程包括：\n\n- 从0开始创建Charts\n- 将Charts及其相关的文件打包为归档格式\n- 将Charts存储于仓库（repository）中并与之交互\n- 在Kubernetes集群中安装或卸载Charts以及管理经Helm安装的应用的版本发行周期\n\n2. 对Helm来说，它具有以下几个关键概念。\n\n- Charts：即一个Helm程序包，它包含了运行一个Kubernetes应用所需要的镜像、依赖关系和资源定义等，必要时还会包含Service的定义；它类似于yum的rpm文件。\n- Repository：Charts仓库，用于集中存储和分发Charts，类似于Python的PyPI。\n- Config：应用程序实例化安装运行时使用的配置信息。\n- Release：应用程序实例化配置后运行于Kubernetes集群中的一个Charts实例；从V3开始，Release\n  不再是全局资源，而是存储在各自命名空间内。\n\n# 三、helm架构\n\n> Helm主要由Helm客户端、Tiller服务器和Charts仓库（repository）组成\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213320583-2a0039a5-e9c5-480f-a9c7-b9cdfb451318.png#align=left&display=inline&height=309&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=309&originWidth=720&size=73462&status=done&style=none&width=720)\n\n\n1. Helm客户端是命令行客户端工具，采用Go语言编写，基于gRPC协议与Tiller\n   server交互。它主要完成如下任务。\n\n- 本地Charts开发。\n- 管理Charts仓库。\n- 与Tiller服务器交互：发送Charts以安装、查询Release的相关信息以及升级或卸载已有的Release。\n\n2. Tiller\n   server（V3已将Tiller的删除，通过ApiServer与k8s交互）是托管运行于Kubernetes集群之中的容器化服务应用，它接收来自Helm客户端的请求，并在必要时与Kubernetes API Server进行交互。它主要完成以下任务。\n\n- 监听来自于Helm客户端的请求。\n- 合并Charts和配置以构建一个Release。\n- 向Kubernetes集群安装Charts并对相应的Release进行跟踪。\n- 升级和卸载Charts。\n\n3. 通常，用户于Helm客户端本地遵循其格式编写Charts文件，而后即可部署于Kuber-netes集群之上运行为一个特定的Release。仅在有分发需求时，才应该将同一应用的Charts文件打包成归档压缩格式提交到特定的Charts仓库。\n\n\n\n# 四、Helm工作原理\n\n![11553600-23077c664811da57.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604213358275-42b64ad0-71cd-40c7-bac2-210d95ac7773.jpeg#align=left&display=inline&height=580&margin=%5Bobject%20Object%5D&name=11553600-23077c664811da57.jpg&originHeight=580&originWidth=790&size=59724&status=done&style=none&width=790)\n\n\n1. Chart Install 过程：\n\n- Helm从指定的目录或者tgz文件中解析出Chart结构信息\n- Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller\n- Tiller根据Chart和Values生成一个Release\n- Tiller将Release发送给Kubernetes用于生成Release\n\n2. Chart Update过程：\n\n- Helm从指定的目录或者tgz文件中解析出Chart结构信息\n- Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller\n- Tiller生成Release并更新指定名称的Release的History\n- Tiller将Release发送给Kubernetes用于更新Release\n\n3. Chart Rollback过程：\n\n- Helm将要回滚的Release的名称传递给Tiller\n- Tiller根据Release的名称查找History\n- Tiller从History中获取上一个Release\n- Tiller将上一个Release发送给Kubernetes用于替换当前Release\n\n', 88, 0, 0, '2020-11-01 19:17:58.681591', '2021-01-26 11:48:53.513565', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (110, 'helm-helm安装', 'Helm的安装方式有两种：预编译的二进制程序和源码编译安装。', 'cover/2020_11_01_19_19_15_088554.jpg', '[TOC]\n\n# 一、安装Helm\n\n\n- 官方参考文档：[https://helm.sh/docs/intro/quickstart/](https://helm.sh/docs/intro/quickstart/)\n- Helm的安装方式有两种：预编译的二进制程序和源码编译安装。\n- Helm项目托管在GitHub之上，项目地址为[https://github.com/helm/helm/releases](https://github.com/helm/helm/releases)。\n- Helm的运行依赖于本地安装并配置完成的kubectl方能与运行于Kubernetes集群之上的Tiller服务器进行通信，因此，运行Helm的节点也应该是可以正常使用kubectl命令的主机，或者至少是有着可用kubeconfig配置文件的主机。\n\n1. 下载合用版本的压缩包并将其展开。\n   `wget https://get.helm.sh/helm-v3.0.2-linux-amd64.tar.gz` \n   `tar -xvf helm-v3.0.2-linux-amd64.tar.gz` \n1. 将其二进制程序文件复制或移动到系统PATH环境变量指向的目录中\n   `cp linux-amd64/helm /usr/local/bin/` \n1. 以添加自动完成的代码：\n   `source <(helm completion bash)` \n1. Helm客户端安装完成后，进行验证。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213646506-586de468-4a93-4848-9db8-d1117bc3a1c6.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=44&originWidth=370&size=73462&status=done&style=none&width=370)\n\n\n# 二、添加Helm的官方仓库\n\n\n1. 添加官方Charts仓库\n   `helm repo add stable https://kubernetes-charts.storage.googleapis.com/` \n1. 更新仓库信息\n   `helm repo update` \n1. 查看官方Charts仓库\n   `helm search repo stable` ', 77, 0, 0, '2020-11-01 19:19:18.370563', '2021-01-27 09:38:31.360326', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (111, 'helm-helm常用命令', '本文主要从一、helm管理命令', 'cover/2020_11_01_19_21_33_592205.jpg', '[TOC]\n\n# 一、helm管理命令\n\n\n1. 查看版本\n   `#helm version` \n1. 增加repo\n   `#helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts` \n   `#helm repo add --username admin --password password myharbor https://harbor.qing.cn/chartrepo/charts` \n1. 更新repo仓库资源\n   `#helm repo update` \n\n\n\n# 二、charts管理\n\n\n1. 查看当前安装的charts\n   `#helm list` \n1. 将helm search hub显示所有可用图表。\n   `#helm search hub redis` \n1. 使用helm search repo，您可以在已添加的存储库中找到charts的名称：\n   `#helm search repo redis` \n1. 打印出指定的Charts的详细信息\n   `#helm show chart stable/redis` \n1. 下载charts到本地\n   `#helm fetch redis` \n1. 安装charts\n   `#helm install redis stable/redis` \n1. 查看charts状态\n   `#helm status redis` \n1. 删除charts\n   `#helm uninstall redis` \n\n\n\n# 三、自定义charts\n\n\n1. 创建charts\n   `#helm create helm_charts` \n1. 检查chart语法正确性\n   `# helm lint myapp` \n1. 打包自定义的chart\n   `# helm package myapp` \n1. 查看生成的yaml文件\n   `#helm template myapp-1.tgz` \n1. 使用默认chart部署到k8s\n   `helm install myapp myapp-1.tgz` \n1. 使用包去做release部署\n   `helm install --name example2 helm-chart-0.1.0.tgz --set  service.type=NodePort` \n\n\n\n# 四、更新与回滚\n\n\n1. 查看当前chart信息\n   `#helm list` \n1. 更新images\n   `#helm upgrade myapp myapp-2.tgz` \n1. 查看版本信息\n   `#helm history myapp` \n1. 回滚指定版本\n   `#helm rollback myapp 1` ', 104, 0, 0, '2020-11-01 19:21:36.218456', '2021-01-27 14:50:05.708032', 1, 1, 0, 1, 1);
INSERT INTO `blog_article` VALUES (112, 'helm-Helm', '一个Charts就是按特定格式组织的目录结构，目录名即为Charts名，目录名称本身不包含版本信息。目录结构中除了charts/和templates/是目录之外，其他的都是文件。它们的基本功用如下。', 'cover/2020_11_01_19_22_53_410268.jpg', '[TOC]\n\n# 一、Charts文件组织结构\n\n\n> 一个Charts就是按特定格式组织的目录结构，目录名即为Charts名，目录名称本身不包含版本信息。目录结构中除了charts/和templates/是目录之外，其他的都是文件。它们的基本功用如下。\n\n\n\n1. Chart.yaml：当前Charts的描述信息，yaml格式的文件。\n1. LICENSE：当前Charts的许可证信息，纯文本文件；此为可选文件。\n1. README.md：易读格式的README文件；可选。\n1. requirements.yaml：当前Charts的依赖关系描述文件；可选。\n1. values.yaml：当前Charts用到的默认配置值。\n1. charts/：目录，存放当前Charts依赖到的所有Charts文件。\n1. templates/：目录，存放当前Charts用到的模板文件，可应用于Charts生成有效的Kuber-netes清单文件。\n1. templates/NOTES.txt：纯文本文件，Templates简单使用注解\n\n- 尽管Charts和Templates目录均为可选，但至少应该存在一个Charts依赖文件或一个模板文件。另外，Helm保留使用charts/和templates/目录以及上面列出的文件名称，其他文件都将被忽略。\n\n\n\n# 二、Chart.yaml文件组织格式\n\n\n> Chart.yaml用于提供Charts相关的各种元数据，如名称、版本、关键词、维护者信息、使用的模板引擎等，它是一个Charts必备的核心文件，主要包含以下字段。\n\n\n\n1. name：当前Charts的名称，必选字段。\n1. version：遵循语义化版本规范第2版的版本号，必选字段。\n1. description：当前项目的单语句描述信息，可选字段。\n1. keywords：当前项目的关键词列表，可选字段。\n1. home：当前项目的主页URL，可选字段。\n1. sources：当前项目用到的源码的来源URL列表，可选字段。\n1. maintainers：项目维护者信息，主要嵌套name、email和URL几个属性组成；可选字段。\n1. engine：模板引擎的名称，默认为gotpl，即go模板。\n1. icon:URL，指向当前项目的图标，SVG或PNG格式的图片；可选字段。\n1. appVersion：本项目用到的应用程序的版本号，可选字段，且不必为语义化版本。\n1. tillerVersion：当前Charts依赖的Tiller版本号，可以是语义化版本号的范围，如“>2.4.0”；可选字段。\n\n\n\n# 三、Charts中的依赖关系\n\n\n> Helm中的一个Charts可能会依赖不止一个其他的Charts，这种依赖关系可经requirements.yaml进行动态链接，也可直接存储于charts/目录中进行手动管理。\n\n\n\n1. requirements.yaml文件\n   requirements.yaml文件本质上只是一个简单的依赖关系列表，可用字段具体如下。\n1. name：被依赖的Charts的名称。\n1. version：被依赖的Charts的版本。\n1. repository：被依赖的Charts所属的仓库及其URL；如果是非官方的仓库，则需要先用helm\n   repo add命令将其添加进本地可用仓库。\n1. alias：为被依赖的Charts创建一个别名，从而让当前Charts可以将所依赖的Charts对应到新名称，即别名；可选字段。\n1. tags：默认情况下所有的Charts都会被装载，若给定了tags，则仅装载那些匹配到的Charts。\n1. condition：类似于tags字段，但需要通过自定义的条件来指明要装载的charts。\n1. import-values：导入子Charts中的的值；被导入的值需要在子charts中导出。\n1. Charts目录\n\n- 若需要对依赖关系进行更多的控制，则所有被依赖到的Charts都能以手工方式直接复制到Charts目录中。一个被依赖到的Charts既可以是归档格式，也可以是展开的目录格式，不过，其名称不能以下划线（_）或点号（.）开头，此类文件会被Charts装载器自动忽略。\n  例如，Wordpress Charts依赖关系在其Charts目录中的反映类似如下所示：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213846878-bc9f0d02-84f6-42e6-8f71-7ddb2cd2c7a7.png#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=540&originWidth=410&size=73462&status=done&style=none&width=410)\n\n- Helm\n  Charts模板（template）遵循Go模板语言格式，并支持50种以上的来自Spring库的模板函数附件，以及为数不少的其他专用函数。所有的模板文件都存储于Templates目录中，在当前Charts被Helm引用时，此目录中的所有模板文件都会传递给模板引擎进行处理。模板文件中用到的值（value）有如下两种提供方式。□通过Charts的values.yaml文件提供，通常用于提供默认值。□在运行“helm\n  install”命令时传递包含所需要的自定义值的YAML文件；此处传递的值会覆盖默认值。', 43, 0, 0, '2020-11-01 19:22:56.315663', '2021-01-26 05:07:06.642471', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (113, 'helm-自定义Charts', '本文从一、创建Chart', 'cover/2020_11_01_19_24_06_112249.jpg', '[TOC]\n\n# 一、创建Chart\n\n\n1. 执行命令helm create myapp，会创建一个myapp目录\n   `# helm create myapp` \n1. 查看myapp目录结构\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213908507-37c1f24c-d722-4b8d-89e9-726b4048e21e.png#align=left&display=inline&height=323&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=323&originWidth=357&size=73462&status=done&style=none&width=357)\n\n\n# 二、修改配置文件\n\n\n1. 编辑自描述文件 Chart.yaml , 修改version和appVersion信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213925534-5c591ba4-21ec-4f43-be6c-076a2a0fceba.png#align=left&display=inline&height=484&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=484&originWidth=333&size=88222&status=done&style=none&width=333)\n\n2. 编辑values.yaml配置文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213941106-5641c73c-ca21-4764-8220-cdd31be40e09.png#align=left&display=inline&height=122&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=122&originWidth=432&size=88222&status=done&style=none&width=432)\n\n\n# 三、打包安装chart\n\n\n1. 检查chart语法正确性\n   `# helm lint myapp` \n1. 打包自定义的chart\n   `# helm package myapp` \n1. 安装chart\n   `# helm install myapp myapp-1.tgz` \n1. 验证\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213964472-76d4300b-58fb-4d79-bf0d-b314b58366c5.png#align=left&display=inline&height=273&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=273&originWidth=703&size=113444&status=done&style=none&width=703)\n\n\n# 四、更新\n\n\n1. 编辑自描述文件 Chart.yaml , 修改version和appVersion信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213997516-bc867f00-45fc-451d-a556-c59a0ac0b832.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=240&size=113444&status=done&style=none&width=240)\n\n2. 重新打包charts\n\n- 检查chart语法正确性\n  `# helm lint myapp` \n- 打包自定义的chart\n  `# helm package myapp` \n\n3. 更新chart\n   `# helm upgrade myapp myapp-2.tgz` \n3. 验证\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214043045-708c97cc-38c9-4a0e-9fac-fb39ac394e30.png#align=left&display=inline&height=160&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=160&originWidth=701&size=113444&status=done&style=none&width=701)\n\n\n# 五、回滚\n\n\n1. 查看当前版本信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214065831-f67387cf-66a6-45e5-a53b-d256e15874b2.png#align=left&display=inline&height=101&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=101&originWidth=1167&size=113444&status=done&style=none&width=1167)\n\n2. 查看历史版本信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214087205-b7454028-6ff3-4849-81a4-3c889d4e81d6.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=817&size=113444&status=done&style=none&width=817)\n\n3. 回滚到指定版本\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214103535-470fd467-4396-4558-ac4a-89553272da98.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=463&size=113444&status=done&style=none&width=463)\n\n4. 验证\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214120007-8cdd1acb-02dc-4d99-8426-4af3db6596d7.png#align=left&display=inline&height=159&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=159&originWidth=706&size=113444&status=done&style=none&width=706)', 50, 0, 0, '2020-11-01 19:24:41.021132', '2021-01-26 11:43:50.651903', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (114, 'k8s高可用部署-kubeadm部署', '阿道夫', 'cover/2020_11_01_19_25_26_379963.jpg', '[TOC]\n\n# 一、部署环境\n\n| 主机名  | ip             | docker  | flannel | Keepalived | Haproxy | 主机配置 | 用途                    |\n| ------- | -------------- | ------- | ------- | ---------- | ------- | -------- | ----------------------- |\n| master1 | 192.168.10.138 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点1               |\n| master2 | 192.168.10.139 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点2               |\n| master3 | 192.168.10.140 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点3               |\n| work1   | 192.168.10.141 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点1               |\n| work2   | 192.168.10.142 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点2               |\n| work3   | 192.168.10.143 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点3               |\n| VIP     | 192.168.10.150 | /       | /       | v1.3.5     | v1.5.18 | /        | 虚拟IP在控制节点上浮动  |\n| client  | 192.168.10.145 | /       | /       | /          |         | 2C2G     | 客户端，连接管理K8S集群 |\n\n\n\n# 二、高可用架构\n\n\n![K8S_Ceph云平台架构.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214397461-37728718-19a0-4719-9c0b-a0c1ead7ee38.png#align=left&display=inline&height=1033&margin=%5Bobject%20Object%5D&name=K8S_Ceph%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84.png&originHeight=1033&originWidth=988&size=298518&status=done&style=none&width=988)\n\n\n1. 主备模式高可用架构说明：\n\n| 核心组件           | 高可用模式 | 高可用实现方式  |\n| ------------------ | ---------- | --------------- |\n| apiserver          | 主备       | keepalived      |\n| controller-manager | 主备       | leader election |\n| scheduler          | 主备       | leader election |\n| etcd               | 集群       | kubeadm         |\n\n\n\n- apiserver 通过haproxy+keepalived实现高可用，当某个节点故障时触发keepalived vip 转移；\n- controller-manager k8s内部通过选举方式产生领导者(由--leader-elect 选型控制，默认为true)，同一时刻集群内只有一个controller-manager组件运行；\n- scheduler k8s内部通过选举方式产生领导者(由--leader-elect 选型控制，默认为true)，同一时刻集群内只有一个scheduler组件运行；\n- etcd 通过运行kubeadm方式自动创建集群来实现高可用，部署的节点数为奇数。如果剩余可用节点数量超过半数，集群可以几乎没有影响的正常工作(3节点方式最多容忍一台机器宕机)\n\n# 三、安装准备工作（所有节点都执行）\n\n\n1. 修改主机名\n\n# hostnamectl set-hostname master1\n\n1. 修改hosts文件\n\n# vim /etc/hosts\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214509642-858c8113-28b9-4a78-ab32-7cfd207a8061.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=307&size=33595&status=done&style=none&width=307)\n\n3. 验证mac地址uuid，保证各节点mac和uuid唯一\n\n# cat /sys/class/net/ens33/address\n\n# cat /sys/class/dmi/id/product_uuid\n\n3. 免密登录\n   配置master1到master2、master3免密登录，本步骤只在master01上执行。\n\n- 创建密钥\n\n# ssh-keygen -t rsa\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214539642-a6e00006-560e-481a-9553-b7ed439e525c.png#align=left&display=inline&height=487&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=487&originWidth=749&size=119303&status=done&style=none&width=749)\n\n- 将密钥同步至master2/master3\n  [root[@master01 ](/master01 ) ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root[@master2 ](/master2 ) \n  [root[@master01 ](/master01 ) ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root[@master3 ](/master3 )\n- 免密登陆测试\n\n# ssh root[@master2 ](/master2 ) \n\n# ssh root[@master3 ](/master3 )\n\n- 安装依赖包\n\n# yum -y install conntrack chrony bash-completion ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git iptables-services\n\n5. 时间同步\n\n- master1节点设置\n\n# vim /etc/chrony.conf\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214570252-f2413383-066a-4340-bf09-977ee3cdd222.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=774&size=119303&status=done&style=none&width=774)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214584872-5b85c70c-5662-417a-86aa-86bd0b025cfb.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=585&size=119303&status=done&style=none&width=585)\n\n# systemctl start chronyd\n\n    # systemctl enable chronyd\n\n    # chronyc sources\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214618022-7c33bb76-d23b-4d5a-bc7f-1439895cb881.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=129&originWidth=960&size=119303&status=done&style=none&width=960)\n\n- 其余节点配置\n\n# vim /etc/chrony.conf\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214632701-a16335b0-9c5f-4324-9faa-f338b1528879.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=401&size=119303&status=done&style=none&width=401)\n\n# systemctl start chronyd\n\n# systemctl enable chronyd\n\n# chronyc sources\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214648770-24194bbf-cfba-4509-8e03-deaed1b39bff.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=125&originWidth=468&size=119303&status=done&style=none&width=468)\n\n- 设置防火墙规则\n\n# systemctl stop firewalld\n\n# systemctl disable firewalld\n\n# systemctl start iptables\n\n# systemctl enable iptables\n\n# iptables -F\n\n# service iptables save\n\n6. 关闭selinux\n\n# setenforce 0\n\n# sed -i \'s/^SELINUX=.*/SELINUX=disabled/\' /etc/selinux/config\n\n6. 关闭swap分区\n\n# swapoff -a\n\n# sed -i \'/ swap / s/^(.*)$/#1/g\' /etc/fstab\n\n6. 升级内核（可选）\n\n- 载入公钥\n\n# rpm --import [https://www.elrepo.org/RPM-GPG-KEY-elrepo.org](https://www.elrepo.org/RPM-GPG-KEY-elrepo.org)\n\n- 升级安装ELRepo\n\n# rpm -Uvh [http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm](http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm)\n\ncentos8使用如下命令\n#yum install [https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm](https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm)\n\n- 载入elrepo-kernel元数据\n\n# yum --disablerepo=* --enablerepo=elrepo-kernel repolist\n\n- 安装最新版本的kernel\n\n# yum --disablerepo=* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y\n\n- 删除旧版本工具包\n\n# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y\n\n- 安装新版本工具包\n\n# yum --disablerepo=* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y\n\n- 查看内核插入顺序\n\n# awk -F \'\'$1==\"menuentry \" {print i++ \" : \" $2}\' /etc/grub2.cfg\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214685031-f8a86358-72bc-48d5-9804-8960eb74418a.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=823&size=119303&status=done&style=none&width=823)\n\n- 设置默认启动\n\n# grub2-set-default 0 // 0代表当前第一行，也就是5.5版本\n\n# grub2-editenv list\n\n- 重启验证\n\n# uname -a\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214700346-4f7688a8-46ea-436e-9cec-5b6350be6e6c.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=471&size=119303&status=done&style=none&width=471)\n\n- 修改内核iptables相关参数\n\n# cat <<EOF > /etc/sysctl.d/kubernetes.conf\n\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n\n# sysctl -p /etc/sysctl.d/kubernetes.conf\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214725442-867f089e-6fae-4b31-a54c-84c5fdce7b71.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=473&size=119303&status=done&style=none&width=473)\n\n\n# 四、kubernetes安装前设置（每个节点）\n\n\n1. kube-proxy开启ipvs的前置条件\n\n# yum -y install ipset ipvsadm\n\n# cat > /etc/sysconfig/modules/ipvs.modules <<EOF\n\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n\n# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash\n\n# /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214760297-066b2aef-f381-49d9-98fa-0038086bcb22.png#align=left&display=inline&height=201&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=201&originWidth=713&size=119303&status=done&style=none&width=713)\n\n- linux kernel 4.19版本已经将nf_conntrack_ipv4 更新为 nf_conntrack\n\n2. docker安装（所有节点）\n\n- 安装前源准备\n  yum install -y yum-utils device-mapper-persistent-data lvm2\n- 配置yum源\n  yum-config-manager --add-repo\n  [http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo](http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo)\n- 查看可安装的docker版本\n  yum list docker-ce --showduplicates | sort -r\n- 安装19.03.0版本docker\n  yum install -y docker-ce-19.03.0*\n- 使用阿里云做镜像加速\n  mkdir -p /etc/docker\n  tee /etc/docker/daemon.json <<-\'EOF\'\n  {\n  \"registry-mirrors\": [\"[https://o2j0mc5x.mirror.aliyuncs.com](https://o2j0mc5x.mirror.aliyuncs.com)\"]\n  }\n  EOF\n  systemctl daemon-reload\n- 启动docker\n  systemctl start docker\n  systemctl enable docker\n- docker 1.13以上版本默认禁用iptables的forward调用链，因此需要执行开启命令：\n  iptables -P FORWARD ACCEPT\n- 修改docker cgroup driver为systemd\n  使用systemd作为docker的cgroup\n  driver可以确保服务器节点在资源紧张的情况更加稳定\n  [root[@master ](/master ) ~]#vim /etc/docker/daemon.json \n  {\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"]\n  }\n  systemctl daemon-reload\n  systemctl restart docker\n  docker info | grep Cgroup\n- 配置阿里云yum源\n  [root[@master ](/master ) ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo \n  [kubernetes]\n  name=Kubernetes\n  baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\n  enabled=1\n  gpgcheck=1\n  repo_gpgcheck=1\n  gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\n  [https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg](https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg)\n  EOF\n\n3. 安装命令补全\n\n# yum -y install bash-completion\n\n# source /etc/profile.d/bash_completion.sh\n\n3. 安装kubelet、kubeadm和kubectl组件（所有节点）\n\n- 安装软件包\n\n# yum install -y kubelet kubeadm kubectl\n\nkubelet 运行在集群所有节点上，用于启动Pod和容器等对象的工具\nkubeadm 用于初始化集群，启动集群的命令工具\nkubectl\n用于和集群通信的命令行，通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件\n\n- 设置kubelet开机启动\n\n# systemctl enable kubelet\n\n- kubectl命令补全\n\n# echo \"source <(kubectl completion bash)\" >> ~/.bash_profile\n\n# source .bash_profile\n\n\n\n# 五、部署keepalived（master节点）\n\n\n- 此处的keeplived的主要作用是为haproxy提供vip（192.168.10.150），在三个haproxy实例之间提供主备，降低当其中一个haproxy失效的时对服务的影响。\n- 系统配置\n\n# cat >> /etc/sysctl.conf << EOF\n\nnet.ipv4.ip_forward = 1\nEOF\nsysctl -p\n\n- 安装keepalived\n  yum install -y keepalived\n\n1. 配置keepalived，修改/etc/keepalived/keepalived.conf文件\n\n- master1配置\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214820644-b211e033-0614-4ed0-bb4c-16b8d7cfc944.png#align=left&display=inline&height=740&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=740&originWidth=441&size=134163&status=done&style=none&width=441)\n\n- master2配置\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214836650-69d4adfb-2cad-4755-a469-810273965fbf.png#align=left&display=inline&height=738&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=738&originWidth=402&size=134163&status=done&style=none&width=402)\n\n- master3配置\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214850883-af21ce4d-4e4c-4351-b5a4-f1baa31a2fff.png#align=left&display=inline&height=739&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=739&originWidth=414&size=134163&status=done&style=none&width=414)\n\n- 启动keepalived\n\n# systemctl start keepalived\n\n# systemctl enable keepalived\n\n2. VIP查看\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214868024-3f8bc14a-1813-4dcf-a913-f0f749fd878f.png#align=left&display=inline&height=372&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=372&originWidth=580&size=142912&status=done&style=none&width=580)\n\n\n# 六、部署haproxy（master节点）\n\n> haproxy为apiserver提供反向代理，haproxy将所有请求轮询转发到每个master节点上。相对于仅仅使用keepalived主备模式仅单个master节点承载流量，这种方式更加合理、健壮。\n\n1. 系统配置\n\n# cat >> /etc/sysctl.conf << EOF\n\nnet.ipv4.ip_nonlocal_bind = 1\nEOF\n\n# sysctl -p\n\n1. 安装haproxy\n\n# yum install -y haproxy\n\n1. 配置haproxy（所有master节点一样）\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214920117-959e5c24-887e-41c0-9155-da3059e25ff6.png#align=left&display=inline&height=130&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=130&originWidth=596&size=142912&status=done&style=none&width=596)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214936866-552df92a-4f60-4c35-ab7c-f6c6db304121.png#align=left&display=inline&height=327&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=327&originWidth=618&size=142912&status=done&style=none&width=618)\n\n4. 启动并检测服务\n\n# systemctl enable haproxy.service\n\n# systemctl start haproxy.service\n\n# ss -lntp | grep 6443\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214954799-5b92803f-88d9-4e8d-a450-757356c74d04.png#align=left&display=inline&height=53&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=53&originWidth=611&size=142912&status=done&style=none&width=611)\n\n\n# 七、初始化master节点（master1节点操作）\n\n\n1. 创建集群配置文件kubeadm-config.yaml\n\n```yaml\napiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.17.2\nimageRepository: registry.aliyuncs.com/google_containers\napiServer:\n  certSANs:    \n  #填写所有kube-apiserver节点的hostname、IP、VIP\n  - master1\n  - master2\n  - master3\n  - work1\n  - work2\n  - work3\n  - 192.168.10.138\n  - 192.168.10.139\n  - 192.168.10.140\n  - 192.168.10.141\n  - 192.168.10.142  \n  - 192.168.10.143\n  - 192.168.10.150\ncontrolPlaneEndpoint: \"192.168.10.150:16443\"\n#VIP:port\nnetworking:\n  podSubnet: \"10.244.0.0/16\"\n```\n\n2. master1节点初始化\n\n# kubeadm init --config=kubeadm-config.yaml\n\n2. 记录kubeadm\n   join的输出，后面需要这个命令将work节点和其他master节点加入集群中。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215043155-d557c36d-f562-4674-90cc-0d34752ff1bf.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=183&originWidth=918&size=142912&status=done&style=none&width=918)\n\n- 如果初始化失败：\n\n# kubeadm reset\n\n# rm -rf $HOME/.kube/config\n\n- 根据提示配置环境变量\n\n# mkdir -p $HOME/.kube\n\n# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n\n# chown $(id -u):$(id -g) $HOME/.kube/config\n\n5. 安装flannel网络\n\n- 下载配置文件\n\n# wget [https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml](https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml)\n\n- 启用flannel\n\n# kubectl apply -f kube-flannel.yml\n\n- 如果镜像不能正常下载，所有节点需提前导入镜像\n\n\n\n# 八、master2、master3节点加入集群\n\n\n1. 证书分发\n\n- master2、master3创建证书目录\n\n# mkdir -p /etc/kubernetes/pki/etcd\n\n- master1运行脚本cert-main-master.sh，将证书分发至master02和master03\n\n```bash\n#cert-main-master.sh \nUSER=root\nCONTROL_PLANE_IPS=\"master2 master3\"\nfor host in ${CONTROL_PLANE_IPS}; do\n    scp /etc/kubernetes/pki/ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/ca.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/sa.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/sa.pub \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/front-proxy-ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/front-proxy-ca.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/etcd/ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/etcd/\n    scp /etc/kubernetes/pki/etcd/ca.key \"${USER}\"@$host:/etc/kubernetes/pki/etcd/\ndone\n```\n\n\n2. master2、master3加入集群\n\n- 执行加入节点命令\n  kubeadm join 192.168.10.150:16443 --token uueq7e.qz5tt55dx5y8vyuf  --discovery-token-ca-cert-hash\n  sha256:ff411ba37bc403c9c87cae64355d222a6d8775737a65ac79b411256f58666eb7 --control-plane\n\n3. 根据提示配置环境变量\n\n# mkdir -p $HOME/.kube\n\n# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n\n# chown $(id -u):$(id -g) $HOME/.kube/config\n\n3. 查看验证\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215152521-160428f7-e9c0-48ed-883e-07352c89d78a.png#align=left&display=inline&height=662&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=662&originWidth=758&size=285628&status=done&style=none&width=758)\n\n\n# 九、work节点加入集群（所有work节点执行）\n\n\n1. work1、2、3执行加入集群命令\n   kubeadm join 192.168.10.150:16443 --token uueq7e.qz5tt55dx5y8vyuf  --discovery-token-ca-cert-hash\n    sha256:ff411ba37bc403c9c87cae64355d222a6d8775737a65ac79b411256f58666eb7\n1. master节点上查看所有节点状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215185744-89e0b31e-5e77-41c8-8d32-e49a169b4902.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=519&size=285628&status=done&style=none&width=519)\n\n\n# 十、client配置\n\n\n1. 设置kubernetes源\n\n```bash\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttps://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n2. 安装kubectl\n\n# yum install -y kubectl\n\n- 安装版本与集群版本保持一致\n\n3. 命令补全\n\n- 安装bash-completion\n\n# yum -y install bash-completion\n\n- bash-completion\n\n# source /etc/profile.d/bash_completion.sh\n\n4. 拷贝admin.conf\n\n# mkdir -p /etc/kubernetes\n\n# scp 192.168.10.138:/etc/kubernetes/admin.conf /etc/kubernetes/\n\n# echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" >>\n\n~/.bash_profile\n\n# source .bash_profile\n\n4. 加载环境变量\n\n# echo \"source <(kubectl completion bash)\" >> ~/.bash_profile\n\n# source .bash_profile\n\n4. kubectl测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215252016-c69542f0-7250-476b-8d3c-3f319949d484.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=321&originWidth=579&size=285628&status=done&style=none&width=579)\n\n\n# 十一、Dashboard搭建（client端操作）\n\n\n1. 因为自动生成的证书很多浏览器无法使用，所以自己创建证书\n1. 新建证书存放目录\n   mkdir /etc/kubernetes/dashboard-certs\n   cd /etc/kubernetes/dashboard-certs/\n1. 创建命名空间\n   kubectl create namespace kubernetes-dashboard\n1. 创建key文件\n   openssl genrsa -out dashboard.key 2048\n1. 证书请求\n   openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj\n   \'/CN=dashboard-cert\'\n1. 自签证书\n   openssl x509 -req -in dashboard.csr -signkey dashboard.key -out\n   dashboard.crt\n1. 创建kubernetes-dashboard-certs对象\n   kubectl create secret generic kubernetes-dashboard-certs\n   --from-file=dashboard.key --from-file=dashboard.crt -n\n   kubernetes-dashboard\n1. 下载并修改配置文件\n\n- 下载配置文件\n  wget [https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml](https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml)\n- 修改配置文件，增加直接访问端口\n  [root[@master ](/master ) ~]# vim recommended.yaml \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215287691-e6bd84d5-5b5a-4663-8aef-1e8d547ceae5.png#align=left&display=inline&height=200&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=200&originWidth=458&size=285628&status=done&style=none&width=458)\n\n- 修改配置文件，注释原kubernetes-dashboard-certs对象声明\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215302751-bdb856e7-4ba8-42d0-8953-c6b0ba08f94f.png#align=left&display=inline&height=256&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=256&originWidth=431&size=285628&status=done&style=none&width=431)\n\n- 运行dashboard\n  [root[@master ](/master ) ~]# kubectl apply -f recommended.yaml \n\n9. 更新配置信息\n\n- 创建Dashboard管理员账号dashboard-admin.yaml，并apply\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n- 赋权dashboard-admin-bind-cluster-role.yaml，并apply\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-bind-cluster-role\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n- 获取token信息\n  kubectl -n kubernetes-dashboard describe secret $(kubectl -n\n  kubernetes-dashboard get secret | grep dashboard-admin | awk \'{print $1}\')\n\n10. 登录访问[https://192.168.10.150:30001](https://192.168.10.150:30001)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215381513-3963f5df-dcd6-4d4c-8965-e03f0964e4d2.png#align=left&display=inline&height=690&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=690&originWidth=1258&size=285628&status=done&style=none&width=1258)\n\n\n# 十二、集群高可用测试（client节点）\n\n\n1. 组件所在节点查看\n1. 查看apiserver所在节点（当前位于master1）\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215414607-ed319661-1b57-4a4f-bc7b-519b766f4555.png#align=left&display=inline&height=104&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=104&originWidth=539&size=285628&status=done&style=none&width=539)\n\n3. 查看scheduler所在节点（当前位于master3）\n\n# kubectl get endpoints kube-controller-manager -n kube-system -o yaml |grep holderIdentity\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215434024-ebbc1b50-8f6a-4629-9412-cabd38f2fa39.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=868&size=285628&status=done&style=none&width=868)[ ](media/6630119cd28b15c13995018641aa4492.png)\n\n4. 查看controller-manager所在节点（当前位于master3）\n   [root[@client ](/client ) ~]# kubectl get endpoints kube-scheduler -n kube-system  -o yaml |grep holderIdentity\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215452891-5232e489-bd6c-4173-b99f-7251656bfaa6.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=866&size=285628&status=done&style=none&width=866)\n\n5. 创建deployment和svc，模拟生产业务\n\n- deployment资源清单文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215468725-ddd2884b-f027-4628-a987-05838dd11018.png#align=left&display=inline&height=500&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=500&originWidth=468&size=285628&status=done&style=none&width=468)\n\n- svc资源清单文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215485214-2295d42f-6e00-4a90-9d1e-db03c41a50c9.png#align=left&display=inline&height=278&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=278&originWidth=266&size=285628&status=done&style=none&width=266)\n\n- client节点访问测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215503065-0b6fac4d-086c-42ab-a2e7-59e75135a6a6.png#align=left&display=inline&height=369&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=369&originWidth=1038&size=285628&status=done&style=none&width=1038)\n\n3. master1节点关机，模拟宕机\n\n- 关闭master1，模拟宕机\n\n# init 0\n\n- 各组件查看\n  apiserver位于master1\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215529784-8387132b-0fda-4550-8f2e-248a44b17a5e.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=539&size=285628&status=done&style=none&width=539)\n  controller-manager位于master3\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215546032-e4b8fa3b-7736-430f-9985-709489609e93.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=74&originWidth=757&size=285628&status=done&style=none&width=757)\n  scheduler位于master3\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215561038-4610c096-801a-4f97-9ab1-5831b751f436.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=758&size=285628&status=done&style=none&width=758)\n\n- 集群节点信息查看\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215579145-23d2239a-6ff7-40c8-a4a9-2e77afb0e562.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=518&size=285628&status=done&style=none&width=518)\n\n- 业务访问测试\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215594824-6bb85bc0-d8b5-42f0-b8e4-f4af13d202fd.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=767&size=285628&status=done&style=none&width=767)\n\n- 结论\n  当有一个master节点宕机时，VIP会发生漂移，集群各项功能不受影响。\n\n4. master3关机，模拟2台节点宕机\n\n- 关闭master3:\n\n# init 0\n\n- 查看VIP：\n\n# ip a|grep 150\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215620990-41c12dba-53f1-4fd0-9d68-332b4dd25cf6.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=541&size=285628&status=done&style=none&width=541)\n    vip漂移至唯一的master2\n\n- 集群功能测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215638949-54d6021e-f694-49c0-b7db-c8bd813ca1de.png#align=left&display=inline&height=101&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=101&originWidth=763&size=285628&status=done&style=none&width=763)\n\n- 由于etcd集群崩溃，整个k8s集群也不能正常对外服务。', 12, 0, 0, '2020-11-01 19:27:08.945128', '2021-01-21 07:08:57.679465', 1, 6, 0, 0, 0);
INSERT INTO `blog_article` VALUES (115, 'k8s高可用部署-其他高可用部署方式', 'k8s高可用部署工具有：一、命令行工具部署：sealos', 'cover/2020_11_01_19_28_36_694024.jpg', '[TOC]\n\n# 一、命令行工具部署：sealos\n\n\n[https://github.com/fanux/sealos](https://github.com/fanux/sealos)\n\n\n# 二、图形界面部署：breeze\n\n\n[https://github.com/wise2c-devops/breeze](https://github.com/wise2c-devops/breeze)\n\n\n# 三、rancher平台部署\n\n\n[https://rancher.com/docs/rancher/v2.x/en/](https://rancher.com/docs/rancher/v2.x/en/)\n\n# 四、KubeSphere 容器平台\n\n\n[https://kubesphere.com.cn/docs/](https://kubesphere.com.cn/docs/)\n\n', 58, 0, 0, '2020-11-01 19:28:40.946551', '2021-01-26 05:38:19.547529', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (116, 'k8s维护-更改证书有效期', 'k8s更改证书有效期分为以下几步：一、基础知识', 'cover/2020_11_01_19_30_51_041678.jpg', '[TOC]\n# 一、基础知识\n\n\n1. k8s使用https双向认证，使用kubeadm安装后证书默认有效期为1年\n1. k8s证书存放路径：/etc/kubernetes/pki/\n1. 查看证书信息：openssl x509 -in apiserver.crt -text -noout\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218118360-1e2d43cb-d470-49ca-ac00-3c68f8e412bb.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=459&size=285628&status=done&style=none&width=459)\n\n4. 修改方式：修改kubeadm源码，更改证书默认有效期\n\n# 二、go 环境部署\n\n\n1. 下载go安装包\n   wget [https://studygolang.com/dl/golang/go1.13.7.linux-amd64.tar.gz](https://studygolang.com/dl/golang/go1.13.7.linux-amd64.tar.gz)\n1. 解压到/usr/local下\n   tar -zxvf go1.13.7.linux-amd64.tar.gz -C /usr/local/\n1. 修改环境变量\n   vim /etc/profile\n   export PATH=$PATH:/usr/local/go/bin\n1. 加载环境变量配置\n   source /etc/profile\n1. 查看版本信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218137951-2f4e6264-6e00-49cb-b2ce-f82bf12558fe.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=366&size=285628&status=done&style=none&width=366)\n\n\n# 三、git源码\n\n\n1. 下载源码至本地\n   git clone [https://github.com/kubernetes/kubernetes.git](https://github.com/kubernetes/kubernetes.git)\n1. 查看当前k8s版本\n   kubeadm version\n1. 切换git到指定版本\n   cd kubernetes\n   git checkout -b remotes/origin/release-1.17.0 v1.17.0\n\n\n\n# 四、修改源码更新证书策略\n\n\n1. 查看相关代码所在文件名称\n   kubernetes]# grep -r kubeadmconstants.CertificateValidity .\n1. 更改源代码\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218156767-a917821f-ed9b-4d9e-8e84-429c85bafa1c.png#align=left&display=inline&height=560&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=560&originWidth=734&size=285628&status=done&style=none&width=734)\n\n3. 编译代码kubeadm\n   kubernetes] #make WHAT=cmd/kubeadm GOFLAGS=-v\n3. 将编译成功的kubeadm复制到/root下\n   kubernetes] #cp _output/bin/kubeadm /root/kubeadm-new\n\n\n\n# 五、更新 kubeadm\n\n\n1. 备份原kubeadm文件\n   cp /usr/bin/kubeadm /usr/bin/kubeadm.old\n1. 替换新kubeadm文件\n   cp /root/kubeadm-new /usr/bin/kubeadm\n1. 赋予执行权限\n   chmod a+x /usr/bin/kubeadm\n\n\n\n# 六、更新master节点证书\n\n\n1. 备份原证书文件\n   cp -r /etc/kubernetes/pki /etc/kubernetes/pki.old\n1. 生成新证书\n   cd /etc/kubernetes/pki\n   kubeadm alpha certs renew all\n1. 查看证书有效期\n   openssl x509 -in apiserver.crt -text -noout | grep Not\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218178574-ee3aea38-bdb9-43ec-9305-51a78495d8dc.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=794&size=285628&status=done&style=none&width=794)\n\n\n# 七、HA其余master节点证书更新\n\n```bash\n#!/bin/bash\nmasterNode=\"192.168.66.20 192.168.66.21\"\n#for host in ${masterNode}; do\n#    scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key}\"\n${USER}\"@$host:/etc/kubernetes/pki/\n#    scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} \"root\"@$host:/etc/kubernetes/pki/etcd\n#    scp /etc/kubernetes/admin.conf \"root\"@$host:/etc/kubernetes/\n#done\nfor host in${CONTROL_PLANE_IPS}; do\n    scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key}\"${USER}\"@$host:/root/pki/\n    scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} \"root\"@$host:/root/etcd   \n    scp /etc/kubernetes/admin.conf \"root\"@$host:/root/kubernetes/\ndone\n\n```\n\n\n\n', 54, 0, 0, '2020-11-01 19:30:56.839009', '2021-01-27 00:33:17.921546', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (118, 'k8s维护-k8s版本升级', 'k8s版本升级分为以下几步：一、准备工作', 'cover/2020_11_01_19_33_12_080189.jpg', '[TOC]\n\n# 一、准备工作\n\n\n1. 在master节点上查看此时的kubernetes的版本\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218267571-0336ccbf-4d1c-4b45-8b46-acb5109de78c.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=884&size=285628&status=done&style=none&width=884)\n\n2. 查询最新版本号\n   yum list kubeadm --showduplicates | sort -r\n\n\n\n# 二、升级操作\n\n\n1. master和node节点执行升级命令\n   yum update -y kubeadm kubectl kubelet\n1. master节点验证升级版本\n   kubeadm upgrade plan\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218285326-7a41d155-c92d-4043-a838-6b1577c15a41.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=80&originWidth=716&size=285628&status=done&style=none&width=716)\n\n3. 升级到指定版本\n   kubeadm upgrade apply v1.17.2\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218305417-9dd74318-4eb7-43e9-8e75-e59015006a0f.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=829&size=285628&status=done&style=none&width=829)\n\n4. master和node重启kubelet\n   systemctl daemon-reload\n   systemctl restart kubelet\n\n\n\n# 三、验证\n\n\n1. 查看node信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218326559-05d1773f-72d6-4970-8a93-d740f6602595.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=440&size=285628&status=done&style=none&width=440)\n\n', 54, 0, 0, '2020-11-01 19:32:35.808434', '2021-01-26 05:38:23.995718', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (119, 'k8s维护-添加work节点', '本文介绍了如果给k8s集群添加work工作节点', 'cover/2020_11_01_19_34_51_518057.jpg', '1. work节点进行初始化操作\n1. master节点查询join命令\n\n`# kubeadm token create --print-join-command`\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218421749-8e84c96b-7788-421b-81ce-ea902b84aa12.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=879&size=285628&status=done&style=none&width=879)\n\n3. work节点执行kubeadm join命令\n3. master节点查看node信息', 60, 0, 0, '2020-11-01 19:34:53.994087', '2021-01-26 05:38:26.648032', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (120, 'k8s维护-控制节点启用pod调度', '默认情况下，出于安全原因，您的群集不会在控制节点上调度Pod。如果您希望能够在控制平面节点上调度Pod，例如用于单机Kubernetes集群进行开发，去除污点标记即可', 'cover/2020_11_01_19_37_11_107846.jpg', '\n> 默认情况下，出于安全原因，您的群集不会在控制节点上调度Pod。如果您希望能够在控制平面节点上调度Pod，例如用于单机Kubernetes集群进行开发，请运行：\n\nkubectl taint nodes --all node-role.kubernetes.io/master-\n\n', 49, 0, 0, '2020-11-01 19:37:14.959935', '2021-01-26 05:38:31.023389', 1, 6, 0, 1, 0);
INSERT INTO `blog_article` VALUES (121, 'k8s维护-日常错误排错', '本文主要从集群状态查看和日志查看两方面介绍日常运维工作中出现问题如何查找日志快速排查错误', 'cover/2020_11_01_19_38_33_321421.jpg', '[TOC]\n# 一、状态查看\n\n\n1. 查看 Pod 状态以及运行节点\n   kubectl get pods -o wide\n   kubectl -n kube-system get pods -o wide\n1. 查看 Pod 事件\n   kubectl describe pod <pod-name>\n1. 查看 Node 状态\n   kubectl get nodes\n   kubectl describe node <node-name>\n\n\n\n# 二、日志查看\n\n\n1. kube-apiserver 日志\n   PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-apiserver 是用 systemd 管理的，则需要登录到 master 节点上，然后使用\n  journalctl -u kube-apiserver 查看其日志。\n\n\n\n2. kube-controller-manager 日志\n   PODNAME=$(kubectl -n kube-system get pod -l\n   component=kube-controller-manager -o jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-controller-manager 是用 systemd 管理的，则需要登录到 master\n  节点上，然后使用 journalctl -u kube-controller-manager 查看其日志。\n\n\n\n3. kube-scheduler 日志\n   PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-scheduler 是用 systemd 管理的，则需要登录到 master 节点上，然后使用\n  journalctl -u kube-scheduler 查看其日志。\n\n\n\n4. kube-dns 日志\n   kube-dns 通常以 Addon 的方式部署，每个 Pod 包含三个容器，最关键的是 kubedns\n   容器的日志：\n   PODNAME=$(kubectl -n kube-system get pod -l k8s-app=kube-dns -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME -c kubedns\n4. Kubelet 日志\n   Kubelet 通常以 systemd 管理。查看 Kubelet 日志需要首先 SSH 登录到 Node\n   上，推荐使用 kubectl-node-shell而不是为每个节点分配公网 IP 地址。比如：··\n\n```bash\n[root@localhost ~]# cat kubectl-node_shell\n#!/bin/sh\nif [ -z \"$1\" ]; then\n  echo \"Please specify node name\"\n  exit 1\nfi\n\nNODE=\"$1\"\nIMAGE=\"alpine\"\nPOD=\"nsenter-$(env LC_CTYPE=C tr -dc a-z0-9 < /dev/urandom | head -c 6)\"\nNAMESPACE=\"\"\n\n# Check the node\nkubectl get node \"$NODE\" >/dev/null || exit 1\n\nOVERRIDES=\"$(cat <<EOT\n{\n  \"spec\": {\n    \"nodeName\": \"$NODE\",\n    \"hostPID\": true,\n    \"containers\": [\n      {\n        \"securityContext\": {\n          \"privileged\": true\n        },\n        \"image\": \"$IMAGE\",\n        \"name\": \"nsenter\",\n        \"stdin\": true,\n        \"stdinOnce\": true,\n        \"tty\": true,\n        \"command\": [ \"nsenter\", \"--target\", \"1\", \"--mount\", \"--uts\", \"--ipc\", \"--net\", \"--pid\", \"--\", \"bash\", \"-l\" ]\n      }\n    ]\n  }\n}\nEOT\n)\"\n\necho \"spawning \"$POD\" on \"$NODE\"\"\nkubectl run --namespace \"$NAMESPACE\" --rm --image alpine --overrides=\"$OVERRIDES\" --generator=run-pod/v1 -ti \"$POD\"\nchmod +x ./kubectl-node_shell\nsudo mv ./kubectl-node-shell /usr/local/bin/kubectl-node_shell\n[root@localhost ~]# ./kubectl-node_shell localhost.localdomain\nspawning \"nsenter-i71opm\" on \"localhost.localdomain\"\nIf you don\'t see a command prompt, try pressing enter.\n[root@localhost /]# journalctl -l -u kubelet\n```\n\n\n6. Kube-proxy 日志\n   Kube-proxy 通常以 DaemonSet 的方式部署，可以直接用 kubectl 查询其日志\n   $ kubectl -n kube-system get pod -l component=kube-proxy\n   NAME READY STATUS RESTARTS AGE\n   kube-proxy-42zpn 1/1 Running 0 1d\n   kube-proxy-7gd4p 1/1 Running 0 3d\n   kube-proxy-87dbs 1/1 Running 0 4d\n   $ kubectl -n kube-system logs kube-proxy-42zpn', 362, 16, 0, '2020-11-01 19:39:33.469438', '2021-01-26 15:26:03.375669', 1, 6, 0, 1, 1);
INSERT INTO `blog_article` VALUES (122, 'k8s高可用部署-源码部署', '摘要', 'cover/2020_11_01_19_40_47_367718.jpg', '', 9, 0, 0, '2020-11-01 19:40:58.433285', '2021-01-21 08:30:53.503658', 1, 6, 0, 0, 0);
INSERT INTO `blog_article` VALUES (124, 'linux运维工程师面试题总结', '总结了近期参加了数十场linux运维工程师面试，总结了常见面试问题，希望可以帮助到大家。', 'cover/2020_12_11_20_32_48_685428.jpg', '> 本人花费为期一周的时间，参加了数十家互联网公司的面试，主要方向是linux、容器运维、自动化运维，其中包括IBM、新浪、完美世界等公司。以下是本人参加面试时遇到的考题，希望可以帮助到大家！\n\n# 一、linux\n\n1. 系统启动流程\n1. linux文件类型\n1. centos6和7怎么添加程序开机自启动？\n1. 如何升级内核，目前最新版本号多少？\n1. nginx日志访问量前十的ip怎么统计？\n1. 删除/var/log/下.log结尾的30天前的日志文件\n1. ansible有哪些模块？功能是什么？\n1. nginx性能为什么比apache高？\n1. 四层负载和七层负载区别是什么？\n1. lvs有哪些工作模式？哪个性能高？\n1. lvs nginx haproxy keeplived区别，优缺点？\n1. 如下url地址，各个部分的含义\nhttps://www.baidu.com/s?word=123&ie=utf-8\n1. tomcat各个目录含义，如何修改端口，如何修改内存数？\n1. nginx反向代理时，如何使后端获取真正的访问来源ip？\n1. nginx负载均衡算法有哪些？\n1. 如何进行压力测试？\n1. curl命令如何发送https请求？如何查看response头信息？如何发送get和post表单信息？\n\n# 二、mysql\n\n1. 索引的为什么使查询加快？有啥缺点？\n1. sql语句左外连接 右外连接 内连接 全连接区别\n1. mysql数据备份方式，如何恢复？你们的备份策略是什么？\n1. 如何配置数据库主从同步，实际工作中是否遇到数据不一致问题？如何解决？\n1. mysql约束有哪些？\n1. 二进制日志（binlog）用途？\n1. mysql数据引擎有哪些？\n1. 如何查询mysql数据库存放路径？\n1. mysql数据库文件后缀名有哪些？用途什么？\n1. 如何修改数据库用户的密码？\n1. 如何修改用户权限？如何查看？\n\n# 三、nosql\n\n1. redis数据持久化有哪些方式？\n1. redis集群方案有哪些？\n1. redis如何进行数据备份与恢复？\n1. MongoDB如何进行数据备份？\n1. kafka为何比redis rabbitmq快？\n\n# 四、docker\n\n1. dockerfile有哪些关键字？用途是什么？\n1. 如何减小dockerfile生成镜像体积？\n1. dockerfile中CMD与ENTRYPOINT区别是什么？\n1. dockerfile中COPY和ADD区别是什么？\n1. docker的cs架构组件有哪些？\n1. docker网络类型有哪些？\n1. 如何配置docker远程访问？\n1. docker核心namespace CGroups 联合文件系统功能是什么？\n1. 命令相关：导入导出镜像，进入容器，设置重启容器策略，查看镜像环境变量，查看容器占用资源\n1. 构建镜像有哪些方式？\n1. docker和vmware虚拟化区别？\n\n\n\n# 五、kubernetes\n\n1. k8s的集群组件有哪些？功能是什么？\n1. kubectl命令相关：如何修改副本数，如何滚动更新和回滚，如何查看pod的详细信息，如何进入pod交互？\n1. etcd数据如何备份？\n1. k8s控制器有哪些？\n1. 哪些是集群级别的资源？\n1. pod状态有哪些？\n1. pod创建过程是什么？\n1. pod重启策略有哪些？\n1. 资源探针有哪些？\n1. requests和limits用途是什么？\n1. kubeconfig文件包含什么内容，用途是什么？\n1. RBAC中role和clusterrole区别，rolebinding和 clusterrolebinding区别？\n1. ipvs为啥比iptables效率高？\n1. sc pv pvc用途，容器挂载存储整个流程是什么？\n1. nginx ingress的原理本质是什么？\n1. 网络类型，描述不同node上的Pod之间的通信流程\n1. k8s集群节点需要关机维护，需要怎么操作\n\n# 六、prometheus\n\n1. prometheus对比zabbix有哪些优势？\n1. prometheus组件有哪些，功能是什么？\n1. 指标类型有哪些？\n1. 在应对上千节点监控时，如何保障性能\n1. （降低采集频率，缩小历史数据保存天数，使用集群联邦和远程存储）\n1. 简述从添加节点监控到grafana成图的整个流程\n1. 在工作中用到了哪些exporter\n\n# 七、ELK\n\n1. Elasticsearch的数据如何备份与恢复？\n1. 你们项目中使用的logstash过滤器插件是什么？实现哪些功能？\n1. 是否用到了filebeat的内置module？用了哪些？\n1. kibana如何自定义图表和仪表盘？\n1. elasticsearch分片副本是什么？你们配置的参数是多少？\n\n# 八、运维开发\n\n1. 备份系统中所有镜像\n1. 编写脚本，定时备份某个库，然后压缩，发送异机\n1. （注意：①公共部分定义函数，如获取时间戳，配置报警接口②异常处理，如数据库大，检测任务是否完成。检测生成文件大小是否是空文件）\n1. 批量获取所有主机的系统信息\n1. django的mtv模式流程\n1. python如何导出、导入环境依赖包\n1. python创建，进入，退出，查看虚拟环境\n1. flask和django区别，应用场景\n1. flask开发一个hello word页面流程\n1. 列举常用的git命令\n1. git gitlab jenkins的CICD流程如何配置\n\n\n\n# 九、日常工作\n\n1. 在日常工作中遇到了什么棘手的问题，如何排查\n\n（①redis弱口令导致中挖矿病毒，排查，优化②k8s中开发的程序在用户上传文件时开启进程，未及时关闭，导致节点超出最大进程数）\n\n3. 日常故障处理流程\n2. 修改线上业务配置文件流程\n2. 业务pv多少？集群规模多少？怎么保障业务高可用？\n\n# 十、开放性问题\n\n1. 你认为初级运维工程师和高级运维工程师的区别？（初级干活的，会操作，顺利完成领导安排的任务。高级优化架构，研究如何避免问题，研究新技术并引用）\n1. 你认为未来运维发展方向（自动化，智能化）\n\n\n\n', 1547, 15, 0, '2020-11-19 20:31:03.800689', '2021-01-27 11:19:47.418053', 1, 8, 0, 1, 1);
INSERT INTO `blog_article` VALUES (125, '运维工程师面试总结(含答案)', '总结了近期面试的所有面试题，并整理相关答案内容，希望可以帮到大家顺利找到称心满意的工作！', 'cover/2020_12_11_20_27_56_948556.jpg', '[TOC]\n\n# 一、linux\n## 1. linux系统启动流程\n\n- 第一步：开机自检，加载BIOS\n- 第二步：读取ＭＢＲ\n- 第三步：Boot Loader　grub引导菜单\n- 第四步：加载kernel内核\n- 第五步：init进程依据inittab文件夹来设定运行级别\n- 第六步：init进程执行rc.sysinit\n- 第七步：启动内核模块\n- 第八步：执行不同运行级别的脚本程序\n- 第九步：执行/etc/rc.d/rc.lo\n\n## 2. linux文件类型\n\n| 文件属性 | 文件类型 |\n| --- | --- |\n| - | 常规文件，即file |\n| d | 目录文件 |\n| b | block device 即块设备文件，如硬盘;支持以block为单位进行随机访问 |\n| c | character device 即字符设备文件，如键盘支持以character为单位进行线性访问 |\n| l | symbolic link 即符号链接文件，又称软链接文件 |\n| p | pipe 即命名管道文件 |\n| s | socket 即套接字文件，用于实现两个进程进行通信 |\n\n\n\n## 3. centos6和7怎么将源码安装的程序添加到开机自启动？\n\n- 通用方法：编辑/etc/rc.d/rc.local文件，在文件末尾添加启动服务命令\n- centos6：①进入到/etc/rc.d/init.d目录下，②新建一个服务启动脚本，脚本中指定chkconfig参数，③添加执行权限，④执行chkconfig --add 添加服务自启动\n- centos7：①进入到/usr/lib/systemd/system目录下，②新建自定义服务文件，文件中包含[Unit] [Service] [Install]相关配置，然后添加下执行权限，③执行systemctl enable 服务名称\n\n## 4. 简述lvm，如何给使用lvm的/分区扩容？\n\n- 功能：可以对磁盘进行动态管理。动态按需调整大小\n- 概念：\n\n①PV - 物理卷：物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备。 \n②VG - 卷组：卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 \n③LV - 逻辑卷：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。\n![SAVE_20201124_085444.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606179298183-d171b82b-46b7-4d7c-9cd6-08942e923776.jpeg#align=left&display=inline&height=360&margin=%5Bobject%20Object%5D&name=SAVE_20201124_085444.jpg&originHeight=360&originWidth=600&size=59325&status=done&style=none&width=600)\n\n- 给/分区扩容步骤：\n\n①添加磁盘\n②使用fdisk命令对新增加的磁盘进行分区\n③分区完成后修改分区类型为lvm\n④使用pvcreate创建物理卷\n⑤使用vgextend命令将新增加的分区加入到根目录分区中\n⑥使用lvextend命令进行扩容\n⑦使用xfs_growfs调整卷分区大小\n\n## 5. 为何du和df统计结果不一致？\n\n- 用户删除了大量的文件被删除后，在文件系统目录中已经不可见了，所以du就不会再统计它。\n- 然而如果此时还有运行的进程持有这个已经被删除的文件句柄，那么这个文件就不会真正在磁盘中被删除，分区超级块中的信息也就不会更改，df仍会统计这个被删除的文件。\n- 可通过 lsof命令查询处于deleted状态的文件，被删除的文件在系统中被标记为deleted。如果系统有大量deleted状态的文件，会导致du和df统计结果不一致。\n\n## 6. 如何升级内核？\n\n- 方法一\n```bash\n# 添加第三方yum源进行下载安装。\nCentos 6 YUM源：http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm\nCentos 7 YUM源：http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm\n# 先导入elrepo的key，然后安装elrepo的yum源：\nrpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm\n# 查看可用的内核相关包\nyum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available \nyum -y --enablerepo=elrepo-kernel install\n```\n\n- 方法二\n```bash\n# 通过下载kernel image的rpm包进行安装。\n官方 Centos 6: http://elrepo.org/linux/kernel/el6/x86_64/RPMS/\n官方 Centos 7: http://elrepo.org/linux/kernel/el7/x86_64/RPMS/\n# 获取下载链接进行下载安装即可\nwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-4.4.185-1.el7.elrepo.x86_64.rpm\nrpm -ivh kernel-lt-4.4.185-1.el7.elrepo.x86_64.rp\n# 查看默认启动顺序\n[root@localhost ~]# awk -F\\\' \'$1==\"menuentry \" {print $2}\' /etc/grub2.cfg\nCentOS Linux (5.2.2-1.el7.elrepo.x86_64) 7 (Core)\nCentOS Linux (4.4.182-1.el7.elrepo.x86_64) 7 (Core)\nCentOS Linux (3.10.0-957.21.3.el7.x86_64) 7 (Core)\nCentOS Linux (3.10.0-957.10.1.el7.x86_64) 7 (Core)\nCentOS Linux (3.10.0-327.el7.x86_64) 7 (Core)\nCentOS Linux (0-rescue-e34fb4f1527b4f2d9fc75b77c016b6e7) 7 (Core)\n由上面可以看出新内核(4.12.4)目前位置在0，原来的内核(3.10.0)目前位置在1\n# 设置默认启动\n[root@localhost ~]# grub2-set-default 0　　// 0代表当前第一行，也就是4.12.4版本\n# 重启验证\n```\n\n## 7. nginx日志访问量前十的ip怎么统计？\n\n`awk \'{array[$1]++}END{for (ip in array)print ip,array[ip]}\' access.log |sort -k2 -rn|head` \n\n## 8. 如何删除/var/log/下.log结尾的30天前的日志？\n\n`find /var/log/  -type f -name .*.log -mtime 30|xargs rm -f` \n\n## 9. ansible有哪些模块？功能是什么？\n\n| 模块 | 功能 |\n| --- | --- |\n| copy | 拷贝文件到被控端 |\n| cron | 定时任务 |\n| fetch | 拷贝被控端文件到本地 |\n| file | 文件模块 |\n| group | 用户组模块 |\n| user | 用户模块 |\n| hostname | 主机名模块 |\n| script | 脚本模块 |\n| service | 服务启动模块 |\n| command | 远程执行命令模块 |\n| shell | 远程执行命令模块，command高级用法 |\n| yum  | 安装包组模块 |\n| setup  | 查看主机系统信息 |\n\n## 10. nginx为什么比apache快？\n\n- nginx采用epoll模型\n- apache采用select模型\n\n## 11. 四层负载和七层负载区别是什么？\n\n- 四层基于IP+端口进行转发\n- 七层就是基于URL等应用层信息的负载均衡\n\n## 12. lvs有哪些工作模式？哪个性能高？\n\n- dr：直接路由模式，请求由 LVS 接受，由真实提供服务的服务器直接返回给用户，返回的时候不经过 LVS。（性能最高）\n- tun：隧道模式，客户端将访问vip报文发送给LVS服务器。LVS服务器将请求报文重新封装，发送给后端真实服务器。后端真实服务器将请求报文解封，在确认自身有vip之后进行请求处理。后端真实服务器在处理完数据请求后，直接响应客户端。\n- nat：网络报的进出都要经过 LVS 的处理。LVS 需要作为 RS 的网关。当包到达 LVS 时，LVS 做目标地址转换（DNAT），将目标 IP 改为 RS 的 IP。RS 接收到包以后，仿佛是客户端直接发给它的一样。RS 处理完，返回响应时，源 IP 是 RS IP，目标 IP 是客户端的 IP。这时 RS 的包通过网关（LVS）中转，LVS 会做源地址转换（SNAT），将包的源地址改为 VIP，这样，这个包对客户端看起来就仿佛是 LVS 直接返回给它的。客户端无法感知到后端 RS 的存在。\n\n- fullnat模式：fullnat模式和nat模式相似，但是与nat不同的是nat模式只做了两次地址转换，fullnat模式却做了四次。 \n\n## 13. lvs nginx haproxy keeplived区别，优缺点？\n\n> 参考链接：[https://blog.51cto.com/816885/2529993?source=dra](https://blog.51cto.com/816885/2529993?source=dra)\n\n## 14. 如下url地址，各个部分的含义\n\n[https://www.baidu.com/s?word=123&ie=utf-8](https://www.baidu.com/s?word=123&ie=utf-8)\n\n- https: 使用https加密协议访问\n- www.baidu.com/s: 请求地址\n- ?word&ie=utf-8: get请求的参数，多个参数&连接\n\n## 15. tomcat各个目录含义，如何修改端口，如何修改内存数？\n\n- bin 存放tomcat命令\n- conf 存放tomcat配置文件\n- lib 存放tomcat运行需要加载的jar包\n- log 存在Tomcat运行产生的日志\n- temp 运行过程中产生的临时文件\n- webapps 站点目录\n- work 存放tomcat运行时的编译后的文件\n- conf/server.xml  修改端口号\n- bin/catalina.sh  修改jvm内存\n\n## 16. nginx反向代理时，如何使后端获取真正的访问来源ip？\n\n在location配置段添加以下内容：\nproxy_set_header Host $http_host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n\n## 17. nginx负载均衡算法有哪些？\n\n- rr 轮训\n- weight 加权轮训\n- ip_hash 静态调度算法\n- fair 动态调度算法\n- url_hash url哈希\n- leat_conn 最小连接数\n\n## 18. 如何进行压力测试？\n\n例如：模拟10个用户，对百度首页发起总共100次请求。\n测试命令： ab -n 100 -c 10  https://www.baidu.com/index.htm\n\n## 19. curl命令如何发送https请求？如何查看response头信息？如何发送get和post表单信息？\n\n- 发送https请求：curl  --tlsv1 \'https://www.bitstamp.net/api/v2/transactions/btcusd/\'\n- response头信息 ：curl -I\n- get：curl 请求地址?key1=value1&key2=value2&key3=value3\n- post： curl -d \"key1=value1&key2=value2&key3=value3\"\n\n# 二、mysql\n\n## 1. 索引的为什么使查询加快？有啥缺点？\n\n默认的方式是根据搜索条件进行全表扫描，遇到匹配条件的就加入搜索结果集合。如果我们对某一字段增加索引，查询时就会先去索引列表中一次定位到特定值的行数，大大减少遍历匹配的行数，所以能明显增加查询的速度\n缺点：\n\n- 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加\n- 索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间，如果需要建立聚簇索引，那么需要占用的空间会更大\n- 以表中的数据进行增、删、改的时候，索引也要动态的维护，这就降低了整数的维护速度\n\n## 2. sql语句左外连接 右外连接 内连接 全连接区别\n![17-24-18-018.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606382686237-38fe6856-8b0c-43f5-963d-001be4978d83.png#align=left&display=inline&height=760&margin=%5Bobject%20Object%5D&name=17-24-18-018.png&originHeight=760&originWidth=966&size=174040&status=done&style=none&width=966)\n\n## 3. mysql数据备份方式，如何恢复？你们的备份策略是什么？\n\n- 物理完全备份\n\n备份所有数据库文件：/var/lib/mysql/*\n备份所有binlog文件:  /var/lib/mysql/mysql-bin.*\n备份选项文件: /etc/my.cnf\n\n- mysqldump逻辑备份\n\nmysqldump -uroot -p --all-databases > /backup/mysqldump/all.db\n\n- 物理备份恢复\n\nmv /var/lib/mysql /var/lib/mysql.old  #先把原来的数据目录改名\ncp -a /backups/mysql /var/lib\n\n- 逻辑备份数据恢复\n\nmysql > use db_name\nmysql > source /backup/mysqldump/db_name.db\n\n## 4. 如何配置数据库主从同步，实际工作中是否遇到数据不一致问题？如何解决？\n\n为每个服务器配置唯一值的server-id\n\n- 主库\n\n开启binlog日志\n创建主从复制用户\n查看master的状态\n\n- 从库\n\nchange master to设置主库信息\nstart slave开始复制\n\n## 5. mysql约束有哪些？\n\n- 非空约束\n- 唯一约束\n- 主键约束\n- 外键约束\n\n## 6. 二进制日志（binlog）用途？\n\nBINLOG记录数据库的变更过程。例如创建数据库、建表、修改表等DDL操作、以及数据表的相关DML操作，这些操作会导致数据库产生变化，开启binlog以后导致数据库产生变化的操作会按照时间顺序以“事件”的形式记录到binlog二进制文件中\n\n## 7. mysql数据引擎有哪些？\n\n- 常用的 myisam、innodb\n- 区别：\n\nInnoDB 支持事务，MyISAM 不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而 MyISAM就不可以了；\nMyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用；\nInnoDB 支持外键，MyISAM 不支持；\nMyISAM 是默认引擎，InnoDB 需要指定；\nInnoDB 不支持 FULLTEXT 类型的索引；\nInnoDB 中不保存表的行数，如 select count(*) from table 时，InnoDB；需要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count(*)语句包含 where 条件时 MyISAM 也需要扫描整个表；\n对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM表中可以和其他字段一起建立联合索引；\n清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表；\nInnoDB 支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like \'%lee%\'\n\n## 8. 如何查询mysql数据库存放路径？\n\n```sql\nmysql> show variables like \'datadir%\';\n+---------------+------------------------+\n| Variable_name | Value                  |\n+---------------+------------------------+\n| datadir       | /usr/local/mysql/data/ |\n+---------------+------------------------+\n1 row in set (0.00 sec)\n```\n## 9. mysql数据库文件后缀名有哪些？用途什么？\n\n- myisam\n\n.frm文件：保护表的定义\n.myd：保存表的数据\n.myi：表的索引文件\n\n- innodb\n\n.frm：保存表的定义\n.ibd：表空间\n\n## 10. 如何修改数据库用户的密码？\n\n- mysql8之前\n```sql\nset password for 用户名@localhost = password(\'新密码\'); \nmysqladmin -u用户名 -p旧密码 password 新密码  \nupdate user set password=password(\'123\') where user=\'root\' and host=\'localhost\';\n```\n\n- mysql8之后\n```sql\n# mysql8初始对密码要求高，简单的字符串不让改。先改成:MyNewPass@123;\nalter user \'root\'@\'localhost\' identified by \'MyNewPass@123\';\n# 降低密码难度\nset global validate_password.policy=0;\nset global validate_password.length=4;\n# 修改成简易密码\nalter user \'root\'@\'localhost\'IDENTIFIED BY \'1111\';\n```\n\n## 11. 如何修改用户权限？如何查看？\n\n- 授权：\n\n`grant all on *.* to user@\'%\' identified by \'passwd\'` \n\n- 查看权限\n\n`show grants for user@\'%\';` \n\n# 三、nosql\n\n## 1. redis数据持久化有哪些方式？\n\n- rdb\n- aof\n\n## 2. redis集群方案有哪些？\n\n- 官方cluster方案\n- twemproxy代理方案\n- 哨兵模式\n- codis\n客户端分片\n\n## 3. redis如何进行数据备份与恢复？\n\n- 备份\n\nredis 127.0.0.1:6379> SAVE \n创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。\n\n- 还原\n\n只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可\nredis 127.0.0.1:6379> CONFIG GET dir\n1) \"dir\"\n2) \"/usr/local/redis/bin\"\n\n## 4. MongoDB如何进行数据备份？\n\nmongoexport / mongoimport\nmongodump  / mongorestore\n\n## 5. kafka为何比redis rabbitmq快？\n\n> [https://www.zhihu.com/question/22480085](https://www.zhihu.com/question/22480085)\n\n# 四、docker\n\n## 1. dockerfile有哪些关键字？用途是什么？\n\n![mmexport1606384205455.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606384293612-6cb1dcb2-f12f-407f-8ebf-47f15050737a.jpeg#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=mmexport1606384205455.jpg&originHeight=398&originWidth=831&size=142623&status=done&style=none&width=831)\n\n## 2. 如何减小dockerfile生成镜像体积？\n\n- 尽量选取满足需求但较小的基础系统镜像，例如大部分时候可以选择debian:wheezy或debian:jessie镜像，仅有不足百兆大小；\n- 清理编译生成文件、安装包的缓存等临时文件；\n- 安装各个软件时候要指定准确的版本号，并避免引入不需要的依赖；\n- 从安全角度考虑，应用要尽量使用系统的库和依赖；\n- 如果安装应用时候需要配置一些特殊的环境变量，在安装后要还原不需要保持的变量值；\n\n## 3. dockerfile中CMD与ENTRYPOINT区别是什么？\n\n- CMD 和 ENTRYPOINT 指令都是用来指定容器启动时运行的命令。\n- 指定 ENTRYPOINT  指令为 exec 模式时，CMD指定的参数会作为参数添加到 ENTRYPOINT 指定命令的参数列表中。\n## 4. dockerfile中COPY和ADD区别是什么？\n\n- COPY指令和ADD指令都可以将主机上的资源复制或加入到容器镜像中\n- 区别是ADD可以从 远程URL中的资源不会被解压缩。\n- 如果是本地的压缩包ADD进去会被解压缩\n\n## 5. docker的cs架构组件有哪些？\n\n![SAVE_20201126_175404.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606384460582-73c1e1ac-e7ee-4da7-b810-d2310371eabe.jpeg#align=left&display=inline&height=934&margin=%5Bobject%20Object%5D&name=SAVE_20201126_175404.jpg&originHeight=934&originWidth=854&size=103298&status=done&style=none&width=854)\n\n## 6. docker网络类型有哪些？\n\n- host模式\n- container模式\n- none模式\n- bridge模式\n\n## 7. 如何配置docker远程访问？\n\n- vim /lib/systemd/system/docker.service\n- 在ExecStart=后添加配置，注意，需要先空格后，再输入 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\n\n## 8. docker核心namespace CGroups 联合文件系统功能是什么？\n\n- namespace：资源隔离\n- cgroup：资源控制\n- 联合文件系统：支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下\n\n## 9. 命令相关：导入导出镜像，进入容器，设置重启容器策略，查看镜像环境变量，查看容器占用资源\n\n- 导入镜像 docker load -i xx.tar\n- 导出镜像docker save -o xx.tar image_name\n- 进入容器docker exec -it 容器命令 /bin/bash\n- 设置容器重启策略启动时 --restart选项\n- 查看容器环境变量  docker exec {containerID} env\n- 查看容器资源占用docker stats test2\n\n## 10. 构建镜像有哪些方式？\n\n- dockerfile\n- 容器提交为镜像\n\n## 11. docker和vmware虚拟化区别？\n\n![mmexport1606385080747.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606385089235-6c5f9d7c-20f1-4cb9-abc2-cff8096e9492.jpeg#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=mmexport1606385080747.jpg&originHeight=465&originWidth=612&size=159635&status=done&style=none&width=612)\n![mmexport1606385039848.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606385095772-d9bae033-5a10-4084-9d7b-1a60b03916ef.jpeg#align=left&display=inline&height=283&margin=%5Bobject%20Object%5D&name=mmexport1606385039848.jpg&originHeight=283&originWidth=606&size=129383&status=done&style=none&width=606)\n\n# 五、kubernetes\n\n\n## 1. k8s的集群组件有哪些？功能是什么？\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606447373367-f429cad4-676c-47cc-b238-57003cf13319.png#align=left&display=inline&height=573&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=573&originWidth=793&size=27098&status=done&style=none&width=793)\n\n## 2. kubectl命令相关：如何修改副本数，如何滚动更新和回滚，如何查看pod的详细信息，如何进入pod交互？\n\n- 修改副本数 kubectl scale deployment redis --replicas=3\n- 活动更新kubectl set image deployments myapp-deploy myapp=myapp:v2\n- 回滚kubectl rollout undo deployments myapp-deploy\n- 查看pod详细信息kubectl describe pods/<pod-name>\n- 进入pod交互kubectl exec -it <pod-name> -c <container-name> bash\n\n## 3. etcd数据如何备份？\n\n-  etcdctl --endpoints=\"https://192.168.32.129:2379,https://192.168.32.130:2379,192.168.32.128:2379\" --cacert=/etc/kubernetes/cert/ca.pem --key=/etc/etcd/cert/etcd-key.pem --cert=/etc/etcd/cert/etcd.pem  snapshot save snashot1.db\n- Snapshot saved at snashot1.db\n\n## 4. k8s控制器有哪些？\n\n- 副本集（ReplicaSet）\n- 部署（Deployment）\n- 状态集（StatefulSet）\n- Daemon集（DaemonSet）\n- 一次任务（Job）\n- 计划任务（CronJob）\n- 有状态集（StatefulSet）\n\n## 5. 哪些是集群级别的资源？\n\n- Namespace\n- Node\n- Role\n- ClusterRole\n- RoleBinding\n- ClusterRoleBinding\n\n## 6. pod状态有哪些？\n\n- Pending           等待中\n- Running           运行中\n- Succeeded      正常终止\n- Failed              异常停止\n- Unkonwn         未知状态\n\n## 7. pod创建过程是什么？\n\n![SAVE_20201126_181652.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606385821622-f36232c8-15a5-46dc-a750-bfc90624b205.jpeg#align=left&display=inline&height=524&margin=%5Bobject%20Object%5D&name=SAVE_20201126_181652.jpg&originHeight=524&originWidth=790&size=92836&status=done&style=none&width=790)\n\n## 8. pod重启策略有哪些？\n\nPod的重启策略有3种，默认值为Always。\n\n- Always ： 容器失效时，kubelet 自动重启该容器；\n- OnFailure ： 容器终止运行且退出码不为0时重启；\n- Never ： 不论状态为何， kubelet 都不重启该容器\n\n## 9. 资源探针有哪些？\n\n- ExecAction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。\n- TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。\n- HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起HTTP GET请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。\n\n## 10. requests和limits用途是什么？\n\n- “requests”属性定义其请求的确保可用值，即容器运行可能用不到这些额度的资源，但用到时必须要确保有如此多的资源可用\n- ”limits”属性则用于限制资源可用的最大值，即硬限制\n\n## 11. kubeconfig文件包含什么内容，用途是什么？\n\n包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context 信息（集群名称、用户名）。\n\n## 12. RBAC中role和clusterrole区别，rolebinding和 clusterrolebinding区别？\n\n- Role 可以定义在一个 namespace 中，如果想要跨 namespace则可以创建ClusterRole，ClusterRole 具有与 Role相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的\n- RoleBinding 适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权\n\n## 13. ipvs为啥比iptables效率高？\n\nIPVS模式与iptables同样基于Netfilter，但是ipvs采用的hash表，iptables采用一条条的规则列表。iptables又是为了防火墙设计的，集群数量越多iptables规则就越多，而iptables规则是从上到下匹配，所以效率就越是低下。因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能\n\n## 14. sc pv pvc用途，容器挂载存储整个流程是什么？\n\n- PVC：Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。\n- PV ：具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。\n- StorageClass：充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606447989005-23283e61-2776-456e-8827-25eebf8fb8a2.png#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=679&originWidth=967&size=44551&status=done&style=none&width=967)\n\n## 15. nginx ingress的原理本质是什么？\n\n- ngress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，\n- 然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，\n- 再写到nginx-ingress-controller的pod里，这个Ingress\ncontroller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，\n- 然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。\n\n## 16. 描述不同node上的Pod之间的通信流程\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606448071815-d05c64b2-9d8a-4570-8f77-4ee2e1513446.png#align=left&display=inline&height=776&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=776&originWidth=1278&size=471377&status=done&style=none&width=1278)\n\n## 17. k8s集群节点需要关机维护，需要怎么操作\n\n- 进行pod驱逐：kubelet drain  <node_name>\n- 检查node上是否无pod运行，切被驱逐的pod已经在其他节点运行正常\n- 关机维护\n- 开机启动相关服务（注意启动顺序）\n- 解除node节点不可调度：kubectl uncordon node\n- 创建测试pod，并使用节点标签测试节点可以被正常调度\n\n## 18. canal和flannel区别\n\n- Flannel（简单、使用居多）：基于Vxlan技术（叠加网络+二层隧道），不支持网络策略\n- Calico（较复杂，使用率少于Flannel）：也可以支持隧道网络，但是是三层隧道（IPIP），支持网络策略\n- Calico项目既能够独立地为Kubernetes集群提供网络解决方案和网络策略，也能与flannel结合在一起，由flannel提供网络解决方案，而Calico此时仅用于提供网络策略。\n\n\n\n# 六、prometheus\n\n## 1. prometheus对比zabbix有哪些优势？\n\n> [https://blog.csdn.net/wangyiyungw/article/details/85774969](https://blog.csdn.net/wangyiyungw/article/details/85774969)**\n\n## 2. prometheus组件有哪些，功能是什么？\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606570207946-efe0855b-2ecb-4d78-af6d-9cf8873e813a.png#align=left&display=inline&height=459&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=459&originWidth=690&size=471377&status=done&style=none&width=690)\n\n## 3. 指标类型有哪些？\n\n- Counter（计数器）\n- Guage（仪表盘）\n- Histogram（直方图）\n- Summary（摘要）\n\n## 4. 在应对上千节点监控时，如何保障性能\n\n- 降低采集频率\n- 缩小历史数据保存天数，\n- 使用集群联邦和远程存储\n\n## 5. 简述从添加节点监控到grafana成图的整个流程\n\n- 被监控节点安装exporter\n- prometheus服务端添加监控项\n- 查看prometheus web界面——status——targets\n- grafana创建图表\n\n## 6. 在工作中用到了哪些exporter\n\n- node-exporter监控linux主机\n- cAdvisor监控容器\n- MySQLD Exporter监控mysql\n- Blackbox Exporter网络探测\n- Pushgateway采集自定义指标监控\n- process exporter进程监控\n\n# 七、ELK\n\n## 1. Elasticsearch的数据如何备份与恢复？\n\n> [https://www.cnblogs.com/tcy1/p/13492361.html](https://www.cnblogs.com/tcy1/p/13492361.html)\n> [https://blog.csdn.net/moxiaomomo/article/details/78401400?locationNum=8&fps=1](https://blog.csdn.net/moxiaomomo/article/details/78401400?locationNum=8&fps=1)\n\n## 2. 你们项目中使用的logstash过滤器插件是什么？实现哪些功能？\n\n- date 日期解析\n- grok 正则匹配解析\n- overwrite 写某个字段\n- dissect 分隔符解析\n- mutate 对字段做处理\n- json 解析\n- geoip 地理位置解析\n- ruby 修改logstash event\n\n## 3. 是否用到了filebeat的内置module？用了哪些？\n\n![13-11-46-046.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606540321821-3c32570e-5d13-4041-977c-449d033548e8.png#align=left&display=inline&height=407&margin=%5Bobject%20Object%5D&name=13-11-46-046.png&originHeight=407&originWidth=654&size=21168&status=done&style=none&width=654)\n\n## 4. elasticsearch分片副本是什么？你们配置的参数是多少？\n\n> [https://juejin.cn/post/6844903862088777736](https://juejin.cn/post/6844903862088777736)\n\n# 八、运维开发\n\n## 1. 备份系统中所有容器镜像\n\n```bash\n#备份镜像列表\ndocker images|awk \'NR>1{print $1\":\"$2}\'|sort > images.list。\n#导出所有镜像为当前目录下文件：\nwhile read img; do\n    echo $img\n    file=\"${img/\\//-}\"\n    sudo docker save --output $file.tar $img\ndone < images.list\n#将本地镜像文件导入为Docker镜像：\nwhile read img; do\n    echo $img\n    file=\"${img/\\//-}\"\n    docker load < $file.tar\ndone < images.list\n```\n\n## 2. 编写脚本，定时备份某个库，然后压缩，发送异机\n\n- 公共部分定义函数，如获取时间戳，配置报警接口\n- 多使用if判断是否存在异常并处理，如数据库大，检测任务是否完成。检测生成文件大小是否是空文件\n\n## 3. 批量获取所有主机的系统信息\n\n- 使用python的paramiko库，ssh登陆主机执行查询操作\n- 使用shell脚本批量ssh登陆主机并执行命令\n- 使用ansible的setup模块获取主机信息\n- prometheus的node_exporter收集主机资源信息\n\n## 4. django的mtv模式流程\n\n![SAVE_20201128_132923.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1606541375034-4cace753-5894-42e8-9a74-3eae8d4b1203.jpeg#align=left&display=inline&height=430&margin=%5Bobject%20Object%5D&name=SAVE_20201128_132923.jpg&originHeight=430&originWidth=992&size=102450&status=done&style=none&width=992)\n\n## 5. python如何导出、导入环境依赖包\n\n- 导出环境\n\n`pip freeze >> requirements.txt` \n\n- 导入环境\n\n`pip install -r requirement.txt` \n\n## 6. python创建，进入，退出，查看虚拟环境\n\n- 安装软件包\n\n`pip3 install virtualenv` \n\n- 检测安装是否成功\n\n`virtualenv --version` \n\n- 创建虚拟环境\n- cd到要创建虚拟环境的目录\n\n`cd github/test/venv/` \n\n- 创建虚拟环境\n\n`virtualenv test` \n\n- 激活虚拟环境\n\n`source test/bin/activate(activate路径)` \n\n- 退出虚拟环境\n\n`deactivate` \n\n## 7. flask和django区别，应用场景\n\n- Django功能大而全，Flask只包含基本的配置 Django的一站式解决的思路，能让开发者不用在开发之前就在选择应用的基础设施上花费大量时间。Django有模板，表单，路由，认证，基本的数据库管理等等内建功能。与之相反，Flask只是一个内核，默认依赖于两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集，其他很多功能都是以扩展的形式进行嵌入使用。\n\n- Flask 比 Django 更灵活 用Flask来构建应用之前，选择组件的时候会给开发者带来更多的灵活性 ，可能有的应用场景不适合使用一个标准的ORM(Object-Relational Mapping 对象关联映射)，或者需要与不同的工作流和模板系统交互。\n\n## 8. 列举常用的git命令\n\n- $ git init\n- $ git config\n- $ git add\n- $ git commit\n- $ git branch\n- $ git checkout\n- $ git tag\n- $ git push\n- $ git status\n- $ git log\n\n## 9. git gitlab jenkins的CICD流程如何配置\n\n- 开发者git提交代码至gitlab仓库\n- jenkins从gitlab拉取代码，触发镜像构建\n- 镜像上传至harbor私有仓库\n- 镜像下载至执行机器\n- 镜像运行\n\n# 九、日常工作\n\n## 1. 在日常工作中遇到了什么棘手的问题，如何排查\n\n- redis弱口令导致中挖矿病毒，排查，优化\n- k8s中开发的程序在用户上传文件时开启进程，未及时关闭，导致节点超出最大进程数\n\n## 2. 日常故障处理流程\n\n- 查看报警内容，快速定位大致故障主机，服务，影响范围\n- 告知运维经理故障，并开始排查\n- 如果需要修改配置文件，重启服务器等操作，告知相关开发人员\n- 完成故障处理\n\n## 3. 修改线上业务配置文件流程\n\n- 先告知运维经理和业务相关开发人员\n- 在测试环境测试，并备份之前的配置文件\n- 测试无误后修改生产环境配置\n- 观察生产环境是否正常，是否有报警\n- 完成配置文件更改\n\n## 4. 业务pv多少？集群规模多少？怎么保障业务高可用？\n\n\n# 十、开放性问题\n\n## 1. 你认为初级运维工程师和高级运维工程师的区别？\n\n## 2. 你认为未来运维发展方向\n\n自动化，智能化\n', 1644, 6, 0, '2020-11-28 21:44:58.348084', '2021-01-27 13:48:45.816649', 1, 8, 1, 1, 1);
INSERT INTO `blog_article` VALUES (126, 'Django使用百度统计进行流量分析', '项目上线后，作为站长肯定迫不及待的想知道页面访问量多少，哪些页面更受用户喜爱，访问用户终端类型占比多少？用户在哪个时间段访问较多？对于这些流量的分析，主流的解决方案主要有①分析nginx的access.log日志②使用第三方统计平台（如百度统计、友盟），本文使用以百度统计为例讲解。', 'cover/2020_12_11_20_26_32_691553.jpg', '[TOC]\n\n> 项目上线后，作为站长肯定迫不及待的想知道页面访问量多少，哪些页面更受用户喜爱，访问用户终端类型占比多少？用户在哪个时间段访问较多？对于这些流量的分析，主流的解决方案主要有①分析nginx的access.log日志②使用第三方统计平台（如百度统计、友盟）\n\n# 一、注册开通百度统计功能\n\n\n1. 打开[链接](https://tongji.baidu.com/web/welcome/login)，点击右上角注册，选择百度统计-站长版\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607526840830-a5905875-d037-4abb-a288-c59c327c2e2d.png#align=left&display=inline&height=267&margin=%5Bobject%20Object%5D&name=image.png&originHeight=267&originWidth=384&size=20155&status=done&style=none&width=384)\n\n2. 填写表单，完成注册\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607526910134-1c56db60-02de-4a43-8d12-943f20153765.png#align=left&display=inline&height=632&margin=%5Bobject%20Object%5D&name=image.png&originHeight=632&originWidth=505&size=54815&status=done&style=none&width=505)\n\n3. 登录账号后，点击管理——网站列表——新增网站，填写网站详细信息\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607527014775-311f0077-29d4-4003-9a08-7c19472d901c.png#align=left&display=inline&height=701&margin=%5Bobject%20Object%5D&name=image.png&originHeight=701&originWidth=1913&size=130572&status=done&style=none&width=1913)\n\n# 二、html页面添加统计代码\n\n1. 在管理——代码管理——代码获取页，点击获取统计代码\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607527101449-f25b0fae-252d-48a1-8564-8d10ed7e1a25.png#align=left&display=inline&height=567&margin=%5Bobject%20Object%5D&name=image.png&originHeight=567&originWidth=1035&size=98974&status=done&style=none&width=1035)\n\n2. 在项目的base页面，添加粘贴相关代码\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607527266653-3c0052e2-d938-4aa3-a08b-0de485da0027.png#align=left&display=inline&height=694&margin=%5Bobject%20Object%5D&name=image.png&originHeight=694&originWidth=1071&size=119281&status=done&style=none&width=1071)\n\n3. 然后点击代码安装检查，验证是否正确安装\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607527337956-029fc1ef-934d-4e3f-aa6c-b4865b244339.png#align=left&display=inline&height=546&margin=%5Bobject%20Object%5D&name=image.png&originHeight=546&originWidth=1044&size=90863&status=done&style=none&width=1044)\n\n- 如果已安装代码，但百度统计提示代码未安装，检查页面的referrer策略，设置为no-referrer-when-downgrade\n\n# 三、项目后台添加相关代码\n\n1. 在setting.py文件中定义变量，便于后期更换token等信息\n\n    ```python\n# 百度统计token\nBAIDU_USERNAME = \'*****\'\nBAIDU_PASSWORD = \'*****\'\nBAIDU_ID = \'*****\'\nBAIDU_TOKEN = \'*****\'\n```\n\n    BAIDU_ID可以查看地址栏siteId的值\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607527641052-a82d74ee-3c6c-499e-a453-ed400ce08d69.png#align=left&display=inline&height=194&margin=%5Bobject%20Object%5D&name=image.png&originHeight=194&originWidth=773&size=23361&status=done&style=none&width=773)\n\n2. 新建baidu_tongji.py文件，专门处理百度统计相关业务逻辑\n\n```python\nimport json\nimport requests\nfrom django.utils import timezone\nfrom myblog.settings import BAIDU_USERNAME, BAIDU_PASSWORD, BAIDU_TOKEN, BAIDU_ID, BAIDU_START_DATE\n\n# 百度统计api\nclass BaiduApi:\n    def __init__(self):\n        self.url = \'https://api.baidu.com/json/tongji/v1/ReportService/getData\'\n        self.username = BAIDU_USERNAME\n        self.password = BAIDU_PASSWORD\n        self.token = BAIDU_TOKEN\n        self.id = BAIDU_ID\n\n    def countAll(self):\n        now_date = timezone.now()\n        end_date = now_date.strftime(\'%Y%m%d\')\n        data = {\n            \"header\": {\n                \"username\": self.username,\n                \"password\": self.password,\n                \"token\": self.token,\n                \"account_type\": 1\n            },\n            \"body\": {\n                \"site_id\": self.id,\n                \"start_date\": BAIDU_START_DATE,\n                \"end_date\": end_date,\n                \"metrics\": \"pv_count,visitor_count,ip_count\",\n                \"method\": \"trend/time/a\",\n                \"area\": \"\"\n            }\n        }\n        r = requests.post(self.url, data=json.dumps(data))\n        result = json.loads(r.text)\n        pv = result[\"body\"][\"data\"][0][\"result\"][\"pageSum\"][0][0]\n        uv = result[\"body\"][\"data\"][0][\"result\"][\"pageSum\"][0][1]\n        ip = result[\"body\"][\"data\"][0][\"result\"][\"pageSum\"][0][2]\n        count_dict = {}\n        count_dict[\'pv\'] = pv\n        count_dict[\'uv\'] = uv\n        count_dict[\'ip\'] = ip\n        return count_dict\n```\n\n* 这里只列出了pv、uv、ip统计代码，其他信息可以参考[开发者手册](https://tongji.baidu.com/api/manual/)\n\n3. views视图函数编写调用部分的逻辑代码\n\n    ```python\n# 运行时间\nnow_data = str(timezone.now().strftime(\'%Y-%m-%d\'))\nd1 = datetime.datetime.strptime(BAIDU_START_DATE, \'%Y%m%d\')\nd2 = datetime.datetime.strptime(now_data, \'%Y-%m-%d\')\nday_count = (d2 - d1).days\n# 流量统计\napi = BaiduApi()\ncount = api.countAll()\nall_pv = count[\'pv\']\nall_uv = count[\'uv\']\nall_ip = count[\'ip\']\n```\n\n4. html文件加载数据\n\n    ```html\n<div class=\"about-me\">\n  <div>\n    <i class=\"fas fa-business-time\"></i>\n    <span>运行时间：{{ aside_dict.day_count }}天</span>\n  </div>\n  <div>\n    <i class=\"fas fa-book-reader\"></i>\n    <span>总访问量：{{ aside_dict.all_pv }}</span>\n  </div>\n  <div>\n    <i class=\"fas fa-user-plus\"></i>\n    <span>访问人数：{{ aside_dict.all_uv }}</span>\n  </div>\n  <div>\n    <i class=\"fas fa-chalkboard-teacher\"></i>\n    <span>访问IP：{{ aside_dict.all_ip }}</span>\n  </div>\n</div>\n```\n\n# 四、最终效果展示\n\n1. 侧边栏统计pv、uv\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607528061086-2c9364a0-b01a-43a0-be2f-26cfed9db3df.png#align=left&display=inline&height=303&margin=%5Bobject%20Object%5D&name=image.png&originHeight=303&originWidth=324&size=20975&status=done&style=none&width=324)\n\n2. 网站后台流量统计\n\n    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607528129501-244b7a56-8ef3-4548-a912-d59099e4bc18.png#align=left&display=inline&height=970&margin=%5Bobject%20Object%5D&name=image.png&originHeight=970&originWidth=1920&size=216750&status=done&style=none&width=1920)', 249, 0, 0, '2020-12-09 23:38:37.541873', '2021-01-26 05:38:42.111554', 1, 4, 0, 1, 1);
INSERT INTO `blog_article` VALUES (127, 'Django项目使用CDN加速', '由于囊中羞涩，所以使用阿里云1M带宽服务器提供服务，但是访问速度十分感人，加载站点需要10多秒钟。主要原因是因为首页有大量图片资源需要加载，所有有必要使用CDN技术，对静态资源进行分发，提高页面加载速度。本文详细记录的Django项目如何使用CDN对静态资源进行加速处理的完整流程。', 'cover/2020_12_11_20_31_09_177596.jpg', '[TOC]\n\n> 由于囊中羞涩，所以使用阿里云1M带宽服务器提供服务，但是访问速度十分感人，加载站点需要10多秒钟。主要原因是因为首页有大量图片资源需要加载，所有有必要使用CDN技术，对静态资源进行分发，提高页面加载速度。本文详细记录的Django项目如何使用CDN对静态资源进行加速处理的完整流程。\n\n# 一、什么是CDN\n\nCDN的全称是Content Delivery Network，即内容分发网络。CDN是构建在现有网络基础之上的智能虚拟网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。\n通俗来讲，就是存储你的站点的静态资源，然后分发到所有的边缘节点，用户在访问站点的时候，就会从就近的 CDN 节点获取资源，从而减轻服务器负载并提升网页加载速度。\n举个例子，当你网上购物时，经常使用淘宝和京东，但是京东下单往往当天就可以收到快递，而淘宝需要等几天的物流。这是因为京东提前采购好商品，存放在全国各地的仓库，下单后直接从离你最近的仓库发货。而淘宝下单后，需要从厂家发货，所以京东收货速度快多了。\nCDN加速也是同样的原理，开启了CDN加速可以大幅度提高网站的加载速度。\n\n# 二、准备工作\n\n1. 已经备案的根域名\n1. 申请cdn加速域名的https证书，推荐一个免费站点（[https://freessl.cn/](https://freessl.cn/)）\n1. 本文使用阿里云DNS解析和华为云CDN\n1. 用户访问流量示意图\n\n![Snipaste_2020-12-10_16-03-25.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607587568517-6edcf520-dfbe-4b10-b80b-155114d3e302.png#align=left&display=inline&height=1120&margin=%5Bobject%20Object%5D&name=Snipaste_2020-12-10_16-03-25.png&originHeight=1120&originWidth=1734&size=226728&status=done&style=none&width=1734)\n\n# 三、配置过程\n\n## 1. 修改nginx配置文件\n\n- 修改nginx配置文件，配置cdn加速域名server配置（配置ssl、开启目录浏览、设置资源路径别名）\n- 目录浏览根据自身情况选择是否开启，开启主要是方便代码调试，修改静态资源。\n\n```bash\nserver {\n        listen 443 ssl http2;\n        server_name  cdn.cuiliangblog.cn;\n        autoindex on;\n        autoindex_exact_size off;\n        autoindex_localtime on;\n        charset utf-8;\n        ssl_certificate    /etc/ssl/cdn.cuiliangblog.cn_chain.crt;#.pem证书路径\n        ssl_certificate_key  /etc/ssl/cdn.cuiliangblog.cn_key.key;#.key证书路径\n        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;\n        ssl_prefer_server_ciphers on;\n        ssl_session_cache shared:SSL:10m;\n        ssl_session_timeout 10m;\n        location ~* \\.(eot|otf|ttf|woff|woff2|svg)$ {\n            root /root/myblog/;\n            add_header Access-Control-Allow-Origin *;\n        }\n        location /static/ {\n            alias /root/myblog/static/;\n        }\n        location /media/ {\n            alias /root/myblog/media/;\n        }\n    }\n```\n\n- 临时添加hosts记录，访问验证，查看cdn.cuiliangblog.cn/static/和media是否可以正常加载\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607587984530-6ee6cbf4-e80e-4e61-ada5-6570a21aaf00.png#align=left&display=inline&height=345&margin=%5Bobject%20Object%5D&name=image.png&originHeight=690&originWidth=1114&size=104578&status=done&style=none&width=557)\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607588015243-1791fe51-c1bb-4964-86dc-8789b83429df.png#align=left&display=inline&height=271&margin=%5Bobject%20Object%5D&name=image.png&originHeight=542&originWidth=1102&size=71933&status=done&style=none&width=551)\n\n## 2. 华为CDN配置\n\n- 控制台——CDN——添加域名，选择源站IP,并填写服务器公网IP地址\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589000512-71c59f93-cd06-48ac-949c-df4b3f9eed60.png#align=left&display=inline&height=714&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1428&originWidth=2126&size=261461&status=done&style=none&width=1063)\n\n- 配置https证书，以及缓存策略\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589187347-1217aa75-0c99-45b0-bd83-e48f89eef06b.png#align=left&display=inline&height=341&margin=%5Bobject%20Object%5D&name=image.png&originHeight=682&originWidth=2286&size=101388&status=done&style=none&width=1143)\n\n- 复制cname域名\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589228751-934d53c6-d8fd-443d-a7c3-2cd090647a17.png#align=left&display=inline&height=395&margin=%5Bobject%20Object%5D&name=image.png&originHeight=790&originWidth=1802&size=105636&status=done&style=none&width=901)\n\n## 3. 阿里云DNS解析配置\n\n- 添加cname记录\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589348977-e1bc55bf-f157-494f-a02a-5331c1ff2aa5.png#align=left&display=inline&height=386&margin=%5Bobject%20Object%5D&name=image.png&originHeight=772&originWidth=1848&size=116546&status=done&style=none&width=924)\n\n- 删除本地host记录，然后访问cdn.cuiliangblog.cn/static/和media是否可以正常加载\n\n## 4. Django项目配置\n\n- 修改setting.py配置文件，修改static和media地址指向cdn加速地址\n\n```bash\n# 静态文件存放位置\n# STATIC_URL = \'/static/\'\nSTATIC_URL = \'https://cdn.cuiliangblog.cn/static/\'\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, \"static\"),  \n]\n\n# 设置文件上传路径，图片上传、文件上传都会存放在此目录里\n# MEDIA_URL = \'/media/\'\nMEDIA_URL = \'https://cdn.cuiliangblog.cn/media/\'\nMEDIA_ROOT = os.path.join(BASE_DIR, \'media\')\n```\n\n- 查看模板文件，修改图片资源路径\n\n```html\n<a href=\"javascript:;\">\n  <img lay-src=\"{{ MEDIA_URL }}{{ userinfo.photo }}\" class=\"layui-nav-img\">{{ user.username }}\n</a>\n```\n\n# 四、结果验证\n\n- 访问站点，查看文档资源请求地址\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589810816-bed7477b-6053-42d7-9cf5-f5a680acd604.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=274&originWidth=716&size=30775&status=done&style=none&width=358)\n\n- 查看图片资源请求地址\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607589852374-fd484095-f3f7-42ff-aad2-f1dcac5d10a5.png#align=left&display=inline&height=132&margin=%5Bobject%20Object%5D&name=image.png&originHeight=264&originWidth=698&size=36102&status=done&style=none&width=349)\n\n', 330, 2, 0, '2020-12-11 20:31:23.801602', '2021-01-26 18:19:04.101722', 1, 4, 0, 1, 1);
INSERT INTO `blog_article` VALUES (128, '使用阿里云SDK监控主机状态', '最近在开发博客网站后台时，想添加主机性能查看功能，以便登录网站后台管理即可随查看服务器的CPU、内存、系统负载、磁盘使用率的情况，本文详细记录了采用阿里云SDK方式获取主机资源指标信息。', 'cover/2020_12_16_22_38_28_153014.jpg', '[TOC]\n\n# 一、需求背景\n\n最近在开发博客网站后台时，想添加主机性能查看功能，以便登录网站后台管理即可随查看服务器的CPU、内存、系统负载、磁盘使用率的情况，解决思路主要有两种：\n\n1. 使用os模块，通过调用shell脚本，实现信息的收集与处理\n1. 使用阿里云SDK，传入相应参数，获取指标信息\n\n由于python项目使用docker容器运行，在容器中执行shell脚本，获取宿主机资源信息还是比较麻烦的，因此采用阿里云SDK方式获取主机资源指标信息。\n\n# 二、参考链接\n\n## 1. ECS主机监控项参数\n\n[https://help.aliyun.com/document_detail/162844.html?spm=a2c4g.11186623.2.9.48337d97ecgWtM#concept-2482301](https://help.aliyun.com/document_detail/162844.html?spm=a2c4g.11186623.2.9.48337d97ecgWtM#concept-2482301)\n\n## 2. python SDK参考文档\n\n[https://help.aliyun.com/document_detail/28622.html?spm=a2c4g.11186623.6.838.595c12d0YrMGqc](https://help.aliyun.com/document_detail/28622.html?spm=a2c4g.11186623.6.838.595c12d0YrMGqc)\n\n# 三、前期准备\n\n## 1. 查询实例名称InstanceId\n\n**在控制台——云服务器ECS——概览**\n**![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608125834299-7fa2bb4e-ebd0-47a6-bb35-a9c08106d7f8.png#align=left&display=inline&height=497&margin=%5Bobject%20Object%5D&name=image.png&originHeight=497&originWidth=1054&size=81243&status=done&style=none&width=1054)**\n\n## 2.所在地域RegionId\n\n**在控制台即可查看主机所在地域，参考文档** [https://help.aliyun.com/document_detail/40654.html?spm=a2c4g.11186623.2.26.61014f55hxVpaH#concept-h4v-j5k-xdb](https://help.aliyun.com/document_detail/40654.html?spm=a2c4g.11186623.2.26.61014f55hxVpaH#concept-h4v-j5k-xdb)，查询**Region ID**\n\n## 3.AccessKey和AccessSecret\n\nRAM访问控制——用户——创建编程访问角色用户\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608126271544-361ded51-b341-4eee-a9a5-51d1f7083269.png#align=left&display=inline&height=553&margin=%5Bobject%20Object%5D&name=image.png&originHeight=553&originWidth=952&size=54881&status=done&style=none&width=952)\n**参考文档：**[https://help.aliyun.com/document_detail/53045.html?spm=a2c4g.11186623.2.34.61014f55BKnY5k#concept-53045-zh](https://help.aliyun.com/document_detail/53045.html?spm=a2c4g.11186623.2.34.61014f55BKnY5k#concept-53045-zh)\n\n# 四、开发流程\n\n## 1. 调试官方demo程序\n\n[https://help.aliyun.com/document_detail/28622.html?spm=a2c4g.11186623.6.838.595c12d0YrMGqc](https://help.aliyun.com/document_detail/28622.html?spm=a2c4g.11186623.6.838.595c12d0YrMGqc)\n\n```python\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.acs_exception.exceptions import ClientException\nfrom aliyunsdkcore.acs_exception.exceptions import ServerException\nfrom aliyunsdkcms.request.v20190101.DescribeMetricListRequest import DescribeMetricListRequest\n\nclient = AcsClient(\'<accessKeyId>\', \'<accessSecret>\', \'cn-beijing\')\n# <accessKeyId>,<accessSecret>,cn-beijing:根据自己查询信息填写\nrequest = DescribeMetricListRequest()\nrequest.set_accept_format(\'json\')\n\nrequest.set_StartTime(\"2019-05-21 10:00:00\")\n# 时间修改为当前小时，响应以免数据过多\nrequest.set_Dimensions(\"{\\\"instanceId\\\":\\\"i-2ze3*******\\\"}\")\n# \"i-2ze3*******\\\"：此处改为ECS服务器实例ID\nrequest.set_Period(\"60\")\nrequest.set_Namespace(\"acs_ecs_dashboard\")\nrequest.set_MetricName(\"CPUUtilization\")\n\nresponse = client.do_action_with_exception(request)\n# python2:  print(response)\nprint(str(response, encoding=\'utf-8\'))\n```\n\n如果信息填写无误，就可以获取到相应内容，内容包含 `\"Minimum\\\":3.44,\\\"Maximum\\\":3.44,\\\"Average\\\":3.44 `字段信息，这就是CPU当前时间段的指标\n\n## 2.setting文件保存配置\n\n尽量将所有配置信息都写到setting中，便于后期更换服务器时，修改变量值即可，避免修改代码工作\n\n```python\n# 阿里云SDK\nALIYUN_KEYId = \'****\'\nALIYUN_SECRET = \'****\'\nALIYUN_LOCATION = \'****\'\nALIYUN_INSTANCE = \'***\'\n```\n\n## 3.tools文件定义SDK类\n\n* **MetricName**参考文档：[https://help.aliyun.com/document_detail/162844.html?spm=a2c4g.11186623.2.9.48337d97ecgWtM#concept-2482301](https://help.aliyun.com/document_detail/162844.html?spm=a2c4g.11186623.2.9.48337d97ecgWtM#concept-2482301)（刚开始在官方文档ECS主机SDK参考文档和云监控监控项说明试了半天，最后才发现监控项名称是这个）\n* 由于我只需要获取CPU、内存、磁盘使用率、系统负载四项，在上一分钟的状态即可，需要在返回的一串json内容中提取主要信息，返回数据结构完全一致，所有就定义到一个方法里面。\n\n```python\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.acs_exception.exceptions import ClientException\nfrom aliyunsdkcore.acs_exception.exceptions import ServerException\nfrom aliyunsdkcms.request.v20190101.DescribeMetricListRequest import DescribeMetricListRequest\nfrom myblog.settings import ALIYUN_INSTANCE, ALIYUN_KEYId, ALIYUN_LOCATION, ALIYUN_SECRET\n\n# 阿里云sdk\nclass AliyunSDK:\n    def __init__(self, metric):\n        self.accessKeyId = ALIYUN_KEYId\n        self.accessSecret = ALIYUN_SECRET\n        self.location = ALIYUN_LOCATION\n        self.instanceId = ALIYUN_INSTANCE\n        self.metric = metric\n\n    def metricInfo(self):\n        startTime = (datetime.datetime.now() - datetime.timedelta(minutes=10)).strftime(\"%Y-%m-%d %H:%M:%S\")\n        client = AcsClient(self.accessKeyId, self.accessSecret, self.location)\n        request = DescribeMetricListRequest()\n        request.set_accept_format(\'json\')\n        request.set_StartTime(startTime)\n        instance = \"{\\\"instanceId\\\":\\\"\" + self.instanceId + \"\\\"}\"\n        request.set_Dimensions(instance)\n        request.set_Period(\"60\")\n        request.set_Namespace(\"acs_ecs_dashboard\")\n        request.set_MetricName(self.metric)\n        response = client.do_action_with_exception(request)\n        result = str(response, encoding=\'utf-8\')\n        result_dict = json.loads(result)\n        if result_dict[\'Code\'] == \'200\':\n            data = eval(result_dict[\'Datapoints\'][1:-1])\n            return data[0][\"Average\"]\n        else:\n            return 0\n```\n\n## 4.views视图函数逻辑处理\n\n```python\nfrom management.tools import AliyunSDK\ndef dashboard(request):\n# 主机性能\n    cpu = AliyunSDK(\"CPUUtilization\")\n    cpu_rate = round(cpu.metricInfo(), 2)\n    memory = AliyunSDK(\"memory_usedutilization\")\n    memory_rate = round(memory.metricInfo(), 2)\n    disk = AliyunSDK(\"diskusage_utilization\")\n    disk_rate = round(disk.metricInfo(), 2)\n    load = AliyunSDK(\"load_15m\")\n    load_rate = round(load.metricInfo() * 100, 2)\n    return render(request, \'management/index.html\', locals())\n```\n\n## 5.前端模板文件显示\n\n```html\n<div class=\"layui-card\">\n  <div class=\"layui-card-header\">主机性能</div>\n  <div class=\"layui-card-body\" id=\"system-info\">\n    <div class=\"layuiadmin-card-list\">\n      <p class=\"layuiadmin-normal-font\">CPU使用率</p>\n      <div class=\"layui-progress layui-progress-big\" lay-showpercent=\"yes\">\n        <div class=\"layui-progress-bar\"\n             lay-percent={{ cpu_rate }}% style=\"width:{{ cpu_rate }}%\"><span\n                                                                             class=\"layui-progress-text\">{{ cpu_rate }}%</span></div>\n      </div>\n    </div>\n    <div class=\"layuiadmin-card-list\">\n      <p class=\"layuiadmin-normal-font\">系统负载</p>\n      <div class=\"layui-progress layui-progress-big\" lay-showpercent=\"yes\">\n        <div class=\"layui-progress-bar\"\n             lay-percent={{ load_rate }}% style=\"width:{{ load_rate }}%\"><span\n                                                                               class=\"layui-progress-text\">{{ load_rate }}%</span></div>\n      </div>\n    </div>\n    <div class=\"layuiadmin-card-list\">\n      <p class=\"layuiadmin-normal-font\">内存使用率</p>\n      <div class=\"layui-progress layui-progress-big\" lay-showpercent=\"yes\">\n        <div class=\"layui-progress-bar\"\n             lay-percent={{ memory_rate }}% style=\"width:{{ memory_rate }}%\"><span\n                                                                                   class=\"layui-progress-text\">{{ memory_rate }}%</span></div>\n      </div>\n    </div>\n    <div class=\"layuiadmin-card-list\">\n      <p class=\"layuiadmin-normal-font\">磁盘使用率</p>\n      <div class=\"layui-progress layui-progress-big\" lay-showpercent=\"yes\">\n        <div class=\"layui-progress-bar\"\n             lay-percent={{ disk_rate }}% style=\"width:{{ disk_rate }}%\"><span\n                                                                               class=\"layui-progress-text\">{{ disk_rate }}%</span></div>\n      </div>\n    </div>\n  </div>\n</div>\n```\n\n# 五、结果查看\n\n1. 主机性能模块信息查看\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608128320661-3bce53e4-0120-4abf-9f42-5c55fef00b09.png#align=left&display=inline&height=396&margin=%5Bobject%20Object%5D&name=image.png&originHeight=396&originWidth=547&size=19454&status=done&style=none&width=547)\n\n2. 后台管理首页\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608128372879-a31c29cd-c5b8-4c54-991d-9a3a366040c9.png#align=left&display=inline&height=971&margin=%5Bobject%20Object%5D&name=image.png&originHeight=971&originWidth=1920&size=273355&status=done&style=none&width=1920)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608128389937-9012aee4-7104-4587-9aed-718e3ceeb616.png#align=left&display=inline&height=971&margin=%5Bobject%20Object%5D&name=image.png&originHeight=971&originWidth=1920&size=276582&status=done&style=none&width=1920)\n\n\n\n', 401, 4, 0, '2020-12-16 22:42:01.026824', '2021-01-26 10:31:38.262439', 1, 4, 0, 1, 1);
INSERT INTO `blog_article` VALUES (129, 'python爬虫自动更换必应壁纸', '最近在做登录页时，对于有选择困难症的我来说，选哪一张图片做背景图片是一件十分头疼的事。既然这样，那就每天定时爬取必应的壁纸作为背景图片。有时候，没有选择就是最好的选择。', 'cover/2020_12_25_10_18_09_462341.jpg', '[TOC]\n> 最近在做登录页时，对于有选择困难症的我来说，选哪一张图片做背景图片是一件十分头疼的事。既然这样，那就每天定时爬取必应的壁纸作为背景图片，毕竟必应每天的图片质量都是挺不错的。有时候，没有选择就是最好的选择。\n\n# 一、获取图片下载地址\n\n## 1. 分析必应网站html代码结构\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608802589204-353df350-392c-474f-947b-45b9314adefa.png#align=left&display=inline&height=801&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1602&originWidth=2880&size=3151207&status=done&style=none&width=1440)\n\n- 分析html代码可知，图片背景图片的地址是域名（[https://cn.bing.com/](https://cn.bing.com/)）与div元素background-image属性地址（/th?id=OHR.WildReindeer_ZH-CN8301029606_UHD.jpg&rf=LaDigue_UHD.jpg&pid=hp&w=1920&h=1080&rs=1&c=4）拼接而来\n\n## 2. 获取div元素background-image属性\n\n- 使用requests库发送get请求，获取html代码，然后解析出background-image属性值，拼接得到实际背景图片下载地址。\n- 从html代码中解析出属性值，可以采用正则方法匹配或者xpath解析，在这儿分享一个快速写xpath表达式的技巧\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608860958203-e2821464-5afe-44a3-9e04-33c61ede81cd.png#align=left&display=inline&height=888&margin=%5Bobject%20Object%5D&name=image.png&originHeight=888&originWidth=668&size=191496&status=done&style=none&width=668)\n\n```python\nurl = \'https://cn.bing.com/\'\n# 设置浏览器参数\nheader = {\n    \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36\',\n    # referer的作用就是记录你在访问一个目标网站时，在访问前你的原网站的地址\n    \'Referer\': \'https://cn.big.com/\'\n}\n# 下载网页html代码\nhtml = requests.get(url, headers=header).text\n# 构造xpath的解析对象\netree_html = etree.HTML(html)\n# 获取图片地址\ndata_url = etree_html.xpath(\'//*[@id=\"bgImgProgLoad\"]/@data-ultra-definition-src\')[0]\nimg_url = \"https://cn.bing.com/\" + data_url\nprint(\'图片下载地址：\', img_url)\n```\n\n# 二、图片压缩\n\n默认下载的图片是1080P的大图，直接设置为背景图片的话，对带宽压力较大。因此需要进行压缩图片处理，可以使用pillow库在保存图片时指定分辨率大小，但会使图片变得模糊。\n在此推荐给大家一个在线压缩图片地址[https://tinypng.com/](https://tinypng.com/)。压缩后图片依然清晰，最重要的是它有python sdk库，方便调用。\n\n## 1. 申请API key\n\n先在他们的网站[TinyPNG Developer API](https://tinypng.com/developers)上申请一个API key用于身份验证。只需要提供你的名字和邮箱地址就可以获得一个API key，API key会以链接的形式发到邮箱里。\n\n## 2. 调试demo程序\n\n参考文档：[https://tinypng.com/developers/reference/python](https://tinypng.com/developers/reference/python)\n\n```python\nimport tinify\ntinify.key = \"YOUR_API_KEY\"\nsource = tinify.from_file(\"large.jpg\")\nresized = source.resize(\n    method=\"fit\",\n    width=150,\n    height=100\n)\nresized.to_file(\"thumbnail.jpg\")\n```\n\n# 三、完整代码\n\n- 使用crontab计划任务，每天凌晨执行一次脚本，下载必应壁纸原图到/root/bgc下,文件名为当天时间戳，然后使用TinyPN压缩图片，生成新文件至/root/myblog/static/images下，命名为bg-img.jpg，从而实现每日壁纸自动更换。\n\n```python\n#!/usr/bin/python3\n# 定时更新登录页背景图\n\nimport os\nimport time\nimport request\nimport click\nimport tinify\nimport requests\n\n\n# 获取必应每日壁纸img下载地址\ndef get_img_url():\n    url = \'https://cn.bing.com/\'\n    # 浏览器参数\n    header = {\n        \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36\',\n        # referer的作用就是记录你在访问一个目标网站时，在访问前你的原网站的地址\n        \'Referer\': \'https://cn.bing.com/\'\n    }\n    # 下载网页\n    html = requests.get(url, headers=header).text\n    # 构造xpath的解析对象\n    etree_html = etree.HTML(html)\n    # 获取图片地址\n    data_url = etree_html.xpath(\'//*[@id=\"bgImgProgLoad\"]/@data-ultra-definition-src\')[0]\n    img_url = \"https://cn.bing.com/\" + data_url\n    print(\'图片下载地址：\', img_url)\n    return img_url\n\n\n# 保存图片到磁盘文件夹dirname中\ndef save_img(img_url, dirname):\n    try:\n        if not os.path.exists(dirname):\n            print(\'文件夹\', dirname, \'不存在，重新建立\')\n            # os.mkdir(dirname)\n            os.makedirs(dirname)\n        # 获得图片文件名，包括后缀\n        date = time.strftime(\'%Y-%m-%d\', time.localtime(time.time()))\n        filename = date + \'.jpg\'\n        # 拼接目录与文件名，得到图片路径\n        filepath = os.path.join(dirname, filename)\n        # 下载图片，并保存到文件夹中\n        print(\"保存地址:\", filepath)\n        urllib.request.urlretrieve(img_url, filepath)\n    except IOError as e:\n        print(\'文件操作失败\', e)\n    except Exception as e:\n        print(\'错误 ：\', e)\n    print(filepath, \"保存成功!\")\n    return filepath\n\n\n# 压缩图片\ndef zip_img(in_file, out_file):\n    tinify.key = \"***\"\n    source = tinify.from_file(in_file)  # 压缩指定文件\n    resized = source.resize(\n        method=\"fit\",\n        width=1024,\n        height=768\n    )\n    resized.to_file(out_file)\n    print(out_file, \"压缩完成！\")\n\n\ndef main():\n    # 下载保存图片\n    dirname = \"/root/bgc\"\n    img_url = get_img_url()\n    file_path = save_img(img_url, dirname)\n\n    # 压缩图片\n    img_dir = \"/root/myblog/static/images\"  # 输出目录\n    img_name = \'bg-img.jpg\'\n    img_path = img_dir+\"/\"+img_name\n    zip_img(file_path, img_path)\n\n\nif __name__ == \'__main__\':\n    main()\n```\n\n# 四、效果演示\n\n1. [登录页](https://www.cuiliangblog.cn/account/loginRegister/)\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608804583530-c0b3b2bc-14b5-45dc-a2fa-6c96572b2e37.png#align=left&display=inline&height=801&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1602&originWidth=2880&size=4267462&status=done&style=none&width=1440)\n\n2. 必应壁纸原图文件\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608804708040-f62b95b2-335d-4574-a26d-92e9d823b0a2.png#align=left&display=inline&height=118&margin=%5Bobject%20Object%5D&name=image.png&originHeight=236&originWidth=1656&size=86148&status=done&style=none&width=828)', 312, 2, 0, '2020-12-25 10:21:40.398792', '2021-01-26 19:30:30.694408', 1, 3, 0, 1, 1);
INSERT INTO `blog_article` VALUES (130, 'Django第三方账号登录（QQ、微博、github）', '博客网站上线一段时间了，发现注册用户很少，大家对于注册账号这种事都不太感兴趣，因此决定开发第三方账号登录功能，参考了相关文档实现了该功能，对整个过程以及其中遇到的问题还有解决办法做了详细的总结，避免大家踩坑。', 'cover/2020_12_27_18_20_15_014245.jpg', '[TOC]\n>博客网站上线一段时间了，发现注册用户很少，大家对于注册账号这种事都不太感兴趣，因此决定开发第三方账号登录功能，参考了相关文档实现了该功能，对整个过程以及其中遇到的问题还有解决办法做了详细的总结，避免大家踩坑。\n\n# 一、什么是OAuth\n\n## 1. 概念\n\nOAuth的英文全称是Open Authorization，它是一种开放授权协议。OAuth目前共有2个版本，2007年12月的1.0版（之后有一个修正版1.0a）和2010年4月的2.0版，1.0版本存在严重安全漏洞，而2.0版解决了该问题。\nOAuth简单说就是一种授权的协议，只要授权方和被授权方遵守这个协议去写代码提供服务，那双方就是实现了OAuth模式。\n\n## 2. 应用场景\n\n用户想要在某个网站应用登录使用时，又不想通过繁琐的步骤注册账号，此时就可以使用OAuth协议，调用第三方平台（如qq 微博等），完成第三方平台登录后，实现自动创建用户并登录的过程。\n\n## 3. 过程解析\n\n以博客网站使用QQ实现认证登录过程为例，详细分析具体流程\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609040146302-ed1c1edb-77f1-4beb-8936-d88ca85aaa7d.png#align=left&display=inline&height=464&margin=%5Bobject%20Object%5D&name=image.png&originHeight=464&originWidth=1165&size=113562&status=done&style=none&width=1165)\n\n- 第一步：点击qq登录图标\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609040263194-d9c9bd77-882d-49b0-8267-c2e6d1d45be3.png#align=left&display=inline&height=547&margin=%5Bobject%20Object%5D&name=image.png&originHeight=547&originWidth=448&size=60272&status=done&style=none&width=448)\n此时f12打开调试工具，发现network发送的具体请求地址是 `https://www.cuiliangblog.cn/login/login/qq` \n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609040400822-f65d62d9-88e5-4e20-9529-654a5199b687.png#align=left&display=inline&height=167&margin=%5Bobject%20Object%5D&name=image.png&originHeight=167&originWidth=822&size=36462&status=done&style=none&width=822)\nDjango服务器会响应一个重定向地址，指向qq授权登录页。`https://graph.qq.com/oauth2.0/authorize?client_id=101921043&client_id=https://www.cuiliangblog.cn/login/complete/qq&state=uu25k4sabcActZYDUB2B1He8BrYS0MYu&response_type=code`\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609040694692-33ab12d1-db0b-4b0b-9c6a-30a9beee2cc0.png#align=left&display=inline&height=209&margin=%5Bobject%20Object%5D&name=image.png&originHeight=209&originWidth=897&size=47938&status=done&style=none&width=897)\n该请求中携带了client_id、state等信息，QQ服务器收到请求后判断来源请求认证网站信息的正确性，确认无误后跳转至登录授权页面\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609041352719-582089f5-dd23-4047-813b-892c343117c5.png#align=left&display=inline&height=224&margin=%5Bobject%20Object%5D&name=image.png&originHeight=224&originWidth=908&size=49553&status=done&style=none&width=908)\n\n- 第二步：qq登录页面输入用户名密码，然后点授权并登录\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609041226042-7c3dd7d7-2eb4-4d07-8f59-e1d778fb3cc3.png#align=left&display=inline&height=388&margin=%5Bobject%20Object%5D&name=image.png&originHeight=388&originWidth=935&size=71717&status=done&style=none&width=935)\n用户输入账号密码点击授权并登录按钮后，访问qq服务器中校验用户名密码的方法，进行用户名密码登录校验\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609041411269-d5f70b39-57b1-4df8-9e95-90fefe674663.png#align=left&display=inline&height=477&margin=%5Bobject%20Object%5D&name=image.png&originHeight=477&originWidth=900&size=108367&status=done&style=none&width=900)\n若校验成功，该方法会响应浏览器一个重定向地址，并附上一个code（授权码）和state。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609041513587-fcc1ea2b-319c-49cd-9d05-12c33150b0e1.png#align=left&display=inline&height=434&margin=%5Bobject%20Object%5D&name=image.png&originHeight=434&originWidth=904&size=85879&status=done&style=none&width=904)\n\n- 第三步：跳回到博客页面，成功登录\n\nDjango服务器拿到的token后，获取根据openid获取用户信息。最后将用户信息储存起来，返回给浏览器其首页的视图。到此OAuth2.0授权结束。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609044025426-d5d6a22f-218e-449c-a3a3-4109d7d0d246.png#align=left&display=inline&height=549&margin=%5Bobject%20Object%5D&name=image.png&originHeight=549&originWidth=1035&size=65282&status=done&style=none&width=1035)\n\n# 二、使用OAuth\n\n## 1. django项目使用OAuth\n\n了解上述过程后，如何在自己的项目中使用OAuth认证呢？\n\n- 方式一：参考文档，发送request请求api接口，完成认证授权。参考文档：[https://wiki.connect.qq.com/%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C_oauth2-0](https://wiki.connect.qq.com/%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C_oauth2-0)\n- 方式二：使用Django库，在此推荐两个库①django-allauth（大而全，从用户注册到发送邮件重置密码全过程、以及第三方登录都功能都有）②social_django（Django项目第三方登录模块）\n\n由于我的项目已经开发了用户认证全部流程，所有在这采用social_django库实现用户第三方登录。参考文档：[https://python-social-auth.readthedocs.io/en/latest/index.html](https://python-social-auth.readthedocs.io/en/latest/index.html)\n\n## 2.准备工作\n\n使用QQ、微博的第三方登录时，需要开发者实名认证，QQ调用网站还需要进行备案。微信不支持个人身份开发者注册，需要填写企业信息。\n\n- QQ申请地址：[https://connect.qq.com/](https://connect.qq.com/)，如果遇到无法填写信息的话，可以登陆开放平台[http://wiki.open.qq.com/](http://wiki.open.qq.com/) 后，点管理中心-账号资料再点编辑就可以上传修改信息，提交审核即可。（QQ互联资质和腾讯开放平台资质是共用的）\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609044107013-a6a93e21-9d07-428e-9c7d-77ad1557b64f.png#align=left&display=inline&height=472&margin=%5Bobject%20Object%5D&name=image.png&originHeight=472&originWidth=1022&size=65958&status=done&style=none&width=1022)\n\n- 微博申请地址：[https://open.weibo.com/](https://open.weibo.com/)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609044270114-b5e8752d-b5bd-4877-84f4-70532c4a3aae.png#align=left&display=inline&height=579&margin=%5Bobject%20Object%5D&name=image.png&originHeight=579&originWidth=971&size=107094&status=done&style=none&width=971)\n\n- github申请地址：[https://github.com/settings/applications/new](https://github.com/settings/applications/new)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609044404191-1409a791-1a51-4429-ae40-f02965b4bebe.png#align=left&display=inline&height=336&margin=%5Bobject%20Object%5D&name=image.png&originHeight=336&originWidth=1029&size=42659&status=done&style=none&width=1029)\n\n- 安装social_django库\n\n`pip install social-auth-app-django` \n\n- 修改setting.py配置\n\n```python\nINSTALLED_APPS = [\n    ……\n    \'social_django\',  # 第三方登录\n    ……\n]\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [os.path.join(BASE_DIR, \'templates\')]\n        ,\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                ………\n                # 第三方账号登录\n                \'social_django.context_processors.backends\',\n                \'social_django.context_processors.login_redirect\',\n                ……\n            ],\n        },\n    },\n]\n\n# 支持邮箱登录\nAUTHENTICATION_BACKENDS = (\n                           \'django.contrib.auth.backends.ModelBackend\',  # 指定Django的modelbackend类\n                           )\n# 第三方登录\nSOCIAL_AUTH_URL_NAMESPACE = \'social\'\n\n# 登陆成功后的回调路由\nSOCIAL_AUTH_LOGIN_REDIRECT_URL = \'https://www.cuiliangblog.cn/account/\'\n# 使用https代理\nSECURE_PROXY_SSL_HEADER = (\'HTTP_X_FORWARDED_PROTO\', \'https\')\n```\n\n- 配置url\n\n```python\nurlpatterns = [\n    ……\n    path(\'login/\', include(\'social_django.urls\', namespace=\'social\')),\n    # 第三方账户登录\n    ……\n]\n```\n\n- 创建数据库相关表\n\n`python manage.py migrate` \n\n生成如下五张表\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609045126235-23b87567-9878-4516-a7d1-782ac0e080c4.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=image.png&originHeight=110&originWidth=204&size=5586&status=done&style=none&width=204)\n\n## 3. github\n\n- setting文件添加配置\n\n```python\n# 登录认证后端\nAUTHENTICATION_BACKENDS = (……\n                           \'social_core.backends.github.GithubOAuth2\',  # github登录\n                       		……\n                           )\n\n# 第三方Github KEY和SECRET\nSOCIAL_AUTH_GITHUB_KEY = \'***********\'\nSOCIAL_AUTH_GITHUB_SECRET = \'***************\'\n# 使用昵称作为用户名\nSOCIAL_AUTH_GITHUB_USE_OPENID_AS_USERNAME = True\n```\n\n- 前端页面添加github登录跳转按钮\n\n```html\n<button type=\"button\" onclick=\"window.location.href = \'{% url \'social:begin\' \'github\' %}\'\" class=\"layui-btn layui-btn-normal\"><i class=\"fab fa-github fa-lg\"></i></button>\n```\n\n- github应用settings配置，Homepage URL和Authorization callback URL都填写域名即可\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609045468864-9882b805-e85f-4697-9bc6-ce5bfff3b766.png#align=left&display=inline&height=774&margin=%5Bobject%20Object%5D&name=image.png&originHeight=774&originWidth=668&size=82454&status=done&style=none&width=668)\n\n- 登录测试（如果出现redirect_uri错误提示，参考后面的注意事项）\n\n## 4. 微博\n\n- setting文件添加配置\n\n```python\n# 登录认证后端\nAUTHENTICATION_BACKENDS = (……\n                           \'social_core.backends.weibo.WeiboOAuth2\',  # 新浪微博登录\n                       		……\n                           )\n\n# 第三方新浪微博KEY和SECRET\nSOCIAL_AUTH_WEIBO_KEY = \'**********\'\nSOCIAL_AUTH_WEIBO_SECRET = \'***************\'\n```\n\n- 前端页面添加github登录跳转按钮\n\n```html\n<button type=\"button\" onclick=\"window.location.href = \'{% url \'social:begin\' \'weibo\' %}\'\" class=\"layui-btn layui-btn-normal\"><i class=\"layui-icon layui-icon-login-weibo\"></i></button>\n```\n\n- 微博开放平台，填写应用授权回调页\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609046006820-4d354b54-0fa7-4414-bfa5-65262845f598.png#align=left&display=inline&height=390&margin=%5Bobject%20Object%5D&name=image.png&originHeight=390&originWidth=970&size=78373&status=done&style=none&width=970)\n\n- 登录测试（如果出现redirect_uri错误提示，参考后面的注意事项）\n\n## 5. QQ\n\n- setting文件添加配置\n\n```python\n# 登录认证后端\nAUTHENTICATION_BACKENDS = (……\n                           \'social_core.backends.qq.QQOAuth2\',  # qq登录\n                       		……\n                           )\n\n# 第三方QQ KEY和SECRET\nSOCIAL_AUTH_QQ_KEY = \'***************\'\nSOCIAL_AUTH_QQ_SECRET = \'*********\'\n```\n\n- 前端页面添加github登录跳转按钮\n\n```html\n<button type=\"button\" onclick=\"window.location.href = \'{% url \'social:begin\' \'qq\' %}\'\" class=\"layui-btn layui-btn-normal\"><i class=\"layui-icon layui-icon-login-qq\"></i></button>\n```\n\n- QQ开放平台，填写应用授权回调页\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609058888662-d2ad40fa-5052-4e1a-b0a0-1599411ec951.png#align=left&display=inline&height=551&margin=%5Bobject%20Object%5D&name=image.png&originHeight=551&originWidth=1002&size=38508&status=done&style=none&width=1002)\n\n\n- 登录测试（如果出现redirect_uri错误提示，参考后面的注意事项）\n\n# 三、其他功能实现\n\n使用默认配置不能获取用户头像，性别信息，微博和QQ的用户名是用户的UID，为了提高用户体验，需要在创建用户时，修改一部分代码逻辑，social_django库支持可扩展的管道机制，开发人员可以在身份验证、登录和注销期间自定义开发其他功能。参考文档：[https://python-social-auth.readthedocs.io/en/latest/pipeline.html#authentication-pipeline](https://python-social-auth.readthedocs.io/en/latest/pipeline.html#authentication-pipeline)\n安装官方文档的模板开发功能，还是比较麻烦的，后期库版本更新了还得调试代码。所有我换了个思路，研究social_django库代码发现，用户其他信息都存放在AbstractUserSocialAuth模型的extra_data字段中\n\n## 1. 调试测试\n\n- 模型文件路径：social_django > migrations > models.py\n\n```python\nclass AbstractUserSocialAuth(models.Model, DjangoUserMixin):\n    \"\"\"Abstract Social Auth association model\"\"\"\n    ……\n    extra_data = JSONField()\n    ……\n\n    def __str__(self):\n        return str(self.user)\n\n    class Meta:\n        app_label = \"social_django\"\n        abstract = True\n    ……\n\n\nclass UserSocialAuth(AbstractUserSocialAuth):\n    \"\"\"Social Auth association model\"\"\"\n\n    class Meta:\n        \"\"\"Meta data\"\"\"\n        app_label = \"social_django\"\n        unique_together = (\'provider\', \'uid\')\n        db_table = \'social_auth_usersocialauth\'\n```\n\n- 数据库查看extra_data字段信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609059856420-d679a1cf-b5da-488c-8851-0858c2eae347.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=image.png&originHeight=162&originWidth=1216&size=46870&status=done&style=none&width=1216)\n\n- python shell 调试获取用户详细信息\n\n```python\n[root@0da62d4c0833 myblog]# python3.9 manage.py shell\nPython 3.9.0 (default, Dec 12 2020, 15:10:05) \n[GCC 8.3.1 20191121 (Red Hat 8.3.1-5)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n>>> from social_django.models import UserSocialAuth\n>>> user1 = UserSocialAuth.objects.get(user_id=62)                \n>>> user1.extra_data[\'username\']\n\'ζั͡ޓއއ陌影\'\n>>> user1.extra_data[\'profile_image_url\']\n\'http://thirdqq.qlogo.cn/g?b=oidb&k=B2EsdNd78qoVu4616YoNVg&s=40&t=1581912070\'\n>>> user1.extra_data[\'gender\']\n\'女\'\n>>> \n```\n\n调试没问题，下面开始写这部分业务逻辑的代码。\n\n## 2. 获取用户头像、用户名、性别\n\n网站的数据库设计时，有一个userinfo表，专门存放用户详细信息，整体逻辑流程就是获取UserSocialAuth当前登录用户的extra_data相关值，保存到userinfo相关字段即可\n\n```python\n# userinfo模型详细信息\nclass UserInfo(models.Model):\n    user = models.OneToOneField(User, on_delete=models.CASCADE, unique=True)\n    # 与自带用户表一对一\n    phone = models.CharField(verbose_name=\'手机号\', max_length=20, default=\"保密\")\n    sex_choice = [(\'1\', \'男\'), (\'2\', \'女\')]\n    sex = models.CharField(verbose_name=\'性别\', max_length=1, choices=sex_choice, default=1)\n    web = models.CharField(verbose_name=\'个人网站\', max_length=50, blank=True, null=True)\n    aboutme = models.TextField(verbose_name=\'个性签名\', max_length=200, default=\"这个人很懒，什么都没留下！\")\n    photo = models.ImageField(upload_to=\'photo/\', verbose_name=\'头像\', default=\'photo/default.jpg\')\n\n    class Meta:\n        verbose_name = \'用户详细信息\'\n        verbose_name_plural = verbose_name\n\n    def __str__(self):\n        return \"user:{}\".format(self.user.username)\n```\n\n主要解决的问题有：\n\n- 用户头像和性别只有QQ、微博这两个平台可以授权获取，github获取不到\n- 用户名github默认获取的昵称作为用户名，而QQ和微博使用UID编码作为用户名\n\n用户授权完成后跳转到个人中心页view部分代码\n\n```python\n# 用户个人中心页面相关view处理逻辑\ndef personalCenter(request):\n    try:\n        userinfo = UserInfo.objects.get(user_id=request.user.id)\n        # print(\"已注册用户登录\")\n    except Exception as e:\n        print(e)\n        # print(\"第三方用户第一次登录\")\n        social_user = UserSocialAuth.objects.get(user_id=request.user.id)\n        if social_user.provider == \'github\':\n            # print(\"github登录\")\n            UserInfo.objects.create(user_id=request.user.id)\n            user = User.objects.get(id=request.user.id)\n        else:\n            # print(\"qq或微博登录\")\n            username = social_user.extra_data[\'username\'].replace(\" \", \"\")\n            photo_url = social_user.extra_data[\'profile_image_url\']\n            username = social_user.extra_data[\'username\'].replace(\" \", \"\")\n            photo_url = social_user.extra_data[\'profile_image_url\']\n            sex = social_user.extra_data[\'gender\']\n            # QQ用户性别存储值为男、女,微博用户性别存储值为f、m\n            if sex == \'女\' or sex == \'f\':\n                sex_id = 2\n            else:\n                sex_id = 1\n            user_info = UserInfo()\n            user_info.user_id = request.user.id\n            user_info.photo = downloadPhoto(photo_url)\n            user_info.sex = sex_id\n            user_info.save()\n            new_user = User.objects.get(id=request.user.id)\n            new_user.username = username\n            new_user.save()\n            user = new_user\n    ………………\n    return render(request, \'layui-mini/account/index.html\', locals())\n```\n\n# 四、注意事项\n\n## 1. 授权回调页地址填写\n\n- github授权回调页填写域名即可，例如[https://www.cuiliangblog.cn/](https://www.cuiliangblog.cn/)\n- qq和微博的授权回调页打开浏览器调试工具，查看redirect_uri地址填写\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609061745454-23dfc7e5-5a21-4c3d-8d91-53ed4602a0ee.png#align=left&display=inline&height=166&margin=%5Bobject%20Object%5D&name=image.png&originHeight=166&originWidth=805&size=44452&status=done&style=none&width=805)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609061930758-16a6362a-e49a-4c5e-8200-67f9a28faf53.png#align=left&display=inline&height=126&margin=%5Bobject%20Object%5D&name=image.png&originHeight=126&originWidth=973&size=32203&status=done&style=none&width=973)\n\n## 2. qq回调地址问题\n\nsocial_django库默认回调地址为****/login/complete/qq/，不符合qq回调页地址要求。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609061985903-bfc7d8fe-2abb-4359-932f-951e20b11b9c.png#align=left&display=inline&height=498&margin=%5Bobject%20Object%5D&name=image.png&originHeight=498&originWidth=1027&size=45994&status=done&style=none&width=1027)\n此时可以修改setting配置，取消默认回调页uri后面的/\n\n```python\n# 回调页uri后面不加/\nSOCIAL_AUTH_TRAILING_SLASH = False\n```\n\n## 3. from表单redirect_uri地址异常问题\n\n正确的form表单redirect_uri地址如下图所示\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609062869306-62cc3d4f-82ad-4adc-b0c5-966260971a72.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=image.png&originHeight=117&originWidth=496&size=16914&status=done&style=none&width=496)\n\n- redirect_uri地址为http://127.0.0.1:8000/***情况解决办法\n\n主要是被代理服务器接收到的header信息不对，修改nginx配置，添加proxy_set_header 配置即可\n\n```bash\nserver {\n        …………\n        location / {\n                proxy_pass http://127.0.0.1:8000;\n                proxy_set_header  Host $host;\n                proxy_set_header  X-Real-IP $remote_addr;\n                proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header X-Forwarded-Proto  $scheme;\n        }\n 				…………\n    }\n```\n\n- redirect_uri地址为http://***，而不是https://***情况解决办法\n\n修改setting配置，强制将所有http的重定向请求转为https\n\n```bash\n# 将所有非SSL请求永久重定向到SSL\nSECURE_PROXY_SSL_HEADER = (\'HTTP_X_FORWARDED_PROTO\', \'https\')\n```\n\n\n# 五、效果演示\n\n## 1. 登录页\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609063401781-4592c8bd-266d-4780-af77-963c635c096e.png#align=left&display=inline&height=970&margin=%5Bobject%20Object%5D&name=image.png&originHeight=970&originWidth=1920&size=3028606&status=done&style=none&width=1920)\n\n## 2. 用户个人中心页 \n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609063568582-977775a2-dd21-41b8-858b-341bf2cd35b6.png#align=left&display=inline&height=970&margin=%5Bobject%20Object%5D&name=image.png&originHeight=970&originWidth=1920&size=175600&status=done&style=none&width=1920)\n\n## 3. 后台管理用户列表\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609063666506-b42e50a5-9243-4d3a-acc1-5a30fc8a26f6.png#align=left&display=inline&height=970&margin=%5Bobject%20Object%5D&name=image.png&originHeight=970&originWidth=1920&size=247169&status=done&style=none&width=1920)', 376, 4, 0, '2020-12-27 19:36:16.487507', '2021-01-26 05:38:57.977315', 1, 4, 0, 1, 1);
INSERT INTO `blog_article` VALUES (131, 'python读取csv处理生成excel', '最近领导安排让我每周定时把grafana导出的csv文件进行统计汇总工作，需要处理的csv文件还是蛮多的，况且还要每周重复汇总处理。干脆写个脚本，每周执行一遍脚本，既方便还不会出错。', 'cover/2021_01_15_17_24_20_635986.jpg', '[TOC]\n> 最近领导安排让我每周定时把grafana导出的csv文件进行统计汇总工作，需要处理的csv文件还是蛮多的，况且还要每周重复汇总处理。干脆写个脚本，每周执行一遍脚本，既方便还不会出错。\n\n# 一、需求分析\n\n## 1. 原始文件分析\n\n  原始文件是多个csv表格，第一列为时间戳，每10分钟统计生成一行，其余列为ip地址在该时间段内的访问次数\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610698222567-7e8b32a0-aec5-4877-b414-785ea384fe0f.png#align=left&display=inline&height=363&margin=%5Bobject%20Object%5D&name=image.png&originHeight=725&originWidth=1127&size=129901&status=done&style=none&width=563.5)\n\n## 2. 处理结果分析\n\n根据要求，统计每个ip地址在当天访问次数求和，汇总生成新表格，结果如下，并将所有csv文件按照文件名，分别汇总到不同的sheet下\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610698451974-8f8446f7-f186-4bef-88d4-e78fb1a55c57.png#align=left&display=inline&height=175&margin=%5Bobject%20Object%5D&name=image.png&originHeight=350&originWidth=1043&size=50175&status=done&style=none&width=521.5)\n\n# 二、代码逻辑\n\n## 1. 流程分析\n\n- 首先遍历指定目录下的.csv文件，提取文件名生成数组 \n- 然后使用pandas库读取csv文件，提取日期和ip，然后统计每个ip当天访问次数，生成新的DataFrame\n- 最后使用xlwings库将pandas处理后的DataFrame数据写入excel文件，指定文件名作为sheet名\n\n## 2. 遍历指定目录下.csv文件\n\n> 主要用到了os模块中的walk()函数，可以遍历文件夹下所有的文件名。\n\n```python\ndef find_csv(path):\n    \"\"\"\n    查找目录下csv文件\n    :param path: 查找csv的目录路径\n    :return: csv文件名list\n    \"\"\"\n    csv_file = []\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if os.path.splitext(file)[1] == \'.csv\':\n                csv_file.append(os.path.join(root, file))\n    return csv_file\n```\n\n## 3. pandas处理csv文件\n\n> pandas是python环境下最有名的数据统计包，对于数据挖掘和数据分析，以及数据清洗等工作，用pandas再合适不过了，官方地址：[https://www.pypandas.cn/](https://www.pypandas.cn/)\n\n```python\ndef summary_data(file):\n    \"\"\"\n    grafana导出的csv文件处理汇总\n    :param file: csv文件路径\n    :return: 处理完成后的pandas对象\n    \"\"\"\n    # 读取整个csv文件\n    csv_data = pd.read_csv(file, \';\')\n    # 提取日期\n    csv_data[\"Time\"] = csv_data[\"Time\"].map(lambda Time: Time[0:10])\n    date = csv_data[\"Time\"].drop_duplicates()\n    # 提取IP\n    ip_list = csv_data.columns.values[1:]\n    # 生成新列表\n    result_data = []\n    for day in list(date):\n        ip_data = []\n        for ip in ip_list:\n            # 统计指定ip地址在指定日期的数据之和\n            ip_sum = csv_data.loc[csv_data[\'Time\'] == day, ip].sum()\n            ip_data.append(ip_sum)\n            # print(\"日期：%s ip：%s 总计:%s\" % (day, ip, ip_sum))\n        result_data.append(ip_data)\n    # 生成新的DataFrame\n    result_df = pd.DataFrame(result_data, index=list(date), columns=ip_list)\n    # 添加行列统计\n    result_df[\'day_sum\'] = result_df.apply(lambda x: x.sum(), axis=1)\n    result_df.loc[\'ip_sum\'] = result_df.apply(lambda x: x.sum())\n    print(file, \"处理完毕!\")\n    return result_df\n```\n\n## 4. excel数据写入\n\n> pandas的to_excel方法也可以写入到excel文件，但是如果需要写入到指定的sheet，就无法满足需求了，此时就需要用的xlwings或者openpyxl库，此处使用xlwings，参考文档：[https://www.xlwings.org/pro](https://www.xlwings.org/pro)\n\n```python\ndef save_excel(data_df, file_name, excel_name):\n    \"\"\"\n    生成并写入新excel文件\n    :param data_df: pandas数据对象\n    :param file_name: 传入文件名，作为生成的sheet名称\n    :param excel_name: 生成excel文件名\n    :return: null\n    \"\"\"\n    sheet_name = file_name[file_name.rfind(\'/\', 1) + 1:file_name.rfind(\'.\', 1)]\n    wb = xlwings.Book(excel_name)\n    sheet = wb.sheets.add(name=sheet_name)\n    sheet.range(\"A1\").value = data_df\n    wb.save()\n    wb.close()\n    print(sheet_name, \"Sheet写入完毕!\")\n```\n\n## 5. 完整代码\n\n```python\nimport os\nimport pandas as pd\nimport xlwings\n\n\ndef find_csv(path):\n    \"\"\"\n    查找目录下csv文件\n    :param path: 查找csv的目录路径\n    :return: csv文件名list\n    \"\"\"\n    csv_file = []\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if os.path.splitext(file)[1] == \'.csv\':\n                csv_file.append(os.path.join(root, file))\n    return csv_file\n\n\ndef summary_data(file):\n    \"\"\"\n    grafana导出的csv文件处理汇总\n    :param file: csv文件路径\n    :return: 处理完成后的pandas对象\n    \"\"\"\n    # 读取整个csv文件\n    csv_data = pd.read_csv(file, \';\')\n    # 提取日期\n    csv_data[\"Time\"] = csv_data[\"Time\"].map(lambda Time: Time[0:10])\n    date = csv_data[\"Time\"].drop_duplicates()\n    # 提取IP\n    ip_list = csv_data.columns.values[1:]\n    # 生成新列表\n    result_data = []\n    for day in list(date):\n        ip_data = []\n        for ip in ip_list:\n            ip_sum = csv_data.loc[csv_data[\'Time\'] == day, ip].sum()\n            ip_data.append(ip_sum)\n            # print(\"日期：%s ip：%s 总计:%s\" % (day, ip, ip_sum))\n        result_data.append(ip_data)\n    result_df = pd.DataFrame(result_data, index=list(date), columns=ip_list)\n    # 添加行列统计\n    result_df[\'day_sum\'] = result_df.apply(lambda x: x.sum(), axis=1)\n    result_df.loc[\'ip_sum\'] = result_df.apply(lambda x: x.sum())\n    print(file, \"处理完毕!\")\n    return result_df\n\n\ndef save_excel(data_df, file_name, excel_name):\n    \"\"\"\n    生成并写入新excel文件\n    :param data_df: pandas数据对象\n    :param file_name: 传入文件名，作为生成的sheet名称\n    :param excel_name: 生成excel文件名\n    :return: null\n    \"\"\"\n    sheet_name = file_name[file_name.rfind(\'/\', 1) + 1:file_name.rfind(\'.\', 1)]\n    wb = xlwings.Book(excel_name)\n    sheet = wb.sheets.add(name=sheet_name)\n    sheet.range(\"A1\").value = data_df\n    wb.save()\n    wb.close()\n    print(sheet_name, \"Sheet写入完毕!\")\n\n\nif __name__ == \'__main__\':\n    # 原始csv文件存放路径\n    path = \'./csv\'\n    # 生成excel文件名\n    excel_name = \'cm.xlsx\'\n    csv_file = find_csv(path)\n    # 创建excel文件\n    new_excel = pd.DataFrame()\n    new_excel.to_excel(excel_name)\n    # 处理并写入excel文件\n    for file in csv_file:\n        data_df = summary_data(file)\n        save_excel(data_df, file, excel_name)\n    # 删除默认Sheet1\n    wb = xlwings.Book(excel_name)\n    wb.sheets[\'Sheet1\'].delete()\n    wb.save()\n    wb.close()\n    print(\"数据汇总完毕,生成文件路径 %s/%s\" % (os.getcwd(), excel_name))\n```\n\n\n\n', 71, 1, 0, '2021-01-15 17:24:39.935862', '2021-01-26 15:25:41.045869', 1, 3, 0, 1, 1);
COMMIT;

-- ----------------------------
-- Table structure for blog_article_tags
-- ----------------------------
DROP TABLE IF EXISTS `blog_article_tags`;
CREATE TABLE `blog_article_tags` (
  `id` int NOT NULL AUTO_INCREMENT,
  `article_id` int NOT NULL,
  `tag_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `blog_article_tags_article_id_tag_id_b78a22e9_uniq` (`article_id`,`tag_id`) USING BTREE,
  KEY `blog_article_tags_tag_id_88eb3ed9_fk_blog_tag_id` (`tag_id`) USING BTREE,
  CONSTRAINT `blog_article_tags_article_id_82c02dd6_fk_blog_article_id` FOREIGN KEY (`article_id`) REFERENCES `blog_article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `blog_article_tags_tag_id_88eb3ed9_fk_blog_tag_id` FOREIGN KEY (`tag_id`) REFERENCES `blog_tag` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=238 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of blog_article_tags
-- ----------------------------
BEGIN;
INSERT INTO `blog_article_tags` VALUES (55, 2, 1);
INSERT INTO `blog_article_tags` VALUES (56, 2, 4);
INSERT INTO `blog_article_tags` VALUES (59, 3, 1);
INSERT INTO `blog_article_tags` VALUES (60, 3, 4);
INSERT INTO `blog_article_tags` VALUES (153, 4, 2);
INSERT INTO `blog_article_tags` VALUES (154, 4, 4);
INSERT INTO `blog_article_tags` VALUES (61, 5, 1);
INSERT INTO `blog_article_tags` VALUES (62, 5, 4);
INSERT INTO `blog_article_tags` VALUES (57, 6, 1);
INSERT INTO `blog_article_tags` VALUES (58, 6, 4);
INSERT INTO `blog_article_tags` VALUES (63, 7, 2);
INSERT INTO `blog_article_tags` VALUES (64, 7, 6);
INSERT INTO `blog_article_tags` VALUES (155, 8, 2);
INSERT INTO `blog_article_tags` VALUES (156, 8, 4);
INSERT INTO `blog_article_tags` VALUES (151, 9, 2);
INSERT INTO `blog_article_tags` VALUES (152, 9, 4);
INSERT INTO `blog_article_tags` VALUES (183, 10, 2);
INSERT INTO `blog_article_tags` VALUES (184, 10, 4);
INSERT INTO `blog_article_tags` VALUES (181, 11, 2);
INSERT INTO `blog_article_tags` VALUES (182, 11, 4);
INSERT INTO `blog_article_tags` VALUES (179, 12, 2);
INSERT INTO `blog_article_tags` VALUES (180, 12, 4);
INSERT INTO `blog_article_tags` VALUES (149, 13, 2);
INSERT INTO `blog_article_tags` VALUES (150, 13, 4);
INSERT INTO `blog_article_tags` VALUES (177, 17, 2);
INSERT INTO `blog_article_tags` VALUES (178, 17, 4);
INSERT INTO `blog_article_tags` VALUES (157, 20, 2);
INSERT INTO `blog_article_tags` VALUES (158, 20, 4);
INSERT INTO `blog_article_tags` VALUES (175, 22, 2);
INSERT INTO `blog_article_tags` VALUES (176, 22, 4);
INSERT INTO `blog_article_tags` VALUES (161, 23, 2);
INSERT INTO `blog_article_tags` VALUES (162, 23, 4);
INSERT INTO `blog_article_tags` VALUES (173, 24, 2);
INSERT INTO `blog_article_tags` VALUES (174, 24, 4);
INSERT INTO `blog_article_tags` VALUES (143, 25, 2);
INSERT INTO `blog_article_tags` VALUES (144, 25, 4);
INSERT INTO `blog_article_tags` VALUES (145, 26, 2);
INSERT INTO `blog_article_tags` VALUES (146, 26, 4);
INSERT INTO `blog_article_tags` VALUES (171, 27, 2);
INSERT INTO `blog_article_tags` VALUES (172, 27, 4);
INSERT INTO `blog_article_tags` VALUES (147, 28, 2);
INSERT INTO `blog_article_tags` VALUES (148, 28, 4);
INSERT INTO `blog_article_tags` VALUES (159, 44, 2);
INSERT INTO `blog_article_tags` VALUES (160, 44, 4);
INSERT INTO `blog_article_tags` VALUES (163, 45, 2);
INSERT INTO `blog_article_tags` VALUES (164, 45, 4);
INSERT INTO `blog_article_tags` VALUES (65, 51, 1);
INSERT INTO `blog_article_tags` VALUES (66, 51, 3);
INSERT INTO `blog_article_tags` VALUES (169, 52, 2);
INSERT INTO `blog_article_tags` VALUES (170, 52, 4);
INSERT INTO `blog_article_tags` VALUES (135, 53, 2);
INSERT INTO `blog_article_tags` VALUES (136, 53, 4);
INSERT INTO `blog_article_tags` VALUES (167, 54, 2);
INSERT INTO `blog_article_tags` VALUES (168, 54, 4);
INSERT INTO `blog_article_tags` VALUES (141, 55, 2);
INSERT INTO `blog_article_tags` VALUES (142, 55, 4);
INSERT INTO `blog_article_tags` VALUES (165, 60, 2);
INSERT INTO `blog_article_tags` VALUES (166, 60, 4);
INSERT INTO `blog_article_tags` VALUES (139, 63, 2);
INSERT INTO `blog_article_tags` VALUES (140, 63, 4);
INSERT INTO `blog_article_tags` VALUES (133, 64, 2);
INSERT INTO `blog_article_tags` VALUES (134, 64, 4);
INSERT INTO `blog_article_tags` VALUES (137, 65, 2);
INSERT INTO `blog_article_tags` VALUES (138, 65, 4);
INSERT INTO `blog_article_tags` VALUES (131, 66, 2);
INSERT INTO `blog_article_tags` VALUES (132, 66, 4);
INSERT INTO `blog_article_tags` VALUES (129, 67, 2);
INSERT INTO `blog_article_tags` VALUES (130, 67, 4);
INSERT INTO `blog_article_tags` VALUES (127, 68, 2);
INSERT INTO `blog_article_tags` VALUES (128, 68, 4);
INSERT INTO `blog_article_tags` VALUES (125, 69, 2);
INSERT INTO `blog_article_tags` VALUES (126, 69, 4);
INSERT INTO `blog_article_tags` VALUES (99, 70, 2);
INSERT INTO `blog_article_tags` VALUES (100, 70, 4);
INSERT INTO `blog_article_tags` VALUES (123, 71, 2);
INSERT INTO `blog_article_tags` VALUES (124, 71, 4);
INSERT INTO `blog_article_tags` VALUES (121, 72, 2);
INSERT INTO `blog_article_tags` VALUES (122, 72, 4);
INSERT INTO `blog_article_tags` VALUES (119, 73, 2);
INSERT INTO `blog_article_tags` VALUES (120, 73, 4);
INSERT INTO `blog_article_tags` VALUES (117, 74, 2);
INSERT INTO `blog_article_tags` VALUES (118, 74, 4);
INSERT INTO `blog_article_tags` VALUES (115, 75, 2);
INSERT INTO `blog_article_tags` VALUES (116, 75, 4);
INSERT INTO `blog_article_tags` VALUES (113, 76, 2);
INSERT INTO `blog_article_tags` VALUES (114, 76, 4);
INSERT INTO `blog_article_tags` VALUES (111, 77, 2);
INSERT INTO `blog_article_tags` VALUES (112, 77, 4);
INSERT INTO `blog_article_tags` VALUES (103, 78, 2);
INSERT INTO `blog_article_tags` VALUES (104, 78, 4);
INSERT INTO `blog_article_tags` VALUES (109, 79, 2);
INSERT INTO `blog_article_tags` VALUES (110, 79, 4);
INSERT INTO `blog_article_tags` VALUES (105, 80, 2);
INSERT INTO `blog_article_tags` VALUES (106, 80, 4);
INSERT INTO `blog_article_tags` VALUES (107, 81, 2);
INSERT INTO `blog_article_tags` VALUES (108, 81, 4);
INSERT INTO `blog_article_tags` VALUES (101, 82, 2);
INSERT INTO `blog_article_tags` VALUES (102, 82, 4);
INSERT INTO `blog_article_tags` VALUES (95, 83, 2);
INSERT INTO `blog_article_tags` VALUES (96, 83, 4);
INSERT INTO `blog_article_tags` VALUES (93, 84, 2);
INSERT INTO `blog_article_tags` VALUES (94, 84, 4);
INSERT INTO `blog_article_tags` VALUES (91, 85, 2);
INSERT INTO `blog_article_tags` VALUES (92, 85, 4);
INSERT INTO `blog_article_tags` VALUES (89, 86, 2);
INSERT INTO `blog_article_tags` VALUES (90, 86, 4);
INSERT INTO `blog_article_tags` VALUES (87, 87, 2);
INSERT INTO `blog_article_tags` VALUES (88, 87, 4);
INSERT INTO `blog_article_tags` VALUES (67, 88, 3);
INSERT INTO `blog_article_tags` VALUES (68, 88, 4);
INSERT INTO `blog_article_tags` VALUES (85, 89, 2);
INSERT INTO `blog_article_tags` VALUES (86, 89, 4);
INSERT INTO `blog_article_tags` VALUES (83, 90, 2);
INSERT INTO `blog_article_tags` VALUES (84, 90, 4);
INSERT INTO `blog_article_tags` VALUES (81, 91, 2);
INSERT INTO `blog_article_tags` VALUES (82, 91, 4);
INSERT INTO `blog_article_tags` VALUES (79, 92, 2);
INSERT INTO `blog_article_tags` VALUES (80, 92, 4);
INSERT INTO `blog_article_tags` VALUES (77, 93, 2);
INSERT INTO `blog_article_tags` VALUES (78, 93, 4);
INSERT INTO `blog_article_tags` VALUES (75, 94, 2);
INSERT INTO `blog_article_tags` VALUES (76, 94, 4);
INSERT INTO `blog_article_tags` VALUES (73, 95, 2);
INSERT INTO `blog_article_tags` VALUES (74, 95, 4);
INSERT INTO `blog_article_tags` VALUES (71, 96, 2);
INSERT INTO `blog_article_tags` VALUES (72, 96, 4);
INSERT INTO `blog_article_tags` VALUES (69, 97, 2);
INSERT INTO `blog_article_tags` VALUES (70, 97, 4);
INSERT INTO `blog_article_tags` VALUES (53, 98, 2);
INSERT INTO `blog_article_tags` VALUES (54, 98, 4);
INSERT INTO `blog_article_tags` VALUES (51, 99, 2);
INSERT INTO `blog_article_tags` VALUES (52, 99, 4);
INSERT INTO `blog_article_tags` VALUES (49, 100, 2);
INSERT INTO `blog_article_tags` VALUES (50, 100, 4);
INSERT INTO `blog_article_tags` VALUES (47, 101, 2);
INSERT INTO `blog_article_tags` VALUES (48, 101, 4);
INSERT INTO `blog_article_tags` VALUES (45, 102, 2);
INSERT INTO `blog_article_tags` VALUES (46, 102, 4);
INSERT INTO `blog_article_tags` VALUES (43, 103, 2);
INSERT INTO `blog_article_tags` VALUES (44, 103, 4);
INSERT INTO `blog_article_tags` VALUES (41, 104, 2);
INSERT INTO `blog_article_tags` VALUES (42, 104, 4);
INSERT INTO `blog_article_tags` VALUES (39, 105, 2);
INSERT INTO `blog_article_tags` VALUES (40, 105, 4);
INSERT INTO `blog_article_tags` VALUES (37, 106, 2);
INSERT INTO `blog_article_tags` VALUES (38, 106, 4);
INSERT INTO `blog_article_tags` VALUES (35, 107, 2);
INSERT INTO `blog_article_tags` VALUES (36, 107, 4);
INSERT INTO `blog_article_tags` VALUES (33, 108, 2);
INSERT INTO `blog_article_tags` VALUES (34, 108, 4);
INSERT INTO `blog_article_tags` VALUES (31, 109, 2);
INSERT INTO `blog_article_tags` VALUES (32, 109, 4);
INSERT INTO `blog_article_tags` VALUES (29, 110, 2);
INSERT INTO `blog_article_tags` VALUES (30, 110, 4);
INSERT INTO `blog_article_tags` VALUES (28, 111, 2);
INSERT INTO `blog_article_tags` VALUES (27, 111, 9);
INSERT INTO `blog_article_tags` VALUES (25, 112, 2);
INSERT INTO `blog_article_tags` VALUES (26, 112, 4);
INSERT INTO `blog_article_tags` VALUES (23, 113, 2);
INSERT INTO `blog_article_tags` VALUES (24, 113, 4);
INSERT INTO `blog_article_tags` VALUES (7, 114, 2);
INSERT INTO `blog_article_tags` VALUES (8, 114, 4);
INSERT INTO `blog_article_tags` VALUES (22, 115, 4);
INSERT INTO `blog_article_tags` VALUES (21, 115, 9);
INSERT INTO `blog_article_tags` VALUES (16, 116, 4);
INSERT INTO `blog_article_tags` VALUES (15, 116, 9);
INSERT INTO `blog_article_tags` VALUES (18, 118, 4);
INSERT INTO `blog_article_tags` VALUES (17, 118, 9);
INSERT INTO `blog_article_tags` VALUES (20, 119, 4);
INSERT INTO `blog_article_tags` VALUES (19, 119, 9);
INSERT INTO `blog_article_tags` VALUES (9, 120, 2);
INSERT INTO `blog_article_tags` VALUES (10, 120, 4);
INSERT INTO `blog_article_tags` VALUES (6, 121, 4);
INSERT INTO `blog_article_tags` VALUES (5, 121, 9);
INSERT INTO `blog_article_tags` VALUES (3, 122, 2);
INSERT INTO `blog_article_tags` VALUES (4, 122, 4);
INSERT INTO `blog_article_tags` VALUES (223, 124, 4);
INSERT INTO `blog_article_tags` VALUES (222, 124, 9);
INSERT INTO `blog_article_tags` VALUES (219, 125, 3);
INSERT INTO `blog_article_tags` VALUES (218, 125, 9);
INSERT INTO `blog_article_tags` VALUES (216, 126, 8);
INSERT INTO `blog_article_tags` VALUES (217, 126, 9);
INSERT INTO `blog_article_tags` VALUES (226, 127, 8);
INSERT INTO `blog_article_tags` VALUES (227, 127, 9);
INSERT INTO `blog_article_tags` VALUES (228, 128, 8);
INSERT INTO `blog_article_tags` VALUES (229, 128, 9);
INSERT INTO `blog_article_tags` VALUES (230, 129, 8);
INSERT INTO `blog_article_tags` VALUES (231, 129, 9);
INSERT INTO `blog_article_tags` VALUES (232, 130, 8);
INSERT INTO `blog_article_tags` VALUES (233, 130, 9);
INSERT INTO `blog_article_tags` VALUES (236, 131, 8);
INSERT INTO `blog_article_tags` VALUES (237, 131, 9);
COMMIT;

-- ----------------------------
-- Table structure for blog_catalogue
-- ----------------------------
DROP TABLE IF EXISTS `blog_catalogue`;
CREATE TABLE `blog_catalogue` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(100) NOT NULL,
  `order` int NOT NULL,
  `level` int NOT NULL,
  `father` int DEFAULT NULL,
  `note_id` int DEFAULT NULL,
  `section_id` int DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `blog_catalogue_note_id_9f765cc4_fk_blog_note_id` (`note_id`),
  KEY `blog_catalogue_section_id_55c0f70a_fk_blog_section_id` (`section_id`),
  CONSTRAINT `blog_catalogue_note_id_9f765cc4_fk_blog_note_id` FOREIGN KEY (`note_id`) REFERENCES `blog_note` (`id`),
  CONSTRAINT `blog_catalogue_section_id_55c0f70a_fk_blog_section_id` FOREIGN KEY (`section_id`) REFERENCES `blog_section` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=209 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of blog_catalogue
-- ----------------------------
BEGIN;
INSERT INTO `blog_catalogue` VALUES (1, '基础知识', 1, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (2, 'kubeadm部署k8s', 2, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (3, 'k8s高可用部署', 1, 2, 1, 1, 1);
INSERT INTO `blog_catalogue` VALUES (4, '概念和术语', 2, 2, 1, 1, 2);
INSERT INTO `blog_catalogue` VALUES (5, '集群组件', 3, 2, 1, 1, 3);
INSERT INTO `blog_catalogue` VALUES (6, '抽象对象', 4, 2, 1, 1, 4);
INSERT INTO `blog_catalogue` VALUES (7, '通过阿里云获取gcr.io镜像文件', 5, 2, 1, 1, 5);
INSERT INTO `blog_catalogue` VALUES (8, '前期准备', 1, 2, 2, 1, 6);
INSERT INTO `blog_catalogue` VALUES (9, '部署kubernets', 2, 2, 2, 1, 7);
INSERT INTO `blog_catalogue` VALUES (10, '部署helm', 3, 2, 2, 1, 8);
INSERT INTO `blog_catalogue` VALUES (11, '部署ingress控制器', 4, 2, 2, 1, 9);
INSERT INTO `blog_catalogue` VALUES (12, '部署calico网络组件', 5, 2, 2, 1, 10);
INSERT INTO `blog_catalogue` VALUES (13, '部署dashboard', 6, 2, 2, 1, 11);
INSERT INTO `blog_catalogue` VALUES (14, '部署metrics-server监控组件', 7, 2, 2, 1, 12);
INSERT INTO `blog_catalogue` VALUES (15, '部署Prometheus+Grafana', 8, 2, 2, 1, 13);
INSERT INTO `blog_catalogue` VALUES (16, '部署elk日志收集', 9, 2, 2, 1, 14);
INSERT INTO `blog_catalogue` VALUES (17, '部署Harbor私有镜像仓库', 10, 2, 2, 1, 15);
INSERT INTO `blog_catalogue` VALUES (18, '命令格式', 1, 2, 25, 1, 16);
INSERT INTO `blog_catalogue` VALUES (19, 'node常用命令', 2, 2, 25, 1, 17);
INSERT INTO `blog_catalogue` VALUES (20, 'pod常用命令', 3, 2, 25, 1, 18);
INSERT INTO `blog_catalogue` VALUES (21, '控制器常用命令', 4, 2, 25, 1, 19);
INSERT INTO `blog_catalogue` VALUES (22, 'service常用命令', 6, 2, 25, 1, 20);
INSERT INTO `blog_catalogue` VALUES (23, '存储常用命令', 5, 2, 25, 1, 21);
INSERT INTO `blog_catalogue` VALUES (24, '日常命令总结', 7, 2, 25, 1, 22);
INSERT INTO `blog_catalogue` VALUES (25, 'kubectl命令', 3, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (26, '资源对象', 4, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (27, 'K8S中的资源对象', 1, 2, 26, 1, 23);
INSERT INTO `blog_catalogue` VALUES (28, 'yuml文件', 2, 2, 26, 1, 24);
INSERT INTO `blog_catalogue` VALUES (29, 'k8s yaml字段大全', 3, 2, 26, 1, 25);
INSERT INTO `blog_catalogue` VALUES (30, '管理Namespace资源', 4, 2, 26, 1, 26);
INSERT INTO `blog_catalogue` VALUES (31, '标签与标签选择器', 5, 2, 26, 1, 27);
INSERT INTO `blog_catalogue` VALUES (32, 'Pod资源对象', 6, 2, 26, 1, 28);
INSERT INTO `blog_catalogue` VALUES (33, 'Pod生命周期', 7, 2, 26, 1, 29);
INSERT INTO `blog_catalogue` VALUES (34, '资源需求与限制', 8, 2, 26, 1, 30);
INSERT INTO `blog_catalogue` VALUES (35, 'Pod服务质量（优先级）', 9, 2, 26, 1, 31);
INSERT INTO `blog_catalogue` VALUES (36, '资源控制器', 5, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (37, 'Pod控制器', 1, 2, 36, 1, 32);
INSERT INTO `blog_catalogue` VALUES (38, 'ReplicaSet控制器', 2, 2, 36, 1, 33);
INSERT INTO `blog_catalogue` VALUES (39, 'Deployment控制器', 3, 2, 36, 1, 34);
INSERT INTO `blog_catalogue` VALUES (40, 'DaemonSet控制器', 4, 2, 36, 1, 35);
INSERT INTO `blog_catalogue` VALUES (41, 'Job控制器', 5, 2, 36, 1, 36);
INSERT INTO `blog_catalogue` VALUES (42, 'CronJob控制器', 6, 2, 36, 1, 37);
INSERT INTO `blog_catalogue` VALUES (43, 'StatefulSet控制器', 7, 2, 36, 1, 38);
INSERT INTO `blog_catalogue` VALUES (44, 'PDB中断预算', 8, 2, 36, 1, 39);
INSERT INTO `blog_catalogue` VALUES (45, 'Service和Ingress', 6, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (46, 'Service资源及模型', 1, 2, 45, 1, 40);
INSERT INTO `blog_catalogue` VALUES (47, '服务发现', 2, 2, 45, 1, 41);
INSERT INTO `blog_catalogue` VALUES (48, 'Service类型', 3, 2, 45, 1, 42);
INSERT INTO `blog_catalogue` VALUES (49, 'Headless Service', 4, 2, 45, 1, 43);
INSERT INTO `blog_catalogue` VALUES (50, 'Ingress资源', 5, 2, 45, 1, 44);
INSERT INTO `blog_catalogue` VALUES (51, 'Ingress案例', 6, 2, 45, 1, 45);
INSERT INTO `blog_catalogue` VALUES (52, '存储', 7, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (53, '配置集合ConfigMap', 1, 2, 52, 1, 46);
INSERT INTO `blog_catalogue` VALUES (54, '敏感信息Secret', 2, 2, 52, 1, 47);
INSERT INTO `blog_catalogue` VALUES (55, '临时存储emptyDir', 3, 2, 52, 1, 48);
INSERT INTO `blog_catalogue` VALUES (56, '节点存储hostPath', 4, 2, 52, 1, 49);
INSERT INTO `blog_catalogue` VALUES (57, '网络存储卷', 5, 2, 52, 1, 50);
INSERT INTO `blog_catalogue` VALUES (58, '持久存储卷', 6, 2, 52, 1, 51);
INSERT INTO `blog_catalogue` VALUES (59, 'downwardAPI存储卷', 7, 2, 52, 1, 52);
INSERT INTO `blog_catalogue` VALUES (60, 'rook', 8, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (61, 'rook简介', 1, 2, 60, 1, 53);
INSERT INTO `blog_catalogue` VALUES (62, 'ceph', 2, 2, 60, 1, 54);
INSERT INTO `blog_catalogue` VALUES (63, 'rook部署', 3, 2, 60, 1, 55);
INSERT INTO `blog_catalogue` VALUES (64, 'rbd块存储服务', 4, 2, 60, 1, 56);
INSERT INTO `blog_catalogue` VALUES (65, 'cephfs共享文件存储', 5, 2, 60, 1, 57);
INSERT INTO `blog_catalogue` VALUES (66, 'RGW对象存储服务', 6, 2, 60, 1, 58);
INSERT INTO `blog_catalogue` VALUES (67, '维护rook存储', 7, 2, 60, 1, 59);
INSERT INTO `blog_catalogue` VALUES (68, '网络', 9, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (69, '网络概述', 1, 2, 68, 1, 60);
INSERT INTO `blog_catalogue` VALUES (70, '网络类型', 2, 2, 68, 1, 61);
INSERT INTO `blog_catalogue` VALUES (71, 'flannel网络插件', 3, 2, 68, 1, 62);
INSERT INTO `blog_catalogue` VALUES (72, '网络策略', 4, 2, 68, 1, 63);
INSERT INTO `blog_catalogue` VALUES (73, '网络与策略实例', 5, 2, 68, 1, 64);
INSERT INTO `blog_catalogue` VALUES (74, '安全', 10, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (75, '访问控制', 1, 2, 74, 1, 65);
INSERT INTO `blog_catalogue` VALUES (76, '认证', 2, 2, 74, 1, 66);
INSERT INTO `blog_catalogue` VALUES (77, '鉴权', 3, 2, 74, 1, 67);
INSERT INTO `blog_catalogue` VALUES (78, '准入控制', 4, 2, 74, 1, 68);
INSERT INTO `blog_catalogue` VALUES (79, '示例', 5, 2, 74, 1, 69);
INSERT INTO `blog_catalogue` VALUES (80, 'pod调度', 11, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (81, '调度器概述', 1, 2, 80, 1, 70);
INSERT INTO `blog_catalogue` VALUES (82, 'node亲和调度', 2, 2, 80, 1, 71);
INSERT INTO `blog_catalogue` VALUES (83, 'pod亲和调度', 3, 2, 80, 1, 72);
INSERT INTO `blog_catalogue` VALUES (84, '污点和容忍度', 4, 2, 80, 1, 73);
INSERT INTO `blog_catalogue` VALUES (85, '固定节点调度', 5, 2, 80, 1, 74);
INSERT INTO `blog_catalogue` VALUES (86, '系统扩展', 12, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (87, '自定义资源类型（CRD）', 1, 2, 86, 1, 75);
INSERT INTO `blog_catalogue` VALUES (88, '自定义控制器', 2, 2, 86, 1, 76);
INSERT INTO `blog_catalogue` VALUES (89, '资源指标与HPA', 13, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (90, '资源监控及资源指标', 1, 2, 89, 1, 77);
INSERT INTO `blog_catalogue` VALUES (91, '监控组件安装', 2, 2, 89, 1, 78);
INSERT INTO `blog_catalogue` VALUES (92, '资源指标及其应用', 3, 2, 89, 1, 79);
INSERT INTO `blog_catalogue` VALUES (93, '自动弹性缩放', 4, 2, 89, 1, 80);
INSERT INTO `blog_catalogue` VALUES (94, 'helm', 14, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (95, 'helm基础', 1, 2, 94, 1, 81);
INSERT INTO `blog_catalogue` VALUES (96, 'helm安装', 2, 2, 94, 1, 82);
INSERT INTO `blog_catalogue` VALUES (97, 'helm常用命令', 3, 2, 94, 1, 83);
INSERT INTO `blog_catalogue` VALUES (98, 'Helm Charts', 4, 2, 94, 1, 84);
INSERT INTO `blog_catalogue` VALUES (99, '自定义Charts', 5, 2, 94, 1, 85);
INSERT INTO `blog_catalogue` VALUES (100, 'k8s高可用部署', 15, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (101, 'kubeadm高可用部署', 1, 2, 100, 1, 86);
INSERT INTO `blog_catalogue` VALUES (102, '离线二进制部署k8s', 2, 2, 100, 1, 87);
INSERT INTO `blog_catalogue` VALUES (103, '其他高可用部署方式', 3, 2, 100, 1, 88);
INSERT INTO `blog_catalogue` VALUES (104, '日常维护', 16, 1, NULL, 1, NULL);
INSERT INTO `blog_catalogue` VALUES (105, '更改证书有效期', 1, 2, 104, 1, 89);
INSERT INTO `blog_catalogue` VALUES (106, 'k8s版本升级', 2, 2, 104, 1, 90);
INSERT INTO `blog_catalogue` VALUES (107, '添加work节点', 3, 2, 104, 1, 91);
INSERT INTO `blog_catalogue` VALUES (108, '控制节点启用pod调度', 4, 2, 104, 1, 92);
INSERT INTO `blog_catalogue` VALUES (109, '集群以外节点控制k8s集群', 5, 2, 104, 1, 93);
INSERT INTO `blog_catalogue` VALUES (110, '删除本地集群', 6, 2, 104, 1, 94);
INSERT INTO `blog_catalogue` VALUES (111, '日志排查', 7, 2, 104, 1, 95);
INSERT INTO `blog_catalogue` VALUES (112, '基础', 1, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (113, 'docker基础', 1, 2, 112, 2, 96);
INSERT INTO `blog_catalogue` VALUES (114, 'docker安装与卸载', 2, 2, 112, 2, 97);
INSERT INTO `blog_catalogue` VALUES (115, '镜像', 2, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (116, '镜像操作命令', 1, 2, 115, 2, 98);
INSERT INTO `blog_catalogue` VALUES (117, '管理镜像', 2, 2, 115, 2, 99);
INSERT INTO `blog_catalogue` VALUES (118, '构建镜像', 3, 2, 115, 2, 100);
INSERT INTO `blog_catalogue` VALUES (119, '导出和导入镜像', 4, 2, 115, 2, 101);
INSERT INTO `blog_catalogue` VALUES (120, '容器', 3, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (121, '容器操作命令总结', 1, 2, 120, 2, 102);
INSERT INTO `blog_catalogue` VALUES (122, '创建容器', 2, 2, 120, 2, 103);
INSERT INTO `blog_catalogue` VALUES (123, '停止容器', 3, 2, 120, 2, 104);
INSERT INTO `blog_catalogue` VALUES (124, '进入容器', 4, 2, 120, 2, 105);
INSERT INTO `blog_catalogue` VALUES (125, '删除容器', 5, 2, 120, 2, 106);
INSERT INTO `blog_catalogue` VALUES (126, '导入和导出容器', 6, 2, 120, 2, 107);
INSERT INTO `blog_catalogue` VALUES (127, '查看容器', 7, 2, 120, 2, 108);
INSERT INTO `blog_catalogue` VALUES (128, '其他容器命令', 8, 2, 120, 2, 109);
INSERT INTO `blog_catalogue` VALUES (129, '仓库', 4, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (130, 'docker hub公共仓库', 1, 2, 129, 2, 110);
INSERT INTO `blog_catalogue` VALUES (131, 'registry私有仓库', 2, 2, 129, 2, 111);
INSERT INTO `blog_catalogue` VALUES (132, 'Harbor私有镜像仓库', 3, 2, 129, 2, 112);
INSERT INTO `blog_catalogue` VALUES (133, 'dockerfile', 5, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (134, '基本结构', 1, 2, 133, 2, 113);
INSERT INTO `blog_catalogue` VALUES (135, '配置命令', 2, 2, 133, 2, 114);
INSERT INTO `blog_catalogue` VALUES (136, '操作命令', 3, 2, 133, 2, 115);
INSERT INTO `blog_catalogue` VALUES (137, '命令区别', 4, 2, 133, 2, 116);
INSERT INTO `blog_catalogue` VALUES (138, '构建镜像', 5, 2, 133, 2, 117);
INSERT INTO `blog_catalogue` VALUES (139, 'dockerfile编写注意事项', 6, 2, 133, 2, 118);
INSERT INTO `blog_catalogue` VALUES (140, '资源限制', 6, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (141, '内存限制', 1, 2, 140, 2, 119);
INSERT INTO `blog_catalogue` VALUES (142, 'CPU限制', 2, 2, 140, 2, 120);
INSERT INTO `blog_catalogue` VALUES (143, '限制磁盘IO', 3, 2, 140, 2, 121);
INSERT INTO `blog_catalogue` VALUES (144, '存储管理', 7, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (145, '存储管理常用命令', 1, 2, 144, 2, 122);
INSERT INTO `blog_catalogue` VALUES (146, '数据卷', 2, 2, 144, 2, 123);
INSERT INTO `blog_catalogue` VALUES (147, '数据卷容器', 3, 2, 144, 2, 124);
INSERT INTO `blog_catalogue` VALUES (148, '利用数据卷容器进行数据迁移', 4, 2, 144, 2, 125);
INSERT INTO `blog_catalogue` VALUES (149, '网络管理', 8, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (150, '端口映射实现容器访问', 1, 2, 149, 2, 126);
INSERT INTO `blog_catalogue` VALUES (151, '互联机制实现便捷互访', 2, 2, 149, 2, 127);
INSERT INTO `blog_catalogue` VALUES (152, '为镜像添加SSH服务', 3, 2, 149, 2, 128);
INSERT INTO `blog_catalogue` VALUES (153, 'docker网络管理', 4, 2, 149, 2, 129);
INSERT INTO `blog_catalogue` VALUES (154, '配置DNS和主机名', 5, 2, 149, 2, 130);
INSERT INTO `blog_catalogue` VALUES (155, '容器防火墙访问控制', 6, 2, 149, 2, 131);
INSERT INTO `blog_catalogue` VALUES (156, 'docker网络分析', 7, 2, 149, 2, 132);
INSERT INTO `blog_catalogue` VALUES (157, 'docker网络类型', 8, 2, 149, 2, 133);
INSERT INTO `blog_catalogue` VALUES (158, '自定义网络', 9, 2, 149, 2, 134);
INSERT INTO `blog_catalogue` VALUES (159, 'docker网络模型', 10, 2, 149, 2, 135);
INSERT INTO `blog_catalogue` VALUES (160, '核心技术', 9, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (161, '基本架构', 1, 2, 160, 2, 136);
INSERT INTO `blog_catalogue` VALUES (162, '命名空间', 2, 2, 160, 2, 137);
INSERT INTO `blog_catalogue` VALUES (163, '控制组', 3, 2, 160, 2, 138);
INSERT INTO `blog_catalogue` VALUES (164, '联合文件系统', 4, 2, 160, 2, 139);
INSERT INTO `blog_catalogue` VALUES (165, '网络虚拟化', 5, 2, 160, 2, 140);
INSERT INTO `blog_catalogue` VALUES (166, '安全', 10, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (167, '命名空间隔离的安全', 1, 2, 166, 2, 141);
INSERT INTO `blog_catalogue` VALUES (168, '控制组资源控制的安全', 2, 2, 166, 2, 142);
INSERT INTO `blog_catalogue` VALUES (169, '内核能力机制', 3, 2, 166, 2, 143);
INSERT INTO `blog_catalogue` VALUES (170, 'Docker服务端的防护', 4, 2, 166, 2, 144);
INSERT INTO `blog_catalogue` VALUES (171, '其他安全特性', 5, 2, 166, 2, 145);
INSERT INTO `blog_catalogue` VALUES (172, 'Docker Machine批量安装管理', 11, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (173, '安装Docker Machine', 1, 2, 172, 2, 146);
INSERT INTO `blog_catalogue` VALUES (174, '管理machine', 2, 2, 172, 2, 147);
INSERT INTO `blog_catalogue` VALUES (175, 'Docker Compose容器编排', 12, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (176, 'Docker Compose简介', 1, 2, 175, 2, 148);
INSERT INTO `blog_catalogue` VALUES (177, 'compose模板文件', 2, 2, 175, 2, 149);
INSERT INTO `blog_catalogue` VALUES (178, 'compose命令', 3, 2, 175, 2, 150);
INSERT INTO `blog_catalogue` VALUES (179, 'compose环境变量', 4, 2, 175, 2, 151);
INSERT INTO `blog_catalogue` VALUES (180, 'compose案例', 5, 2, 175, 2, 152);
INSERT INTO `blog_catalogue` VALUES (181, 'Docker Swarm集群管理', 13, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (182, 'Docker Swarm简介', 1, 2, 181, 2, 153);
INSERT INTO `blog_catalogue` VALUES (183, '部署swarm集群', 2, 2, 181, 2, 154);
INSERT INTO `blog_catalogue` VALUES (184, 'swarm高可用', 3, 2, 181, 2, 155);
INSERT INTO `blog_catalogue` VALUES (185, 'service访问', 4, 2, 181, 2, 156);
INSERT INTO `blog_catalogue` VALUES (186, 'docker其他与总结', 14, 1, NULL, 2, NULL);
INSERT INTO `blog_catalogue` VALUES (187, 'Docker图形化管理', 1, 2, 186, 2, 157);
INSERT INTO `blog_catalogue` VALUES (188, 'docker system命令', 2, 2, 186, 2, 158);
INSERT INTO `blog_catalogue` VALUES (189, 'docker镜像常见问题', 3, 2, 186, 2, 159);
INSERT INTO `blog_catalogue` VALUES (190, 'docker容器常见问题', 4, 2, 186, 2, 160);
INSERT INTO `blog_catalogue` VALUES (191, 'docker仓库常见问题', 5, 2, 186, 2, 161);
INSERT INTO `blog_catalogue` VALUES (192, 'docker配置常见问题', 6, 2, 186, 2, 162);
INSERT INTO `blog_catalogue` VALUES (193, 'docker其他问题', 7, 2, 186, 2, 163);
INSERT INTO `blog_catalogue` VALUES (194, 'docker命令总结', 8, 2, 186, 2, 164);
INSERT INTO `blog_catalogue` VALUES (195, 'prometheus基础', 1, 1, NULL, 3, NULL);
INSERT INTO `blog_catalogue` VALUES (196, 'prometheus简介', 1, 2, 195, 3, 165);
INSERT INTO `blog_catalogue` VALUES (197, '部署prometheus', 2, 2, 195, 3, 166);
INSERT INTO `blog_catalogue` VALUES (198, '数据模型', 3, 2, 195, 3, 167);
INSERT INTO `blog_catalogue` VALUES (199, '指标类型', 4, 2, 195, 3, 168);
INSERT INTO `blog_catalogue` VALUES (200, 'Jobs和Instances', 5, 2, 195, 3, 169);
INSERT INTO `blog_catalogue` VALUES (201, 'PromQL', 2, 1, NULL, 3, NULL);
INSERT INTO `blog_catalogue` VALUES (202, '查询表达式', 1, 2, 201, 3, 170);
INSERT INTO `blog_catalogue` VALUES (203, '操作符', 2, 2, 201, 3, 171);
INSERT INTO `blog_catalogue` VALUES (204, '匹配模式', 3, 2, 201, 3, 172);
INSERT INTO `blog_catalogue` VALUES (205, '聚合操作', 4, 2, 201, 3, 173);
INSERT INTO `blog_catalogue` VALUES (206, '内置函数', 5, 2, 201, 3, 174);
INSERT INTO `blog_catalogue` VALUES (207, '在HTTP API中使用PromQL', 6, 2, 201, 3, 175);
INSERT INTO `blog_catalogue` VALUES (208, '最佳实践', 7, 2, 201, 3, 176);
COMMIT;

-- ----------------------------
-- Table structure for blog_category
-- ----------------------------
DROP TABLE IF EXISTS `blog_category`;
CREATE TABLE `blog_category` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of blog_category
-- ----------------------------
BEGIN;
INSERT INTO `blog_category` VALUES (1, 'Linux');
INSERT INTO `blog_category` VALUES (2, 'MySQL');
INSERT INTO `blog_category` VALUES (3, 'Python');
INSERT INTO `blog_category` VALUES (4, 'Django');
INSERT INTO `blog_category` VALUES (5, 'docker');
INSERT INTO `blog_category` VALUES (6, 'kubernets');
INSERT INTO `blog_category` VALUES (7, 'CI/CD');
INSERT INTO `blog_category` VALUES (8, '其他');
COMMIT;

-- ----------------------------
-- Table structure for blog_note
-- ----------------------------
DROP TABLE IF EXISTS `blog_note`;
CREATE TABLE `blog_note` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(50) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of blog_note
-- ----------------------------
BEGIN;
INSERT INTO `blog_note` VALUES (1, 'Kubernetes');
INSERT INTO `blog_note` VALUES (2, 'Docker');
INSERT INTO `blog_note` VALUES (3, 'Prometheus');
COMMIT;

-- ----------------------------
-- Table structure for blog_section
-- ----------------------------
DROP TABLE IF EXISTS `blog_section`;
CREATE TABLE `blog_section` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(50) NOT NULL,
  `body` longtext NOT NULL,
  `view` int unsigned NOT NULL,
  `like` int unsigned NOT NULL,
  `comment` int unsigned NOT NULL,
  `created_time` datetime(6) NOT NULL,
  `modified_time` datetime(6) NOT NULL,
  `is_release` tinyint(1) NOT NULL,
  `author_id` int NOT NULL,
  `note_id` int NOT NULL,
  `collection` int unsigned NOT NULL,
  PRIMARY KEY (`id`),
  KEY `blog_content_author_id_34e2b09c_fk_auth_user_id` (`author_id`),
  KEY `blog_content_note_id_c87e992c_fk_blog_note_id` (`note_id`),
  CONSTRAINT `blog_content_author_id_34e2b09c_fk_auth_user_id` FOREIGN KEY (`author_id`) REFERENCES `auth_user` (`id`),
  CONSTRAINT `blog_content_note_id_c87e992c_fk_blog_note_id` FOREIGN KEY (`note_id`) REFERENCES `blog_note` (`id`),
  CONSTRAINT `blog_section_chk_1` CHECK ((`view` >= 0)),
  CONSTRAINT `blog_section_chk_2` CHECK ((`like` >= 0)),
  CONSTRAINT `blog_section_chk_3` CHECK ((`comment` >= 0)),
  CONSTRAINT `blog_section_chk_4` CHECK ((`collection` >= 0))
) ENGINE=InnoDB AUTO_INCREMENT=177 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of blog_section
-- ----------------------------
BEGIN;
INSERT INTO `blog_section` VALUES (1, 'Kubernetes特性', '\n>Kubernetes是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平台。\n用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。Kubernetes提供了接口和可组合的平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。简单总结起来，它具有以下几个重要特性。\n\n1. 自动装箱\n   建构于容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以提升资源利用率。\n1. 自我修复（自愈）\n   支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制。\n1. 水平扩展\n   支持通过简单命令或UI手动水平扩展，以及基于CPU等资源负载率的自动水平扩展机制。\n1. 服务发现和负载均衡\n   Kubernetes通过其附加组件之一的KubeDNS（或CoreDNS）为系统内置了服务发现功能，它会为每个Service配置DNS名称，并允许集群内的客户端直接使用此名称发出访问请求，而Service则通过iptables或ipvs内建了负载均衡机制。\n1. 自动发布和回滚\n   Kubernetes支持“灰度”更新应用程序或其配置信息，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作。\n1. 密钥和配置管理\n   Kubernetes的ConfigMap实现了配置数据与Docker镜像解耦，需要时，仅对配置做出变更而无须重新构建Docker镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用户名和密码、令牌、密钥等信息，Kubernetes专门提供了Secret对象为其解耦，既便利了应用的快速开发和交付，又提供了一定程度上的安全保障。\n1. 存储编排\n   Kubernetes支持Pod对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储（如AWS和GCP等），以及网络存储系统（例如，NFS、iSCSI、GlusterFS、Ceph、Cinder和Flocker等）。\n1. 批量处理执行\n   除了服务型应用，Kubernetes还支持批处理作业及CI（持续集成），如果需要，一样可以实现容器故障后恢复。', 99, 1, 0, '2020-12-08 21:38:30.784809', '2021-01-26 09:20:40.309382', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (2, '概念和术语', '[TOC]\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603895717319-4987bb90-1d1d-49ba-b97d-de8cd21211f8.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=720&originWidth=594&size=119660&status=done&style=none&width=594)\n\n\n# 一、集群组件\n\n1. Master：\n指的是集群的控制节点，每个 k8s 集群里至少需要一个 Master 节点来负责整个集群的管理和控制，所有控制命令都是发给它，它来负责具体的调度和执行。\n\n2. Node：\n是 k8s 集群中用于运行 Pod 的机器，Node 为整个集群提供可用的集群资源，比如用于保持数据、运行作业、创建网络路由等。如果某个 Node节点宕机，其上的工作负载会被 Master 自动转移到其它节点上去。\n\n# 二、资源抽象\n\n1. 容器组（Pod）:\nKubernetes中最小的资源单位。由位于同一节点上若干容器组成，彼此共享网络命名空间和存储卷（Volume）。Pod是Kubernetes中进行管理的最小资源单位，是最为基础的概念。跟容器类似，Pod是短暂的，随时可变的，通常不带有状态。一般每个Pod中除了应用容器外，还包括一个初始的pause容器，完成网络和存储空间的初始化；\n\n2. 服务（Service）：\n对外提供某个特定功能的一组Pod（可通过标签来选择）和所关联的访问配置。由于Pod的地址是不同的，而且可能改变，直接访问Pod将无法获得稳定的业务。Kubernetes通过服务提供唯一固定的访问地址（如IP地址或者域名），不随后面Pod改变而变化。用户无须关心具体的Pod信息；\n\n3. 存储卷（Volume）：\n存储卷类似Docker中的概念，提供数据的持久化存储（如Pod重启后），并支持更高级的生命周期管理和参数指定功能，支持多种本地和云存储类型；\n\n4. 命名空间（Namespace）:\nKubernetes通过命名空间来实现虚拟化，将同一组物理资源虚拟为不同的抽象集群，避免不同租户的资源发生命名冲突，另外可以进行资源限额。\n\n# 三、控制器抽象\n\n1. 副本集（ReplicaSet）：\n基于Pod的抽象。使用它可以让集群中始终维持某个Pod的指定副本数的健康实例。副本集中的Pod相互并无差异，可以彼此替换。\n\n2. 部署（Deployment）：\n管理Pod或副本集，并且支持升级操作。部署控制器可以提供提供比副本集更方便的操作，推荐使用；\n\n3. 状态集（StatefulSet）：\n管理带有状态的应用。相比部署控制器，状态集可以为Pod分配独一无二的身份，确保在重新调配等操作时也不会相互替换。自1.9版本开始正式支持；\n\n4. Daemon集（DaemonSet）：\n确保节点上肯定运行某个Pod，一般用来采集日志（如logstash）、监控节点（如collectd）或提供存储（如glusterd）使用；\n\n5. 任务（Job）：\n适用于短期处理场景。任务将创建若干Pod，并确保给定数目的Pod最终正常退出（完成指定的处理）；\n\n6. 横向Pod扩展器（Horizontal Pod Autoscaler, HPA）：\n类似云里面的自动扩展组，根据Pod的使用率（典型如CPU）自动调整一个部署里面Pod的个数，保障服务可用性；\n\n7. 入口控制器（Ingress Controller）：\n定义外部访问集群中资源的一组规则，用来提供七层代理和负载均衡服务。\n\n# 四、辅助概念\n\n1. 标签（Label）：\n键值对，可以标记到资源对象上，用来对资源进行分类和筛选；\n\n2. 选择器（Selector）：\n基于标签概念的一个正则表达式，可通过标签来筛选出一组资源；\n\n3. 注解（Annotation）：\n键值对，可以存放大量任意数据，一般用来添加对资源对象的细说明，可供其他工具处理。\n\n4. 秘密数据（Secret）：\n存放敏感数据，例如用户认证的口令等；\n\n5. 名字（Name）：\n用户提供给资源的别名，同类资源不能重名；\n\n6. 持久化存储（PersistentVolume）：\n确保数据不会丢失；\n\n7. 资源限额（Resource Quotas）：\n用来限制某个命名空间下对资源的使用，开始逐渐提供多租户支持；\n\n8. 安全上下文（Security Context）：\n应用到容器上的系统安全配置，包括uid、gid、capabilities、SELinux角色等；\n\n9. 服务账号（Service Accounts）：\n操作资源的用户账号。', 53, 1, 0, '2020-12-08 21:41:29.628084', '2021-01-26 09:21:06.655433', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (3, '集群组件', '[TOC]\n\n# 一、结构拓扑\n\n![12.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603896164490-77596429-04f6-4278-8fad-8dbf9c1ce799.png#align=left&display=inline&height=573&margin=%5Bobject%20Object%5D&name=12.png&originHeight=573&originWidth=793&size=16777&status=done&style=none&width=793)\n\n1. 一个典型的Kubernetes集群由多个工作节点（worker node）和一个集群控制平面（control plane，即Master），以及一个集群状态存储系统（etcd）组成。其中Master节点负责整个集群的管理工作，为集群提供管理接口，并监控和编排集群中的各个工作节点。\n\n1. Master节点主要由apiserver、controller-manager和scheduler三个组件，以及一个用于集群状态存储的etcd存储服务组成，而每个Node节点则主要包含kubelet、kube-proxy及容器引擎（Docker是最为常用的实现）等组件。此外，完整的集群服务还依赖于一些附加组件，如KubeDNS等。\n\n# 二、Master组件\n\n> Kubernetes的集群控制平面由多个组件组成，这些组件可统一运行于单一Master节点，也可以以多副本的方式同时运行于多个节点，以为Master提供高可用功能，甚至还可以运行于Kubernetes集群自身之上。Master主要包含以下几个组件。\n\n1. API服务器（API Server）\nAPI Server负责输出RESTful风格的Kubernetes API，它是发往集群的所有REST操作命令的接入点，并负责接收、校验并响应所有的REST请求，结果状态被持久存储于etcd中。因此，API Server是整个集群的网关。\n\n2. 控制器管理器（Controller Manager）\nController-Manager Serve用于执行大部分的集群层次的功能，它既执行生命周期功能(例如：命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集、node垃圾收集)，也执行API业务逻辑（例如：pod的弹性扩容）。控制管理提供自愈能力、扩容、应用生命周期管理、服务发现、路由、服务绑定和提供。\n\n3. 调度器（Scheduler）\nKubernetes是用于部署和管理大规模容器应用的平台，根据集群规模的不同，其托管运行的容器很可能会数以千计甚至更多。API Server确认Pod对象的创建请求之后，便需要由Scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策。另外，Kubernetes还支持用户自定义调度器。\n\n# 三、集群状态存储（ETCD）\n\n1. Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中，etcd是由CoreOS基于Raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障（如数据库主节点选择、分布式锁等）。\n1. etcd是独立的服务组件，并不隶属于Kubernetes集群自身。生产环境中应该以etcd集群的方式运行以确保其服务可用性。\n1. etcd不仅能够提供键值数据存储，而且还为其提供了监听（watch）机制，用于监听和推送变更。Kubernetes集群系统中，etcd中的键值发生变化时会通知到API Server，并由其通过watch API向客户端输出。基于watch机制，Kubernetes集群的各组件实现了高效协同。\n\n# 四、Node组件\n\n> Node负责提供运行容器的各种依赖环境，并接受Master的管理。每个Node主要由以下几个组件构成。\n\n1. Node的核心代理程序kubelet\nkubelet是运行于工作节点之上的守护进程，它从API Server接收关于Pod对象的配置信息并确保它们处于期望的状态（desired state，后文不加区别地称之为“目标状态”）。kubelet会在API Server上注册当前工作节点，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点的资源占用状况。\n每个Node都要提供一个容器运行时（Container Runtime）环境，它负责下载镜像并运行容器。kubelet并未固定链接至某容器运行时环境，而是以插件的方式载入配置的容器环境。这种方式清晰地定义了各组件的边界。目前，Kubernetes支持的容器运行环境至少包括Docker、RKT、cri-o和Fraki等。\n\n2. kube-proxy\n每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod对象。\n\n# 五、核心附件\n\n> Kubernetes集群还依赖于一组称为“附件”（add-ons）的组件以提供完整的功能，它们通常是由第三方提供的特定应用程序，且托管运行于Kubernetes集群之上。\n\n1. KubeDNS：\n在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他Pod可使用此DNS服务解决主机名。Kubernetes自1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kube-dns项目，而SkyDNS则是更早一代的项目。\n\n2. Kubernetes Dashboard\nKubernetes集群的全部功能都要基于Web的UI，来管理集群中的应用甚至是集群自身。\n\n3. Ingress Controller\nService是一种工作于传统层的负载均衡器，而Ingress是在应用层实现的HTTP（s）负载均衡机制。不过，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器（Ingress Controller）发挥作用。目前，此类的可用项目有Nginx、Traefik、Envoy及HAProxy等。', 39, 1, 0, '2020-12-08 21:44:31.145070', '2021-01-26 09:22:41.675560', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (4, '抽象对象', '[TOC]\n\n> Kubernetes对集群中的资源进行了不同级别的抽象，每个资源都是一个REST对象，通过API进行操作，通过json或yaml格式的模板文件进行定义。\n\n# 一、抽象资源对象\n\n## 1.  容器组（Pod）\n\n在Kubernetes中，并不直接操作容器，最小的管理单位是容器组（Pod）。容器组由一个或多个容器组成，Kubernetes围绕容器组进行创建、调度、停止等生命周期管理。\n同一个容器组中，各个容器共享命名空间（包括网络、IPC、文件系统等容器支持的命名空间）、cgroups限制和存储卷。这意味着同一个容器组中，各个应用可以很方便地相互进行访问，比如通过localhost地址进行网络访问，通过信号量和共享内存进行进程间通信等，类似经典场景中运行在同一个操作系统中的一组进程。可以简单地将一个Pod当作是一个抽象的“虚拟机”，里面运行若干个不同的进程（每个进程实际上就是一个容器）。\n实现上，是先创建一个gcr.io/google_containers/pause容器，创建相关命名空间，然后创建Pod中的其他应用容器，并共享pause容器的命名空间。\n组成容器组的若干容器往往是存在共同的应用目的，彼此关联十分紧密，例如一个Web应用与对应的日志采集应用、状态监控应用。如果单纯把这些相关的应用放一个容器里面，又会造成过度耦合，管理、升级都不方便。\n容器组既保持了容器轻量解耦的特性，又提供了调度操作的便利性，在实践中提供了比单个容器更为灵活和更有意义的抽象。\n\n## 2. 服务（Service）\n\n服务（Service）的提出，主要是要解决Pod地址可变的问题。由于Pod随时可能发生故障，并可能在其他节点上被重启，它的地址是不能保持固定的。因此，用一个服务来代表提供某一类功能（可以通过标签来筛选）的一些Pod，并分配不随Pod位置变化而改变的虚拟访问地址（Cluster IP）。\n典型情况是，比如网站的后端服务，可能有多个Pod都运行了后端处理程序，它们可以组成一个服务。前端只需通过服务的唯一虚拟地址来访问即可，而无须关心具体是访问到了哪个Pod。可见，服务跟负载均衡器实现的功能很相似。\n根据访问方式的不同，服务可以分为如下几种类型：\n\n- ClusterIP：提供一个集群内部的地址，该地址只能在集群内解析和访问。ClusterIP是默认的服务类型；\n- NodePort：在每个集群节点上映射服务到一个静态的本地端口（默认范围为30000～32767）。从集群外部可以直接访问，并自动路由到内部自动创建的ClusterIP；\n- LoadBalancer：使用外部的路由服务，自动路由访问到自动创建的NodePort和ClusterIP；\n- ExternalName：将服务映射到externalName域指定的地址，需要1.7以上版本kube-dns的支持。\n\n组成一个服务的Pod可能是属于不同复制控制器的，但服务自身是不知道复制控制器的存在的。\n\n## 3. 存储卷（Volume）\n\n即容器挂载的数据卷，跟Pod有一致的生命周期，Pod生存过程（包括重启）中，数据卷跟着存在；Pod退出，则数据卷跟着退出。\n几个比较常见的数据卷类型包括：emptyDir、hostPath、gcePersistentDisk、awsElastic-BlockStore、nfs、gitRepo、secret。\n\n- emptyDir：当Pod创建的时候，在节点上创建一个空的挂载目录，挂载到容器内。当Pod从节点离开（例如删除掉）的时候，自动删除挂载目录内数据。节点上的挂载位置可以为物理硬盘或内存。这一类的挂载适用于非持久化的存储，例如与Pod任务相关的临时数据等。除此之外，其他存储格式大都是持久化的；\n- hostPath：将节点上某已经存在的目录挂载到Pod中，Pod退出后，节点上的数据将保留；\n- gcePersistentDisk：使用GCE的Persistent Disk服务，Pod退出后，会保留数据；\n- awsElasticBlockStore：使用AWS的EBS Volume服务，数据也会持久化保留；\n- nfs：使用NFS协议的网络存储，也是持久化数据存储；\n- gitRepo：挂载一个空目录到Pod，然后clone指定的git仓库代码到里面，适用于直接从仓库中给定版本的代码来部署应用；\n- secret：用来传递敏感信息（如密码等），基于内存的tmpfs，挂载临时秘密文件。\n- 持久化的存储以插件的形式提供为PersistentVolume资源，用户通过请求某个类型的PersistentVolumeClaim资源，来从匹配的持久化存储卷上获取绑定的存储。\n\n# 二、控制器抽象对象\n\n> 控制器抽象对象是基于对所操控对象的进一步抽象，附加了各种资源的管理功能，目前主要包括副本集、部署、状态集、Daemon集、任务等。\n\n## 1. 副本集（ReplicaSet）和部署（Deployment）\n\n在Kubernetes看来，Pod资源是可能随时发生故障的，并不需要保证Pod的运行，而是在发生失败后重新生成。Kubernetes通过复制控制器来实现这一功能。\n用户申请容器组后，复制控制器将负责调度容器组到某个节点上，并保证它的给定份数（replica）正常运行。当实际运行Pod份数超过数目，则终止某些Pod；当不足，则创建新的Pod。一般建议，即使Pod份数是1，也要使用复制控制器来创建，而不是直接创建Pod。\n可以将副本集类比为进程的监管者（supervisor）的角色，只不过它不光能保持Pod的持续运行，还能保持集群中给定类型Pod同时运行的个数为指定值。Pod是临时性的，可以随时由副本集创建或者销毁，这意味着要通过Pod自身的地址访问应用是不能保证一致性的。Kubernetes通过服务的概念来解决这个问题。\n从1.2.0版本开始，Kubernetes将正式引入部署机制来支持更灵活的Pod管理，从而用户无须直接跟复制控制器打交道了。部署代表用户对集群中应用的一次更新操作，在副本集的基础上还支持更新操作。每次滚动升级（rolling-update），会自动将副本集中旧版本的Pod逐渐替换为新的版本。\n另外，副本集也可以支持成为“横向Pod扩展器”的操作对象。\n\n## 2. 状态集（StatefulSet）\n\n通常情况下，使用容器的应用都是不带状态的，意味着部署同一个应用的多个Pod彼此可以替换，而且生命周期可以是很短暂的。任何一个Pod退出后，Kubernetes在集群中可以自动创建一个并按照调度策略调度到节点上。无状态的应用时候，关心的主要是副本的个数，而不关心名称、位置等。与此对应，某些应用需要关心Pod的状态（包括各种数据库和配置服务等），挂载独立的存储。一旦当某个Pod故障退出后，Kubernetes会创建同一命名的Pod，并挂载原来的存储，以便Pod中应用继续执行，实现了该应用的高可用性。\n状态集正是针对这种需求而设计的，提供比副本集和部署更稳定可靠的运行支持。\n\n## 3. Daemon集（DaemonSet）\n\nDaemon集适合于长期运行在后台的伺服类型应用，例如对节点的日志采集或状态监控等后台支持服务。\nDaemon集的应用会确保在指定类型的每个节点上都运行一个该应用的Pod。可能是集群中所有节点，也可能是指定标签的一类节点。\n\n## 4. 任务（Job）\n\n不同于长期运行的应用，任务（Job）代表批处理类型的应用。任务中应用完成某一类的处理即可退出，有头有尾。例如，计算Pi到多少位，可以指定若干个Pod成功完成计算，即算任务成功执行。\n\n## 5. 横向Pod扩展器（Horizontal Pod Autoscaler, HPA）\n\n横向Pod扩展器（Horizontal Pod Autoscaler, HPA）解决应用波动的情况。类似云里面的自动扩展组，扩展器根据Pod的使用率（典型如CPU、内存等）自动调整一个部署里面Pod的个数，保障服务在不同压力情况下保证平滑的输出效果。\n控制管理器会定期检查性能指标，在满足条件时候触发横向伸缩。Kubernetes 1.6版本开始支持基于多个指标的伸缩。\n\n# 三、其他抽象对象\n\n## 1. 标签（Label）\n\n标签（Label）是一组键值对，用来标记所绑定对象（典型的就是Pod）的识别属性，进而可以分类。比如name=apache|nginx、type=web|db、release=alpha|beta|stable、tier=frontend|backend等。另外，Label键支持通过/来添加前缀，可以用来标注资源的组织名称等。一般的，前缀不能超过253个字符，键名不能超过63个字符。\n标签所定义的属性是不唯一的，这意味着不同资源可能带有相同的标签键值对。这些属性可以将业务的相关信息绑定到对象上，用来对资源对象进行分类和选择过滤。\n\n## 2. 注解（Annotation）\n\n注解（Annotation）跟标签很相似，也是键值对，但并非用来标识对象，同时可以存储更多更复杂的信息。不同的是，注解并不是为了分类资源对象，而是为了给对象增加更丰富的描述信息。这些信息是任意的，数据量可以很大，可以包括各种结构化、非结构化的数据。\n常见的注解包括时间戳、发行信息、开发者信息等，一般是为了方便用户查找相关线索。\n\n## 3. 选择器（Selector）\n\n基于资源对象上绑定的标签信息，选择器（Selector）可以通过指定标签键值对来过滤出一组特定的资源对象。\n选择器支持的语法包括基于等式（Equality-based）的，和基于集合（Set-based）的。\n基于等式的选择，即通过指定某个标签是否等于某个值，例如env=production或者tier! =frontend等。多个等式可以通过AND逻辑组合在一起。\n基于集合的选择，即通过指定某个标签的值是否在某个集合内，例如env in (staging, production)。\n\n## 4. 秘密数据（Secret）\n\n秘密数据（Secret）资源用来保存一些敏感的数据，这些数据（例如密码）往往不希望别的用户看到，但是在启动某个资源（例如Pod）的时候需要提供。通过把敏感数据放到Secret里面，用户只需要提供Secret的别名即可使用。\n在对应容器（secret-test-pod.test-container）内，通过环境变量$SECRET_USERNAME和$SECRET_PASSWORD即可获取到原始的用户名和密码信息。\n此外，还可以采用数据卷的方式把秘密数据的值以文件形式放到容器内。通常，秘密数据不要超过1 MB。\n在整个过程中，只有秘密数据的所有人和最终运行的容器（准确的说，需要是同一个服务账号下面的）能获取原始敏感数据，只接触到Pod定义模板的人是无法获取到的。\n\n## 5. UID和名字\n\nKubernetes用UID和名字（Name）来标识对象。其中，UID是全局唯一的，并且不能复用；而名字则仅仅要求对某种类型的资源（在同一个命名空间内）内是唯一的，并且当某个资源移除后，其名字可以被新的资源复用。\n这意味着，可以创建一个Pod对象，命名为test，同样可以创建一个复制控制器，命名也为test。一般的，名字字符串的长度不要超过253个字符。\n\n## 6. 命名空间\n\n命名空间（Namespace）用来隔离不同用户的资源，类似租户或项目的概念。默认情况下，相同命名空间中的对象将具有相同的访问控制策略。\n同一个命名空间内，资源不允许重名，但不同命名空间之间，允许存在重名。用户在创建资源的时候可以通过--namespace=<some_namespace>来指定所属的命名空间。\nKubernetes集群启动后，会保留两个特殊的命名空间：\n\n- default：资源未指定命名空间情况下，默认属于该空间；\n- kube-system：由Kubernetes系统自身创建的资源。\n\n另外，大部分资源都属于某个命名空间，但部分特殊资源，如节点、持久存储等不属于任何命名空间。\n\n## 7. 污点和容忍\n\n污点（Taint）和容忍（Toleration）用于辅助管理Pod到工作节点的调度过程。具有某个污点的工作节点，在不容忍的Pod看来，要尽量避免调度到它上面去。\n通常情况下，可以为一个工作节点注明若干污点，只有对这些污点容忍的Pod，才可以被调度到这些具有污点的节点上。', 25, 1, 0, '2020-12-08 21:45:41.187703', '2021-01-26 09:24:46.150892', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (5, '通过阿里云获取gcr.io镜像文件', '[TOC]\n\n# 一、常用国内加速地址\n\n## 1. 使用方法\n\n• 原镜像下载操作\n` docker pull quay.io/deis/go-dev:v1.10.0 ` \n• 修改国内地址操作\n`docker pull quay.azk8s.cn/deis/go-dev:v1.10.0` \n• 修改镜像标签\n`docker tag quay.azk8s.cn/deis/go-dev:v1.10.0 quay.io/deis/go-dev:v1.10.0` \n\n## 2. 加速地址\n\ngcr.azk8s.cn/google_containers/:\ngcr.mirrors.ustc.edu.cn/xxx/yyy:zzz\ndockerhub.azk8s.cn/xxx/yyy:zzz\n\n# 二、使用阿里镜像服务加速\n\n1. github创建仓库，新建Dockerfile文件，内容为FROM 镜像名称\n![1.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897552599-323da0d8-66e1-40f1-bb04-88d72783b405.png#align=left&display=inline&height=366&margin=%5Bobject%20Object%5D&name=1.png&originHeight=366&originWidth=1045&size=32133&status=done&style=none&width=1045)\n\n2. 登陆阿里云的容器镜像服务然后点击创建镜像仓库\n![2.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897572140-b7e88118-70bb-4b1b-bea6-c35e93e84ba4.png#align=left&display=inline&height=695&margin=%5Bobject%20Object%5D&name=2.png&originHeight=695&originWidth=777&size=25057&status=done&style=none&width=777)\n\n3. 代码源绑定github账号\n![3.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897600107-67ccca44-50f6-4f47-bcf6-06635625c716.png#align=left&display=inline&height=377&margin=%5Bobject%20Object%5D&name=3.png&originHeight=377&originWidth=1260&size=49582&status=done&style=none&width=1260)\n\n4. 构建——添加规则\n![4.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897615621-1a52d45d-fc1c-4274-b8de-baadff193d97.png#align=left&display=inline&height=466&margin=%5Bobject%20Object%5D&name=4.png&originHeight=466&originWidth=598&size=15270&status=done&style=none&width=598)\n\n5. 根据提示使用镜像\n![5.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603897638784-ba27762a-6b42-4655-8dd9-374b1e977272.png#align=left&display=inline&height=310&margin=%5Bobject%20Object%5D&name=5.png&originHeight=310&originWidth=768&size=14613&status=done&style=none&width=768)\n\n6. pull镜像，修改tag\n   `docker tag registry.cn-hangzhou.aliyuncs.com/cuiliang_images/cuiliang_images:1 k8s.gcr.io/metrics-server-amd64:v0.3.6` \n6. 导出镜像\n   `docker save -o metrics-server.tar k8s.gcr.io/metrics-server-amd64:v0.3.6` \n6. 导入镜像\n   `docker load -i metrics-server.tar` \n\n', 29, 2, 0, '2020-12-08 21:47:37.353055', '2021-01-26 09:39:56.485867', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (6, '前期准备', '[TOC]\n\n# 一、概述\n\n## 1. kubernetes组件架构图\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603980733741-61d38ca3-b46f-47df-b5fc-e7933d41abb7.png#align=left&display=inline&height=705&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=705&originWidth=931&size=353797&status=done&style=none&width=931)\n\n## 2. 节点信息\n\n| 主机名 | IP             | 角色      | 组件                                          | 配置 |\n| ------ | -------------- | --------- | --------------------------------------------- | ---- |\n| master | 192.168.10.100 | 管理节点  | kube-apiserver kube-controller-manager docker | 2C2G |\n| node1  | 192.168.10.101 | 计算节点1 | kube-proxy kube-flannel docker                | 2C2G |\n| node2  | 192.168.10.102 | 计算节点2 | kube-porxy kube-flannel docker                | 2C2G |\n| harbor | 192.168.10.103 | 私有仓库  | harbor docker                                 | 2C2G |\n\n## 3. kubernetes与docker版本\n\n- kubernets1.18支持最新docker版本为19.03.8\n\n## 4. 整体步骤\n\n- 基础环境配置\n- kubernetes安装前设置(源、镜像及相关配置)\n- kubeadm部署(master)\n- 启用基于flannel的Pod网络\n- kubeadm加入node节点\n- dashboard组件安装与使用\n- heapster监控组件安装与使用\n- 访问测试\n\n1. 官方参考文档\n   [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n\n# 二、基础环境配置（所有节点）\n\n\n## 1. 修改主机名与hosts文件\n\n`vim /etc/hosts ` \n![0e6e3f33849a339ede77a5c556f154cb.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981603961-6ca446b6-ffab-42c1-b8ff-0cce9be381a7.png#align=left&display=inline&height=78&margin=%5Bobject%20Object%5D&name=0e6e3f33849a339ede77a5c556f154cb.png&originHeight=78&originWidth=295&size=3474&status=done&style=none&width=295)\n\n## 2. 验证mac地址uuid，保证各节点mac和uuid唯一\n\n`cat /sys/class/net/ens33/address` \n`cat /sys/class/dmi/id/product_uuid` \n\n## 3. 安装依赖包\n\n`yum -y install conntrack chrony bash-completion ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git` \n\n## 4. 时间同步\n\n- master节点设置\n\n\n`[root@master ~]#yum -y install chrony` \n`[root@master ~]#vim /etc/chrony.conf` \n![ef248b652982ded900b7f5f94aafdcaf.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981889979-0ed27d84-b3b6-42a3-b81a-7f9f4056a69f.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=ef248b652982ded900b7f5f94aafdcaf.png&originHeight=144&originWidth=774&size=32894&status=done&style=none&width=774)\n![482275c6cc13a702571c9e5807fed028.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603981870828-323a2ec7-37b5-49f5-bb5a-eb18768f450d.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=482275c6cc13a702571c9e5807fed028.png&originHeight=81&originWidth=585&size=5695&status=done&style=none&width=585)  \n`[root@master ~]#systemctl start chronyd ` \n    `[root@master  ~]#systemctl enable chronyd ` \n    `[root@master  ~]#timedatectl set-timezone Asia/Shanghai`  \n    `[root@master  ~]#chronyc sources ` \n![0a8ef4abc1a05c8fbc2e997c1174a028.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982167245-efe52879-5c33-43cc-873f-abf85841f396.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=0a8ef4abc1a05c8fbc2e997c1174a028.png&originHeight=129&originWidth=960&size=9122&status=done&style=none&width=960)\n\n- node节点配置\n  `[root@node1  ~]# yum -y install chrony ` \n  `[root@node1  ~]# vim /etc/chrony.conf ` \n  ![5dd88d85cbae94d1aef235d0396d22da.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982277805-b1538afa-3289-40cd-9481-b5341dd5cbc6.png#align=left&display=inline&height=127&margin=%5Bobject%20Object%5D&name=5dd88d85cbae94d1aef235d0396d22da.png&originHeight=127&originWidth=498&size=9109&status=done&style=none&width=498)\n  `[root@node1  ~]# systemctl start chronyd ` \n  `[root@node1  ~]# systemctl enable chronyd ` \n  `[root@node1  ~]# chronyc sources ` \n\n![3e9e432bcf81b01ef223a64c83d6e012.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982328125-f607aaf1-2a57-4c8f-b568-fcb293afb5d9.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=3e9e432bcf81b01ef223a64c83d6e012.png&originHeight=125&originWidth=961&size=8831&status=done&style=none&width=961)\n\n\n## 5. 设置防火墙规则\n\n`[root@master  ~]# systemctl stop firewalld ` \n`[root@master  ~]# systemctl disable firewalld ` \n`[root@master  ~]# yum -y install iptables-services ` \n`[root@master  ~]# systemctl start iptables ` \n`[root@master  ~]# systemctl enable iptables ` \n`[root@master  ~]# iptables -F ` \n`[root@master  ~]# service iptables save ` \n\n## 6. 关闭selinux\n\n`[root@master  ~]# setenforce 0 ` \n`[root@master  ~]# sed -i \'s/^SELINUX=.*/SELINUX=disabled/\' /etc/selinux/config` \n\n## 7. 关闭swap分区\n\n`[root@master  ~]# swapoff -a ` \n`[root@master  ~]# sed -i \'/ swap / s/^\\(.*\\)$/#\\1/g\' /etc/fstab ` \n\n## 8. 修改内核iptables相关参数,启用iptables查看桥接流量\n\n```bash\n[root@master  ~]# cat <<EOF > /etc/sysctl.d/kubernetes.conf `\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n```\n\n`[root@master  ~]# sysctl -p /etc/sysctl.d/kubernetes.conf ` \n\n- centos8会有如下报错\n\n![8484ea61fb135eecdd54290e47345121.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603982683093-f204be5e-65d7-40d2-a0cf-5a0374a655ec.png#align=left&display=inline&height=126&margin=%5Bobject%20Object%5D&name=8484ea61fb135eecdd54290e47345121.png&originHeight=126&originWidth=1027&size=21806&status=done&style=none&width=1027)\n\n- 临时解决，重启失效\n  modprobe br_netfilter\n- 开机加载上面这个模块\n\n```bash\ncat > /etc/rc.sysinit << EOF\n#!/bin/bash\nfor file in /etc/sysconfig/modules/*.modules ; do\n[ -x $file ] && $file\ndone\nEOF\ncat > /etc/sysconfig/modules/br_netfilter.modules << EOF\nmodprobe br_netfilter\nEOF\n```\n\n`[root@master  ~]#chmod 755 /etc/sysconfig/modules/br_netfilter.modules` \n`[root@master  ~]#lsmod |grep br_netfilter` \n\n\n## 9. 升级内核（可选）\n\n\n- 载入公钥\n  `[root@master  ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org` \n- 升级安装ELRepo\n\n`[root@master  ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm` \n\n- centos8使用如下命令\n  `[root@master  ~]#yum install https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm` \n- 载入elrepo-kernel元数据\n  `[root@master  ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist` \n- 安装最新版本的kernel\n  `[root@master  ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y` \n- 删除旧版本工具包\n  `[root@master  ~]# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y` \n- 安装新版本工具包\n  `[root@master  ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y` \n- 查看内核插入顺序\n  `[root@server-1  ~]# awk -F \\\' \'$1==\"menuentry \" {print i++ \" : \" $2}\'  /etc/grub2.cfg` \n\n\n![aea20dffb5b646d6a0ccc29759e28e2b.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983161220-53c8857e-2143-4c1e-ba67-15a1cd92d97f.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=aea20dffb5b646d6a0ccc29759e28e2b.png&originHeight=249&originWidth=987&size=25959&status=done&style=none&width=987)\n\n- 设置默认启动\n  `[root@server-1  ~]# grub2-set-default 0 // 0代表当前第一行，也就是5.3版本 ` \n  `[root@server-1  ~]# grub2-editenv list ` \n- 重启验证\n\n\n\n# 二、kubernetes安装前设置（每个节点）\n\n\n## 1. kube-proxy开启ipvs的前置条件（每个节点执行）\n\n`[root@node1  ~]# yum -y install ipset ipvsadm` \n\n```bash\n[root@node1  ~]# cat > /etc/sysconfig/modules/ipvs.modules <<EOF \n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n```\n\n`[root@node2  ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash ` \n`[root@node2  ~]# /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack` \n![d307861136126af61481ba884f655357.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983371668-ff0e07ae-19b2-4653-a724-177b625a7874.png#align=left&display=inline&height=322&margin=%5Bobject%20Object%5D&name=d307861136126af61481ba884f655357.png&originHeight=322&originWidth=1109&size=34102&status=done&style=none&width=1109)\n\n- linux kernel 4.19版本已经将nf_conntrack_ipv4 更新为 nf_conntrack\n\n## 2. docker安装（所有节点）\n\n- 安装前源准备\n  `yum install -y yum-utils device-mapper-persistent-data lvm2` \n- 配置yum源\n  `yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo` \n- 查看可安装的docker版本\n  `yum list docker-ce --showduplicates | sort -r` \n- 安装19.03.8版本docker\n  `yum install -y docker-ce-19.03.8*` \n- 使用阿里云做镜像加速\n\n```bash\nmkdir -p /etc/docker\ntee /etc/docker/daemon.json <<-\'EOF\'\n{\n\"registry-mirrors\": [\"https://o2j0mc5x.mirror.aliyuncs.com\"]\n}\nEOF\n```\n\n`systemctl daemon-reload` \n\n- 启动docker\n  `systemctl start docker` \n  `systemctl enable docker` \n\n## 3. docker 1.13以上版本默认禁用iptables的forward调用链，因此需要执行开启命令：\n\n`iptables -P FORWARD ACCEPT` \n\n## 4. 修改docker cgroup driver为systemd\n\n- 使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定\n\n```bash\n[root@master  ~]#vim /etc/docker/daemon.json \n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n```\n\n`systemctl daemon-reload` \n       `systemctl restart docker` \n       `docker info | grep Cgroup` \n![c752b0f068f75826340b6d11aea08b14.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603983714681-9e9973a4-79e0-4449-bb18-0617507ba2b8.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=c752b0f068f75826340b6d11aea08b14.png&originHeight=49&originWidth=513&size=4262&status=done&style=none&width=513)\n\n## 5. 配置阿里云yum源\n\n```bash\n[root@master  ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo \n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttps://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n## 6. 安装kubeadm、kubectl、kubelet\n\n`[root@master  ~]# yum -y install kubeadm kubectl kubelet` \n`[root@master  ~]# systemctl enable kubelet ` ', 45, 0, 0, '2020-12-08 21:50:03.540996', '2021-01-15 11:04:33.924801', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (7, '部署kubernets', '[TOC]\n# 一、kubeadm部署(master)\n\n## 1. 执行安装命令\n\n`[root@master  ~]#kubeadm init --apiserver-advertise-address=192.168.10.100 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.16.3 --pod-network-cidr=10.244.0.0/16` \n\n- --apiserver-advertise-address\n  指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的interface。\n- --pod-network-cidr\n  指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对--pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用flannel 网络方案，必须设置成这个 CIDR。\n- --image-repository\n  Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问gcr.io，在1.13版本中我们可以增加–image-repository参数，默认值是k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。\n- --kubernetes-version=v1.16.3\n  关闭版本探测，因为它的默认值是stable-1，会导致从[https://dl.k8s.io/release/stable-1.txt](https://dl.k8s.io/release/stable-1.txt)下载最新的版本号\n\n![a5fb6787d8acdc6284242fd1c2f58b9b.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984052634-f103d264-1a32-4447-aec5-79d8c0a7501b.png#align=left&display=inline&height=430&margin=%5Bobject%20Object%5D&name=a5fb6787d8acdc6284242fd1c2f58b9b.png&originHeight=430&originWidth=1040&size=32994&status=done&style=none&width=1040)\n\n## 2. 根据提示初始化kubectl\n\n`[root@master  ~]# mkdir -p $HOME/.kube ` \n`[root@master  ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config ` \n`[root@master  ~]# chown $(id -u):$(id -g) $HOME/.kube/config ` \n\n## 3. 启用 kubectl 命令自动补全功能（注销重新登录生效）\n\n`[root@master  ~]# yum -y install bash-completion ` \n`[root@master  ~]# echo \"source <(kubectl completion bash)\" >> ~/.bashrc` \n\n## 4. 测试kubectl\n\n![b344bf365bb90a67a8703581c77f9f8f.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984121864-bd375b49-f0ba-40c1-b390-88124f8d2226.png#align=left&display=inline&height=79&margin=%5Bobject%20Object%5D&name=b344bf365bb90a67a8703581c77f9f8f.png&originHeight=79&originWidth=516&size=5373&status=done&style=none&width=516)\n\n# 二、启用基于flannel的Pod网络\n\n\n## 1. 下载配置文件\n\n`[root@master  ~]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml` \n\n## 2. 启用flannel\n\n`[root@master  ~]# kubectl apply -f kube-flannel.yml ` \n\n## 3. 验证操作\n\n`[root@master  ~]# kubectl get pods --all-namespaces ` \n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984276434-4fdf16c7-fea2-4025-b899-c4685fa63b59.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=249&originWidth=987&size=353797&status=done&style=none&width=987)\n\n\n# 三、kubeadm加入node节点\n\n## 1. 将节点加入到集群\n\n`[root@node1  docker]# kubeadm join 192.168.10.100:6443 --token twjk7a.wkjilo6g39urowbj  --discovery-token-ca-cert-hash sha256:efeb65c86da8587794ee0503258201d6d15e793f833e3b716dce9f35eb72b83b` \n\n## 2. 查看集群信息\n\n`[root@master  ~]# kubectl get nodes` \n![b052f62ac2e1b7357fe1648a328cacd7.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984372036-e3573546-3674-4aea-a5b5-9c9558ec3de8.png#align=left&display=inline&height=128&margin=%5Bobject%20Object%5D&name=b052f62ac2e1b7357fe1648a328cacd7.png&originHeight=128&originWidth=507&size=8785&status=done&style=none&width=507)\n\n## 3. 后期有新节点加入时，使用kubeadm token list可以查询token信息，执行join操作', 49, 0, 0, '2020-12-08 21:51:42.072953', '2021-01-13 18:34:38.127581', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (8, '部署helm', '[TOC]\n\n# 一、安装Helm\n\n\n- 官方参考文档：[https://helm.sh/docs/intro/quickstart/](https://helm.sh/docs/intro/quickstart/)\n- Helm的安装方式有两种：预编译的二进制程序和源码编译安装。\n- Helm项目托管在GitHub之上，项目地址为[https://github.com/helm/helm/releases](https://github.com/helm/helm/releases)。\n- Helm的运行依赖于本地安装并配置完成的kubectl方能与运行于Kubernetes集群之上的Tiller服务器进行通信，因此，运行Helm的节点也应该是可以正常使用kubectl命令的主机，或者至少是有着可用kubeconfig配置文件的主机。\n\n## 1. 下载合适版本的压缩包并将其展开。\n\n`wget https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz` \n`tar -zxvf helm-v3.4.1-linux-amd64.tar.gz` \n\n## 2. 将其二进制程序文件复制或移动到系统PATH环境变量指向的目录中\n\n`cp linux-amd64/helm /usr/local/bin/` \n\n## 3. 以添加自动完成的代码：\n\n`source <(helm completion bash)` \n\n## 4. Helm客户端安装完成后，进行验证。\n\n![0f56509ab3c33a7f284b4ceddbda3e1c.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603984527925-c322f0ab-67db-479f-9670-e70a7258b463.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=0f56509ab3c33a7f284b4ceddbda3e1c.png&originHeight=44&originWidth=370&size=14541&status=done&style=none&width=370)\n\n\n# 二、添加Helm的官方仓库\n\n\n## 1. 添加官方Charts仓库\n\n `helm repo add stable https://charts.helm.sh/stable ` \n\n## 2. 更新仓库信息\n\n`helm repo update` \n\n## 3. 查看官方Charts仓库\n\n`helm search repo stable` ', 40, 1, 0, '2020-12-08 21:53:12.177463', '2021-01-16 16:36:52.950969', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (9, '部署ingress控制器', '[TOC]\n\n# 一、使用yaml配置文件部署\n\n\n## 1. 参考地址\n\n[github地址](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.2/docs/deploy/index.md)\n[ingress-nginx官网](https://kubernetes.github.io/ingress-nginx/)\n\n## 2. 创建ingress基础环境资源\n\n`# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.31.1/deploy/static/provider/aws/deploy-tls-termination.yaml` \n\n\n- 下载慢可以去Github下载\n  [https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml)\n- 创建资源\n  `# kubectl apply -f mandatory.yaml` \n- 查看pod资源信息\n  `# kubectl get pod -n ingress-nginx` \n\n\n\n## 3. 采用nodepod暴露服务\n\n`# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml` \n\n\n## 4. 查看svc资源信息\n\n`kubectl get svc -n ingress-nginx` \n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603985541050-69cc3be0-848d-481f-ad04-91473d710c25.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=909&size=353797&status=done&style=none&width=909)\n\n# 二、使用helm部署\n\n\n## 1. 创建ingress名称空间\n\n`# kubectl create namespace ingress` \n\n## 2. 添加仓库\n\n`# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx` \n\n## 3. 部署ingress\n\n`# helm install my-release ingress-nginx/ingress-nginx --namespace ingress` \n\n## 4. 查看验证\n\n`# kubectl get pod -n ingress` \n`# kubectl get svc -n ingress` ', 57, 0, 0, '2020-12-08 21:54:58.620845', '2021-01-08 07:48:22.192432', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (10, '部署calico网络组件', '[TOC]\n\n# 一、Calico官网：\n\n[https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel](https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel)\n\n# 二、安装部署\n\n## 1. 下载calico的canal插件：\n\n`# curl https://docs.projectcalico.org/manifests/canal.yaml -O` \n\n- 如果使用的是pod cidr 10.244.0.0/16，请跳到下一步。如果您使用的是不同的pod cidr，请使用以下命令来设置包含pod cidr的环境变量pod cidr，并将清单中的10.244.0.0/16替换为pod cidr。\n\nPOD_CIDR=\"<your-pod-cidr>\"\n`sed -i -e \"s?10.244.0.0/16?$POD_CIDR?g\" canal.yaml` \n\n## 2. 部署canal插件：\n\n`kubectl apply -f canal.yaml` \n\n## 3. 使用kubectl get pods -n kube-system中查看安装进程。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986218413-be39a69c-279a-4114-badb-6128ed215913.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=742&size=353797&status=done&style=none&width=742)', 17, 0, 0, '2020-12-08 21:56:22.438170', '2021-01-16 19:56:18.546077', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (11, '部署dashboard', '[TOC]\n\n# 一、dashboard组件安装-token\n\n\n- 官方参考文档：\n  [https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui)\n- github项目地址：\n  [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)\n- 说明\n  1.7及之后的版本默认在部署时仅定义了运行Dashboard所需要的最小权限，仅能够在Master主机上通过“kubectl\n  proxy”命令创建代理后于本机进行访问，它默认禁止了来自于其他任何主机的访问请求。\n\n\n\n## 1. 因为自动生成的证书很多浏览器无法使用，所以自己创建证书\n\n\n- 新建证书存放目录\n  `mkdir /etc/kubernetes/dashboard-certs` \n  `cd /etc/kubernetes/dashboard-certs/` \n- 创建命名空间\n  `kubectl create namespace kubernetes-dashboard` \n- 创建key文件\n  `openssl genrsa -out dashboard.key 2048` \n- 证书请求\n  `openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj \'/CN=dashboard-cert\'` \n- 自签证书\n  `openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt` \n- 创建kubernetes-dashboard-certs对象\n  `kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard` \n\n## 2. 下载并修改配置文件\n\n\n- 下载配置文件\n  `wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml` \n- 修改配置文件，增加直接访问端口\n  `[root@master  ~]# vim recommended.yaml ` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986440425-d0aa96ba-83dc-49b1-8e18-599b381bfefc.png#align=left&display=inline&height=354&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=354&originWidth=353&size=353797&status=done&style=none&width=353)\n\n- 修改配置文件，注释原kubernetes-dashboard-certs对象声明\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986459006-3930aa82-bee0-49aa-b245-428149b6d2c2.png#align=left&display=inline&height=256&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=256&originWidth=431&size=353797&status=done&style=none&width=431)\n\n## 3. 运行dashboard\n\n`[root@master  ~]# kubectl apply -f recommended.yaml ` \n\n## 4. 更新配置信息\n\n\n- 创建Dashboard管理员账号dashboard-admin.yaml，并apply\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n\n- 赋权dashboard-admin-bind-cluster-role.yaml，并apply\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-bind-cluster-role\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n\n## 3. 获取token信息\n\n`kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk \'{print $1}\')` \n\n## 4. 登录访问[https://192.168.10.100:30](https://192.168.10.100:30001)010\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986577911-f4796102-2fff-49df-b021-e5c90dd0ec31.png#align=left&display=inline&height=731&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=731&originWidth=1109&size=353797&status=done&style=none&width=1109)\n\n\n# 二、dashboard组件安装-kubeconfig\n\n1. 设置变量\n- 获取dashboard-admin-token的名称\n  `kubectl get secrets --all-namespaces | grep dashboard-admin-token`\n\n- 获取token\n  `DASH_TOCKEN=$(kubectl -n kubernetes-dashboard get secret dashboard-admin-token-9k522 -o jsonpath={.data.token}| base64 -d)`\n\n2. 初始化集群信息，提供API Server的URL，以及验证API Server证书所用到的CA证书等\n`kubectl config set-cluster kubernetes --server=192.168.10.100:6443 --kubeconfig=/root/dashbord-admin.conf` \n\n3. 获取dashboard-admin的token，并将其作为认证信息。\n`kubectl config set-credentials dashboard-admin --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf` \n\n4. 设置context列表，定义一个名为dashboard-admin的context：\n`kubectl config set-context dashboard-admin@kubernetes  --cluster=kubernetes  --user=dashboard-admin --kubeconfig=/root/dashbord-admin.conf` \n\n5.  使用的context为前面定义的名为dashboard-admin的context：\n`kubectl config use-context dashboard-admin@kubernetes  --kubeconfig=/root/dashbord-admin.conf` \n\n6.  sz发送kubeconfig，然后使用kubeconfig登录\n\n6.  切换到admin用户\n`kubectl config use-context kubernetes-admin@kubernetes ` ', 16, 0, 0, '2020-12-08 22:00:02.205253', '2021-01-17 04:35:09.254550', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (12, '部署metrics-server监控组件', '[TOC]\n\n# 一、部署metrics-server\n\n[github地址](https://github.com/kubernetes-sigs/metrics-server)\n\n1. 克隆项目代码的仓库至本地node节点目录以获得其资源配置清单\n`# git clone https://github.com/kubernetes-incubator/metrics-server.git` \n\n2. 需要修改metrics-server/deploy/1.8+/metrics-server-deployment.yaml清单文件\n![Snipaste_2020-01-29_20-35-06.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603986822431-7916f44e-cf76-40b6-a690-806b975707cb.png#align=left&display=inline&height=142&margin=%5Bobject%20Object%5D&name=Snipaste_2020-01-29_20-35-06.png&originHeight=142&originWidth=1107&size=57139&status=done&style=none&width=1107)\n\n    ```yaml\n    image: mirrorgooglecontainers/metrics-server-amd64:v0.3.6\n    imagePullPolicy: IfNotPresent\n    command:\n    - /metrics-server\n    - --kubelet-insecure-tls\n    - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n    ```\n\n3. 为master节点添加label\n`# kubectl label nodes master metrics=yes` \n\n4. 部署metrics-server\n`# kubectl create -f metrics-server/deploy/1.8+/` \n\n5. 检验相应的API群组metrics.k8s.io是否出现在Kubernetes集群的API群组列表中\n`# kubectl api-versions | grep metrics` \n\n6. 确认相关的Pod对象运行正常\n`# kubectl get pods -n kube-system -l k8s-app=metrics-server` \n\n7. 使用kubectl top node查看结果', 18, 1, 0, '2020-12-08 22:05:41.434501', '2021-01-08 07:48:34.378874', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (13, '部署Prometheus+Grafana', '[TOC]\n\n# 一、Prometheus介绍\n\n\n> 如果已安装metrics-server需要先卸载，否则冲突\n\n\n\n1. Prometheus（普罗米修斯）是一套开源的监控&报警&时间序列数据库的组合，起始是由SoundCloud公司开发的。随着发展，越来越多公司和组织接受采用Prometheus，社会也十分活跃，他们便将它独立成开源项目。现在最常见的Kubernetes容器管理系统中，通常会搭配Prometheus进行监控。\n1. Prometheus基本原理是通过HTTP协议周期性抓取被监控组件的状态，这样做的好处是任意组件只要提供HTTP接口就可以接入监控系统，不需要任何SDK或者其他的集成过程。这样做非常适合虚拟化环境比如VM或者Docker\n   。\n1. 输出被监控组件信息的HTTP接口被叫做exporter\n   。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux\n   系统信息 (包括磁盘、内存、CPU、网络等等)\n1. 与其他监控系统相比，Prometheus的主要特点是：\n\n\n\n- 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸）；\n- 非常高效的存储；\n- 一种灵活的查询语言；\n- 不依赖分布式存储，单个服务器节点；\n- 时间集合通过HTTP上的PULL模型进行；\n- 通过中间网关支持推送时间；\n- 通过服务发现或静态配置发现目标；\n- 多种模式的图形和仪表板支持。\n\n\n\n# 二、Grafana介绍\n\n\n> Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。它主要有以下六大特点：\n\n\n\n- 展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；\n- 数据源：Graphite，InfluxDB，OpenTSDB，Prometheus，Elasticsearch，CloudWatch和KairosDB等；\n- 通知提醒：以可视方式定义最重要指标的警报规则，Grafana将不断计算并发送通知，在数据达到阈值时通过Slack、PagerDuty等获得通知；\n- 混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数据源；\n- 注释：使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据和标记；\n- 过滤器：Ad-hoc过滤器允许动态创建新的键/值过滤器，这些过滤器会自动应用于使用该数据源的所有查询。\n\n\n\n# 三、组件说明\n\n\n1. MetricServer：是kubernetes集群资源使用情况的聚合器，收集数据给kubernetes集群内使用，如kubectl,hpa,scheduler等。\n1. PrometheusOperator：是一个系统监测和警报工具箱，用来存储监控数据。\n1. NodeExporter：用于各node的关键度量指标状态数据。\n1. KubeStateMetrics：收集kubernetes集群内资源对象数据，制定告警规则。\n1. Prometheus：采用pull方式收集apiserver，scheduler，controller-manager，kubelet组件数据，通过http协议传输。\n1. Grafana：是可视化数据统计和监控平台。\n\n\n\n# 四、安装部署\n\n\n## 1. git项目至本地\n\n`# git clone https://github.com/coreos/kube-prometheus.git` \n\n## 2. 修改资源清单文件\n\n\n- 修改 kube-prometheus/manifests/grafana-service.yaml 文件，使用 nodepode 方式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987218245-ea1245d4-ff2b-48c2-b93d-540d6231cd20.png#align=left&display=inline&height=370&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=370&originWidth=308&size=353797&status=done&style=none&width=308)\n\n- 修改修改 kube-prometheus/manifests/prometheus-service.yaml，改为nodepode方式\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987249483-629b3c1a-085f-4c6b-b898-300dcc3d99d7.png#align=left&display=inline&height=414&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=414&originWidth=349&size=353797&status=done&style=none&width=349)\n\n- 修改 kube-prometheus/manifests/alertmanager-service.yaml，改为 nodepode\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987271069-764c1ea4-05e7-4e79-a755-fc612d0fe718.png#align=left&display=inline&height=416&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=416&originWidth=354&size=353797&status=done&style=none&width=354)\n\n\n## 3. 创建资源对象\n\n\n- 创建monitoring名称空间\n  `# kubectl create namespace monitoring` \n- 创建crd资源\n  `# kubectl apply -f kube-prometheus/manifests/setup` \n- 创建其他资源\n  `# kubectl apply -f kube-prometheus/manifests/` \n\n\n\n## 4. 验证查看\n\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987440659-7b24fa61-0fd4-4d2a-967d-65f8f07da024.png#align=left&display=inline&height=319&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=319&originWidth=603&size=353797&status=done&style=none&width=603)\n\n- 查看top信息\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987475444-35f7d2c4-09d0-464e-b17a-4ce0f210fa99.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=181&originWidth=580&size=353797&status=done&style=none&width=580)\n\n\n## 5. web访问验证\n\n\n- 访问prometheus http://192.168.10.100:30200/targets，查看节点状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987514446-de0637dd-cacd-4998-8ffc-4686b59b4a00.png#align=left&display=inline&height=726&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=726&originWidth=952&size=353797&status=done&style=none&width=952)\n\n- 访问 grafana http://192.168.10.100:30100/login，默认用户名和密码是admin/admin\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987570120-647e1df6-8337-49c5-9b93-e2844cb059cd.png#align=left&display=inline&height=598&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=598&originWidth=1020&size=353797&status=done&style=none&width=1020)\n\n- 配置数据源\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987591308-acf44c8c-1d55-47eb-9fdc-45867ae5575b.png#align=left&display=inline&height=499&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=499&originWidth=1146&size=353797&status=done&style=none&width=1146)\n\n- 使用默认配置，点击测试\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987610894-a48d6427-f28b-45b8-a2a1-abd9549cb449.png#align=left&display=inline&height=356&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=356&originWidth=1125&size=353797&status=done&style=none&width=1125)\n\n- 导入默认仪表盘\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987630982-25258bd8-f3c5-4023-8396-70873f2809f9.png#align=left&display=inline&height=327&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=327&originWidth=1246&size=353797&status=done&style=none&width=1246)\n\n- 点击home选择仪表盘查看\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987649716-c9122fc8-fc64-4de7-85ca-9cde5bf85686.png#align=left&display=inline&height=659&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=659&originWidth=1240&size=353797&status=done&style=none&width=1240)\n\n\n# 五、配置prometheus\n\n\n1.  两个监控任务没有对应的目标\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603987690255-d187beca-93da-470f-9a9b-c1192bfb4f15.png#align=left&display=inline&height=157&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=157&originWidth=600&size=353797&status=done&style=none&width=600)\n\n\n- 查看prometheus-serviceMonitorKubeScheduler文件中，selector匹配的是service的标签\n\n```bash\n[root@k8s-1 manifests]# cat prometheus-serviceMonitorKubeScheduler.yaml \napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    k8s-app: kube-scheduler\n  name: kube-scheduler\n  namespace: monitoring\nspec:\n  endpoints:\n  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n    interval: 30s\n    port: https-metrics\n    scheme: https\n    tlsConfig:\n      insecureSkipVerify: true\n  jobLabel: k8s-app\n  namespaceSelector:\n    matchNames:\n    - kube-system\n    # selector匹配的资源标签为k8s-app=kube-scheduler\n  selector:\n    matchLabels:\n      k8s-app: kube-scheduler\n```\n\n- 但是namespace中并没有k8s-app=kube-scheduler的service\n\n2. 新建prometheus-kubeSchedulerService.yaml并apply创建资源\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-scheduler\n  labels:\n    k8s-app: kube-scheduler #与servicemonitor中的selector匹配\nspec:\n  selector: \n    component: kube-scheduler # 与scheduler的pod标签一直\n  ports:\n  - name: http-metrics\n    port: 10251\n    targetPort: 10251\n    protocol: TCP \n```\n\n- 新建prometheus-kubeControllerManagerService.yaml并apply创建资源\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n   namespace: kube-system\n   name: kube-controller-manager\n   labels:\n    k8s-app: kube-controller-manager\nspec:\n  selector:\n    component: kube-controller-manager\n  ports:\n  - name: http-metrics\n    port: 10252\n    targetPort: 10252\n    protocol: TCP \n```\n\n3. 再次查看targets信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988018072-dc53fffd-80ce-403d-9dbd-451a2fe041d8.png#align=left&display=inline&height=527&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=527&originWidth=711&size=353797&status=done&style=none&width=711)\n\n# 六、部署pushgateway\n\n1. pushgetway目录下，创建这三个yaml文件。\n\n- prometheus-pushgatewayServiceMonitor.yaml\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    prometheus: k8s\n  name: prometheus-pushgateway\n  namespace: monitoring\nspec:\n  endpoints:\n  - honorLabels: true\n    port: http\n  jobLabel: k8s-app\n  selector:\n    matchLabels:\n      app: prometheus-pushgateway\n```\n\n- prometheus-pushgatewayService.yaml\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: prometheus-pushgateway\n  name: prometheus-pushgateway\n  namespace: monitoring\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 9091\n    nodePort: 30400\n    targetPort: metrics\n  selector:\n    app: prometheus-pushgateway\n#  type: ClusterIP\n```\n\n- prometheus-pushgatewayDeployment.yaml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: prometheus-pushgateway\n  name: prometheus-pushgateway\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-pushgateway\n  template:\n    metadata:\n      labels:\n        app: prometheus-pushgateway\n    spec:\n      containers:\n      - image: prom/pushgateway:v0.8.0\n        livenessProbe:\n          httpGet:\n            path: /#/status\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        name: prometheus-pushgateway\n        ports:\n        - containerPort: 9091\n          name: metrics\n        readinessProbe:\n          httpGet:\n            path: /#/status\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 50m\n            memory: 100Mi\n```\n\n- 然后使用\n\n```\nkubectl apply -f .\n```', 24, 1, 0, '2020-12-08 22:07:11.797680', '2021-01-08 07:48:40.752908', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (14, '部署elk日志收集', '[TOC]\n\n# 一、准备工作\n\n\n- 添加elk仓库\n  `# helm repo add elastic https://helm.elastic.co` \n- 创建elk名称空间\n  `# kubectl create namespace elk` \n\n\n\n# 二、部署Elasticsearch\n\n\n1. 将chart包下载本地\n   `# helm fetch elastic/elasticsearch` \n1. 修改elasticsearch/values.yaml配置文件（降低配置要求）\n\n- master节点数改为1\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988104443-96e8f591-55d0-4546-8cc2-6bf630067240.png#align=left&display=inline&height=23&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=23&originWidth=238&size=353797&status=done&style=none&width=238)\n\n- 不使用持久卷\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988121224-df3248d2-2773-488d-9f58-cbda613e98cd.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=229&size=353797&status=done&style=none&width=229)\n\n3. helm安装chart包\n   `helm install elasticsearch -f values.yaml elastic/elasticsearch --namespace elk`\n   \n3. 验证结果\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988170952-93a09828-203d-4852-a624-b8ced3feb626.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=843&size=353797&status=done&style=none&width=843)\n\n\n# 三、部署Filebeat\n\n\n1. 将chart包下载本地\n`# helm fetch elastic/filebeat` \n\n2. 安装chart包\n   `# helm install filebeat elastic/filebeat --namespace elk` \n   \n2. 验证结果\n\n- 查看pod状态\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988224463-d69aea9a-ed5e-42be-a4ed-19d00048e37a.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=805&size=353797&status=done&style=none&width=805)\n\n# 四、部署Logstash\n\n\n1. 将chart包下载本地\n   `# helm fetch elastic/logstash` \n1. 修改logstash/values.yaml配置文件\n\n- 不使用持久卷\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988254598-7efac7c6-4c7c-4736-b209-78e45adb4f3f.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=226&size=353797&status=done&style=none&width=226)\n\n3. helm安装chart包\n   `# helm install logstash -f values.yaml elastic/logstash --namespace elk` \n3. 验证结果\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988289176-966e6a1d-f874-4dae-9813-042e6f893b31.png#align=left&display=inline&height=69&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=69&originWidth=705&size=353797&status=done&style=none&width=705)\n\n\n# 五、部署 kibana\n\n\n1. 将chart包下载至本地\n   `# helm fetch elastic/kibana` \n1. 编辑kibana/values.yaml配置文件\n\n- 服务类型改为nodeport\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988322205-6e247a9e-ac99-4118-8426-704e3fe0b05f.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=247&size=353797&status=done&style=none&width=247)\n\n3. helm安装chart包\n   `# helm install kibana -f values.yaml elastic/kibana --namespace elk` \n3. 结果验证\n\n- 查看pod状态\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988354212-f94d6364-6125-4de4-9e36-4bf6c95d5664.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=718&size=353797&status=done&style=none&width=718)\n\n\n# 六、web访问测试\n\n![bdf9ab3eedce76bccd980f4710f2a803.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988405166-430b6b6a-de6f-4c2a-a34c-4b2dec59ed36.png#align=left&display=inline&height=640&margin=%5Bobject%20Object%5D&name=bdf9ab3eedce76bccd980f4710f2a803.png&originHeight=640&originWidth=1244&size=244291&status=done&style=none&width=1244)\n![aaba2f13bc7bf131c6571b4707c0b163.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988414154-8ce60590-b0d8-4a96-b811-839723df0084.png#align=left&display=inline&height=934&margin=%5Bobject%20Object%5D&name=aaba2f13bc7bf131c6571b4707c0b163.png&originHeight=934&originWidth=1245&size=420555&status=done&style=none&width=1245)', 33, 0, 0, '2020-12-08 22:12:32.162785', '2021-01-20 01:07:52.544732', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (15, '部署Harbor私有镜像仓库', '[TOC]\n\n# 一、Harbor私有镜像仓库\n\n\n1. 安装docker\n1. 安装docker-compose\n1. 下载harbor离线安装包\n   [参考链接](https://github.com/goharbor/harbor/releases)\n   `[root@harbor  ~] wget https://github.com/vmware/harbor/releases/download/v1.8.6/harbor-offline-installer-v1.8.6.tgz` \n   `[root@harbor  ~]# tar -xvf harbor-offline-installer-v1.8.6.tgz ` \n1. 修改harbor.yml配置文件\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988544911-e6a87aea-59f4-4c87-a388-865be20e9fa4.png#align=left&display=inline&height=127&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=127&originWidth=362&size=353797&status=done&style=none&width=362)\n   *注释https相关配置\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988572290-24b5f355-9396-409b-9901-d58307a2832e.png#align=left&display=inline&height=155&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=155&originWidth=590&size=353797&status=done&style=none&width=590)\n\n5. 运行install.sh脚本\n   `[root[@harbor ](/harbor ) harbor]# ./install.sh`\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988591722-52fc5191-6221-497d-b50f-838f395648e8.png#align=left&display=inline&height=105&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=105&originWidth=894&size=353797&status=done&style=none&width=894)\n\n6. 访问Harbor并登陆。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988612916-ff68ff22-8de8-4cd0-9a06-e61f16a24983.png#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=629&originWidth=669&size=353797&status=done&style=none&width=669)\n\n 初始用户名admin\n 初始密码Harbor12345\n\n7. 创建systemd服务管理脚本\n\n```bash\n	[Unit]\n	Description=Harbor\n	After=docker.service systemd-networkd.service systemd-resolved.service\n	Requires=docker.service\n	Documentation=http://github.com/vmware/harbor\n	\n	[Service]\n	Type=simple\n	Restart=on-failure\n	RestartSec=5\n	ExecStart=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml up\n	ExecReload=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml restart\n	ExecStop=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml down\n	\n	[Install]\n	WantedBy=multi-user.target\n\n```\n\n\n# 二、docker授权访问harbor仓库（所有安装docker的节点）\n\n\n1. docker配置文件私有仓库设置\n   `[root@master  ~]# vim /etc/docker/daemon.json ` \n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988680971-a7382b3b-9276-40ff-8820-157818f39e64.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=804&size=353797&status=done&style=none&width=804)\n\n2. 重启docker\n   `systemctl daemon-reload` \n   `systemctl restart docker` \n2. master节点登陆测试\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988703787-3dfcd92f-1287-4bc4-ba33-1d90d54cc0ef.png#align=left&display=inline&height=197&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=197&originWidth=948&size=353797&status=done&style=none&width=948)\n\n4. 推送镜像测试\n    ![4b3c38a2ca924a141215c8050cca5f3c.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988723097-0794693f-ba4f-4332-bfab-d01d4530b421.png#align=left&display=inline&height=229&margin=%5Bobject%20Object%5D&name=4b3c38a2ca924a141215c8050cca5f3c.png&originHeight=229&originWidth=513&size=15430&status=done&style=none&width=513)\n\n\n# 三、kubernets访问harbor仓库\n\n\n- 由于harbor采用了用户名密码认证，所以在镜像下载时需要配置sercet\n\n\n\n1. 创建认证secret\n   `kubectl create secret docker-registry registry-secret --namespace=default --docker-server=192.168.10.103 --docker-username=admin --docker-password=Harbor12345` \n1. 查看secret\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1603988773016-f09118ac-6a2b-4a40-b80f-02cc37e9a45f.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=787&size=353797&status=done&style=none&width=787)\n\n3. 使用相应的私有registry中镜像的Pod资源的定义，即可通过imagePullSecrets字段使用此Secret对象\n\n    ```yaml\n	apiVersion: v1\n	kind: Pod \n	metadata:\n	  name: secret-imagepull-demo\n	  namespace: default\n	spec:\n	  imagePullSecrets:\n	  - name: registry-secret\n	  containers:\n	  - image: harbor.cy.bj/k8s/nginx\n	    name: myapp\n```\n\n4. 删除\n   `kubectl delete secret registry-secret` ', 30, 1, 0, '2020-12-08 22:16:13.414559', '2021-01-08 07:49:04.952201', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (16, '命令格式', '[TOC]\n\n# 一、命令格式\n\n\n> **kubectl [command] [TYPE] [NAME] [flags]**\n\n\n\n# 二、command\n\n\n1. 指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048264044-a061f530-f07e-4b0e-9d8b-5b78ce9ef139.png#align=left&display=inline&height=825&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=825&originWidth=533&size=178039&status=done&style=none&width=533)\n\n# 三、TYPE\n\n\n1. 指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:\n\n```bash\n# kubectl get pod pod1\n# kubectl get pods pod1\n# kubectl get po pod1\n```\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048361139-80f4120c-f561-43e3-9196-a5544e38bc78.png#align=left&display=inline&height=823&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=823&originWidth=533&size=178039&status=done&style=none&width=533)\n\n# 四、NAME\n\n\n1. 指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息。 `kubectl get pods` \n1. 在对多个资源执行操作时，可以按类型和名称指定每个资源，或指定一个或多个文件\n1. 要对所有类型相同的资源进行分组，执行以下操作：TYPE1 name1 name2  name。 `kubectl get pod example-pod1 example-pod2` \n1. 分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3。 `kubectl get pod/example-pod1 replicationcontroller/example-rc1` \n1. 用一个或多个文件指定资源：-f file1 -f file2 -f file 。 `kubectl get pod -f ./pod.yaml` \n\n# 五、flags\n\n1. 指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API服务器的地址和端口。\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048939406-551d39d8-f417-46f1-a6d2-cfc3fc0afb9d.png#align=left&display=inline&height=586&margin=%5Bobject%20Object%5D&name=image.png&originHeight=586&originWidth=533&size=178039&status=done&style=none&width=533)\n\n# 六、输出格式\n\n1. kubectl命令还包含了多种不同的输出格式（如表3-2所示），它们为用户提供了非常灵活的自定义输出机制，如输出为YAML或JSON格式等。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604048960887-c5457ab4-2497-495e-a1c5-1fa388f7616d.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=527&size=178039&status=done&style=none&width=527)\n\n# 七、帮助命令\n\n1. 可以通过 `kubectl help [subcommand]` 命令查看命令格式和支持的子命令信息。', 12, 1, 0, '2020-12-09 21:25:59.777077', '2021-01-17 03:00:33.256408', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (17, 'node常用命令', '1. 显示Node的详细信息\n\n    `$ kubectl describe nodes <node-name>` \n\n2. 驱逐node节点的pod\n\n   `$ kubectl drain node2 --delete-local-data` \n\n3. 删除node节点\n\n    `$ kubectl delete node nodename` \n\n4. 为node节点设置“disktype=ssd”标签以标识其拥有SSD设备：\n\n     `$ kubectl label nodes node2 disktype=ssd` \n\n4. 查看具有键名SSD的标签的Node资源：\n\n    `$ kubectl get nodes -l \'disktype\' -L disktype` ', 12, 1, 0, '2020-12-09 21:27:53.428483', '2021-01-15 22:13:23.652526', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (18, 'pod常用命令', '[TOC]\n\n# 一、创建资源对象\n\n\n1. 根据yaml配置文件一次性创建service和rc\n   `$ kubectl create -f my-service.yaml -f my-rc.yaml` \n1. 根据<directory>目录下所有.yaml、.yml、.json文件的定义进行创建操作\n   `$ kubectl create -f <directory>` \n\n\n\n# 二、查看pod对象\n\n\n1. 查看所有Pod列表\n   `$ kubectl get pods` \n1. 显示Pod的更多信息\n   `kubectl get pod <pod-name> -o wide` \n1. 以yaml格式显示Pod的详细信息\n   `kubectl get pod <pod-name> -o yaml` \n1. 查看命名空间\n   `kubectl get namespaces` \n1. 查看所有命令空间pod\n   `kubectl get pod --all-namespaces`  或者 `kubectl get pod -A`\n1. 查看指定命名空间pod信息\n  `kubectl get pod -n kube` \n7. 查看所有pod标签信息\n   `kubectl get pods --show-labels` \n7. 查看指定标签的pod\n   `kubectl get pods -l app=rs-demo` \n7. 格式化输出自定义列信息\n\n```bash\n[root@k8s-1 ~]# kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image\npod_name                                     pod_image\nmytomcat-84884dbbfc-22csl                    jy-k8s-registry.jiayuan.idc/project:v5\nmytomcat-84884dbbfc-cl2m7                    jy-k8s-registry.jiayuan.idc/project:v5\nmytomcat-84884dbbfc-ngpmw                    jy-k8s-registry.jiayuan.idc/project:v5\nonline-message-deployment-59c75ddc8d-pfk4s   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\nonline-message-deployment-59c75ddc8d-s2b77   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\nonline-message-deployment-59c75ddc8d-s5wv5   jy-k8s-registry.jiayuan.idc/online-message:v20.10.20-104430\n```\n\n\n# 三、查看pod对象的详细信息\n\n\n1. 显示Pod的详细信息\n   `$ kubectl describe pods/<pod-name>` \n   `$ kubectl describe pods <pod-name>` \n\n\n\n# 四、查看容器中的日志信息\n\n\n1. 查看容器的日志\n   `kubectl logs <pod-name>` \n1. 实时查看日志\n   `kubectl logs -f <pod-name>` \n\n\n\n# 五、在pod中执行命令\n\n\n1. 执行Pod的data命令，默认是用Pod中的第一个容器执行\n   `$ kubectl exec <pod-name> data` \n1. 指定Pod中某个容器执行data命令\n   `$ kubectl exec <pod-name> -c <container-name> data` \n1. 通过bash获得Pod中某个容器的TTY，相当于登录容器\n   `$ kubectl exec -it <pod-name> -c <container-name> bash` \n\n\n\n# 六．删除pod对象\n\n\n1. 基于Pod.yaml定义的名称删除Pod\n   `$ kubectl delete -f pod.yaml` \n1. 删除所有包含某个label的Pod\n   `$ kubectl delete pods -l name=<label-name>` \n1. 删除所有Pod\n   `$ kubectl delete pods --all` \n\n\n\n# 七、其他相关命令\n\n\n1. 编辑名为 docker-registry 的 pod\n`$ kubectl edit pod docker-registry ` \n2. 获取相关的使用帮助\n   `$ kubectl explain pods` \n   `$ kubectl explain pods.spec` \n2. 给pod资源添加lables标签\n   `kubectl label pods/pod-with-labels version=v1` \n2. 修改已有pod资源标签\n   `kubectl label pods/pod-with-labels version=v2 --overwrite` ', 15, 0, 0, '2020-12-09 21:29:25.849657', '2021-01-08 07:49:16.331585', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (19, '控制器常用命令', '[TOC]\n\n# 一、ReplicaSet\n\n\n1. 查看replicaset信息\n   `$ kubectl get replicaset` \n   `$ kubectl get replicaset rs-example -o wide` \n1. 查看replicasets控制器下某资源详细信息\n   `$ kubectl describe replicasets/rs-example` \n1. 显示由RC管理的Pod的信息\n   `$ kubectl describe pods <rc-name>` \n1. Pod的扩容与缩容\n   `$ kubectl scale rc redis --replicas=3` \n    *当我们需要进行永久性扩容时，不要忘记修改rc配置文件中的replicas数量。\n  `$ kubectl patch statefulset myapp -p \'{\"spec\":{\"replicas\":3}}\'` \n5. 更新镜像\n   `$ kubectl set image deployments myapp-deploy  myapp=192.168.10.110/k8s/myapp:v2` \n\n# 二、Deployment\n\n\n1. 查看版本信息\n   `$ kubectl rollout history deployment myapp-deploy` \n1. 查看更新状态：\n   `$ kubectl rollout status deployment nginx` \n1. 终止升级\n   `$ kubectl rollout pause deployment/nginx` \n1. 继续升级\n   `$ kubectl rollout resume deployment/nginx` \n1. 回滚到上一个版本\n   `$ kubectl rollout undo deployments myapp-deploy` \n1. 回滚到指定版本\n   `$ kubectl rollout undo deployments myapp-deploy --to-revision=1`\n\n\n\n# 三、DaemonSet控制器\n\n\n1. 更新镜像\n   `$ kubectl set image daemonsets filebeat-ds filebeat=ikubernetes/filebeat:5.6.6` \n\n# 四、Job控制器\n\n\n1. 以Job控制器名称为标签进行匹配：\n   `$ kubectl get pods -l job-name=job-example` \n1. job扩容\n   `$ kubectl scale jobs job-multi --replicas=2` \n\n\n\n# 五、CronJob控制器\n\n\n1. 以CronJob控制器名称为标签进行匹配：\n   `$ kubectl get jobs -l app=mycronjob-jobs` ', 12, 0, 0, '2020-12-09 21:31:01.588510', '2021-01-26 09:02:56.712543', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (20, 'service常用命令', '[TOC]\n\n# 一、service\n\n1. 查看svc信息\n   `$ kubectl get svc` \n2. 删除service\n   `$kubectl delete svc 服务名称` \n\n# 二、ingress\n\n\n1. 查看ingress-nginx信息\n   `$ kubectl get svc -n ingress-nginx` \n1. 查看ingress规则\n   `$ kubectl get ingress` ', 12, 0, 0, '2020-12-09 21:34:57.028105', '2021-01-15 22:00:09.836938', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (21, '存储常用命令', '[TOC]\n\n# 一、ConfigMap\n\n\n1. 在命令行直接给出键值对来创建ConfigMap对象\n   `$ kubectl create configmap configmap_name --from-literal=key-name-1=value-1` \n1. 查看ConfigMap对象special-config的相关信息\n   `$ kubectl get configmaps special-config -o yaml` \n   `$ kubectl describe configmaps special-config` \n1. 基于文件创建ConfigMap对象\n   `# kubectl create configmap resolv.conf --from-file=/etc/resolv.conf` \n1. 基于目录创建ConfigMap对象\n   `# kubectl create configmap docker-config-files --from-file=/etc/docker/` \n\n\n\n# 二、secret\n\n\n1. 创建了一个名为mysql-auth的Secret对象，用户名为root，密码为123.com\n   `# kubectl create secret generic mysql-auth --from-literal=username=root --from-literal=password=123.com` \n1. 将ssh密钥认证文件创建secret对象\n   `# kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-file=ssh-publickey=/root/.ssh/id_rsa.pub` \n1. 查看secret资源详细信息\n   `# kubectl get secrets ssh-key-secret -o yaml` ', 12, 0, 0, '2020-12-09 21:36:27.003455', '2021-01-26 09:04:15.576023', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (22, '日常命令总结', '[TOC]\n\n# 一、查看命令\n\n\n1. 查看所有namespace的pods运行情况\n   `kubectl get pods --all-namespaces` \n1. 查看具体pods，记得后边跟namespace名字哦\n   `kubectl get pods kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system` \n1. 查看pods具体信息\n   `kubectl get pods -o wide kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system` \n1. 获取所有deployment\n   `kubectl get deployment --all-namespaces` \n1. 查看kube-system namespace下面的pod/svc/deployment 等等（-o wide选项可以查看存在哪个对应的节点）\n   `kubectl get pod /svc/deployment -n kube-system` \n1. 列出该 namespace 中的所有 pod 包括未初始化的\n   `kubectl get pods --include-uninitialized` \n1. 查看deployment\n   `kubectl get deployment nginx-app` \n1. 查看rc和servers\n   `kubectl get rc,services` \n1. 查看pods结构信息（重点，通过这个看日志分析错误）对控制器和服务，node同样有效\n   `kubectl describe pods xxxxpodsname --namespace=xxxnamespace` \n    其他控制器类似，就是kubectl get 控制器 控制器具体名称\n10. 查看pod日志\n    `kubectl logs $POD_NAME` \n10. 查看pod变量\n    `kubectl exec my-nginx-5j8ok -- printenv | grep SERVICE` \n\n\n\n# 二、集群查看\n\n\n1. 查看集群健康情况\n   `kubectl get cs` \n1. 集群核心组件运行情况\n   `kubectl cluster-info` \n1. 查看表空间名\n   `kubectl get namespaces` \n1. 查看版本\n   `kubectl version` \n1. 查看API\n   `kubectl api-versions` \n1. 查看事件\n   `kubectl get events` \n1. 获取全部节点\n   `kubectl get nodes` \n1. 删除节点\n   `kubectl delete node k8s2` \n\n\n\n# 三、创建资源\n\n\n1. 创建资源\n   `kubectl create -f ./nginx.yaml` \n1. 创建+更新，可以重复使用\n   `kubectl apply -f xxx.yaml` \n1. 创建当前目录下的所有yaml资源\n   `kubectl create -f .` \n1. 使用多个文件创建资源\n   `kubectl create -f ./nginx1.yaml -f ./mysql2.yaml` \n1. 使用目录下的所有清单文件来创建资源\n   `kubectl create -f ./dir` \n1. 使用 url 来创建资源\n   `kubectl create -f https://git.io/vPieo` \n1. 创建带有终端的pod\n   `kubectl run -i --tty busybox --image=busybox` \n1. 启动一个 nginx 实例\n   `kubectl run nginx --image=nginx` \n1. 启动多个pod\n   `kubectl run mybusybox --image=busybox --replicas=5` \n1. 获取 pod 和 svc 的文档\n   `kubectl explain pods,svc` \n\n\n\n# 四、更新\n\n\n1. 滚动更新 pod frontend-v1\n   `kubectl rolling-update python-v1 -f python-v2.json` \n1. 更新资源名称并更新镜像\n   `kubectl rolling-update python-v1 python-v2 --image=image:v2` \n1. 更新 frontend pod 中的镜像\n   `kubectl rolling-update python --image=image:v2` \n1. 退出已存在的进行中的滚动更新\n   `kubectl rolling-update python-v1 python-v2 --rollback` \n1. 基于 stdin 输入的 JSON 替换 pod\n   `cat pod.json | kubectl replace -f -` \n1. 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口\n   `kubectl expose rc nginx --port=80 --target-port=8000` \n1. 更新单容器 pod 的镜像版本（tag）到 v4\n   `kubectl get pod nginx-pod -o yaml | sed \'s/\\(image:myimage\\):.*$/\\1:v4/\' | kubectl replace -f –` \n1. 添加标签\n   `kubectl label pods nginx-pod new-label=awesome` \n1. 添加注解\n   `kubectl annotate pods nginx-pod icon-url=http://goo.gl/XXBTWq` \n1. 自动扩展 deployment\n   `kubectl autoscale deployment foo --min=2 --max=10` \n\n\n\n# 五、编辑资源\n\n\n1. 编辑名为 docker-registry 的 service\n   `kubectl edit svc/docker-registry` \n1. 修改启动参数\n   `vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf` \n\n\n\n# 六、动态伸缩pod\n\n\n1. 将foo副本集变成3个\n   `kubectl scale --replicas=3 rs/foo` \n1. 缩放“foo”中指定的资源。\n   `kubectl scale --replicas=3 -f foo.yaml` \n1. 将deployment/mysql从2个变成3个\n   `kubectl scale --current-replicas=2 --replicas=3 deployment/mysql` \n1. 变更多个控制器的数量\n   `kubectl scale --replicas=5 rc/foo rc/bar rc/baz` \n1. 查看变更进度\n   `kubectl rollout status deploy deployment/mysql` \n\n\n\n# 七、label 操作\n\n\n1. 增加节点lable值\n   `kubectl label nodes node1 zone=north` \n1. 增加lable值 [key]=[value]\n   `kubectl label pod redis-master-1033017107-q47hh role=master` \n1. 删除lable值\n   `kubectl label pod redis-master-1033017107-q47hh role-` \n1. 修改lable值\n   `kubectl label pod redis-master-1033017107-q47hh role=backend --overwrite` \n\n\n\n# 八、滚动升级\n\n\n1. 配置文件滚动升级\n   `kubectl rolling-update redis-master -f redis-master-controller-v2.yaml` \n1. 命令升级\n   `kubectl rolling-update redis-master --image=redis-master:2.0` \n1. pod版本回滚\n   `kubectl rolling-update redis-master --image=redis-master:1.0 --rollback` \n\n\n\n# 九、etcdctl 常用操作\n\n\n1. 检查网络集群健康状态\n   `etcdctl cluster-health` \n1. 带有安全认证检查网络集群健康状态\n   `etcdctl --endpoints=https://192.168.71.221:2379 cluster-health` \n1. 查看集群成员\n   `etcdctl member list` \n1. 设置网络配置\n   `etcdctl set /k8s/network/config ‘{ “Network”: “10.1.0.0/16” }’` \n1. 获取网络配置\n   `etcdctl get /k8s/network/config` \n\n\n\n# 十、删除资源\n\n\n1. 根据label删除：\n`kubectl delete pod -l app=flannel -n kube-system`   \n2. 删除 pod.json 文件中定义的类型和名称的 pod\n   `kubectl delete -f ./pod.json` \n2. 删除名为“baz”的 pod 和名为“foo”的 service\n   `kubectl delete pod,service baz foo` \n2. 删除具有 name=myLabel 标签的 pod 和 serivce\n   `kubectl delete pods,services -l name=myLabel` \n2. 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的\n   `kubectl delete pods,services -l name=myLabel --include-uninitialized` \n2. 删除 my-ns namespace下的所有 pod 和 serivce，包括尚未初始化的\n   `kubectl -n my-ns delete po,svc --all` \n2. 强制删除\n   `kubectl delete pods prometheus-7fcfcb9f89-qkkf7 --grace-period=0 --force` \n2. 删除指定deployment\n   `kubectl delete deployment kubernetes-dashboard --namespace=kube-system` \n2. 删除指定svc\n   `kubectl delete svc kubernetes-dashboard --namespace=kube-system` \n2. 根据资源清单文件删除\n   `kubectl delete -f kubernetes-dashboard.yaml` \n2. 强制替换，删除后重新创建资源。会导致服务中断。\n   `kubectl replace --force -f ./pod.json` \n\n# 十一、交互\n\n\n1. dump 输出 pod 的日志（stdout）\n   `kubectl logs nginx-pod` \n1. dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）\n   `kubectl logs nginx-pod -c my-container` \n1. 流式输出 pod 的日志（stdout）\n   `kubectl logs -f nginx-pod` \n1. 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）\n   `kubectl logs -f nginx-pod -c my-container` \n1. 交互式 shell 的方式运行 pod\n   `kubectl run -i --tty busybox --image=busybox -- sh` \n1. 连接到运行中的容器\n   `kubectl attach nginx-pod -i` \n1. 转发 pod 中的 6000 端口到本地的 5000 端口\n   `kubectl port-forward nginx-pod 5000:6000` \n1. 在已存在的容器中执行命令（只有一个容器的情况下）\n   `kubectl exec nginx-pod -- ls /` \n1. 在已存在的容器中执行命令（pod 中有多个容器的情况下）\n   `kubectl exec nginx-pod -c my-container -- ls /` \n1. 显示指定 pod和容器的指标度量\n   `kubectl top pod POD_NAME --containers` \n1. 进入pod\n   `kubectl exec -ti podName /bin/bash` \n\n\n\n# 十二、调度配置\n\n\n1. 标记 my-node 不可调度\n   `kubectl cordon k8s-node` \n1. 清空 my-node 以待维护\n   `kubectl drain k8s-node` \n1. 标记 my-node 可调度\n   `kubectl uncordon k8s-node` \n1. 显示 my-node 的指标度量\n   `kubectl top node k8s-node` \n1. 将当前集群状态输出到 stdout\n   `kubectl cluster-info dump` \n1. 将当前集群状态输出到 /path/to/cluster-state\n   `kubectl cluster-info dump --output-directory=/path/to/cluster-state` \n   如果该键和影响的污点（taint）已存在，则使用指定的值替换\n7. 查看kubelet进程启动参数\n   `kubectl taint nodes foo dedicated=special-user:NoSchedule` \n7. 查看日志\n   `journalctl -u kubelet -f` \n\n# 十三、导出配置文件\n\n\n1. 导出proxy\n   `kubectl get ds -n kube-system -l k8s-app=kube-proxy -o yaml>kube-proxy-ds.yaml` \n1. 导出kube-dns\n   `kubectl get deployment -n kube-system -l k8s-app=kube-dns -o yaml >kube-dns-dp.yaml` \n   `kubectl get services -n kube-system -l k8s-app=kube-dns -o yaml >kube-dns-services.yaml` \n1. 导出所有 configmap\n   `kubectl get configmap -n kube-system -o wide -o yaml > configmap.yaml` \n\n\n\n# 十四、复杂操作命令\n\n\n1. 删除kube-system 下Evicted状态的所有pod\n   `kubectl get pods -n kube-system |grep Evicted| awk ‘{print $1}’|xargs` ', 9, 0, 0, '2020-12-09 21:40:59.881346', '2021-01-26 09:04:21.192000', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (23, 'K8S中的资源对象', '[TOC]\n\n> Kubernetes的API对象大体可分为工作负载（Workload）、发现和负载均衡（Discovery& LB）、配置和存储（Config &Storage）、集群（Cluster）以及元数据（Metadata）五个类别。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604066905178-6f18d9f5-6dfe-4325-a4d1-5374cc83c0e6.png#align=left&display=inline&height=559&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=559&originWidth=1102&size=353797&status=done&style=none&width=1102)\n\n# 一、工作负载型资源\n\n\n> Pod是工作负载型资源中的基础资源，它负责运行容器，并为其解决环境性的依赖。但Pod可能会因为资源超限或节点故障等原因而终止，这些非正常终止的Pod资源需要被重建，不过，这类工作将由工作负载型的控制器来完成，它们通常也称为pod控制器。\n\n\n\n1. ReplicationController：用于确保每个Pod副本在任一时刻均能满足目标数量，换言之，它用于保证每个容器或容器组总是运行并且可访问；它是上一代的无状态Pod应用控制器，现已被Deployment和ReplicaSet取代。\n1. ReplicaSet：新一代ReplicationController，它与ReplicationController的唯一不同之处仅在于支持的标签选择器不同，ReplicationController只支持等值选择器，而ReplicaSet还额外支持基于集合的选择器。\n1. Deployment：用于管理无状态的持久化应用，例如HTTP服务器；它用于为Pod和ReplicaSet提供声明式更新，是建构在ReplicaSet之上的更为高级的控制器。\n1. StatefulSet：用于管理有状态的持久化应用，如database服务程序；其与Deployment的不同之处在于StatefulSet会为每个Pod创建一个独有的持久性标识符，并会确保各Pod之间的顺序性。\n1. DaemonSet：用于确保每个节点都运行了某Pod的一个副本，新增的节点一样会被添加此类Pod；在节点移除时，此类Pod会被回收；DaemonSet常用于运行集群存储守护进程——如glusterd和ceph，还有日志收集进程——如fluentd和logstash，以及监控进程——如Prometheus的Node Exporter、collectd、Datadog agent和Ganglia的gmond等。\n1. Job：用于管理运行完成后即可终止的应用，例如批处理作业任务；换句话讲，Job创建一个或多个Pod，并确保其符合目标数量，直到Pod正常结束而终止。\n1. CronJob：用于管理Job控制器资源的运行时间。Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及重复运行的方式\n\n# 二、发现和负载均衡\n\n\n> Pod资源可能会因为任何意外故障而被重建，于是它需要固定的可被“发现”的方式。另外，Pod资源仅在集群内可见，它的客户端也可能是集群内的其他Pod资源，若要开放给外部网络中的用户访问，则需要事先将其暴露到集群外部，并且要为同一种工作负载的访问流量进行负载均衡。\n\n\n\n1. Service：基于标签选择器将一组pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的对象上。并对客户端隐藏了真实的处理用户请求的pod资源。Service资源会通过API Service持续监视着标签选择器匹配到的后端pod对象，并实时跟踪个对象的变动；\n1. Endpoint：存储在etcd中，用来记录一个service对应的所有pod的访问地址，创建Service资源对象时，其关联的Endpoint对象会自动创建。\n1. Ingress：利用nginx，haproxy，envoy,traefik等负载均衡器来暴露集群内部服务，利用Ingress可以解决内部资源访问外部资源的方式，和四层调度替换为七层调度的问题。\n\n# 三、配置与存储\n\n\n> Docker容器分层联合挂载的方式决定了不宜在容器内部存储需要持久化的数据，于是它通过引入挂载外部存储卷的方式来解决此类问题\n\n\n\n1. Volume(存储卷)：本质上，Kubernetes Volume 是一个目录，当 Volume 被 mount 到Pod，Pod 中的所有容器都可以访问这个Volume。Kubernetes支持众多类型的存储设备或存储系统，如GlusterFS、CEPH、RBD和Flocker等。\n1. CSI：容器存储接口,可以扩展各种各样的第三方存储卷)特殊类型的存储卷\n1. ConfigMap：用于为容器中的应用提供配置数据以定制程序的行为\n1. Secret：保存敏感数据，如敏感的配置信息，例如密钥、证书等\n1. DownwardAPI：把外部环境中的信息输出给容器\n\n# 四、集群级资源\n\n\n> 用于定义集群自身配置信息的对象，它们仅应该由集群管理员进行操作\n\n1. Namespace：资源对象名称的作用范围，绝大多数对象都隶属于某个名称空间，默认时隶属于“default”。\n1. Node:Kubernetes集群的工作节点，其标识符在当前集群中必须是唯一的。\n1. Role：名称空间级别的由规则组成的权限集合，可被RoleBinding引用。\n1. ClusterRole:Cluster级别的由规则组成的权限集合，可被RoleBinding和ClusterRoleBinding引用。\n1. RoleBinding：将Role中的许可权限绑定在一个或一组用户之上，它隶属于且仅能作用于一个名称空间；绑定时，可以引用同一名称空间中的Role，也可以引用全局名称空间中的ClusterRole。\n1. ClusterRoleBinding：将ClusterRole中定义的许可权限绑定在一个或一组用户之上；它能够引用全局名称空间中的ClusterRole，并能通过Subject添加相关信息。\n\n# 五、元数据型资源\n\n\n> 用于为集群内部的其他资源配置其行为或特性\n\n\n\n1. HPA：自动伸缩工作负载类型的资源对象的规模\n1. PodTemplate：为pod资源的创建预制模板\n1. LimitRange：为名称空间的资源设置其CPU和内存等系统级资源的数量限制等。', 17, 0, 0, '2020-12-10 22:14:41.791993', '2021-01-20 21:12:39.907753', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (24, 'yuml文件', '[TOC]\n\n# 一、yuml文档格式\n\n\n## 1. 注释\n\n使用#作为注释，YAML中只有行注释。\n\n## 2. 基本格式要求\n\n- YAML大小写敏感；\n- 使用缩进代表层级关系；\n- 缩进只能使用空格，不能使用TAB，不要求空格个数，只需要相同层级左对齐（一般2个或4个空格）\n\n## 3. YAML支持的数据结构\n\n- 对象：键值对集合，又称为映射、哈希、字典\n- 数组：一组按照次序排列的值，又称为序列、列表\n- 纯量：单个、不可再分的值\n\n## 4. k8s yaml 文件中字段类型\n\n- <Object> 对象类型\n\n```yaml\nmetadata：\n    name：\n  	namespace:\n```\n\n- <[]Object> 对象列表类型\n\n```yaml\n	containers:\n	-  name: name1\n	   images:\n	-  name: name2\n   	 images:\n```\n\n- <string> 字符串类型\n  namespace: default\n- <integer> 整型\n  replica: 3\n- <boolean> 布尔类型 true or false\n  hostIPC: false\n- <map[string]string>字符串嵌套\n  nodeSelector:\n      label: lablename\n- <[]> 列表类型\n\n```yaml\n写法1：\ncommand:\n	- \"string1\"\n	- \"string2\"\n写法二：\ncommand: [\"string1\",\"string2\",\"string3\"]\n```\n\n# 二、必要字段\n\n\n## 1. 模板\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n	name: dev\nspec:\n  finalizers:\n  - kubernetes\n```\n\n## 2. 字段说明\n\n| 字段       | 类型   | 说明                                                         |\n| ---------- | ------ | ------------------------------------------------------------ |\n| apiVersion | string | k8s API版本，可以使用kubectl api-versions查询                |\n| kind       | string | yaml文件定义的资源类型和角色，有Pod、Deployment、Endpoints、Service |\n| metadata   | object | 元数据对象                                                   |\n| spec       | object | 详细定义对象，用来描述所期望的对象应该具有的状态             |\n\n\n\n# 三、metadata字段\n\n| 字段      | 类型   | 说明                                                         |\n| --------- | ------ | ------------------------------------------------------------ |\n| namespace | string | 指定当前对象隶属的名称空间，默认值为default。                |\n| name：    | string | 设定当前对象的名称，在其所属的名称空间的同一类型中必须唯一。 |\n| labels：  | string | 用于标识当前对象的标签，键值数据，常被用作挑选条件。         |\n\n\n\n# 四、spec字段\n\n\n## 1. spec.containers[]：spec对象的容器列表定义\n\n| spec.containers[].name  | string | 定义容器的名字     |\n| ----------------------- | ------ | ------------------ |\n| spec.containers[].image | string | 定义用到的镜像名称 |\n\n| spec.containers[].imagePullPolicy | string | 定义镜像拉取策略 always：默认，每次都hub拉取 \nnever：仅使用本机镜像\n ifnotprestnt：本机没有就hub拉取 |\n| spec.containers[].command[] | list | 指定一个或多个容器启动命令 |\n| spec.containers[].args[] | list | 指定容器启动命令参数，Dockerfile中CMD参数 |\n| spec.containers[].workingDir | string | 指定容器工作目录 |\n\n\n\n## 2. spec.containers[].volumeMounts[]：指定容器内部的存储卷配置\n\n| spec.containers[].volumeMounts[].name      | string | 容器挂载的存储卷名称                         |\n| ------------------------------------------ | ------ | -------------------------------------------- |\n| spec.containers[].volumeMounts[].mountPath | string | 容器挂载的存储卷路径                         |\n| spec.containers[].volumeMounts[].readOnly  | string | 存储卷读写模式，ture或false， 默认为读写模式 |\n\n\n\n## 3. spec.containers[].ports[]：指定容器端口列表\n\n| spec.containers[].ports[].name          | string | 指定端口名称                                                 |\n| --------------------------------------- | ------ | ------------------------------------------------------------ |\n| spec.containers[].ports[].containerPort | string | 指定容器监听的端口号                                         |\n| spec.containers[].ports[].hostPort      | string | 指定容器所在主机监听的端口号，设置了hostPort后同一台主机无法启动相同副本 |\n| spec.containers[].ports[].protocol      | string | 指定端口协议，默认为TCP                                      |\n\n\n\n## 4. spec.containers[].env[]：指定容器运行前环境变量\n\n| spec.containers[].env[].name  | string | 指定环境变量名称 |\n| ----------------------------- | ------ | ---------------- |\n| spec.containers[].env[].value | string | 指定环境变量值   |\n\n\n\n## 5. spec.containers[].resources.limits：指定容器运行时资源的上限\n\n| spec.containers[].resources.limits.cpu    | string | 指定cpu限制，单位为core数    |\n| ----------------------------------------- | ------ | ---------------------------- |\n| spec.containers[].resources.limits.memory | string | 指定内存限制，单位为MIB，GIB |\n\n\n\n## 6. spec.containers[].resources.requests：指定容器启动和调度时资源的上限\n\n| spec.containers[].resources.requests.cpu    | string | 指定cpu限制，单位为core数    |\n| ------------------------------------------- | ------ | ---------------------------- |\n| spec.containers[].resources.requests.memory | string | 指定内存限制，单位为MIB，GIB |\n\n\n\n## 7. 探针\n\n| spec.containers[].livenessProbe                | object | 存活检测                                                     |\n| ---------------------------------------------- | ------ | ------------------------------------------------------------ |\n| spec.containers[].ReadinessProbe               | object | 就绪检测                                                     |\n| spec.containers[].检测方式.initialDelaySeconds | Int    | 初始化延迟的时间，监测从多久之后开始运行，单位是秒           |\n| spec.containers[].检测方式.timeoutSeconds      | Int    | 监测的超时时间，如果超过这个时长后，则认为监测失败           |\n| spec.containers[].检测方式.periodSeconds       | int    | 存活性探测的频度，显示为period属性，默认为10s，最小值为1     |\n| spec.containers[].检测方式.successThreshold    | int    | 处于失败状态时，探测操作至少连续多少次的成功才被认为是通过检测，显示为#success属性，默认值为1，最小值也为1。 |\n| spec.containers[].检测方式.failureThreshold：  | int    | 处于成功状态时，探测操作至少连续多少次的失败才被视为是检测不通过，显示为#failure属性，默认值为3，最小值为1。 |\n\n\n\n## 8. exec检测\n\n| spec.containers[].检测方式.exec         | Object | exec方式执行命令检测 |\n| --------------------------------------- | ------ | -------------------- |\n| spec.containers[].检测方式.exec.command | List   | 执行的检测命令依据   |\n\n\n\n## 9. httpGet检测\n\n| spec.containers[].检测方式.httpGet             | Object | http探针，依据状态码                                |\n| ---------------------------------------------- | ------ | --------------------------------------------------- |\n| spec.containers[].检测方式.httpGet.host        | string | 请求的主机地址，默认为Pod IP；                      |\n| spec.containers[].检测方式.httpGet.port        | string | 请求的端口，必选字段。                              |\n| spec.containers[].检测方式.httpGet.httpHeaders | Object | 自定义的请求报文首部                                |\n| spec.containers[].检测方式.httpGet.path        | string | 请求的HTTP资源路径，即URL                           |\n| spec.containers[].检测方式.httpGet.scheme      | string | 建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。 |\n\n\n\n## 10. tcpSocket检测\n\n| spec.containers[].检测方式.tcpSocket      | Object | tcpSocket检测，依据端口是否开放      |\n| ----------------------------------------- | ------ | ------------------------------------ |\n| spec.containers[].检测方式.tcpSocket.host | string | 请求连接的目标IP地址，默认为Pod IP。 |\n| spec.containers[].检测方式.tcpSocket.port | string | 请求连接的目标端口，必选字段。       |\n\n\n\n## 11. 其他spec\n\n| spec.restartPolicy    | string  | 定义Pod重启策略 always：pod一旦终止就重启 onfailure：pod只有以非零退出码终止时（正常结束退出码为0），才会重启 never：pod终止后，不会重启 |\n| --------------------- | ------- | ------------------------------------------------------------ |\n| spec.nodeSelector     | object  | 定义node的label过滤标签，以key:value指定                     |\n| spec.imagePullSecrets | object  | 定义pull镜像时，使用secret名称，以key:value指定              |\n| spec.hostNetwork      | Boolean | 定义是否使用主机网络模式，默认使用docker网桥，如果设置true无法启动相同副本 |\n\n\n\n# 五、控制器字段\n\n\n## 1. ReplicaSet\n\n| spec.replicas                  | integer | 期望的Pod对象副本数                    |\n| ------------------------------ | ------- | -------------------------------------- |\n| spec.selector                  | Object  | 当前控制器匹配Pod对象副本的标签选择器  |\n| spec.selector.matchLabels      | string  | matchLabels标签选择器                  |\n| spec.selector.matchExpressions | string  | matchExpressions标签选择器             |\n| spec.template                  | Object  | 用于补足Pod副本数量时使用的Pod模板资源 |\n\n\n\n## 2. Deployment\n\n| spec.strategy.rollingUpdate. maxSurge      | integer/percent | 升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是一个期望值的百分比 |\n| ------------------------------------------ | --------------- | ------------------------------------------------------------ |\n| spec.strategy.rollingUpdate.maxUnavailable | integer/percent | 升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比 |\n| spec.revisionHistoryLimit                  | integer         | 控制器可保存的历史版本数量                                   |\n| spec.minReadySeconds                       | integer         | 新建的Pod对象，在启动后的多长时间内如果其容器未发生崩溃等异常情况即被视为“就绪”；默认为0秒，表示一旦就绪性探测成功，即被视作可用 |\n\n\n\n## 3. DaemonSet控制器\n\n- .spec.selector 是必填字段，且指定该字段时，必须与.spec.template.metata.labels 字段匹配（不匹配的情况下创建 DaemonSet将失败）。DaemonSet 创建以后，.spec.selector字段就不可再修改。如果修改，可能导致不可预见的结果。\n\n## 4. Job控制器\n\n- Pod模板中的spec.restartPolicy默认为“Always”，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为“Never”或“OnFailure”。\n\n| .spec.parallelism | integer | 能够定义作业执行的并行度，将其设置为2或者以上的值即可实现并行多队列作业同时运行 |\n| ----------------- | ------- | ------------------------------------------------------------ |\n| spec.completions  | integer | 使用的是默认值1，则表示并行度即作业总数                      |\n\n\n\n- 将.spec.completions属性值设置为大于.spec.parallelism的属性值，则表示使用多队列串行任务作业模式\n\n| .spec.activeDeadlineSeconds | integer | Job的deadline，用于为其指定最大活动时间长度，超出此时长的作业将被终止。 |\n| --------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.backoffLimit          | integer | 将作业标记为失败状态之前的重试次数，默认值为6。              |\n\n\n\n## 5. CronJob控制器\n\n| jobTemplate                      | Object               | Job控制器模板，用于为CronJob控制器生成Job对象；必选字段      |\n| -------------------------------- | -------------------- | ------------------------------------------------------------ |\n| schedule                         | string               | Cron格式的作业调度运行时间点；必选字段。                     |\n| concurrencyPolicy                | string               | 并发执行策略，可用值有“Allow”（允许）、“Forbid”（禁止）和“Replace”（替换），用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 |\n| failedJobHistoryLimit            | integer              | 为失败的任务执行保留的历史记录数，默认为1。                  |\n| successfulJobsHistoryLimit       | integer              | 为成功的任务执行保留的历史记录数，默认为3                    |\n| startingDeadlineSeconds          | integer              | 因各种原因缺乏执行作业的时间点所导致的启动作业错误的超时时长，会被记入错误历史记录 |\n| suspend                          | boolean              | 是否挂起后续的任务执行，默认为false，对运行中的作业不会产生影响。 |\n| .spec.successfulJobsHistoryLimit | integer              | 保留多少完成的 Job。默认没有限制，所有成功和失败的 Job 都会被保留。 当运行一个 Cron Job 时，Job 可以很快就堆积很多，推荐设置这两个字段的值。设置限制的值为 0，相关类型的 Job 完成后将不会被保留。 |\n| .spec.failedJobsHistoryLimit     | integer              | 保留多少失败的 Job                                           |\n| .spec.concurrencyPolicy          | Allow/Forbid/Replace | 属性控制作业并存的机制，其默认值为“Allow”，即允许前后Job，甚至属于同一个CronJob的更多Job同时运行。 “Forbid”用于禁止前后两个Job同时运行，如果前一个尚未结束，后一个则不予启动（跳过）， “Replace”用于让后一个Job取代前一个，即终止前一个并启动后一个。 |\n\n\n\n## 6. PDB\n\n| .spec.selector       | Object | 当前PDB对象使用的标签选择器，一般是与相关的Pod控制器使用同一个选择器。 |\n| -------------------- | ------ | ------------------------------------------------------------ |\n| .spec.minAvailable   | string | Pod自愿中断的场景中，至少要保证可用的Pod对象数量或比例，要阻止任何Pod对象发生自愿中断，可将其设置为100% |\n| .spec.maxUnavailable | string | Pod自愿中断的场景中，最多可转换为不可用状态的Pod对象数量或比例，0值意味着不允许Pod对象进行自愿中断；此字段与minAvailable互斥 |\n\n\n\n# 六、service\n\n| .spec.selector:        | Object  | 当前svc使用的标签选择器，用来管理pod中一样的标签资源。 |\n| ---------------------- | ------- | ------------------------------------------------------ |\n| .spec.type             | string  | service类型                                            |\n| .spec.clusterIP        | string  | 虚拟服务IP地址                                         |\n| .spec.ExternalName     | string  | ExternalName模式域名                                   |\n| .spec.sessionAffinity  | string  | 是否支持session会话绑定                                |\n| .spec.ports.name       | string  | 端口名称                                               |\n| .spec.ports.protocol   | string  | 端口协议，默认tcp                                      |\n| .spec.ports.port       | integer | 服务监听端口号                                         |\n| .spec.ports.targetPort | integer | 转发到后端的服务端口号                                 |\n| .spec.ports.nodePort   | integer | nodeport模式绑定物理主机端口                           |\n\n\n\n# 七、ingress\n\n| .spec.rules               | Object  | 用于定义当前Ingress资源的转发规则列表                        |\n| ------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.rules.host          | string  | 指定访问地址，留空表示通配所有的主机名                       |\n| .spec.backend             | Object  | 为负载均衡器指定一个全局默认的后端                           |\n| .spec.backend.serviceName | string  | 流量转发的后端目标Service资源的名称                          |\n| .spec.backend.servicePort | integer | 流量转发的后端目标Service资源的端口                          |\n| .spec.tls                 | Object  | TLS配置，目前仅支持通过默认端口443提供服务                   |\n| .spec.tls.hosts           | string  | 使用的TLS证书之内的主机名称，此处使用的主机名必须匹配tlsSecret中的名称。 |\n| .spec.tls.secretName      | string  | SSL会话的secret对象名称，在基于SNI实现多主机路由的场景中，此字段为可选。 |\n\n\n\n# 八、存储\n\n\n## 1. configmap\n\n| .spec.volumes.configMap.items.key  | string  | 要引用的键名称，必选字段                                     |\n| ---------------------------------- | ------- | ------------------------------------------------------------ |\n| .spec.volumes.configMap.items.path | string  | 对应的键于挂载点目录中生成的文件的相对路径，可以不同于键名称，必选字段 |\n| .spec.volumes.configMap.items.mode | integer | 文件的权限模型，可用范围为0到0777。                          |\n\n', 9, 0, 0, '2020-12-10 22:16:43.255432', '2021-01-19 23:28:09.796097', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (25, 'k8s yaml字段大全', '\n![yaml字段大全.svg](https://cdn.nlark.com/yuque/0/2020/svg/2308212/1607519437815-a6cc95c7-f533-40d6-bbd7-424ab8dbf64f.svg#align=left&display=inline&height=8106&margin=%5Bobject%20Object%5D&name=yaml%E5%AD%97%E6%AE%B5%E5%A4%A7%E5%85%A8.svg&originHeight=8106&originWidth=2130&size=5406682&status=done&style=none&width=2130)', 13, 0, 0, '2020-12-10 22:17:48.517747', '2021-01-08 07:49:47.998923', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (26, '管理Namespace资源', '[TOC]\n\n# 一、简介\n\n\n1. 名称空间（Namespace）是Kubernetes集群级别的资源，用于将集群分隔为多个隔离的逻辑分区以配置给不同的用户、租户、环境或项目使用，例如，可以为development、qa和production应用环境分别创建各自的名称空间。\n1. Kubernetes的绝大多数资源都隶属于名称空间级别（另一个是全局级别或集群级别），名称空间资源为这类的资源名称提供了隔离的作用域，同一名称空间内的同一类型资源名必须是唯一的，但跨名称空间时并无此限制。不过，Kubernetes还是有一些资源隶属于集群级别的，如Node、Namespace和PersistentVolume等资源，它们不属于任何名称空间，因此资源对象的名称必须全局唯一\n\n\n\n# 二、Namespaces 的常用操作\n\n\n1. 查看命名空间\n   `# kubectl get namespaces ` \n2. Kubernetes默认有三个命名空间\n    default:默认的命名空间\n    kube-system:由Kubernetes系统对象组成的命名空间\n    kube-public:该空间由系统自动创建并且对所有用户可读性，做为集群公用资源的保留命名空间\n3. 查看特定名称空间的详细信息\n   `# kubectl describe namespaces default ` \n3. 创建命名空间\n   `# kubectl create namespace test-cluster ` \n3. 查询命名空间中的资源\n   `# kubectl get all --namespace=test-cluster ` \n   `# kubectl get all -n test-clutser ` \n   `# kubectl get nodes ` \n   `# kubectl get pods -n kube-system ` \n3. 修改默认的namespace配置\n   `# kubectl config view`\n- 先查看是否设置了current-context\n  `# kubectl config set-context default --namespace=bs-test` \n- 设置default配置的namespace参数\n  `# kubectl config set current-context default`  //设置当前环境变量为 default\n- 通过这段代码设置默认的命名空间后，就不用每次在输入命令的时候带上--namespace参数了。\n\n\n\n# 三、其他操作\n\n\n1. 查看命名空间中的资源。\n   `# kubectl api-resources --namespaced=true ` \n1. 查看不在命名空间中的资源\n   `# kubectl api-resources --namespaced=false ` ', 10, 0, 0, '2020-12-10 22:19:14.620701', '2021-01-17 15:29:48.800096', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (27, '标签与标签选择器', '[TOC]\n\n# 一、标签概述\n\n\n1. 标签就是“键值”类型的数据，它们可于资源创建时直接指定，也可随时按需添加于活动对象中，而后即可由标签选择器进行匹配度检查从而完成资源挑选。一个对象可拥有不止一个标签，而同一个标签也可被添加至多个资源之上。\n1. 为资源附加多个不同纬度的标签以实现灵活的资源分组管理功能，例如，版本标签、环境标签、分层架构标签等，用于交叉标识同一个资源所属的不同版本、环境及架构层级等\n\n\n\n# 二、管理标签资源\n\n\n1. 创建资源时，可直接在其metadata中嵌套使用“labels”字段以定义要附加的标签项。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126204022-ef2634c8-2778-44a2-b01f-612421a11e08.png#align=left&display=inline&height=262&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=262&originWidth=304&size=353797&status=done&style=none&width=304)\n\n2. 在“kubectl get pods”命令中使用“--show-labels”选项，以额外显示对象的标签信息：\n   $ kubectl get pods --show-labels\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126243588-ce05cf44-b574-475d-9cd9-4297f2734d7a.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=907&size=353797&status=done&style=none&width=907)\n\n3. 按需进行添加标签操作。\n   $ kubectl label pods/pod-with-labels version=v1\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126267070-315fe539-5f71-41be-875b-17665cd15740.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=1051&size=353797&status=done&style=none&width=1051)\n\n4. 对于已经附带了指定键名的标签，使用“--overwrite”命令以强制覆盖原有的键值\n   $ kubectl label pods/pod-with-labels version=v2 --overwrite\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126304496-c1da9364-ba38-4fbd-9304-7d0534a0a15e.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1050&size=353797&status=done&style=none&width=1050)\n\n5. 删除指定键名的标签，使用“标签名-”，即可删除\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604126350783-a6297040-652a-4d19-bb52-27847a7cd3a5.png#align=left&display=inline&height=274&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=274&originWidth=930&size=353797&status=done&style=none&width=930)\n\n\n# 三、标签选择器\n\n\n1. Kubernetes API目前支持两个选择器：基于等值关系（equality-based）以及基于集合关系（set-based）。\n1. 基于等值关系的标签选择器的可用操作符有“=”“==”和“!=”三种，其中前两个意义相同，都表示“等值”关系，最后一个表示“不等”关系\n1. “kubectl get”命令的“-l”选项能够指定使用标签选择器，例如，显示键名env的值不为qa的所有Pod对象：\n   $ kubectl get pods -l \"env! =qa\" -L env\n1. 基于集合关系的标签选择器，它们的使用格式及意义具体如下。\n\n- KEY in (VALUE1, VALUE2, …)：指定的键名的值存在于给定的列表中即满足条件。\n- KEY notin (VALUE1, VALUE2, …)：指定的键名的值不存在于给定的列表中即满足条件。\n- KEY：所有存在此键名标签的资源。\n- ! KEY：所有不存在此键名标签的资源。\n  例如，显示标签键名env的值为production或dev的所有Pod对象：\n  `$ kubectl get pods -l \"env in (production, dev)\" -L env` \n\n5. Kubernetes的诸多资源对象必须以标签选择器的方式关联到Pod资源对象，例如Service、Deployment和ReplicaSet类型的资源等，它们在spec字段中嵌套使用嵌套的“selector”字段，通过“matchLabels”来指定标签选择器，有的甚至还支持使用“matchExpressions”构造复杂的标签选择机制。\n\n- matchLabels：通过直接给定键值对来指定标签选择器。\n- matchExpressions：基于表达式指定的标签选择器列表，每个选择器都形如“{key:KEY_NAME,operator: OPERATOR, values: [VALUE1, VALUE2,…]}”，选择器列表间为“逻辑与”关系；使用In或NotIn操作符时，其values不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空。\n\n\n\n# 四、Pod节点选择器nodeSelector\n\n\n1. Pod节点选择器是标签及标签选择器的一种应用，它能够让Pod对象基于集群中工作节点的标签来挑选倾向运行的目标节点。\n1. Pod对象的spec.nodeSelector可用于定义节点标签选择器，用户事先为特定部分的Node资源对象设定好标签，而后配置Pod对象通过节点标签选择器进行匹配检测，从而完成节点亲和性调度。\n1. 为Node资源对象附加标签的方法同Pod资源，使用“kubectl label nodes/NODE”命令即可。\n\n- 例如，可为node2节点设置“disktype=ssd”标签以标识其拥有SSD设备：\n  $ kubectl label nodes node2 disktype=ssd\n- 查看具有键名SSD的标签的Node资源：\n  $ kubectl get nodes -l \'disktype\' -L disktype\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127096983-ebf1cade-2be2-4b56-96ff-c70e5f5cb0f4.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=693&size=353797&status=done&style=none&width=693)\n\n4. 如果某Pod资源需要调度至这些具有SSD设备的节点之上，那么只需要为其使用spec.nodeSelector标签选择器即可，例如下面的资源清单文件pod-with-nodeselector.yaml示例：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n	name: pod-with-nodeselector\n	labels:\n		env: testing\nspec:\n	containers:\n	- name: myapp\n		image: busybox\n	nodeSelector:\n		disktype: ssd\n```\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127214235-d7685357-a292-43ec-9842-df7717ecf03a.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=831&size=353797&status=done&style=none&width=831)', 22, 0, 0, '2020-12-10 22:21:17.699301', '2021-01-21 03:58:38.467273', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (28, 'Pod资源对象', '[TOC]\n\n# 一、Pod概念的产生\n\n\n1. 在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。\n1. 而Kubernetes项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。\n1. 分别运行于各自容器的进程之间无法实现基于IPC的通信机制，此时，容器间的隔离机制对于依赖于此类通信方式的进程来说却又成了阻碍。Pod资源抽象正是用来解决此类问题\n\n# 二、Pod特点\n\n\n1. 在 Kubernetes项目里， Pod的实现需要使用一个中间容器，这个容器叫作Infra容器。在这个 Pod中，Infra容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace的方式，与 Infra容器关联在一起。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604127738905-9d9b0827-feae-4f96-9ece-cca30a0315bd.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=321&originWidth=533&size=353797&status=done&style=none&width=533)\n\n2. 对于 Pod 里的容器 A 和容器 B 来说：\n\n    它们可以直接使用 localhost 进行通信；\n    它们看到的网络设备跟 Infra 容器看到的完全一样；\n    一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP地址；\n    其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；\n    Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。\n\n3. 把整个虚拟机想象成为一个\n   Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。\n\n\n\n# 三、Pod模型\n\n\n1. Sidecar\n   pattern（边车模型或跨斗模型）：即为Pod的主应用容器提供协同的辅助应用容器，每个应用独立运行\n最为典型的代表是将主应用容器中的日志使用agent收集至日志服务器中时，可以将agent运行为辅助应用容器，即sidecar。另一个典型的应用是为主应用容器中的database server启用本地缓存。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128061676-353480da-9726-418a-98c7-d6c208cfaed8.png#align=left&display=inline&height=225&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=225&originWidth=526&size=353797&status=done&style=none&width=526)\n\n2. Ambassador\n   pattern（大使模型）：即为远程服务创建一个本地代理，代理应用运行于容器中，主容器中的应用通过代理容器访问远程服务\n一个典型的使用示例是主应用容器中的进程访问“一主多从”模型的远程Redis应用时，可在当前Pod容器中为Redis服务创建一个Ambassador container，主应用容器中的进程直接通过localhost接口访问Ambassador container即可。即便是Redis主从集群架构发生变动时，也仅需要将Ambassador container加以修改即可，主应用容器无须对此做出任何反应。\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128174063-7f3ab819-1d02-4d4f-bb7f-6dbebe1eef8d.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=531&size=353797&status=done&style=none&width=531)\n\n3. Adapter\n   pattern（适配器模型）：此种模型一般用于将主应用容器中的内容进行标准化输出，\n    例如，日志数据或指标数据的输出，这有助于调用者统一接收数据的接口，另外，某应用滚动升级后的版本不兼容旧的版本时，其报告信息的格式也存在不兼容的可能性，使用Adapter\n  container有助于避免那些调用此报告数据的应用发生错误。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128214702-12cfa4b4-29ac-4bff-b4cc-04251107cddf.png#align=left&display=inline&height=187&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=187&originWidth=462&size=353797&status=done&style=none&width=462)\n\n4. Kubernetes系统的Pod资源对象用于运行单个容器化应用，此应用称为Pod对象的主容器（main container），同时Pod也能容纳多个容器，不过额外的容器一般工作为Sidecar模型，用于辅助主容器完成工作职能。\n\n\n\n# 四、yaml文件Pod与Container对象\n\n\n1. 基本原则\n\n    凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的，凡是跟容器的Linux Namespace 相关的属性，也一定是 Pod 级别的，凡是 Pod中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义\n\n2. 应用举例\n\n- NodeSelector：供用户将 Pod 与 Node 进行绑定\n- HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容\n- shareProcessNamespace=true：这个 Pod 里的容器要共享 PID Namespace', 8, 0, 0, '2020-12-10 22:23:41.767022', '2021-01-11 23:15:12.706922', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (29, 'Pod生命周期', '[TOC]\n\n# 一、Pod状态\n\n\n1. Pod 生命周期的变化，主要体现在 Pod API 对象的 Status 部分，这是它除了Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod的当前状态，它有如下几种可能的情况：\n\n- Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。\n- Running。这个状态下，Pod已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。\n- Succeeded。这个状态意味着，Pod里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。\n- Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0的返回码）退出。这个状态的出现，意味着你得想办法 Debug这个容器的应用，比如查看 Pod 的 Events 和日志。\n- Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给kube-apiserver，这很有可能是主从节点（Master 和Kubelet）间的通信出现了问题。\n\n# 二、Pod创建过程\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604128919581-117903ea-eb30-41e5-bdee-7547aa1dadc6.png#align=left&display=inline&height=524&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=524&originWidth=790&size=353797&status=done&style=none&width=790)\n\n1. 用户通过kubectl或其他API客户端提交Pod Spec给API Server。\n1. API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。\n1. API Server开始反映etcd中的状态变化。\n1. 所有的Kubernetes组件均使用“watch”机制来跟踪检查API Server上的相关的变动。\n1. kube-scheduler（调度器）通过其“watcher”觉察到API Server创建了新的Pod对象但尚未绑定至任何工作节点。\n1. kube-scheduler为Pod对象挑选一个工作节点并将结果信息更新至API Server。\n1. 调度结果信息由API Server更新至etcd存储系统，而且API Server也开始反映此Pod对象的调度结果。\n1. Pod被调度到的目标工作节点上的kubelet尝试在当前节点上调用Docker启动容器，并将容器的结果状态回送至API Server。\n1. API Server将Pod状态信息存入etcd系统中。\n1. 在etcd确认写入操作成功完成后，API Server将确认信息\n\n\n\n# 三、Pod生命周期中行为\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129048585-fe284a47-712e-4210-99b0-9ced83bbd745.png#align=left&display=inline&height=537&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=537&originWidth=1052&size=353797&status=done&style=none&width=1052)\n\n\n# 四、初始化容器\n\n\n1. 初始化容器（init container）即应用程序的主容器启动之前要运行的容器，常用于为主容器执行一些预置操作，它们具有两种典型特征。\n\n    初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么Kubernetes需要重启它直到成功完成。\n    每个init容器都必须在下一个init容器启动之前成功完成\n\n2. 初始化容器作用\n\n    包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的\n    包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要FROM另一个镜像，只需要在安装过程中使用类似sed、awk、python或dig这样的工具。\n    应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。\n    Init容器使用LinuxNamespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问Secret的权限，而应用程序容器则不能。\n    它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以Init容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。\n\n3. 特殊说明\n\n- 在Pod启动过程中，Init容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出\n- 如果由于运行时或失败退出，将导致容器启动失败，它会根据Pod的restartPolicy指定的策略进行重试。然而，如果Pod的restartPolicy设置为Always，Init容器失败时会使用RestartPolicy策略\n- 在所有的Init容器没有成功之前，Pod将不会变成Ready状态。Init容器的端口将不会在Service中进行聚集。正在初始化中的Pod处于Pending状态，但应该会将Initializing状态设置为true\n- 如果Pod重启，所有Init容器必须重新执行\n- 对Init容器spec的修改被限制在容器image字段，修改其他字段都不会生效。更改Init容器的image字段，等价于重启该Pod\n- Init容器具有应用容器的所有字段。除了readinessProbe，因为Init容器无法定义不同于完成（completion）的就绪（readiness）之外的其他状态。这会在验证过程中强制执行\n- 在Pod中的每个app和Init容器的名称必须唯一；与任何其它容器共享同一个名称，会在验证时抛出错误\n\n\n\n# 五、生命周期钩子函数\n\n\n1. 容器生命周期钩子使它能够感知其自身生命周期管理中的事件，并在相应的时刻到来时运行由用户指定的处理程序代码。Kubernetes为容器提供了两种生命周期钩子。\n\n    postStart：于容器创建完成之后立即运行的钩子处理器（handler）\n    preStop：于容器终止操作之前立即运行的钩子处理器，它以同步的方式调用，因此在其完成之前会阻塞删除容器的操作的调用。\n\n2. 钩子处理器的实现方式有“Exec”和“HTTP”两种，\n\n    exec：执行一段命令\n    HTTP：发送HTTP请求\n\n\n\n# 六、容器探测\n\n\n1. kubelet对容器周期性执行的健康状态诊断，诊断操作由容器的处理器（handler）进行定义。Kubernetes支持三种处理器用于Pod探测。\n\n    ExecAction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。\n    TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。\n    HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起HTTP GET请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。\n\n2. 每次探测都将获得以下三种结果之一：\n\n    成功：容器通过了诊断。\n    失败：容器未通过诊断。\n    未知：诊断失败，因此不会采取任何行动\n\n\n\n# 七、容器的重启策略\n\n\n1. 容器程序发生崩溃或容器申请超出限制的资源等原因都可能会导致Pod对象的终止，此时是否应该重建该Pod对象则取决于其重启策略（restartPolicy）属性的定义。\n\n    Always：但凡Pod对象终止就将其重启，此为默认设定。\n    OnFailure：仅在Pod对象出现错误时方才将其重启。\n    Never：从不重启。\n\n2. 需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一节点上重新启动Pod对象的相关容器。首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么终止，直到节点发生故障或被删除。\n\n\n\n# 八、存活性检测\n\n\n1. 用于判定容器是否处于“运行”（Running）状态；一旦此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为“Success”。\n1. 设置exec探针：通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，若命令状态返回值为0则表示“成功”通过检测，其值均为“失败”状态。\n\n- 示例：使用busybox镜像创建/tem/headthy文件，1分钟后删除。如果文件存在，检测通过，否则为失败\n- 定义liveness-exec.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129450300-7de6b935-0f7d-4304-9695-68ca88d9536f.png#align=left&display=inline&height=303&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=303&originWidth=922&size=353797&status=done&style=none&width=922)\n\n- 在60秒之内使用“kubectl describe pods/liveness-exec”查看其详细信息，其存活性探测不会出现错误。而超过60秒之后，再次运行“kubectl describe pods/liveness-exec”查看其详细信息可以发现，存活性探测出现了故障\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129556467-7e1ed46a-62a7-43bf-b07a-9b56ebf0fd63.png#align=left&display=inline&height=257&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=257&originWidth=1252&size=353797&status=done&style=none&width=1252)\n\n- 待容器重启完成后再次查看，容器已经处于正常运行状态，直到文件再次被删除，存活性探测失败而重启。从下面的命令显示可以看出，liveness-exec已然重启\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129581406-ffa73e9c-0e70-421e-befa-0ee84a11f274.png#align=left&display=inline&height=76&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=76&originWidth=563&size=353797&status=done&style=none&width=563)\n\n3. 设置HTTP探针\n   基于HTTP的探测（HTTPGetAction）向目标容器发起一个HTTP请求，根据其响应码进行结果判定。“spec.containers.livenessProbe.httpGet”字段用于定义此类检测，它的可用配置字段包括如下几个。\n\n- host <string>：请求的主机地址，默认为Pod IP\n- port <string>：请求的端口，必选字段。\n- httpHeaders <[]Object>：自定义的请求报文首部。\n- path <string>：请求的HTTP资源路径，即URL path。\n- scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。\n- 示例：通过nginx生成一个index.html页面文件，请求的资源路径为“/index.html”，地址默认为Pod\n  IP，端口使用了容器中定义的端口名称HTTP，然后删除文件，再次查看pod信息\n- 定义liveness-http.yaml文件\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129720758-fa6f5e9c-a6f6-4159-9ab5-b4838a6c27d7.png#align=left&display=inline&height=350&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=350&originWidth=391&size=353797&status=done&style=none&width=391)\n\n- 而后查看其健康状态检测相关的信息，健康状态检测正常时，容器也将正常运行：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129771625-6781a43c-b156-4d15-960f-2b0ce778fbb8.png#align=left&display=inline&height=187&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=187&originWidth=1241&size=353797&status=done&style=none&width=1241)\n\n- 接下来借助于“kubectl exec”命令删除index.html测试页面：\n  `$ kubectl exec liveness-httpget rm /usr/share/nginx/html/index.html` \n- 而后再次使用“kubectl describe pod/liveness-httpget”查看其详细的状态信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129802054-f380b57f-9fb4-45e9-9f86-e1a6d31b7d1e.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=1252&size=353797&status=done&style=none&width=1252)\n\n4. 设置TCP探针\n   向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。“spec.containers.livenessProbe.tcpSocket”字段用于定义此类检测，它主要包含以下两个可用的属性。\n\n- host <string>：请求连接的目标IP地址，默认为Pod IP。\n- port <string>：请求连接的目标端口，必选字段。\n- 下面是一个定义在资源清单文件liveness-tcp.yaml中的示例，它向Pod IP的80/tcp端口发起连接请求，并根据连接建立的状态判定测试结果：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129833989-50bb841c-6c68-47eb-b5e1-7d0fadefce20.png#align=left&display=inline&height=328&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=328&originWidth=352&size=353797&status=done&style=none&width=352)\n\n\n# 九、就绪性检测\n\n\n1. 用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着其尚未准备就绪，端点控制器（如Service对象）会将其IP从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中。\n1. 与存活性探测机制相同，就绪性探测也支持Exec、HTTP GET和TCP Socket三种探测方式。\n1. 与存活性探测触发的操作不同的是，探测失败时，就绪性探测不会杀死或重启容器以保证其健康性，而是通知其尚未就绪，并触发依赖于其就绪状态的操作（例如，从Service对象中移除此Pod对象）以确保不会有客户端请求接入此Pod对象。', 7, 0, 0, '2020-12-10 22:26:38.449553', '2021-01-08 07:50:58.706566', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (30, '资源需求与限制', '[TOC]\n\n# 一、资源\n\n\n1. CPU：1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1core）相当于1000个微核心（millicores），因此500m相当于是0.5个核心，即二分之一个核心。\n1. 内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K作为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。\n\n# 二、资源需求\n\n\n1. “requests”属性定义其请求的确保可用值，即容器运行可能用不到这些额度的资源，但用到时必须要确保有如此多的资源可用\n1. 示例\n\n- 定义一个Pod中stress容器，确保128Mi的内存及五分之一个CPU核心（200m）资源可用，\n- 它运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时它也会尽可能多地占用CPU资源\n- 另外再启动一个专用的CPU压力测试进程（-c 1）。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604129999463-c42e210a-efd4-489c-961a-4fa735cf04c2.png#align=left&display=inline&height=309&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=309&originWidth=825&size=353797&status=done&style=none&width=825)\n\n- 查看资源使用情况\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130015373-6098717c-cb94-4248-9acc-a868a327098a.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=894&size=353797&status=done&style=none&width=894)\n\n- 每个测试进程CPU占用率为25%，内存占用为262m，此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用的范围内尽量多地占用相关的资源\n\n\n\n# 三、资源限制\n\n\n1. ”limits”属性则用于限制资源可用的最大值，即硬限制\n1. 示例：一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而导致“OOMKillled”为止：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130039468-61e8c5a9-6fe0-41d6-9bd2-5d493eca5695.png#align=left&display=inline&height=328&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=328&originWidth=373&size=353797&status=done&style=none&width=373)\n\n\n- Pod资源的默认重启策略为Always，于是在memleak因内存资源达到硬限制而被终止后会立即重启。不过，多次重复地因为内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制，即每次重启的时间间隔会不断地拉长。于是，用户看到的Pod资源的相关状态通常为“CrashLoopBackOff”：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604130056913-1d317d32-9210-4d62-bb94-392dc6a42c41.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=623&size=353797&status=done&style=none&width=623)', 8, 0, 0, '2020-12-10 22:27:48.793502', '2021-01-15 16:03:43.745572', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (31, 'Pod服务质量（优先级）', '\n\n1. Kubernetes允许节点资源对limits的过载使用，当节点无法同时满足其上的所有Pod对象以资源满载的方式运行。\n1. 在内存资源紧缺时，通过Pod对象的优先级完成先后终止哪些Pod对象判定。根据Pod对象的requests和limits属性，Kubernetes将Pod对象归类到BestEffort、Burstable和Guaranteed三个服务质量（Quality  of Service, QoS）类别下，具体说明如下。\n1. Guaranteed：每个容器都为CPU资源设置了具有相同值的requests和limits属性，以及每个容器都为内存资源设置了具有相同值的requests和limits属性的Pod资源会自动归属于此类别，这类Pod资源具有最高优先级。\n1. Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别要求的Pod资源将自动归属于此类别，它们具有中等优先级。\n1. BestEffort：未为任何一个容器设置requests或limits属性的Pod资源将自动归属于此类别，它们的优先级为最低级别。\n1. 内存资源紧缺时，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够在可用时做到尽可能多地占用资源。若已然不存任何BestEffort类别的容器，则接下来是有着中等优先级的Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod资源存在。\n\n', 10, 0, 0, '2020-12-10 22:28:17.161423', '2021-01-26 09:05:48.071093', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (32, 'Pod控制器', '[TOC]\n\n# 一、Pod控制器概述\n\n\n1. Master的各组件中，API Server仅负责将资源存储于etcd中，并将其变动通知给各相关的客户端程序，如kubelet、kube-scheduler、kube-proxy和kube-controller-manager等，kube-scheduler监控到处于未绑定状态的Pod对象出现时遂启动调度器为其挑选适配的工作节点。\n1. Kubernetes的核心功能之一还在于要确保各资源对象的当前状态（status）以匹配用户期望的状态（spec），使当前状态不断地向期望状态“和解”（reconciliation）来完成容器应用管理，而这些则是kube-controller-manager的任务。\n1. 创建为具体的控制器对象之后，每个控制器均通过API Server提供的接口持续监控相关资源对象的当前状态，并在因故障、更新或其他原因导致系统状态发生变化时，尝试让资源的当前状态向期望状态迁移和逼近。\n\n\n\n# 二、控制器与Pod对象\n\n\n1. Pod控制器资源通过持续性地监控集群中运行着的Pod资源对象来确保受其管控的资源严格符合用户期望的状态，例如资源副本的数量要精确符合期望等。\n1. 一个Pod控制器资源至少应该包含三个基本的组成部分。\n\n\n\n- 标签选择器：匹配并关联Pod资源对象，并据此完成受其管控的Pod资源计数。\n- 期望的副本数：期望在集群中精确运行着的Pod资源的对象数量。\n- Pod模板：用于新建Pod资源对象的Pod模板资源。\n\n\n\n# 三、Pod资源模板\n\n\n1. Pod模板的配置信息中不需要apiVersion和kind字段，但除此之外的其他内容与定义自主式Pod对象所支持的字段几乎完全相同，这包括metadata和spec及其内嵌的其他各个字段。\n1. Pod控制器类资源的spec字段通常都要内嵌replicas、selector和template字段，其中template即为Pod模板的定义。\n1. 下面是一个定义在Deployment资源中的模板资源示例：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604149660295-808544dd-968b-44c6-a2c7-d83a6599d800.png#align=left&display=inline&height=786&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=786&originWidth=953&size=353797&status=done&style=none&width=953)\n\n', 12, 0, 0, '2020-12-10 22:36:53.852069', '2021-01-09 16:41:02.809239', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (33, 'ReplicaSet控制器', '[TOC]\n\n# 一、概述\n\n\n1. 作用：用于确保由其管控的Pod对象副本数在任一时刻都能精确满足期望的数量。ReplicaSet控制器资源启动后会查找集群中匹配其标签选择器的Pod资源对象，当前活动对象的数量与期望的数量不吻合时，多则删除，少则通过Pod模板创建以补足。\n1. 功能：\n\n- 确保Pod资源对象的数量：ReplicaSet需要确保由其控制运行的Pod副本数量精确吻合配置中定义的期望值，否则就会自动补足所缺或终止所余。\n- 确保Pod健康运行：探测到由其管控的Pod对象因其所在的工作节点故障而不可用时，自动请求由调度器于其他工作节点创建缺失的Pod副本。\n- 弹性伸缩：业务规模因各种原因时常存在明显波动，在波峰或波谷期间，可以通过ReplicaSet控制器动态调整相关Pod资源对象的数量。\n- 通过HPA（HroizontalPodAutoscaler）控制器实现Pod资源规模的自动伸缩。\n\n\n\n# 二、创建ReplicaSet\n\n\n1. 可以使用YAML或JSON格式的清单文件定义其配置它的spec字段一般嵌套使用以下几个属性字段。\n\n| replicas        | integer | 期望的Pod对象副本数                                          |\n| --------------- | ------- | ------------------------------------------------------------ |\n| selector        | Object  | 当前控制器匹配Pod对象副本的标签选择器，支持matchLabels和matchExpressions两种匹配机制 |\n| template        | Object  | 用于补足Pod副本数量时使用的Pod模板资源                       |\n| minReadySeconds | integer | 新建的Pod对象，在启动后的多长时间内如果其容器未发生崩溃等异常情况即被视为“就绪”；默认为0秒，表示一旦就绪性探测成功，即被视作可用 |\n\n2. 示例：\n\n\n\n- 创建rs-example.yaml文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150057289-9c0e5277-dadb-413c-92e9-bf95932faf7d.png#align=left&display=inline&height=471&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=471&originWidth=355&size=353797&status=done&style=none&width=355)\n\n- 创建rs资源：\n  `$ kubectl apply -f rs-example.yaml` \n- 查看名称为\"rs-demo\"的pod资源\n  `$ kubectl get pods -l app=rs-demo` \n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150119870-500b735e-d7fc-4d37-9656-ca7e59f05869.png#align=left&display=inline&height=90&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=90&originWidth=573&size=353797&status=done&style=none&width=573)\n\n- 查看replicaset控制器资源状态\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150153396-7114ff2a-e5fd-4fa9-aaf5-055d722a2164.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=966&size=353797&status=done&style=none&width=966)\n\n\n# 三、 ReplicaSet管控Pod对象\n\n\n1. 缺少Pod副本\n\n> 任何原因导致的相关Pod对象丢失，都会由ReplicaSet控制器自动补足。\n\n- 手动删除上面列出的一个Pod对象\n  `$ kubectl delete pods rs-example-5ncrr` \n- 再次列出相关Pod对象的信息，可以看到rs-example-5ncrr被删除，而新的Pod对象rs-example-jfp4k被rs-example控制器创建：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150220585-57639cbe-c988-411d-b3e5-6b02d0e7ad08.png#align=left&display=inline&height=233&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=233&originWidth=604&size=353797&status=done&style=none&width=604)\n    强行修改隶属于控制器rs-example的Pod资源标签，会导致它不再被控制器作为副本计数，这也将触发控制器的Pod对象副本缺失补足机制。\n    例如，将rs-example-26fnb的标签app的值改为rs：\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150258258-7632160f-0e20-4519-a94d-c3898b9eeb4e.png#align=left&display=inline&height=251&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=251&originWidth=818&size=353797&status=done&style=none&width=818)\n\n2. 多出pod副本\n\n> 一旦被标签选择器匹配到的Pod资源数量因任何原因超出期望值，多余的部分都将被控制器自动删除。\n\n\n\n- 例如，为pod-example手动为其添加“app: rs-demo”标签：\n  `$ kubectl label pods rs-example-26fnb app=rs-demp --overwrite` \n- 再次列出相关的Pod资源，可以看到rs-example控制器启动了删除多余Pod的操作，pod-example正处于终止过程中：\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150440661-0a3cae78-9e65-47f1-a519-9bc724d88e49.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=869&size=353797&status=done&style=none&width=869)\n  这就意味着，任何自主式的或本隶属于其他控制器的Pod资源其标签变动的结果一旦匹配到了其他的副本数足额的控制器，就会导致这类Pod资源被删除。\n\n\n\n# 三、查看replicaset资源信息\n\n\n1. 查看所有replicaset（子资源）信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150483115-c206c0ca-55a1-432d-a673-ce20af5202fd.png#align=left&display=inline&height=175&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=175&originWidth=729&size=353797&status=done&style=none&width=729)\n\n2. 查看所有replicaset（子资源）详细信息\n   `$ kubectl describe replicasets` \n   `$ kubectl describe replicasets/rs-example` \n\n\n\n# 四、更新ReplicaSet控制器\n\n\n1. 更改Pod模板：升级应用\n   ReplicaSet控制器的Pod模板可随时按需修改，但它仅影响这之后由其新建的Pod对象，对已有的副本不会产生作用。但在用户逐个手动关闭其旧版本的Pod资源后就能以新代旧，实现控制器下应用版本的滚动升级。\n\n\n\n- 修改原ReplicaSet.yaml文件镜像\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604150699261-68a5e8ef-c8f1-462e-b37f-312974f4b97d.png#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=465&originWidth=349&size=353797&status=done&style=none&width=349) \n\n- apply文件，查看image信息\n  `kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152520384-d67836b0-b1a6-4f75-a93c-61cdf6edf676.png#align=left&display=inline&height=66&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=66&originWidth=340&size=353797&status=done&style=none&width=340)\n  rs-example管控的现存Pod对象使用的仍然是原来版本中定义的镜像\n- 删除pod后自动生成新版pod\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152554444-293ba76c-c8a1-4109-a4f9-0732607aa9c7.png#align=left&display=inline&height=156&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=156&originWidth=591&size=353797&status=done&style=none&width=591)\n\n\n2. 扩容与缩容\n   改动ReplicaSet控制器对象配置中期望的Pod副本数量（replicas字段）会由控制器实时做出响应，从而实现应用规模的水平伸缩。\n   kubectl还提供了一个专用的子命令scale用于实现应用规模的伸缩，它支持从资源清单文件中获取新的目标副本数量，也可以直接在命令行通过“--replicas”选项进行读取，\n\n- 将rs-example控制器的Pod副本数量提升至5个：\n  `$ kubectl scale replicasets rs-example --replicas=5` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152704085-4a04a44f-3c87-46de-8e07-6c1e664efef8.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=751&size=353797&status=done&style=none&width=751)\n- 由下面显示的rs-example资源的状态可以看出，将其Pod资源副本数量扩展至5个的操作已经成功完成：\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152735637-f7e6e92c-8fd1-4d01-b6a3-8fdfb7befb3d.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=110&originWidth=751&size=353797&status=done&style=none&width=751)\n- 收缩规模的方式与扩展相同，只需要明确指定目标副本数量即可。\n\n\n\n# 五、删除ReplicaSet控制器资源\n\n\n1. 使用kubectl delete命令删除ReplicaSet对象时默认会一并删除其管控的各Pod对象。\n   `$ kubectl delete replicasets rs-example` \n\n\n\n# 六、故障转移\n\n\n1. 目前有3个副本分别运行在node1和node2上。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152785665-9435a84e-0b0c-481a-883c-bfba05ac4192.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=963&size=353797&status=done&style=none&width=963)\n\n2. 现在模拟 k8s-node2 故障，关闭该节点。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152808782-980c4975-72b6-4e70-af69-d009d3bf013b.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=963&size=353797&status=done&style=none&width=963)\n\n3. 等待一段时间，Kubernetes 会检查到 k8s-node2 不可用，将 k8s-node2 上的 Pod 标记为 terminating 状态，并在 k8s-node1 上新创建两个 Pod，维持总副本数为 3。\n\n    ![clip_image005.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604152843254-c1dae6ef-9b20-4191-8af8-7b2d9d1e34e9.jpeg#align=left&display=inline&height=145&margin=%5Bobject%20Object%5D&name=clip_image005.jpg&originHeight=145&originWidth=554&size=18296&status=done&style=none&width=554)\n\n4. 当 k8s-node2 恢复后，terminating的 Pod 会被删除，不过已经运行的 Pod不会重新调度回 k8s-node2。\n\n    ![clip_image006.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604152887640-accca54f-ac4c-49e7-b132-1a766f7b3be2.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=clip_image006.png&originHeight=129&originWidth=514&size=9344&status=done&style=none&width=514)\n![clip_image008.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604152893646-ab1ac390-f5a8-4534-baf2-21a747c4d4df.jpeg#align=left&display=inline&height=102&margin=%5Bobject%20Object%5D&name=clip_image008.jpg&originHeight=102&originWidth=554&size=12510&status=done&style=none&width=554)', 9, 0, 0, '2020-12-10 22:45:40.177864', '2021-01-25 20:47:17.916289', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (34, 'Deployment控制器', '[TOC]\n\n# 一、Deployment\n\n\n1. Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet的属性。而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153315362-8e0e0d83-bebf-4fb2-a40f-a1ec140a1ee9.png#align=left&display=inline&height=341&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=341&originWidth=790&size=114249&status=done&style=none&width=790)\n\n2. Deployment具备ReplicaSet的全部功能，同时还增添了部分特性。\n\n- 事件和状态查看：必要时可以查看Deployment对象升级的详细进度和状态。\n- 回滚：升级操作完成后发现问题时，支持使用回滚机制将应用返回到前一个或由用户指定的历史记录中的版本上。\n- 版本记录：对Deployment对象的每一次操作都予以保存，以供后续可能执行的回滚操作使用。\n- 暂停和启动：对于每一次升级，都能够随时暂停和启动。\n- 多种自动更新方案：一是Recreate，即重建更新机制，全面停止、删除旧有的Pod后用新版本替代；另一个是RollingUpdate，即滚动升级机制，逐步替换旧有的Pod至新的版本。\n\n\n\n# 二、创建Deployment\n\n\n1. 除了控制器类型和名称之外，它与前面ReplicaSet控制器示例中的内容几乎没有什么不同。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153398464-73c3b45e-3a30-4d10-9172-4e0e440867b4.png#align=left&display=inline&height=464&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=464&originWidth=514&size=114249&status=done&style=none&width=514)\n\n2. 创建资源对象\n   `kubectl apply -f Deployment.yaml` \n2. 查看Deployment资源\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153432874-ee4ee178-774b-4e14-be71-dbac7888e084.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=572&size=114249&status=done&style=none&width=572)\n\n4. 查看ReplicaSets资源\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153451104-e1492757-4c1e-4236-ac4c-a4d7bcb92183.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=656&size=114249&status=done&style=none&width=656)\n\n5. 查看Pod资源\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153467807-65815c87-a3a4-4d4b-ac7d-0df5d2fcac7f.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=732&size=114249&status=done&style=none&width=732)\n\n6. 由此印证了Deployment借助于ReplicaSet管理Pod资源的机制，于是可以得知，其大部分管理操作与ReplicaSet相同\n\n# 三、更新\n\n\n1. 更新策略\n\n> Deployment控制器支持两种更新策略：滚动更新和重新创建。\n\n- 滚动更新是默认的更新策略，它在删除一部分旧版本Pod资源的同时，补充创建一部分新版本的Pod对象进行应用升级。\n- 重新创建首先删除现有的Pod对象，而后由控制器基于新模板重新创建出新版本资源对象。\n\n2. 滚动更新时，应用升级期间还要确保可用的Pod对象数量不低于某阈值以确保可以持续处理客户端的服务请求，变动的方式和Pod对象的数量范围将通过spec.strategy.rollingUpdate.maxSurge和spec.strategy.rollingUpdate.maxUnavailable两个属性协同进行定义，\n\n- maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；例如，如果期望值为3，当前的属性值为1，则表示Pod对象的总数不能超过4个。\n- maxUnavailable：升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；默认值为1，该值意味着如果期望值是3，则升级期间至少要有两个Pod对象处于正常提供服务的状态\n\n3. 为了保存版本升级的历史，需要在创建Deployment对象时于命令中使用“--record”选项。\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153634802-1f3bb9df-0a02-4446-aee2-16837ec514ea.png#align=left&display=inline&height=278&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=278&originWidth=722&size=132639&status=done&style=none&width=722)\n\n4. 使用命令临时更新镜像\n\n- 使用192.168.10.110/k8s/myapp:v2镜像文件修改Pod模板中的myapp容器，启动Deployment控制器的滚动更新\n  `$ kubectl set image deployments myapp-deploy myapp=192.168.10.110/k8s/myapp:v2` \n- 访问验证\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153671032-f135ccfe-6076-41be-b887-6653feb726d0.png#align=left&display=inline&height=254&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=254&originWidth=892&size=132639&status=done&style=none&width=892)\n\n5. 灰度发布（金色雀发布）\n\n- 待第一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一小部分新版本的应用，主体部分还是旧的版本。然后，再根据用户特征精心筛选出小部分用户的请求路由至新版本的Pod应用，并持续观察其是否能稳定地按期望的方式运行。\n- 采用首批添加1个Pod资源的方式。将Deployment控制器的maxSurge属性的值设置为1，并将maxUnavailable属性的值设置为0：\n  `$ kubectl patch deployments myapp-deploy -p \'{\"spec\":{\"strategy\":{\"rollingUpdate\": {\"maxSurge\": 1, \"maxUnavailable\":0}}}}\'` \n\n6. rollout pause和resume\n   kubectl rollout pause会用来停止触发下一次rollout，正在执行的滚动历程是不会停下来的，而是会继续正常的进行滚动，直到完成。等下一次，用户再次触发rollout时，Deployment就不会真的去启动执行滚动更新了，而是等待用户执行了kubectl rollout resume，流程才会真正启动执行。\n6. 灰度发布、滚动发布和蓝绿发布\n\n**假设replicaSet=10 maxSurge &maxUnavailable不能同时为0 **\n\n| 类型     | 描述                                               | maxSurge                 | maxUnavailable       |\n| -------- | -------------------------------------------------- | ------------------------ | -------------------- |\n| 灰度发布 | 又名金丝雀发布。先极个别更新，通过后再一次全部更新 | 1或10%                   | 视对服务可用度的需求 |\n| 滚动发布 | （部分更新，投入使用）* 直到全部更新完成           | 1<x<（具体看更新的粒度） | 视对服务可用度的需求 |\n| 蓝绿发布 | 新旧版共存，靠切换流量完成更新                     | 10                       | 0                    |\n\n# 四、回滚\n\n\n1. 将myapp-deploy回滚至此前的版本：\n   `$ kubectl rollout undo deployments myapp-deploy` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153850895-6ba381ca-45c9-4574-861d-749aa775727e.png#align=left&display=inline&height=271&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=271&originWidth=1217&size=132639&status=done&style=none&width=1217)\n\n2. 若要回滚到号码为1的revision记录，则使用如下命令即可完成：\n   `$ kubectl rollout undo deployments myapp-deploy --to-revision=1` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604153877271-28d59a3b-630c-49d9-bd88-0f585f681252.png#align=left&display=inline&height=164&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=164&originWidth=1222&size=132639&status=done&style=none&width=1222)\n\n- 回滚操作中，其revision记录中的信息会发生变动，回滚操作会被当作一次滚动更新追加进历史记录中，而被回滚的条目则会被删除。\n- 如果此前的滚动更新过程处于“暂停”状态，那么回滚操作就需要先将Pod模板的版本改回到之前的版本，然后“继续”更新，否则，其将一直处于暂停状态而无法回滚。', 10, 0, 0, '2020-12-10 22:51:32.978630', '2021-01-26 09:06:12.327972', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (35, 'DaemonSet控制器', '[TOC]\n\n# 一、简介\n\n\n1. 用于在集群中的全部节点上同时运行一份指定的Pod资源副本，后续新加入集群的工作节点也会自动创建一个相关的Pod对象，当从集群移除节点时，此类Pod对象也将被自动回收而无须重建。管理员也可以使用节点选择器及节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。\n1. DaemonSet是一种特殊的控制器，它有特定的应用场景，通常运行那些执行系统级操作任务的应用，其应用场景具体如下。\n\n- 运行集群存储的守护进程，如在各个节点上运行glusterd或ceph。\n- 在各个节点上运行日志收集守护进程，如fluentd和logstash。\n- 在各个节点上运行监控系统的代理守护进程，如Prometheus Node Exporter、collectd、Datadog agent、New Relic agent或Ganglia gmond等。\n\n# 二、创建DaemonSet资源对象\n\n\n1. 下面的资源清单，定义一个DaemonSet资源：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154069406-b2317718-9b12-4d64-b139-60b2bc376974.png#align=left&display=inline&height=507&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=507&originWidth=605&size=132639&status=done&style=none&width=605)\n\n- DaemonSet必须使用selector来匹配Pod模板中指定的标签，而且它也支持matchLabels和matchExpressions两种标签选择器。\n\n1. 创建资源对象\n   `$ kubectl apply -f filebeat-ds.yaml` \n1. 查看资源信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154128565-006da16b-8510-4365-a64f-ead2672e71c3.png#align=left&display=inline&height=185&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=185&originWidth=813&size=132639&status=done&style=none&width=813)\n\n\n# 三、更新DaemonSet对象\n\n\n> DaemonSet支持更新机制，相关配置定义在spec.update-Strategy嵌套字段中。目前，它支持RollingUpdate（滚动更新）和OnDelete（删除时更新）两种更新策略，滚动更新为默认的更新策略，工作逻辑类似于Deployment控制，不过仅支持使用maxUnavailabe属性定义最大不可用Pod资源副本数（默认值为1），而删除时更新的方式则是在删除相应节点的Pod资源后重建并更新为新版本。\n\n\n\n1. 将此前创建的filebeat-ds中Pod模板中的容器镜像升级\n   `$ kubectl set image daemonsets filebeat-daemonset filebeat=192.168.10.110/k8s/filebeat:5.6.6` \n1. 查看镜像信息\n   `$ kubectl get pod -o custom-columns=pod_name:metadata.name,pod_image:spec.containers[0].image` \n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154226654-76542800-4f32-40cd-b245-9f84c1e6ef4a.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=676&size=132639&status=done&style=none&width=676)\n\n', 9, 0, 0, '2020-12-10 22:55:13.009421', '2021-01-26 09:07:35.088135', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (36, 'Job控制器', '[TOC]\n\n# 一、简介\n\n\n1. Job控制器用于调配Pod对象运行一次性任务，容器中的进程在正常运行结束后不会对其进行重启，而是将Pod对象置于“Completed”（完成）状态。若容器中的进程因错误而终止，则需要依配置确定重启与否，未运行完成的Pod对象因其所在的节点故障而意外终止后会被重新调度。\n1. 有的作业任务可能需要运行不止一次，用户可以配置它们以串行或并行的方式运行。\n\n- 单工作队列（work queue）的串行式Job：即以多个一次性的作业方式串行执行多次作业，直至满足期望的次数；这次Job也可以理解为并行度为1的作业执行方式，在某个时刻仅存在一个Pod资源对象。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154406275-f1e9a768-9ef6-4fce-9ba7-0f39e8f752f5.png#align=left&display=inline&height=131&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=131&originWidth=528&size=132639&status=done&style=none&width=528)\n\n- 多工作队列的并行式Job：这种方式可以设置工作队列数，即作业数，每个队列仅负责运行一个作业；也可以用有限的工作队列运行较多的作业，即工作队列数少于总作业数，相当于运行多个串行作业队列。工作队列数即为同时可运行的Pod资源数。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154520208-c4617373-9f89-46de-ba37-51605af44dba.png#align=left&display=inline&height=263&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=263&originWidth=529&size=132639&status=done&style=none&width=529)\n\n\n# 二、创建Job对象\n\n\n1. Job控制器的spec字段内嵌的必要字段仅为template，它的使用方式与Deployment等控制器并无不同。Job会为其Pod对象自动添加“job-name=JOB_NAME”和“controller-uid=UID”标签，并使用标签选择器完成对controller-uid标签的关联。需要注意的是，Job位于API群组“batch/v1”之内。\n1. 定义了一个Job控制器：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154553161-b877a31a-6d56-45c6-9a0f-422441da367b.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=571&size=132639&status=done&style=none&width=571)\n\n- Pod模板中的spec.restartPolicy默认为“Always”，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为“Never”或“OnFailure”\n\n3. 创建资源\n   `$ kubectl apply -f job-example.yaml` \n3. 查看资源信息\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154632504-816d524f-03e8-4ebd-9624-5eb9fc5b5fa2.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=531&size=132639&status=done&style=none&width=531)\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154619453-46dc5077-807b-45be-a969-6296a8eeb289.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=611&size=132639&status=done&style=none&width=611)\n\n- 两分钟后，待sleep命令执行完成并成功退出后，Pod资源即转换为Completed状态，completions也从0/1变为1/1\n\n# 三、并行式Job\n\n\n1. spec.parallelism的值设置为1，并设置总任务数．spec.completion属性便能够让Job控制器以串行方式运行多任务。\n1. 串行运行5次任务的Job控制器示例：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154660694-4c82614b-2fd9-4ca7-bee0-ab81f15beb96.png#align=left&display=inline&height=305&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=305&originWidth=575&size=132639&status=done&style=none&width=575)\n\n3. 使用kubectl get pod -w监控其变动\n\n# 四、Job扩容\n\n\n1. Job控制器的.spec.parallelism定义的并行度表示同时运行的Pod对象数，此属性值支持运行时调整从而改变其队列总数，实现扩容和缩容。\n1. 例如在其运行过程中（未完成之前）将job-multi的并行度扩展为三路：\n   `$ kubectl scale jobs job-multi --replicas=3` \n1. 执行命令后可以看到，其同时运行的Pod对象副本数量立即扩展到了三个：\n   `$ kubectl get pods -l job-name=job-multi` \n\n# 五、删除Job\n\n1. Job控制器待其Pod资源运行完成后，将不再占用系统资源。用户可按需保留或使用资源删除命令将其删除。不过，如果某Job控制器的容器应用总是无法正常结束运行，而其restartPolicy又定为了重启，则它可能会一直处于不停地重启和错误的循环当中。所幸的是，Job控制器提供了两个属性用于抑制这种情况的发生，具体如下。\n\n- spec.activeDeadlineSeconds <integer>:Job的deadline，用于为其指定最大活动时间长度，超出此时长的作业将被终止。\n- spec.backoffLimit <integer>：将作业标记为失败状态之前的重试次数，默认值为6。\n\n2. 例如，下面的配置片断表示其失败重试的次数为5，并且如果超出100秒的时间仍未运行完成，那么其将被终止：\n\n```yaml\nspec:\n	backoffLimit: 5\n	activeDeadlineSeconds: 100\n```\n\n', 8, 0, 0, '2020-12-10 22:56:28.480346', '2021-01-26 09:08:15.825661', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (37, 'CronJob控制器', '[TOC]\n\n# 一、简介\n\n\n1. CronJob控制器用于管理Job控制器资源的运行时间。Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及重复运行的方式\n1. 具体如下。\n    在未来某时间点运行作业一次。\n    在指定的时间点重复运行作业。\n\n3. CronJob对象支持使用的时间格式类似于Crontab，略有不同的是，CronJob控制器在指定的时间点时，“?”和“*”的意义相同，都表示任何可用的有效值。\n\n# 二、创建CronJob对象\n\n\n1. CronJob控制器的spec字段可嵌套使用以下字段。\n\n| jobTemplate                | Object  | Job控制器模板，用于为CronJob控制器生成Job对象；必选字段      |\n| -------------------------- | ------- | ------------------------------------------------------------ |\n| schedule                   | string  | Cron格式的作业调度运行时间点；必选字段。                     |\n| concurrencyPolicy          | string  | 并发执行策略，可用值有“Allow”（允许）、“Forbid”（禁止）和“Replace”（替换），用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 |\n| failedJobHistoryLimit      | integer | 为失败的任务执行保留的历史记录数，默认为1。                  |\n| successfulJobsHistoryLimit | integer | 为成功的任务执行保留的历史记录数，默认为3                    |\n| startingDeadlineSeconds    | integer | 因各种原因缺乏执行作业的时间点所导致的启动作业错误的超时时长，会被记入错误历史记录 |\n| suspend                    | boolean | 是否挂起后续的任务执行，默认为false，对运行中的作业不会产生影响。 |\n\n\n\n2. 下面是一个定义在资源清单文件中的CronJob资源对象示例，它每隔2分钟运行一次由jobTemplate定义的简单任务：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154905848-2962f9ab-09ef-40fe-b2f1-a49d7da0535d.png#align=left&display=inline&height=555&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=555&originWidth=797&size=132639&status=done&style=none&width=797)\n\n3. 查看资源信息，命令结果中的SCHEDULE是指其调度时间点，SUSPEND表示后续任务是否处于挂起状态，即暂停任务的调度和运行，ACTIVE表示活动状态的Job对象的数量，而LAST SCHEDULE则表示上次调度运行至此刻的时长：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154926608-f72b89ca-04e5-464d-9282-0c1a1df12b04.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=789&size=132639&status=done&style=none&width=789)\n\n\n# 三、CronJob的控制机制\n\n\n1. CronJob控制器是一个更高级别的资源，它以Job控制器资源为其管控对象，并借助它管理Pod资源对象。\n1. 使用类似如下命令来查看某CronJob控制器创建的Job资源对象，其中的标签“mycronjob-jobs”是在创建cronjob-example时为其指定。不过，只有相关的Job对象被调度执行时，此命令才能将其正常列出。可列出的Job对象的数量取决于CronJob资源的．spec.successfulJobsHistoryLimit的属性值，默认为3。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604154953836-48d6af5d-faca-4924-8050-43fb0500d2b7.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=666&size=132639&status=done&style=none&width=666)\n\n3. 如果作业重复执行时指定的时间点较近，而作业执行时长（普遍或偶尔）跨过了其两次执行的时间长度，则会出现两个Job对象同时存在的情形。有些Job对象可能会存在无法或不能同时运行的情况，这个时候就要通过．spec.concurrencyPolicy属性控制作业并存的机制，其默认值为“Allow”，即允许前后Job，甚至属于同一个CronJob的更多Job同时运行。\n3. 其他两个可用值中，“Forbid”用于禁止前后两个Job同时运行，如果前一个尚未结束，后一个则不予启动（跳过），“Replace”用于让后一个Job取代前一个，即终止前一个并启动后一个。', 10, 0, 0, '2020-12-10 22:57:47.006131', '2021-01-08 07:52:25.568408', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (38, 'StatefulSet控制器', '[TOC]\n\n# 一、介绍\n\n\n1. StatefulSet是Kubernetes提供的管理有状态应用的负载管理控制器API。用于部署和扩展有状态应用的Pod资源，确保它们的运行顺序及每个Pod资源的唯一性。其与ReplicaSet控制器不同的是，虽然所有的Pod对象都基于同一个spec配置所创建，但StatefulSet需要为每个Pod维持一个唯一且固定的标识符，必要时还要为其创建专有的存储卷\n1. StatefulSet适用于具有以下特点的应用：\n\n- 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于 PVC来实现。\n- 稳定的网络标识符，即 Pod 重新调度后其 PodName 和 HostName 不变。\n- 有序部署，有序扩展，基于 init containers 来实现。\n- 有序收缩。\n\n3. Statefulset的启停顺序：\n\n- 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。\n- 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。\n- 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。\n\n4. 一个完整的StatefulSet控制器需要由一个Headless Service、一个StatefulSet和一个volumeClaimTemplate组成。\n\n- HeadlessService用于为Pod资源标识符生成可解析的DNS资源记录，\n- StatefulSet用于管控Pod资源\n- volumeClaimTemplate则基于静态或动态的PV供给方式为Pod资源提供专有且固定的存储\n\n5. 为什么要有headless？？\n   在deployment中，每一个pod是没有名称，是随机字符串，是无序的。而statefulset中是要求有序的，每一个pod的名称必须是固定的。当节点挂了，重建之后的标识符是不变的，每一个节点的节点名称是不能改变的。pod名称是作为pod识别的唯一标识符，必须保证其标识符的稳定并且唯一。为了实现标识符的稳定，这时候就需要一个headless service 解析直达到pod，还需要给pod配置一个唯一的名称。\n5. 为什么要有volumeClainTemplate？？\n   大部分有状态副本集都会用到持久存储，比如分布式系统来说，由于数据是不一样的，每个节点都需要自己专用的存储节点。而在deployment中pod模板中创建的存储卷是一个共享的存储卷，多个pod使用同一个存储卷，而statefulset定义中的每一个pod都不能使用同一个存储卷，由此基于pod模板创建pod是不适应的，这就需要引入volumeClainTemplate，当在使用statefulset创建pod时，会自动生成一个PVC，从而请求绑定一个PV，从而有自己专用的存储卷。\n\n# 二、创建StatefulSet对象\n\n\n1. 先定义了一个名为myapp-svc的Headless Service资源，用于为关联到的每个Pod资源创建DNS资源记录。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155203726-2e0bc9c2-2211-464f-ae29-2199f39acded.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=266&size=132639&status=done&style=none&width=266)\n\n2. 定义多个使用NFS存储后端的PV，空间大小为2GB，仅支持单路的读写操作。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155234926-9a27af5d-d5d7-49fb-b5bd-e436beca4a48.png#align=left&display=inline&height=337&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=337&originWidth=493&size=132639&status=done&style=none&width=493)\n\n3. 定义了一个名为myapp的StatefulSet资源，它通过Pod模板创建了两个Pod资源副本，并基于volumeClaimTemplates（存储卷申请模板）向nfs存储类请求动态供给PV，从而为每个Pod资源提供大小为1GB的专用存储卷。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155259323-30bd5f94-9a87-4e8c-a633-a10ae7316816.png#align=left&display=inline&height=772&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=772&originWidth=524&size=132639&status=done&style=none&width=524)\n\n\n- headless保证它的网络，statefulset存储模版来保证每个pod存储的唯一性，这样才解决了有状态应用的两大痛点\n\n4. 查看资源信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155284426-81ddce5e-b176-4d25-b848-764b0b8c1417.png#align=left&display=inline&height=186&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=186&originWidth=474&size=132639&status=done&style=none&width=474)\n\n\n# 三、Pod资源标识符及存储卷\n\n\n> 由StatefulSet控制器创建的Pod资源拥有固定、唯一的标识和专用存储卷，即便重新调度或终止后重建，其名称也依然保持不变，且此前的存储卷及其数据不会丢失。\n\n\n\n1. Pod资源的固定标识符\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155383969-f69a81d1-6828-4a15-a29c-14049f3823fe.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=900&size=132639&status=done&style=none&width=900)\n\n\n- 这些名称标识会由StatefulSet资源相关的Headless\n  Service资源创建为DNS资源记录。在Pod资源创建后，与其相关的DNS资源记录格式为“$(pod_name).$(service_name).$(namespace).svc.cluster.local”\n- 使用coredns解析测试\n\n`# dig -t A myapp-0.myapp-svc.default.svc.cluster.local@10.244.2.7`\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155432312-0f86dbc3-a4f2-4c7a-b9a8-2b48b11bcf45.png#align=left&display=inline&height=52&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=52&originWidth=691&size=132639&status=done&style=none&width=691)\n\n- 终端中删除Pod资源myapp-1，删除完成后控制器将随之开始重建Pod资源，其名称标识符的确未发生改变：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155449464-973cc6e4-fc82-4f95-a402-a24a747cf36f.png#align=left&display=inline&height=419&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=419&originWidth=891&size=132639&status=done&style=none&width=891)\n\n\n2. Pod资源的专有存储卷\n   控制器通过volumeClaimTemplates为每个Pod副本自动创建并关联一个PVC对象，它们分别绑定了一个动态供给的PV对象：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155475814-dc6293b5-559b-41b9-bc95-bbf762a1c426.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=843&size=132639&status=done&style=none&width=843)\n\n- 重新调度或终止后重建，此前的存储卷及其数据不会丢失。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155491334-774ec032-62ec-4e64-a3a3-1839dce06c2c.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=865&size=132639&status=done&style=none&width=865)\n\n\n# 四、StatefulSet资源扩缩容\n\n\n> 通过修改资源的副本数来改动其目标Pod资源数量。对StatefulSet资源来说，kubectl scale和kubectl patch命令均可实现此功能，也可以使用kubectl edit命令直接修改其副本数，或者在修改配置文件之后，由kubectl apply命令重新声明。\n\n\n\n1. 将myapp中的Pod副本数量扩展至5个：\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155539864-0389cb14-9c34-40e2-ad0c-eb9f8966c05d.png#align=left&display=inline&height=206&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=206&originWidth=677&size=132639&status=done&style=none&width=677)\n\n- StatefulSet资源的扩展过程与创建过程的Pod资源生成策略相同，默认为顺次进行，而且其名称中的序号也将以现有Pod资源的最后一个序号向后进行\n\n2. 执行缩容操作只需要将其副本数量调低即可，例如，这里可使用kubectl patch命令将StatefulSet资源myapp的副本数量修补为3个：\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155636738-209e1c54-172d-460f-88f0-bc9b47116650.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=848&size=132639&status=done&style=none&width=848)\n\n\n- 终止Pod资源后，其存储卷并不会被删除，因此缩减规模后若再将其扩展回来，那么此前的数据依然可用，且Pod资源名称保持不变\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155653645-b31fca57-f650-41be-aed4-6f8d81593356.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=842&size=132639&status=done&style=none&width=842)\n\n\n# 五、StatefulSet资源升级\n\n\n1. 滚动更新\n   滚动更新StatefulSet控制器的Pod资源以逆序的形式从其最大索引编号的Pod资源逐一进行。对于主从复制类的集群应用来说，这样也能保证起主节点作用的Pod资源最后进行更新，确保兼容性。\n\n- 更新Pod中的容器镜像可以使用“kubectl set image”命令进行，例如下面的命令可将myapp控制器下的Pod资源镜像版本升级为“myapp:v2”：\n  `# kubectl set image statefulset myapp nginx=192.168.10.110/k8s/myapp:v2` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155742907-0d4cc8fa-9343-4dbf-8cb4-7596ecb6eaeb.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=117&originWidth=1197&size=132639&status=done&style=none&width=1197)\n\n2. 暂存更新\n   当用户需要设定一个更新操作，但又不希望它立即执行时，可将更新操作予以“暂存”，待条件满足后再手动触发其执行更新。将．spec.update-Strategy.rollingUpdate.partition字段的值设置为Pod资源的副本数量，即比Pod资源的最大索引号大1，这就意味着，所有的Pod资源都不会处于可直接更新的分区之内，那么于其后设定的更新操作也就不会真正执行，直到用户降低分区编号至现有Pod资源索引号范围之内\n\n- 首先将StatefulSet资源myapp的滚动更新分区值设定为3：\n  `# kubectl patch statefulset myapp -p \'{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":3}}}}\'` \n- 而后，将myapp控制器的Pod资源镜像版本更新为“myapp:v3”\n  `# kubectl set image statefulset myapp nginx=192.168.10.110/k8s/myapp:v3` \n- 接着检测各Pod资源的镜像文件版本信息，可以发现其版本并未发生改变：\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155807850-acac1dc8-3e80-483f-8820-56355730d2e6.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1192&size=132639&status=done&style=none&width=1192)\n\n- 即便删除某Pod资源，它依然会基于旧的版本镜像进行重建。\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155836218-e6808a92-3773-4a9d-92b7-ee5758a73805.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=1196&size=132639&status=done&style=none&width=1196)\n\n\n3. 灰度部署\n   将处于暂存状态的更新操作的partition定位于Pod资源的最大索引号，即可放出一只金丝雀，由其测试第一轮的更新操作，在确认无误后通过修改partition属性的值更新其他的Pod对象是一种更为稳妥的更新操作。\n\n- 将暂停的更新StatefulSet控制器myapp资源的分区号设置为Pod资源的最大索引号2，将会触发myapp-2的更新操作：\n  `# kubectl patch statefulset myapp -p \'{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":2}}}}\'` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155904099-a4eee6db-3995-4855-ba67-e7b3f905e9b7.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=1192&size=132639&status=done&style=none&width=1192)\n\n- 将副本数目修改大于3后，也会触发更新操作\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604155922305-42d24c7d-3f0b-487a-8996-3fb6f6fc3546.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=184&originWidth=1197&size=132639&status=done&style=none&width=1197)', 8, 0, 0, '2020-12-10 23:01:41.571481', '2021-01-26 09:08:29.520964', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (39, 'PDB中断预算', '[TOC]\n\n# 一、简介\n\n1. 控制器无法保证在某一时刻一定会存在指定数量或比例的Pod对象，然而这种需求在某些强调服务可用性的场景中却是必备的。Pod中断预算（PodDisruptionBudget，简称PDB）类型的资源，用于为那些自愿的（Voluntary）中断做好预算方案（Budget），限制可自愿中断的最大Pod副本数或确保最少可用的Pod副本数，以确保服务的高可用性。\n1. 部署在Kubernetes的每个应用程序都可以创建一个对应的PDB对象以限制自愿中断时最大可以中断的副本数或者最少应该保持可用的副本数，从而保证应用自身的高可用性。\n\n- 非自愿中断是指那些由不可控外界因素导致的Pod中断退出操作，例如，硬件或系统内核故障、网络故障以及节点资源不足导致Pod对象被驱逐等；\n- 由用户特地执行的管理操作导致的Pod中断则称为“自愿中断”，例如排空节点、人为删除Pod对象、由更新操作触发的Pod对象重建等。\n\n3. 最常见的要保护的对象是是以下kubernetes内置的controller创建的应用对象之一:\n\n- Deployment\n- ReplicaSet\n- ReplicaSet\n- StatefulSet\n\n4. 定义PDB资源时，其spec字段主要嵌套使用以下三个字段。\n\n| .spec.selector       | Object | 当前PDB对象使用的标签选择器，一般是与相关的Pod控制器使用同一个选择器。 |\n| -------------------- | ------ | ------------------------------------------------------------ |\n| .spec.minAvailable   | string | Pod自愿中断的场景中，至少要保证可用的Pod对象数量或比例，要阻止任何Pod对象发生自愿中断，可将其设置为100% |\n| .spec.maxUnavailable | string | Pod自愿中断的场景中，最多可转换为不可用状态的Pod对象数量或比例，0值意味着不允许Pod对象进行自愿中断；此字段与minAvailable互斥 |\n\n\n\n2. 应用场景\n\n| 场景             | 关注点                       | 解决方案                                                     |\n| ---------------- | ---------------------------- | ------------------------------------------------------------ |\n| 无状态的前端     | 服务能力不能减少超过10%      | 使用一个包含minAvailable 90%值的PDB                          |\n| 单实例有状态应用 | 不要在不知情情况下中断       | 1:不使用PDB,容易偶尔的宕机 2:使用PDB,设置maxUnavailable=0.当集群管理员想要终止pod的时候,他需要联系你,然后删除掉PDB以准备应对中断,然后重新创建.(如果maxUnavailable=0则不能进行自愿中断操作) |\n| 多实例有状态应用 | 运行的实例数不能低于法定数量 | 1:把maxUnavailable to 1(根据不同集群要求不同,可以设置为不同的值) 2:把minAvailable设置为法定数量. |\n\n\n\n# 二、示例\n\n\n1. 由Deployment控制器myapp-deploy创建的Pod对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156057563-1fda37f9-d9dd-4156-a997-141a7f1ed45a.png#align=left&display=inline&height=473&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=473&originWidth=520&size=132639&status=done&style=none&width=520)\n\n2. 设置Pod中断预算，要求其最少可用的Pod对象数量为2个\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156073960-c9034938-fa57-42ad-98a6-1fa6514857e6.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=218&originWidth=338&size=132639&status=done&style=none&width=338)\n\n- PDB的app与pod中的app信息一致\n\n3. 创建PDB对象\n   `$ kubectl apply -f pdb.yaml` \n3. 查看PDB的状态\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156111406-b14602ea-f3fe-45df-8e91-e4afedec9afe.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=218&originWidth=338&size=132639&status=done&style=none&width=338)', 10, 0, 0, '2020-12-10 23:02:49.526824', '2021-01-14 02:53:56.728693', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (40, 'Service资源及模型', '[TOC]\n\n# 一、service资源概述\n\n\n1. Service资源基于标签选择器将一组Pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的Pod对象之上，它向客户端隐藏了真实的、处理用户请求的Pod资源，使得客户端的请求看上去就像是由Service直接处理并进行响应的一样。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156865423-4f504971-71fe-43f5-bcd3-a855d7d6650d.png#align=left&display=inline&height=503&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=503&originWidth=924&size=132639&status=done&style=none&width=924)\n\n2. Service对象的IP地址位于为Kubernetes集群配置指定专用IP地址的范围之内，而且是一种虚拟IP地址，它在Service对象创建后即保持不变，并且能够被同一集群中的Pod资源所访问。Service端口用于接收客户端请求并将其转发至其后端的Pod中应用的相应端口之上。\n2. 通过其标签选择器匹配到的后端Pod资源不止一个时，Service资源能够以负载均衡的方式进行流量调度，实现了请求流量的分发机制。Service与Pod对象之间的关联关系通过标签选择器以松耦合的方式建立，它可以先于Pod对象创建而不会发生错误。\n2. Service并不直接链接至Pod对象，它们之间还有一个中间层——Endpoints资源对象，它是一个由IP地址和端口组成的列表，这些IP地址和端口则来自于由Service的标签选择器匹配到的Pod资源。默认情况下，创建Service资源对象时，其关联的Endpoints对象会自动创建。\n\n\n\n# 二、VIP和service代理\n\n\n1. 一个Service对象就是工作节点上的一些iptables或ipvs规则，用于将到达Service对象IP地址的流量调度转发至相应的Endpoints对象指向的IP地址和端口之上。工作于每个工作节点的kube-proxy组件通过API Server持续监控着各Service及与其关联的Pod对象，并将其创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604156949122-c04e92d4-5141-467f-a708-10f181051132.png#align=left&display=inline&height=524&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=524&originWidth=886&size=182488&status=done&style=none&width=886)\n\n2. Service\n   IP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现Kubernetes集群网络的内部通信，并且仅能够将规则中定义的转发服务的请求作为目标地址予以响应，这也是它被称为虚拟IP的原因之一。kube-proxy将请求代理至相应端点的方式有三种：userspace（用户空间）、iptables和ipvs。\n\n\n\n# 三、代理模式\n\n\n1. userspace代理模型\n\n- kube-proxy负责跟踪API\n  Server上Service和Endpoints对象的变动，并据此调整Service资源的定义。对于每个Service对象，它会随机打开一个本地端口（运行于用户空间的kube-proxy进程负责监听），任何到达此代理端口的连接请求都将被代理至当前Service资源后端的各Pod对象上，至于会挑中哪个Pod对象则取决于当前Service资源的调度方式，默认的调度算法是轮询（round-robin）\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157032077-690cbd7d-c176-452e-b9b1-871e769de63f.png#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=683&originWidth=1016&size=318322&status=done&style=none&width=1016)\n\n- 这种代理模型中，请求流量到达内核空间后经由套接字送往用户空间的kube-proxy，而后再由它送回内核空间，并调度至后端Pod。\n\n2. iptables代理模型\n\n- kube-proxy负责跟踪API\n  Server上Service和Endpoints对象的变动（创建或移除），并据此做出Service资源定义的变动。同时，对于每个Service，它都会创建iptables规则直接捕获到达ClusterIP和Port的流量，并将其重定向至当前Service的后端。对于每个Endpoints对象，Service资源会为其创建iptables规则并关联至挑选的后端Pod资源，默认算法是随机调度\n  ![](media/7100bed3fc22161c86e8117142a82db3.png#alt=Node%20X%20%20user%5C%2A%27ace%20%20Server%20%20Client%20%20Pod%201%20%20Pod%20%20kernel%20space%20%20kubeapiserver%20%20Service%20I%20P%20%20%28iptables%29%20%20Node%20Y%20%20Pod%202%20%20user%C2%BBace%20%20Pod%203%20%20kernel%20space)\n- 在创建Service资源时，集群中每个节点上的kube-proxy都会收到通知并将其定义为当前节点上的iptables规则，用于转发工作接口接收到的与此Service资源的ClusterIP和端口的相关流量。客户端发来的请求被相关的iptables规则进行调度和目标地址转换（DNAT）后再转发至集群内的Pod对象之上。\n  相对于用户空间模型来说，iptables模型无须将流量在用户空间和内核空间来回切换，因而更加高效和可靠。不过，其缺点是iptables代理模型不会在被挑中的后端Pod资源无响应时自动进行重定向，而userspace模型则可以。\n\n\n\n1. ipvs代理模型\n\n\n\n- kube-proxy跟踪API\n  Server上Service和Endpoints对象的变动，据此来调用netlink接口创建ipvs规则，并确保与API\n  Server中的变动保持同步。它与iptables规则的不同之处仅在于其请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。\n  ![](media/db76f927fe8770c7421fb3a4b8e602d8.png#alt=dl%20%20aoeds%20%20POCI%20%20a.%27rdsnsn%20%20A%20OPON%20%20aoeds%20%20pod%20%201%20POCI%20%20aot%E2%80%A2dsnsn%20%20X%20OPON)\n- 类似于iptables模型，ipvs构建于netfilter的钩子函数之上，但它使用hash表作为底层数据结构并工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性。\n- ipvs支持众多调度算法，有rr：轮询调度lc：最小连接数dh：目标哈希sh：源哈希sed：最短期望延迟nq：不排队调度\n\n\n\n# 四、会话粘性\n\n\n1. 当客户端访问Pod中的应用程序时，如果有基于客户端身份保存某些私有信息，并基于这些私有信息追踪用户的活动等一类的需求时，那么应该启用session\n   affinity机制。\n1. Session\n   affinity的效果仅会在一定时间期限内生效，默认值为10800秒，超出此时长之后，客户端的再次访问会被调度算法重新调度。另外，Service资源的Session\n   affinity机制仅能基于客户端IP地址识别客户端身份，它会把经由同一个NAT服务器进行源地址转换的所有客户端识别为同一个客户端，调度粒度粗糙且效果不佳，因此，实践中并不推荐使用此种方法实现粘性会话。\n1. Service资源通过．spec.sessionAffinity和．spec.sessionAffinityConfig两个字段配置粘性会话。spec.sessionAffinity字段用于定义要使用的粘性会话的类型，它仅支持使用“None”和“ClientIP”两种属性值。\n\n None：不使用sessionAffinity，默认值。\n ClientIP：基于客户端IP地址识别客户端身份，把来自同一个源IP地址的请求始终调度至同一个Pod对象。\n\n1. 在启用粘性会话机制时，.spec.sessionAffinityConfig用于配置其会话保持的时长，它是一个嵌套字段，使用格式如下所示，其可用的时长范围为“1～86400”，默认为10800秒：\n\n```yaml\nspec:\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n  clientIP:\n  timeoutSeconds: <integer>\n```\n\n\n', 8, 0, 0, '2020-12-14 23:18:06.187095', '2021-01-15 07:45:09.059993', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (41, '服务发现', '[TOC]\n\n# 一、概述\n\n\n1. 服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。\n1. 根据服务发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。\n\n 客户端发现：由客户端到服务注册中心发现其依赖到的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。\n \n 服务端发现：这种方式需要额外用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。\n\n3. Kubernetes自1.3版本开始，其用于服务发现的DNS更新为了kubeDNS，自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。不过，Kubernetes依然支持使用环境变量进行服务发现。\n\n# 二、服务发现方式：环境变量\n\n> 创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。\n\n1. Kubernetes Service环境变量\n   Kubernetes为每个Service资源生成包括以下形式的环境变量在内的一系列环境变量，在同一名称空间中创建的Pod对象都会自动拥有这些变量。\n\n {SVCNAME}_SERVICE_HOST\n {SVCNAME}_SERVICE_PORT\n\n2. Docker Link形式的环境变量\n   Docker使用--link选项实现容器连接时所设置的环境变量形式。在创建Pod对象时，Kubernetes也会将与此形式兼容的一系列环境变量注入Pod对象中。\n\n 在Service资源myapp-svc创建后创建的Pod对象中查看可用的环境变量，其中以KUBERNETES_SERVICE开头的表示Kubernetes Service环境变量，名称中不包含“SERVICE”字符串的环境变量为Docker Link形式的环境变量：\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157621104-88986080-a2b6-495a-b300-d7abc5381e1d.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=210&originWidth=715&size=393875&status=done&style=none&width=715)\n\n3. 基于环境变量的服务发现其功能简单、易用，但存在一定的局限，例如，仅有那些与创建的Pod对象在同一名称空间中且事先存在的Service对象的信息才会以环境变量的形式注入，那些处于非同一名称空间，或者是在Pod资源创建之后才创建的Service对象的相关环境变量则不会被添加。\n\n\n\n# 三、ClusterDNS和服务发现\n\n\n1. 集群中创建的每个Service对象，都会由其自动生成相关的资源记录。默认情况下，集群内各Pod资源会自动配置其作为名称解析服务器，并在其DNS搜索列表中包含它所属名称空间的域名后缀。\n1. 无论是使用kubeDNS还是CoreDNS，它们提供的基于DNS的服务发现解决方案都会负责解析以下资源记录（Resource Record）类型以实现服务发现。\n1. 拥有ClusterIP的Service资源，需要具有以下类型的资源记录。\n\n A记录：<service>.<ns>.svc.<zone>. <ttl> IN A <cluster-ip>\n SRV记录：_<port>._<proto>.<service>.<ns>.svc.<zone>. <ttl> IN SRV <weight> <priority><port-number> <service>.<ns>.svc.<zone>\n PTR记录：<d>.<c>.<b>.<a>.in-addr.arpa. <ttl> IN PTR <service>.<ns>.svc.<zone>\n\n4. Headless类型的Service资源，需要具有以下类型的资源记录。\n\n A记录：<service>.<ns>.svc.<zone>. <ttl> IN A <endpoint-ip>\n SRV记录：_<port>._<proto>.<service>.<ns>.svc.<zone>. <ttl> IN SRV <weight> <priority><port-number> <hostname>.<service>.<ns>.svc.<zone>\n PTR记录：<d>.<c>.<b>.<a>.in-addr.arpa. <ttl> IN PTR <hostname>.<service>.<ns>.svc.<zone>\n\n5. ExternalName类型的Service资源，需要具有CNAME类型的资源记录。\n\n- CNAME记录：<service>.<ns>.svc.<zone>. <ttl> IN CNAME <extname>\n\n\n\n# 四、服务发现方式：DNS\n\n\n1. 创建Service资源对象时，ClusterDNS会为它自动创建资源记录用于名称解析和服务注册，于是，Pod资源可直接使用标准的DNS名称来访问这些Service资源。每个Service对象相关的DNS记录包含如下两个。\n\n {SVCNAME}.{NAMESPACE}.{CLUSTER_DOMAIN}\n {SVCNAME}.{NAMESPACE}.svc.{CLUSTER_DOMAIN}\n\n2. 系统初始化时默认会将“cluster.local.”和主机所在的域“ilinux.io.”作为DNS的本地域使用，这些信息会在Pod创建时以DNS配置的相关信息注入它的/etc/resolv.conf配置文件中。例如，在此前创建的用于交互式Pod资源的客户端中查看其配置，命令如下：\n\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604157821727-9263b27f-00f6-4b83-8432-741cfa186a7d.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=848&size=393875&status=done&style=none&width=848)\n\n3. 上述search参数中指定的DNS各搜索域，是以次序指定的几个域名后缀，具体如下所示。\n\n- {NAMESPACE}.svc.{CLUSTER_DOMAIN}：如default.svc.cluster.local。\n- svc.{CLUSTER_DOMAIN}：如svc.cluster.local。\n- {CLUSTER_DOMAIN}：如cluster.local。\n- {WORK_NODE_DOMAIN}：如ilinux.io。', 7, 0, 0, '2020-12-14 23:19:52.326050', '2021-01-13 15:55:25.426249', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (42, 'Service类型', '[TOC]\n\n# 一、ClusterIP\n\n1. 通过集群内部IP地址暴露服务，此地址仅在集群内部可达，而无法被集群外部的客户端访问\n1. clusterIP 主要在每个 node 节点使用 iptables，将发向 clusterIP对应端口的数据，转发到 kube-proxy 中。然后 kube-proxy自己内部实现有负载均衡的方法，并可以查询到这个 service 下对应 pod的地址和端口，进而把数据转发给对应的 pod 的地址和端口\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158749855-8f640730-a749-45ac-a9a0-ec0aa71af7f2.png#align=left&display=inline&height=445&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=445&originWidth=766&size=393875&status=done&style=none&width=766)\n\n3. 实现过程\n\n apiserver：用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求后将数据存储到etcd中\n kube-proxy：kubernetes的每个节点中都有一个叫做kube-porxy的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables规则中\n iptables：使用NAT等技术将virtualIP的流量转至endpoint中\n\n4. 示例\n\n- 创建myapp-deployment.yaml文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158809520-ea795138-e9b1-446c-a917-d5f681544165.png#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=465&originWidth=514&size=393875&status=done&style=none&width=514)\n\n- 创建ClusterIP service\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158847124-b75de6f7-713c-4993-9511-c430cc25796e.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=264&size=393875&status=done&style=none&width=264)\n\n- 查看创建的svc信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158862063-2c4b9711-3dd6-42c0-b4e9-c7e65e74a5a5.png#align=left&display=inline&height=100&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=100&originWidth=769&size=393875&status=done&style=none&width=769)\n\n- 访问验证\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158901956-aaa0fc00-c604-459d-87b1-6e1ca3f52b2f.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=410&size=393875&status=done&style=none&width=410)\n\n- 查看ipvs规则\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158914101-5ced9d92-d0f7-4b69-9543-8dfa075cc4d5.png#align=left&display=inline&height=371&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=371&originWidth=763&size=393875&status=done&style=none&width=763)\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604158938473-732c1404-334a-48cb-a2ca-46b096437a68.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=901&size=393875&status=done&style=none&width=901)\n\n# 二、NodePort\n\n\n1. 这种类型建立在ClusterIP类型之上，其在每个node节点的IP地址的某静态端口（NodePort）暴露服务，因此，它依然会为Service分配集群IP地址，并将此作为NodePort的路由目标。\n1. NodePort类型就是在工作节点的IP地址上选择一个端口用于将集群外部的用户请求转发至目标Service的ClusterIP和Port，因此，这种类型的Service既可如ClusterIP一样受到集群内部客户端Pod的访问，也会受到集群外部客户端通过套接字<NodeIP>:<NodePort>进行的请求。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159004760-2ac7a32a-0ddb-4e45-af51-653641da9fb5.png#align=left&display=inline&height=397&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=397&originWidth=536&size=393875&status=done&style=none&width=536)\n\n3. 示例\n\n- 创建myapp-service.yaml文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159046138-f8311f23-cb36-4061-a0ea-3545a1787d7a.png#align=left&display=inline&height=307&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=307&originWidth=270&size=393875&status=done&style=none&width=270)\n\n- 查看service信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159091442-ed04a51c-bfcf-4847-a6eb-050a305a01ab.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=803&size=393875&status=done&style=none&width=803)\n\n- 访问验证（nodeIP:port）,任意节点都可以\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159071112-51550bc2-67c0-4753-bd4a-31684ae4d188.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=491&size=393875&status=done&style=none&width=491)\n\n- 查看ipvs规则\n  master\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159125280-b35416ab-c557-4d7a-b403-3d1487ddc392.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=321&size=393875&status=done&style=none&width=321)\n  node1\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159138957-88281852-a397-4220-b817-8d029cac0417.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=320&size=393875&status=done&style=none&width=320)\n  node2\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159153620-9452453c-631e-4707-a871-80b9fa21b979.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=317&size=393875&status=done&style=none&width=317)\n\n# 三、LoadBalancer\n\n\n1. 这种类型建构在NodePort类型之上，创建一个具有公网IP地址的云供应商提供的负载均衡器，由它接入外部客户端的请求并调度至集群节点相应的NodePort之上。因此LoadBalancer一样具有NodePort和ClusterIP。\n1. 简而言之，一个LoadBalancer类型的Service会指向关联至Kubernetes集群外部的、切实存在的某个负载均衡设备，该设备通过工作节点之上的NodePort向集群内部发送请求流量。\n1. 例如Amazon云计算环境中的ELB实例即为此类的负载均衡设备。此类型的优势在于，它能够把来自于集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而不是依赖于客户端自行决定连接至哪个节点，从而避免了因客户端指定的节点故障而导致的服务不可用。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159236311-acb3dc53-e8cd-44c3-b96e-fa69f586b59d.png#align=left&display=inline&height=382&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=382&originWidth=532&size=393875&status=done&style=none&width=532)\n\n4. 示例\n\n- 创建LoadBalancer.yaml文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159253194-4c07583e-61f9-402d-939c-2e3f91a42fd4.png#align=left&display=inline&height=306&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=306&originWidth=277&size=393875&status=done&style=none&width=277)\n\n\n# 四、ExternalName\n\n\n1. 通过将Service映射至由externalName字段的内容指定的主机名来暴露服务，此主机名需要被DNS服务解析至CNAME类型的记录。\n1. 此种类型并非定义由Kubernetes集群提供的服务，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部的Service的一种实现方式。\n1. 这种类型的Service没有ClusterIP和NodePort，也没有标签选择器用于选择Pod资源，因此也不会有Endpoints存在。\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159278073-c8ca1c94-b6e5-4f39-8415-29cd7070cffc.png#align=left&display=inline&height=286&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=286&originWidth=490&size=393875&status=done&style=none&width=490)\n\n4. 示例\n\n- 创建资源清单ExternalName.yaml\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159301221-20b1ae87-dbf1-4181-9d9a-563d03af833b.png#align=left&display=inline&height=170&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=170&originWidth=419&size=393875&status=done&style=none&width=419)\n\n- 查看svc资源信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159341968-6c860be2-2b75-40d3-b239-47a795f07495.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=94&originWidth=675&size=393875&status=done&style=none&width=675)\n\n- 解析验证\n\n    `# dig -t A myapp-service.default.svc.cluster.local. @10.244.2.189`\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159363802-6fa98cf0-d06f-454c-932d-559a39359f3f.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=774&size=393875&status=done&style=none&width=774)', 8, 0, 0, '2020-12-14 23:23:00.361805', '2021-01-26 09:03:39.341572', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (43, 'Headless Service', '[TOC]\n\n# 一、概述\n\n1. 当客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由。\n1. 如何为此类Service资源配置IP地址，则取决于它的标签选择器的定义。\n\n 具有标签选择器：端点控制器（Endpoints Controller）会在API中为其创建Endpoints记录，并将ClusterDNS服务中的A记录直接解析到此Service后端的各Pod对象的IP地址上。\n \n 没有标签选择器：端点控制器（Endpoints Controller）不会在API中为其创建Endpoints记录，ClusterDNS的配置分为两种情形，对ExternalName类型的服务创建CNAME记录，对其他三种类型来说，为那些与当前Service共享名称的所有Endpoints对象创建一条记录。\n\n3. 根据Headless Service的工作特性可知，它记录于ClusterDNS的A记录的相关解析结果是后端Pod资源的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源\n\n# 二、实例\n\n\n1. 创建yaml文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159521030-3e093ac5-b820-4a56-8f43-f01a654b8c82.png#align=left&display=inline&height=263&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=263&originWidth=301&size=393875&status=done&style=none&width=301)\n\n2. 查看svc信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159539059-81c05a3d-3bb7-440e-9b52-c0015f9a7fc7.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=786&size=393875&status=done&style=none&width=786)\n\n3. 查看ClusterDNS地址信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159550860-c8e88b33-2487-46ef-8025-fd599500062a.png#align=left&display=inline&height=145&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=145&originWidth=1018&size=393875&status=done&style=none&width=1018)\n\n4. dns命令解析资源\n\n    `# dig -t A myapp-headless.default.svc.cluster.local. @10.244.2.189`\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159565071-509c5c61-1709-4338-89b9-95b6b673dd52.png#align=left&display=inline&height=99&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=99&originWidth=694&size=393875&status=done&style=none&width=694)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159584863-5547af4d-5530-4eae-a81a-a91b392693a1.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=896&size=393875&status=done&style=none&width=896)\n\n- 客户端向此Service对象发起的请求将直接接入到Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源则是由DNS服务器接收到查询请求时以轮询（roundrobin）的方式返回的IP地址。', 7, 0, 0, '2020-12-14 23:27:35.579850', '2021-01-08 07:53:11.046893', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (44, 'Ingress资源', '[TOC]\n\n# 一、Ingress和Ingress Controller\n\n\n1. Ingress就是一组基于DNS名称（host）或URL路径把请求转发至指定的Service资源的规则，用于将集群外部的请求流量转发至集群内部完成服务发布。然而，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则要想真正发挥作用还需要其他功能的辅助，如监听某套接字，然后根据这些规则的匹配机制路由请求流量。这种能够为Ingress资源监听套接字并转发流量的组件称为Ingress控制器（Ingress Controller）。\n1. Ingress控制器并不直接运行为kube-controller-manager的一部分，它是Kubernetes集群的一个重要附件，类似于CoreDNS，需要在集群上单独部署。\n1. Ingress控制器可以由任何具有反向代理（HTTP/HTTPS）功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器自身也是运行于集群中的Pod资源对象，它与被代理的运行为Pod资源的应用运行于同一网络中\n1. 使用Ingress资源进行流量分发时，Ingress控制器可基于某Ingress资源定义的规则将客户端的请求流量直接转发至与Service对应的后端Pod资源之上，这种转发机制会绕过Service资源，从而省去了由kube-proxy实现的端口代理开销。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604159965183-66a3167e-cb45-40c0-b443-57fd4ff239a6.png#align=left&display=inline&height=375&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=375&originWidth=835&size=393875&status=done&style=none&width=835)\n\n\n# 二、部署Ingress控制器（Nginx）\n\n\n1. Ingress控制器自身是运行于Pod中的容器应用，一般是Nginx或Envoy一类的具有代理及负载均衡功能的守护进程，它监视着来自于API\n   Server的Ingress对象状态，并以其规则生成相应的应用程序专有格式的配置文件并通过重载或重启守护进程而使新配置生效。\n1. 对于Nginx来说，Ingress规则需要转换为Nginx的配置信息。简单来说，Ingress控制器其实就是托管于Kubernetes系统之上的用于实现在应用层发布服务的Pod资源，它将跟踪Ingress资源并实时生成配置规则。\n1. 参考地址\n   github地址\n   ingress-nginx官网\n1. 部署ingress-nginx\n   • 创建ingress基础环境资源\n\n    `kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml` \n\n- 下载慢可以去Github下载\n\n    [https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml](https://github.com/kubernetes/ingress-nginx/blob/nginx-0.26.1/deploy/static/mandatory.yaml)\n\n- 创建资源\n\n    `kubectl apply -f mandatory.yaml` \n\n- 查看pod资源信息\n\n    `kubectl get pod -n ingress-nginx` \n\n- 采用nodepod暴露服务\n\n    `kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml` \n\n- 查看svc资源信息\n\n    `kubectl get svc -n ingress-nginx` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604160889107-c3e1612a-6c4f-42b5-afcf-07f4f6102778.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=909&size=393875&status=done&style=none&width=909)\n\n# 三、Ingress资源类型\n\n\n1. 单Service资源型Ingress\n   使用Ingress来暴露服务，此时只需要为Ingress指定“default backend”即可\n\n- 例如下面的示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n	name: my-ingress\nspec:\n	backend:\n		serviceName: my-svc\n		servicePort: 80\n```\n\n- Ingress控制器会为其分配一个IP地址接入请求流量，并将它们转至示例中的my-svc后端。\n\n2. 基于URL路径进行流量分发\n   垂直拆分或微服务架构中，每个小的应用都有其专用的Service资源暴露服务，但在对外开放的站点上，可通过主域名的URL路径（path）分别接入。\n\n- 例如，对www.ilinux.io/api的请求统统转发至API Service资源，将对www.ilinux.io/wap的请求转发至WAP Service资源\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n	name: test\n	annotations:\n		ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: www.ilinux.io\n    http:\n      paths:\n      - path: /wap\n        backend:\n        	serviceName: wap\n        	servicePort: 80\n      - path: /api\n      	backend:\n      		serviceName: api\n      		servicePort: 80\n```\n\n\n3. 基于主机名称的虚拟主机\n   将每个应用分别以独立的FQDN主机名进行输出，如wap.ik8s.io和api.ik8s.io，这两个主机名解析到external LB（如图6-12所示）的IP地址之上，分别用于发布集群内部的WAP和API这两个Service资源。这种实现方案其实就是Web站点部署中的“基于主机名的虚拟主机”，将多个FQDN解析至同一个IP地址，然后根据“主机头”进行转发。\n\n- 以独立FQDN主机形式发布服务的Ingress资源示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: api.ik8s.io\n    http:\n      paths:\n      - backend:\n          serviceName: api\n          servicePort: 80\n  - host: wap.ik8s.io\n    http:\n      paths:\n      - backend:\n          serviceName: wap\n          servicePort: 80\n```\n\n\n4. TLS类型的Ingress资源\n   用于以HTTPS发布Service资源，基于一个含有私钥和证书的Secret对象即可配置TLS协议的Ingress资源，目前来说，Ingress资源仅支持单TLS端口，并且还会卸载TLS会话。在Ingress资源中引用此Secret即可让Ingress控制器加载并配置为HTTPS服务。\n\n- 下面是一个简单的TLS类型的Ingress资源示例：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: no-rules-map\nspec:\n  tls:\n  - secretName: ikubernetesSecret\n  backend:\n    serviceName: homesite\n    servicePort: 80\n```', 7, 0, 0, '2020-12-14 23:29:38.365526', '2021-01-08 07:53:13.082519', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (45, 'Ingress案例', '[TOC]\n\n> 注意要点：\n>\n> - ingress中的serviceName要与service中的metadata-name保持一致\n> - service中的selector-app要与deployment中的selector-app保持一致\n> - deployment中的matchlables-app要与template中的matadata-lables-app保持一致\n\n\n\n# 一、部署myapp1实例\n\n\n1. 使用Deployment控制器部署myapp1相关的Pod对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161078673-8008e035-9397-4cbd-9417-3f9a0ac2c938.png#align=left&display=inline&height=486&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=486&originWidth=518&size=393875&status=done&style=none&width=518)\n\n- 查看deployment状态\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161105531-f0d96d61-1e0e-415b-b00f-927f4b9d3f16.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=726&size=393875&status=done&style=none&width=726)\n\n2. 使用ClusterIP控制器部署svc1相关的对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161136924-e382777c-8169-4cb5-b90d-857ba5ec6d91.png#align=left&display=inline&height=281&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=281&originWidth=262&size=393875&status=done&style=none&width=262)\n\n- 查看svc\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161222118-85b4411f-1f5b-4be4-a18c-1c575e7bd277.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=766&size=393875&status=done&style=none&width=766)\n\n\n# 二、部署myapp2实例\n\n\n1. 使用Deployment控制器部署myapp2相关的Pod对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161337774-1af7c1e6-885d-4cf5-8a92-48d18e85cda0.png#align=left&display=inline&height=468&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=468&originWidth=515&size=393875&status=done&style=none&width=515)\n\n\n- 查看deployment状态\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161360694-7b7a345f-1284-4a74-bf14-29ba844add46.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=725&size=393875&status=done&style=none&width=725)\n\n2. 使用ClusterIP控制器部署svc2相关的对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604161389154-aa28fb9e-a29e-4a49-b66f-4b12d4302ea3.png#align=left&display=inline&height=281&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=281&originWidth=256&size=393875&status=done&style=none&width=256)\n\n\n- 查看svc\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190423156-2fd9afa2-9888-4476-b31b-380b86a66fc4.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=771&size=393875&status=done&style=none&width=771)\n\n\n# 三、创建Ingress实例\n\n\n1. 编写ingress使访问myapp1.cuiliang.com跳转至myapp1，访问myapp2.cuiliang.com跳转至myapp2\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190463056-17d83770-7537-4377-842b-262123804060.png#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=627&originWidth=442&size=393875&status=done&style=none&width=442)\n\n2. 查看svc服务信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190514114-a0414d50-c6b0-4d45-9e96-5dd805c51b1a.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=912&size=393875&status=done&style=none&width=912)\n\n3. 查看ingress规则\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190541804-f9278af0-de86-4374-842d-0b1ad7c96d6b.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=732&size=393875&status=done&style=none&width=732)\n\n4. 查看ingress-nginx配置文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190575330-f4d45647-a004-4dbb-9b6a-e4cbf4eb53a2.png#align=left&display=inline&height=112&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=112&originWidth=1098&size=393875&status=done&style=none&width=1098)\n\n5. 修改host文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190609730-834d5652-0391-44e1-b185-bd16c5e17266.png#align=left&display=inline&height=39&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=39&originWidth=281&size=393875&status=done&style=none&width=281)\n\n6. 访问测试\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190624017-a76acd75-7698-410b-9e2f-f358e85e82dc.png#align=left&display=inline&height=109&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=109&originWidth=395&size=393875&status=done&style=none&width=395)\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190638087-37168b87-a052-45d3-870b-43985e215323.png#align=left&display=inline&height=103&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=103&originWidth=399&size=393875&status=done&style=none&width=399)\n\n\n# 四、Ingress https代理\n\n\n1. 创建证书，以及 cert 存储\n    `# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=nginxsvc/O=nginxsvc\"` \n    `# kubectl create secret tls tls-secret --key tls.key --cert tls.crt` \n1. 使用Deployment控制器部署myapp3相关的Pod对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190674237-a5e7f8be-f1ac-4368-8d50-5bde14004d4c.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=517&size=393875&status=done&style=none&width=517)\n\n\n- 查看deployment状态\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190714394-a2c8fb61-6176-4cf6-9a41-3c11daac1170.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=723&size=393875&status=done&style=none&width=723)\n\n\n3. 使用ClusterIP控制器部署svc3相关的对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190733320-ddec8958-ae10-4957-9c67-337883920bbc.png#align=left&display=inline&height=282&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=282&originWidth=266&size=393875&status=done&style=none&width=266)\n\n\n- 查看svc\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190751272-8106e34f-26ba-4ccd-a1d3-64831724b2c7.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=757&size=393875&status=done&style=none&width=757)\n\n\n4. 创建Ingress实例\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190776734-07edae7e-4d51-43a7-bf72-c307d9fb0834.png#align=left&display=inline&height=399&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=399&originWidth=437&size=393875&status=done&style=none&width=437)\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190792336-dfd622ec-cb5a-45dc-80a7-55f1bdeed01f.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=384&size=393875&status=done&style=none&width=384)\n\n\n- 查看svc-ingress信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190808435-d18b6ea9-5c6e-4b38-b149-0c6bcac9b4ca.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=909&size=393875&status=done&style=none&width=909)\n\n\n5. 修改host文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190839193-c09e4d82-4ad6-4e90-815b-d166ad10851f.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=291&size=393875&status=done&style=none&width=291)\n\n6. 访问测试\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190855301-14c7cc0d-76c6-4148-bad4-9d8dca6ad206.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=394&size=393875&status=done&style=none&width=394)\n\n\n# 五、BasicAuth用户认证\n\n\n1. 创建证书，以及 cert 存储\n    `#yum -y install httpd` \n    `#htpasswd -c auth foo` \n    `#kubectl create secret generic basic-auth --from-file=auth` \n1. 使用Deployment控制器部署myapp4相关的Pod对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190903397-0e217b91-39fa-4d08-8b14-8ec145db1995.png#align=left&display=inline&height=462&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=462&originWidth=517&size=393875&status=done&style=none&width=517)\n\n- 查看deployment状态\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190923687-eba4cd71-0816-45e9-9e7b-3ae490db26cc.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=723&size=393875&status=done&style=none&width=723)\n\n3. 使用ClusterIP控制器部署svc4相关的对象\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191407683-77b76b2f-fc37-4b6f-aeb3-e0a84dee3ee4.png#align=left&display=inline&height=284&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=284&originWidth=258&size=393875&status=done&style=none&width=258)\n\n- 查看svc\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191431934-e4b20de3-7e39-45a7-868f-7d7ea63de0dd.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=753&size=393875&status=done&style=none&width=753)\n\n4. 创建Ingress实例\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191453753-bed7e60a-64ed-453f-b337-6332e04a2624.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=874&size=393875&status=done&style=none&width=874)\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191472913-f40f56ad-a02b-4706-bc23-9b2641422430.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=50&originWidth=316&size=393875&status=done&style=none&width=316)\n\n5. 查看svc-ingress信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191484691-b571c89d-6be7-4f92-b566-73275831966e.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=140&originWidth=685&size=393875&status=done&style=none&width=685)\n\n6. 修改host文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191500579-19b7894e-f1e8-4865-9355-ac29b9f70a03.png#align=left&display=inline&height=85&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=85&originWidth=284&size=393875&status=done&style=none&width=284)\n\n7. 访问测试\n\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191522260-a2bb4287-174c-4aad-9114-f2ca4709f5c4.png#align=left&display=inline&height=320&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=320&originWidth=625&size=393875&status=done&style=none&width=625)\n\n\n# 六、nginx重写\n\n\n- 当用户访问myapp5.cuiliang.com时跳转到myapp3.cuiliang.com\n\n\n\n1. 创建Ingress实例\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191542667-c01ca764-53de-4661-a262-f5753723e075.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=214&originWidth=944&size=393875&status=done&style=none&width=944)\n\n2. 查看svc-ingress信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191562237-e4d0e9f2-43c2-457d-ac15-daebf049efcd.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=161&originWidth=762&size=393875&status=done&style=none&width=762)\n\n3. 修改host文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191576591-03079a9d-a253-413a-a95f-6a239b5c2d85.png#align=left&display=inline&height=106&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=106&originWidth=287&size=393875&status=done&style=none&width=287)\n\n4. 访问测试\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191595792-a13bdd4b-2d6e-447b-965c-a77501731a91.png#align=left&display=inline&height=202&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=202&originWidth=468&size=393875&status=done&style=none&width=468)\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604191614503-5dffb7cd-62a7-4ab4-a707-81a580799584.png#align=left&display=inline&height=87&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=87&originWidth=401&size=393875&status=done&style=none&width=401)', 10, 0, 0, '2020-12-14 23:38:29.590212', '2021-01-16 10:52:04.801489', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (46, '配置集合ConfigMap', '[TOC]\n\n# 一、简介\n\n\n1. 一个ConfigMap对象就是一系列配置数据的集合，这些数据可“注入”到Pod对象中，并为容器应用所使用，注入方式有挂载为存储卷和传递为环境变量两种。\n1. ConfigMap对象将配置数据以键值对的形式进行存储，这些数据可以在Pod对象中使用或者为系统组件提供配置。无论应用程序如何使用ConfigMap对象中的数据，用户都完全可以通过在不同的环境中创建名称相同但内容不同的ConfigMap对象，从而为不同环境中同一功能的Pod资源提供不同的配置信息。\n\n\n\n# 二、创建ConfigMap对象\n\n\n1. 用户可以根据目录、文件或直接值创建ConfigMap对象。命令的语法格式为：\n\n\n\n> kubectl create configmap <map-name> <data-source>\n\n\n\n- <map-name>即为ConfigMap对象的名称，而<data-source>是数据源，它可以通过直接值、文件或目录来获取。无论是哪一种数据源供给方式，它都要转换为ConfigMap对象中的Key-Value数据，其中Key由用户在命令行给出或是文件数据源的文件名，它仅能由字母、数字、连接号和点号组成，而Value则是直接值或文件数据源的内容。\n\n\n\n2. 利用值直接创建\n\n\n\n- 使用“--from-literal”选项可在命令行直接给出键值对来创建ConfigMap对象，重复使用此选项则可以传递多个键值对\n  `# kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm` \n\n\n\n- 查看ConfigMap对象special-config的相关信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188818032-e6abcf3c-7584-4c93-85e0-d07d6a28dff6.png#align=left&display=inline&height=302&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=302&originWidth=712&size=393875&status=done&style=none&width=712)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188935512-412a37f2-f393-4cea-af64-db5cab05c8be.png#align=left&display=inline&height=346&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=346&originWidth=679&size=393875&status=done&style=none&width=679)\n\n4. 基于文件创建\n\n- 使用“--from-file”选项即可基于文件内容来创建ConfigMap对象，可以重复多次使用“--from-file”选项以传递多个文件内容：\n  `# kubectl create configmap resolv.conf --from-file=/etc/resolv.conf` \n- 查看ConfigMap对象，其数据存储的键为文件名，值为文件内容\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604188962605-677a5bc9-ea87-4736-b838-51c76867e72d.png#align=left&display=inline&height=351&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=351&originWidth=683&size=393875&status=done&style=none&width=683)\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189011074-546cadac-f68f-450f-92ce-cf4a062cbd62.png#align=left&display=inline&height=349&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=349&originWidth=650&size=393875&status=done&style=none&width=650)\n\n\n5. 基于目录创建\n\n- 将“--from-file”选项后面所跟的路径指向一个目录路径就能将目录下的所有文件一同创建于同一ConfigMap资源中，将/etc/docker/目录下的所有文件都保存于docker-config-files对象中：\n  `# kubectl create configmap docker-config-files --from-file=/etc/docker/` \n- 此目录中包含daemon.json\n  key.json两个配置文件。创建ConfigMap资源时，它们会被分别存储为两个键值数据\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189079700-558b4e5a-1f13-4d3e-b965-4cfa56422d2b.png#align=left&display=inline&height=459&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=459&originWidth=813&size=393875&status=done&style=none&width=813)\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189100514-3c3639d1-d011-49ff-9997-65e9a13180da.png#align=left&display=inline&height=510&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=510&originWidth=770&size=393875&status=done&style=none&width=770)\n\n6. 使用清单创建\n\n- 基于配置文件创建ConfigMap资源时，它所使用的字段包括通常的apiVersion、kind和metadata字段，以及用于存储数据的关键字段“data”\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189145232-017a469f-3278-489d-8dee-8dfb0664b962.png#align=left&display=inline&height=171&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=171&originWidth=373&size=393875&status=done&style=none&width=373)\n\n- 查看资源信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189166519-c38d7d80-649f-426d-aed5-6db10fa6b88a.png#align=left&display=inline&height=393&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=393&originWidth=737&size=393875&status=done&style=none&width=737)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189194626-11411c8c-7032-48ba-8441-c2e7d8799e1a.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=707&size=393875&status=done&style=none&width=707)\n\n# 三、向Pod环境变量传递ConfigMap对象键值数据\n\n\n1. 使用 ConfigMap 来导入环境变量\n\n- 创建configmap资源清单\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189231795-79a058e1-e9a3-41ef-99ec-93fdc512035a.png#align=left&display=inline&height=329&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=329&originWidth=294&size=393875&status=done&style=none&width=294)\n\n- 查看资源信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189374366-05a440f6-8764-4264-82dd-4dd9fdfa909e.png#align=left&display=inline&height=237&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=237&originWidth=840&size=393875&status=done&style=none&width=840)\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189349188-858235b3-2af5-4292-a5eb-70c6f0610ef3.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=161&originWidth=788&size=393875&status=done&style=none&width=788)\n\n- 创建pod资源，设置env信息\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189419345-66e6d93b-1c1f-4d1f-9bcd-326ce239f298.png#align=left&display=inline&height=559&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=559&originWidth=498&size=393875&status=done&style=none&width=498)\n\n> 字段name的值为要引用的ConfigMap对象的名称，字段key可用于指定要引用ConfigMap对象中某键的键名在容器中使用envFrom字段直接将ConfigMap资源中的所有键值一次性地完成导入\n\n- 查看pod日志环境变量信息\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189530036-db8ebbe9-6189-4e65-af42-6bd4672282ee.png#align=left&display=inline&height=141&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=141&originWidth=800&size=393875&status=done&style=none&width=800)\n\n\n2. 用 ConfigMap 设置命令行参数\n\n- 创建pod资源，echo变量信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189621739-91b331c2-eba5-40b7-84ac-e9d54dd4feb9.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=922&size=393875&status=done&style=none&width=922)\n\n- 查看pod输出信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189658188-38063454-4bd5-48bd-b01b-e3b4d73e8743.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=50&originWidth=525&size=393875&status=done&style=none&width=525)\n\n\n3. 通过数据卷使用整个ConfigMap\n\n- 创建pod资源，将configmap挂载到/etc/config\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189724438-db5db602-290a-4c45-b930-010f89d99935.png#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=405&originWidth=539&size=393875&status=done&style=none&width=539)\n\n- 进入pod容器查看文件内容\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189837412-e1ad1758-2d42-4a2b-acf2-bc7838de34b4.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=886&size=393875&status=done&style=none&width=886)\n\n\n4. 通过数据卷使用部分ConfigMap值\n\n\n\n- 创建pod资源，将configmap挂载到/etc/config\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604189972621-d6ba1569-2a73-48ba-ad58-153ffd60210d.png#align=left&display=inline&height=491&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=491&originWidth=535&size=393875&status=done&style=none&width=535)\n\n- 进入pod查看文件内容\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190237357-872a821e-bc75-4465-9e26-5543ebec112a.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=94&originWidth=876&size=393875&status=done&style=none&width=876)\n\n\n# 四、configmap热更新\n\n\n1. 创建资源清单，变量log_level: INFO，数据卷挂载到deployment中\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190268910-9567ca1d-1f13-4df1-abab-22572576903e.png#align=left&display=inline&height=791&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=791&originWidth=516&size=393875&status=done&style=none&width=516)\n\n\n- 进入pod容器查看文件内容\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190309173-04a66fc7-1883-4ebe-abb5-4fd58f23cbcc.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=985&size=393875&status=done&style=none&width=985)\n\n\n2. 修改configmap\n   `# kubectl edit configmap log-config` \n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190329882-6e897bbf-890c-40ea-8832-32b018c15199.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=214&originWidth=639&size=393875&status=done&style=none&width=639)\n\n- 进入pod查看文件内容\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604190355134-71479c41-aafc-4a6f-9238-b700a8090786.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=989&size=393875&status=done&style=none&width=989)\n\n\n# 五、注意事项\n\n\n1. ConfigMap是名称空间级的资源，因此，引用它的Pod必须处于同一名称空间中。\n1. ConfigMap 更新后滚动更新 Pod更新 ConfigMap 目前并不会触发相关 Pod\n   的滚动更新，可以通过修改 pod annotations 的方式强制触发滚动更新\n1. 在.spec.template.metadata.annotations中添加version/config，每次通过修改version/config来触发滚动更新\n1. 更新 ConfigMap 后：使用该 ConfigMap 挂载的 Env 不会同步更新使用该 ConfigMap\n   挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新', 12, 0, 0, '2020-12-15 00:01:06.667793', '2021-01-20 04:42:35.716204', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (47, '敏感信息Secret', '[TOC]\n\n# 一、secret概述\n\n\n1. Secret对象存储数据以键值方式存储数据，在Pod资源中通过环境变量或存储卷进行数据访问。Secret对象仅会被分发至调用了此对象的Pod资源所在的工作节点，且只能由节点将其存储于内存中。Secret对象的数据的存储及打印格式为Base64编码的字符串，因此用户在创建Secret对象时也要提供此种编码格式的数据。\n1. Secret对象主要有两种用途，一是作为存储卷注入到Pod上由容器应用程序所使用，二是用于kubelet为Pod里的容器拉取镜像时向私有仓库提供认证信息。使用ServiceAccount资源自建的Secret对象是一种更具安全性的方式。\n1. Secret资源主要由四种类型组成\n\n- Opaque：自定义数据内容；base64编码，用来存储密码、密钥、信息、证书等数据，类型标识符为generic。\n- kubernetes.io/service-account-token:Service Account的认证信息，可在创建Service Accout时由Kubernetes自动创建。\n- kubernetes.io/dockerconfigjson：用来存储Docker镜像仓库的认证信息，类型标识为docker-registry。\n- kubernetes.io/tls：用于为SSL通信模式存储证书和私钥文件，命令式创建时类型标识为tls。\n\n# 二、Opaque\n\n\n1. 通过kubectl create命令创建资源\n\n> 使用 `“kubectl createsecret generic <SECRET_NAME>--from-literal=key=value”` 命令直接进行创建\n\n- 创建了一个名为mysql-auth的Secret对象，用户名为root，密码为123.com\n  `# kubectl create secret generic mysql-auth --from-literal=username=root --from-literal=password=123.com` \n- 查看资源详细信息\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198075538-c8f3cac0-9423-46a7-ba5b-c723c46df897.png#align=left&display=inline&height=324&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=324&originWidth=661&size=393875&status=done&style=none&width=661)\n\n> 使用“ `kubectl create secret generic <SECRET_NAME> --from-file[=KEY1]=/PATH/TO/FILE` ”命令加载认证文件内容并生成为Secret对象\n\n- 将ssh密钥认证文件创建secret对象\n  `# kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-file=ssh-publickey=/root/.ssh/id_rsa.pub` \n- 查看资源详细信息\n  `# kubectl get secrets ssh-key-secret -o yaml` \n\n2. 使用Secret清单创建\n\n- Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198184047-2c609a1a-7b48-407e-9481-99b1af4967f4.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=480&size=393875&status=done&style=none&width=480)\n\n- 创建资源清单文件\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198226076-438789a7-c299-4112-81d0-bd4557e4b1c1.png#align=left&display=inline&height=194&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=194&originWidth=320&size=393875&status=done&style=none&width=320)\n\n3. 将secret挂载到volume中\n\n- 创建pod资源清单\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198261324-da05873a-e527-4f35-8683-1799f284def4.png#align=left&display=inline&height=424&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=424&originWidth=469&size=393875&status=done&style=none&width=469)\n\n- 查看pod信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198285307-9d55a2aa-e4ea-4284-a5b2-f063e4522a21.png#align=left&display=inline&height=95&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=95&originWidth=824&size=393875&status=done&style=none&width=824)\n\n4. 将secret导入到环境变量中\n\n- 创建pod资源清单\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198311560-5d51f6b2-d5c6-45d5-bb12-8e43a17c69e0.png#align=left&display=inline&height=492&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=492&originWidth=471&size=393875&status=done&style=none&width=471)\n\n- 查看pod信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604198327864-5551fd7c-acb9-4e17-9594-52a0b3692c5a.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=763&size=393875&status=done&style=none&width=763)\n\n\n# 三、Service Account\n\n\n1. Service Account 用来访问 Kubernetes API，由 Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中\n1. 验证\n\n- 创建nginx资源\n  `# kubectl run nginx --image nginx` \n- 查看资源信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199092161-4259f67d-89e1-46d6-9b4d-283071aa20cb.png#align=left&display=inline&height=90&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=90&originWidth=1120&size=393875&status=done&style=none&width=1120)\n\n\n# 四、docker config json\n\n\n1. 使用 Kuberctl 创建 docker registry 认证的 secret\n   `kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL` \n1. 在创建 Pod 的时候，通过imagePullSecrets来引用刚创建的 `myregistrykey`\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199137237-f24dc658-8d31-4bcc-9703-f4d3ac6b147d.png#align=left&display=inline&height=237&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=237&originWidth=516&size=393875&status=done&style=none&width=516)\n\n\n# 五、TLS\n\n\n1. 使用“kubectl create secret tls <SECRET_NAME> --cert=--key=”命令加载TLS文件内容并生成secret对象\n\n- 将TLS密钥认证文件创建secret对象\n  `# kubectl create secret tls nginx-ssl --key=nginx.key --cert=nginx.crt` \n\n', 7, 0, 0, '2020-12-15 00:02:36.087048', '2021-01-11 22:54:09.666471', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (48, '临时存储emptyDir', '[TOC]\n\n# 一、概述\n\n\n1. emptyDir存储卷是Pod对象生命周期中的一个临时目录，当 Pod被分配给节点时，首先创建emptyDir卷，并且只要该 Pod在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除Pod 时，emptyDir中的数据将被永久删除\n1. emptyDir的用法有：\n\n 同一Pod内的多个容器间文件的共享\n \n 作为容器数据的临时存储目录用于数据缓存系统等\n\n3. emptyDir存储卷则定义于．spec.volumes.emptyDir嵌套字段中，可用字段主要包含两个。\n\n- medium：此目录所在的存储介质的类型，可取值为“default”或“Memory”，默认为default，表示使用节点的默认存储介质；“Memory”表示使用基于RAM的临时文件系统tmpfs，空间受限于内存，但性能非常好，通常用于为容器中的应用提供缓存空间。\n- sizeLimit：当前存储卷的空间限额，默认值为null，表示不限制；不过，在medium字段值为“Memory”时建议务必定义此限额。\n\n# 二、实例\n\n\n> 存储卷名称为html，挂载于容器nginx的/usr/share/nginx/html目录，以及容器page的/html目录。容器page每隔10秒向存储卷上的index.html文件中追加一行信息，而容器nginx中的nginx进程则以其为站点主页。\n\n\n\n1. 创建pod资源清单\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199304031-2145f16a-8b81-4c30-9411-e3d80cac0370.png#align=left&display=inline&height=576&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=576&originWidth=623&size=393875&status=done&style=none&width=623)\n\n2. 访问测试\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199332803-315500c9-ba4a-498f-ba29-0817a6b2ba73.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=115&originWidth=494&size=393875&status=done&style=none&width=494)', 7, 0, 0, '2020-12-15 00:03:42.801898', '2021-01-20 15:58:14.932236', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (49, '节点存储hostPath', '[TOC]\n\n# 一、概述\n\n\n1. hostPath类型的存储卷是指将工作节点上某文件系统的目录或文件挂载于Pod中的一种存储卷，它可独立于Pod资源的生命周期，因而具有持久性。但它是工作节点本地的存储空间，仅适用于特定情况下的存储卷使用需求。\n1. hostPath的用途：\n\n 运行需要访问 Docker 内部的容器\n \n 使用/var/lib/docker的hostPath在容器中运行 cAdvisor\n \n 使用/dev/cgroups的hostPath允许 pod 指定给定的 hostPath 是否应该在 pod运行之前存在，是否应该创建，以及它应该以什么形式存在\n\n3. 配置hostPath存储卷的嵌套字段共有两个：一个是用于指定工作节点上的目录路径的必选字段path；另一个是指定存储卷类型的type，它支持使用的卷类型包含如下几种。\n\n DirectoryOrCreate：指定的路径不存时自动将其创建为权限是0755的空目录，属主属组均为kubelet。\n Directory：必须存在的目录路径。\n FileOrCreate：指定的路径不存时自动将其创建为权限是0644的空文件，属主和属组同是kubelet。\n File：必须存在的文件路径。\n Socket：必须存在的Socket文件路径。\n CharDevice：必须存在的字符设备文件路径。\n BlockDevice：必须存在的块设备文件路径。\n\n4. 使用这种卷类型的注意：\n\n- 由于每个节点上的文件都不同，具有相同配置（例如从 podTemplate 创建的）的 pod在不同节点上的行为可能会有所不同\n- 当 Kubernetes 按照计划添加资源感知调度时，将无法考虑hostPath使用的资源\n- 在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root身份运行进程，或修改主机上的文件权限以便写入hostPath卷\n\n# 二、实例\n\n\n> 存储卷名称为html，将本地/html下的文件挂载于容器nginx的/usr/share/nginx/html目录下，向index.html文件中追加时间信息，而容器nginx中的nginx进程则以其为站点主页。\n\n\n\n1. 提前在每个节点创建/html目录，并生成index.html文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199463708-c7f624e5-94b6-4bf8-b289-aa8efd964b4a.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=437&size=393875&status=done&style=none&width=437)\n\n2. 创建pod资源清单\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199480199-0f654744-8c85-4f7e-ac2f-4599a2eea1c4.png#align=left&display=inline&height=364&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=364&originWidth=460&size=393875&status=done&style=none&width=460)\n\n3. 访问验证\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199645221-9f4abee8-1045-46c4-aa90-bbbb31496407.png#align=left&display=inline&height=43&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=43&originWidth=416&size=393875&status=done&style=none&width=416)\n\n4. 进入容器修改index.html文件\n\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199669305-886fe79e-bb47-4052-98eb-856c3d653e8b.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=91&originWidth=460&size=393875&status=done&style=none&width=460)\n\n5. 宿主机查看文件是否更改\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604199692831-1caf46fd-d2b7-4128-aede-e1c2a2c17014.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=395&size=393875&status=done&style=none&width=395)', 7, 0, 0, '2020-12-15 00:05:08.124348', '2021-01-18 14:01:17.215456', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (50, '网络存储卷', '[TOC]\n\n> Kubernetes拥有众多类型的用于适配专用存储系统的网络存储卷。这类存储卷包括传统的NAS或SAN设备（如NFS、iSCSI、fc）、分布式存储（如GlusterFS、RBD）、云端存储（如gcePersistentDisk、azureDisk、cinder和awsElasticBlockStore）以及建构在各类存储系统之上的抽象管理层（如flocker、portworxVolume和vsphereVolume）等。\n\n\n\n# 一、NFS存储卷\n\n\n1. Kubernetes的NFS存储卷用于将某事先存在的NFS服务器上导出（export）的存储空间挂载到Pod中以供容器使用。与emptyDir不同的是，NFS存储卷在Pod对象终止后仅是被卸载而非删除。另外，NFS是文件系统级共享服务，它支持同时存在的多路挂载请求。\n1. 定义NFS存储卷时，常用到以下字段。\n\n    server <string>:NFS服务器的IP地址或主机名，必选字段。\n    path <string>:NFS服务器导出（共享）的文件系统路径，必选字段。\n    readOnly <boolean>：是否以只读方式挂载，默认为false。\n\n3. 示例\n\n- Pod资源拥有一个关联至NFS服务器192.168.10.110的存储卷，Redis容器将其挂载于/data目录上，它是运行于容器中的redis-server数据的持久保存位置。\n- 创建pod资源清单\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204302441-cd531776-3b59-4b69-bf69-67bbcdb4d597.png#align=left&display=inline&height=512&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=512&originWidth=389&size=393875&status=done&style=none&width=389)\n\n- 通过其命令客户端redis-cli创建测试数据\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204321708-c395f70e-983f-4170-9dd4-58c9b554318c.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=184&originWidth=660&size=393875&status=done&style=none&width=660)\n\n- 删除pod，重新创建，查看redis数据\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204337393-4c9e2084-5cc0-40a3-a0fa-357a3f0e1b5c.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=657&size=393875&status=done&style=none&width=657)\n\n- 这表明在删除Pod资源时，其关联的外部存储卷并不会被一同删除。\n\n# 二、RBD存储卷\n\n1. Ceph是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和REST三种存储接口。它是一个高度可配置的系统，并提供了一个命令行界面用于监视和控制其存储集群。\n1. 要配置Pod资源使用RBD存储卷，需要事先满足如下几个前提条件。\n\n  存在某可用的Ceph RBD存储集群，否则就需要创建一个。\n 在Ceph RBD集群中创建一个能满足Pod资源数据存储需要的存储映像（image）。\n 在Kubernetes集群内的各节点上安装Ceph客户端程序包（ceph-common）。\n\n3. 在配置RBD类型的存储卷时，需要指定要连接的目标服务器和认证信息等，这一点通常使用以下嵌套字段进行定义。\n\n- monitors <[]string>:Ceph存储监视器，逗号分隔的字符串列表；必选字段。\n- image <string>:rados image的名称，必选字段。\n- pool <string>:rados存储池名称，默认为RBD。\n- user <string>:rados用户名，默认为admin。\n- keyring <string>:RBD用户认证时使用的keyring文件路径，默认为/etc/ceph/keyring。\n- secretRef <Object>:RBD用户认证时使用的保存有相应认证信息的Secret对象，会覆盖由keyring字段提供的密钥信息。\n- readOnly <boolean>：是否以只读的方式进行访问。\n- fsType：要挂载的存储卷的文件系统类型，至少应该是节点操作系统支持的文件系统，如ext4、xfs、ntfs等，默认为ext4。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vol-rbd-pod\nspec:\n  containers:\n  - name: redis\n    image: redis:4-alpine\n    ports:\n    - containerPort: 6379\n      name: redisport\n    volumeMounts:\n    - mountPath: /data\n      name: redis-rbd-vol\n  volumes:\n    - name: redis-rbd-vol\n      rbd:\n        monitors:\n        - \'172.16.0.56:6789\'\n        - \'172.16.0.57:6789\'\n        - \'172.16.0.58:6789\'\n        pool: kube\n        image: redis\n        fsType: ext4\n        readOnly: false\n        user: admin\n        secretRef:\n            name: ceph-secret\n```', 6, 0, 0, '2020-12-15 00:06:23.089832', '2021-01-26 09:09:03.384621', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (51, '持久存储卷', '[TOC]\n\n# 一、概念\n\n\n1. PersistentVolume（PV）\n   是由管理员设置的存储，它是群集的一部分。它是对底层共享存储的抽象，将共享存储作为一种可由用户申请使用的资源。PV 是Volume 之类的卷插件，但具有独立于使用 PV 的 Pod的生命周期。PV是集群级别的资源，不属于任何名称空间\n1. pv与volume区别\n\n PV只能是网络存储（区别于上述的hostPath本地存储），不属于任何Node，但可以在每个Node上访问。\n PV并不是定义在Pod上的，而是独立于Pod之外定义。\n PV的生命周期与Pod是独立的。\n\n3. PersistentVolumeClaim（PVC）\n   是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或只读多次模式挂载）\n3. 静态 pv\n   集群管理员创建一些 PV。它们带有可供群集用户使用的实际存储的细节。它们存在于Kubernetes API 中，可用于消费\n3. 动态pv\n   当管理员创建的静态 PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试动态地为 PVC创建卷。此配置基于StorageClasses：PVC 必须请求\n   [存储类]，并且管理员必须创建并配置该类才能进行动态创建。声明该类为\"\"可以有效地禁用其动态配置要启用基于存储级别的动态存储配置，集群管理员需要启用 API server上的DefaultStorageClass[准入控制器]。例如，通过确保DefaultStorageClass位于API server组件的--admission-control标志，使用逗号分隔的有序值列表中，可以完成此操作\n3. pv与pvc绑定\n   master 中的控制环路监视新的 PVC，寻找匹配的PV（如果可能），并将它们绑定在一起。如果为新的 PVC 动态调配PV，则该环路将始终将该 PV 绑定到PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦 PV和 PVC 绑定后，PersistentVolumeClaim绑定是排他性的，不管它们是如何绑定的。PVC 跟PV 绑定是一对一的映射\n3. 持久化卷声明的保护\n   PVC 保护的目的是确保由 pod 正在使用的 PVC不会从系统中移除，因为如果被移除的话可能会导致数据丢失当启用PVC 保护 alpha功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC的删除将被推迟，直到 PVC 不再被任何 pod 使用\n3. PVC、PV、StorageClass关系\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204659479-e39d12d7-3b95-4a93-b62d-291bf8ec5ced.png#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=679&originWidth=967&size=393875&status=done&style=none&width=967)\n\n- PVC：Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。\n- PV ：具体的 Volume 的属性，比如 Volume的类型、挂载目录、远程存储服务器地址等。\n- StorageClass：充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和PVC，才可以绑定在一起。当然，StorageClass 的另一个重要作用，是指定 PV 的Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning的话，Kubernetes 就可以自动为你创建 PV 了。\n\n# 二、创建PV\n\n1. PV容量(capacity)：设定当前PV的容量，单位为Gi、Mi\n1. 访问模式：PersistentVolume可以以资源提供者支持的任何方式挂载到主机上。每个PV的访问模式都将被设置为该卷支持的特定模式。\n\n| Volume Plugin        | ReadWriteOnce         | ReadOnlyMany          | ReadWriteMany                      |\n| -------------------- | --------------------- | --------------------- | ---------------------------------- |\n| AWSElasticBlockStore | ✓                     | -                     | -                                  |\n| AzureFile            | ✓                     | ✓                     | ✓                                  |\n| AzureDisk            | ✓                     | -                     | -                                  |\n| CephFS               | ✓                     | ✓                     | ✓                                  |\n| Cinder               | ✓                     | -                     | -                                  |\n| CSI                  | depends on the driver | depends on the driver | depends on the driver              |\n| FC                   | ✓                     | ✓                     | -                                  |\n| FlexVolume           | ✓                     | ✓                     | depends on the driver              |\n| Flocker              | ✓                     | -                     | -                                  |\n| GCEPersistentDisk    | ✓                     | ✓                     | -                                  |\n| Glusterfs            | ✓                     | ✓                     | ✓                                  |\n| HostPath             | ✓                     | -                     | -                                  |\n| iSCSI                | ✓                     | ✓                     | -                                  |\n| Quobyte              | ✓                     | ✓                     | ✓                                  |\n| NFS                  | ✓                     | ✓                     | ✓                                  |\n| RBD                  | ✓                     | ✓                     | -                                  |\n| VsphereVolume        | ✓                     | -                     | - (works when Pods are collocated) |\n| PortworxVolume       | ✓                     | -                     | ✓                                  |\n| ScaleIO              | ✓                     | ✓                     | -                                  |\n| StorageOS            | ✓                     | -                     | -                                  |\n\n\n\n ReadWriteOnce——该卷可以被单个节点以读/写模式挂载\n ReadOnlyMany——该卷可以被多个节点以只读模式挂载\n ReadWriteMany——该卷可以被多个节点以读/写模式挂载\n\n3. 在命令行中，访问模式缩写为：\n\n RWO - ReadWriteOnce\n ROX - ReadOnlyMany\n RWX - ReadWriteMany\n\n4. persistentVolumeReclaimPolicy（回收策略）\n\n Retain（保留）——管理员手动回收\n Recycle（回收）——基本擦除（rm -rf /thevolume/*），目前仅NFS和hostPath支持此操作。\n Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和OpenStack Cinder 卷）将被删除\n\n5. StorageClass的名称(storageClassName)：当前PV所属的StorageClass的名称；pvc与pv绑定时根据此name值匹配\n5. 卷可以处于以下的某种状态：\n\n Available（可用）——一块空闲资源还没有被任何声明绑定\n Bound（已绑定）——卷已经被声明绑定\n Released（已释放）——声明被删除，但是资源还未被集群重新声明\n Failed（失败）——该卷的自动回收失败命令行会显示绑定到 PV 的 PVC 的名称\n\n7. 示例\n\n- 定义了一个使用NFS存储后端的PV，空间大小为10GB，支持多路的读写操作。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204799340-b2f90ca3-8702-4b19-a7ec-826825ba637c.png#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=378&originWidth=495&size=393875&status=done&style=none&width=495)\n\n- 查看创建的pv信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204827887-5693ba9c-1700-4f2d-a839-cb7bc3108cdb.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=928&size=393875&status=done&style=none&width=928)\n\n# 三、创建pvc\n\n1. PersistentVolumeClaim是存储卷类型的资源，它通过申请占用某个PersistentVolume而创建，它与PV是一对一的关系，用户无须关心其底层实现细节。申请时，用户只需要指定目标空间的大小、访问模式、PV标签选择器和StorageClass等相关信息即可。\n1. PVC的Spec字段的可嵌套字段具体如下。\n\n accessMode：当前PVC的访问模式，其可用模式与PV相同。\n resources：当前PVC存储卷需要占用的资源量最小值\n selector：绑定时对PV应用的标签选择器（matchLabels）或匹配条件表达式（matchEx-pressions），用于挑选要绑定的PV；如果同时指定了两种挑选机制，则必须同时满足两种选择机制的PV才能被选出。\n storageClassName：所依赖的存储类的名称。\n volumeName：用于直接指定要绑定的PV的卷名。\n\n3. 示例\n\n- 安装nfs服务器\n\n```bash\nyum install -y nfs-common nfs-utils rpcbind\nmkdir /nfsdata\nchmod 666 /nfsdata\nchown nfsnobody /nfsdata\ncat /etc/exports\n/nfsdata *(rw,no_root_squash,no_all_squash,sync)\nsystemctl start rpcbind\nsystemctl enable rpcbind\nsystemctl start nfs\nsystemctl enable nfs\n```\n\n- 客户端测试\n  `showmount -e 192.168.10.110` \n  `mount -t nfs 192.168.10.110:/nfsdata /mnt` \n- 创建pvc资源清单，绑定release为stable且storageClassName为slow的pv\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204913606-3af22987-0ce9-4b94-9c5a-9baa256a8209.png#align=left&display=inline&height=351&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=351&originWidth=357&size=393875&status=done&style=none&width=357)\n\n- 查看pvc详细信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204942724-69897a11-3e03-4399-96ba-30c0beb7d224.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=806&size=393875&status=done&style=none&width=806)\n\n# 四、pod中使用pvc\n\n1. 在Pod资源中调用PVC资源，只需要在定义volumes时使用persistentVolumeClaims字段嵌套指定两个字段即可，具体如下。\n\n claimName：要调用的PVC存储卷的名称，PVC卷要与Pod在同一名称空间中。\n readOnly：是否将存储卷强制挂载为只读模式，默认为false。\n\n2. 示例\n\n- 清单定义了一个Pod资源，调用前面刚刚创建的名为pv-nfs的PVC资源：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604204982524-a526ad83-f400-4529-8dd8-a8bacfc1e96d.png#align=left&display=inline&height=421&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=421&originWidth=365&size=393875&status=done&style=none&width=365)\n\n- 进入pod添加测试数据\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205004635-b5f9b30e-8dfb-4cb1-ad71-c4556daf2b44.png#align=left&display=inline&height=232&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=232&originWidth=652&size=393875&status=done&style=none&width=652)\n\n- 删除pod，重新创建，查看数据\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205019145-5b5110d9-013a-4366-ac12-2f036c807fbe.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=657&size=393875&status=done&style=none&width=657)\n\n# 五、PV和PVC的生命周期\n\n\n1. 存储供给\n   存储供给（Provisioning）是指为PVC准备可用PV的机制。Kubernetes支持静态供给和动态供给。\n\n- 静态供给\n  静态供给是指由集群管理员手动创建一定数量的PV的资源供应方式。这些PV负责处理存储系统的细节，并将其抽象成易用的存储资源供用户使用。\n- 动态供给\n  不存在某静态的PV匹配到用户的PVC申请时，Kubernetes集群会尝试为PVC动态创建符合需求的PV，此即为动态供给。这种方式依赖于存储类的辅助，PVC必须向一个事先存在的存储类发起动态分配PV的请求，没有指定存储类的PVC请求会被禁止使用动态创建PV的方式。另外，为了支持使用动态供给机制，集群管理员需要为准入控制器（admission controller）启用“DefaultStorageClass”选项\n\n2. 存储绑定\n   用户基于一系列存储需求和访问模式定义好PVC后，Kubernetes系统的控制器即会为其查找匹配的PV，并于找到之后在此二者之间建立起关联关系，而后它们二者之间的状态即转为“绑定”（Binding）。若PV是为PVC而动态创建的，则该PV专用于其PVC。\n   若是无法为PVC找到可匹配的PV，则PVC将一直处于未绑定（unbound）状态，直到有符合条件的PV出现并完成绑定方才可用。\n\n- 存储使用（Using）\n  Pod资源基于persistenVolumeClaim卷类型的定义，将选定的PVC关联为存储卷，而后即可为内部的容器所使用。对于支持多种访问模式的存储卷来说，用户需要额外指定要使用的模式。一旦完成将存储卷挂载至Pod对象内的容器中，其应用即可使用关联的PV提供的存储空间。\n- PVC保护（Protection）\n  为了避免使用中的存储卷被移除而导致数据丢失，Kubernetes自1.9版本起引入了“PVC保护机制”。启用了此特性后，万一有用户删除了仍处于某Pod资源使用中的PVC时，Kubernetes不会立即予以移除，而是推迟到不再被任何Pod资源使用后方才执行删除操作。处于此种阶段的PVC资源的status字段为“Termination”，并且其Finalizers字段中包含“kubernetes.io/pvc-protection”。\n\n3. 存储回收（Reclaiming）\n   完成存储卷的使用目标之后，即可删除PVC对象以便进行资源回收。不过，至于如何操作则取决于PV的回收策略。目前，可用的回收策略有三种：Retained、Recycled和Deleted。\n\n- 留存（Retain）\n  留存策略意味着在删除PVC之后，Kubernetes系统不会自动删除PV，而仅仅是将它置于“释放”（released）状态。不过，此种状态的PV尚且不能被其他PVC申请所绑定，因为此前的申请生成的数据仍然存在，需要由管理员手动决定其后续处理方案。这就意味着，如果想要再次使用此类的PV资源，则需要由管理员按下面的步骤手动执行删除操作。\n  1）删除PV，这之后，此PV的数据依然留存于外部的存储之上。\n  2）手工清理存储系统上依然留存的数据。\n  3）手工删除存储系统级的存储卷（例如，RBD存储系统上的image）以释放空间，以便再次创建，或者直接将其重新创建为PV。\n- 回收（Recycle）\n  如果可被底层存储插件支持，资源回收策略会在存储卷上执行数据删除操作并让PV资源再次变为可被Claim。另外，管理员也可以配置一个自定义的回收器Pod模板，以便执行自定义的回收操作。不过，此种回收策略行将废弃。\n- 删除（Delete）\n  对于支持Deleted回收策略的存储插件来说，在PVC被删除后会直接移除PV对象，同时移除的还有PV相关的外部存储系统上的存储资产（asset）。支持这种操作的存储系统有AWS\n  EBS、GCE PD、Azure\n  Disk或Cinder。动态创建的PV资源的回收策略取决于相关存储类上的定义，存储类上相关的默认策略为Delete，大多数情况下，管理员都需要按用户期望的处理机制修改此默认策略，以免导致数据非计划内的误删除。\n\n4. 扩展PVC\n   Kubernetes自1.8版本起增加了扩展PV空间的特性，截至目前，它所支持的扩展PVC机制的存储卷共有以下几种。\n\n- gcePersistentDisk\n- awsElasticBlockStore\n- Cinder\n- glusterfs\n- rbd\n  “PersistentVolumeClaimResize”准入插件负责对支持空间大小变动的存储卷执行更多的验证操作，管理员需要事先启用此插件才能使用PVC扩展机制，那些将“allowVolume\n  Expansion”字段的值设置为“true”的存储类即可动态扩展存储卷空间。随后，用户改动Claim请求更大的空间即能触发底层PV空间扩展从而带来PVC存储卷的扩展。\n  对于包含文件系统的存储卷来说，只有在有新的Pod资源基于读写模式开始使用PVC时才会执行文件系统的大小调整操作。换句话说，如果某被扩展的存储卷已经由Pod资源所使用，则需要重建此Pod对象才能触发文件系统大小的调整操作。支持空间调整的文件系统仅有XFS和EXT3/EXT4。', 7, 0, 0, '2020-12-15 00:08:49.721560', '2021-01-26 09:09:23.284708', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (52, 'downwardAPI存储卷', '[TOC]\n\n> 运行于Kubernetes的Pod对象中的容器化应用偶尔也需要获取其所属Pod对象的IP、主机名、标签、注解、UID、请求的CPU及内存资源量及其限额，甚至是Pod所在的节点名称等，容器可以通过环境变量或downwardAPI存储卷访问此类信息，不过，标签和注解仅支持通过存储卷暴露给容器。\n\n\n\n# 一、环境变量式元数据注入\n\n1. 因为在进程启动完成后将无法再向其告知变量值的变动，于是，环境变量也就不支持中途的更新操作。\n1. 可通过fieldRef字段引用的信息。\n\n- spec.nodeName：节点名称。\n- status.hostIP：节点IP地址。\n- metadata.name:Pod对象的名称。\n- metadata.namespace:Pod对象隶属的名称空间。\n- status.podIP:Pod对象的IP地址。\n- spec.serviceAccountName:Pod对象使用的ServiceAccount资源的名称。\n- metadata.uid:Pod对象的UID。\n- metadata.labels[\'<KEY>\']:Pod对象标签中的指定键的值\n- metadata.annotations[\'<KEY>\']:Pod对象注解信息中的指定键的值\n  通过resourceFieldRef字段引用的信息是指当前容器的资源请求及资源限额的定义，包括requests.cpu、limits.cpu、requests.memory和limits.memory四项。\n\n3. 示例\n   资源配置清单中定义的Pod对象通过环境变量向容器env-test-container中注入了Pod对象的名称、隶属的名称空间、标签app的值以及容器自身的CPU资源限额和内存资源请求等信息：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205214421-989c895f-25c5-419a-a9fc-bacde06932dd.png#align=left&display=inline&height=850&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=850&originWidth=479&size=393875&status=done&style=none&width=479)\n\n- 查看环境变量信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205235343-edfe6078-e574-4ee7-aedf-608cbe24b4a5.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=664&size=393875&status=done&style=none&width=664)\n\n\n# 二、存储卷式元数据注入\n\n\n1. 在downwardAPI存储卷中使用fieldRef引用如下两个数据源。\n\n- metadata.labels:Pod对象的所有标签信息。\n- metadata.annotations:Pod对象的所有注解信息。\n\n2. 示例\n   清单中定义的Pod对象通过downwardAPI存储卷向容器volume-test-container中注入了Pod对象隶属的名称空间、标签、注解以及容器自身的CPU资源限额和内存资源请求等信息。存储卷在容器中的挂载点为/etc/podinfo目录\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205259501-2659279b-d1e8-4092-8bec-661a77dc438e.png#align=left&display=inline&height=791&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=791&originWidth=432&size=393875&status=done&style=none&width=432)\n\n\n- 测试访问上述的映射文件，例如，查看Pod对象的标签列表：\n\n```bash\nkubectl exec dapi-vol-pod -- cat /etc/podinfo/pod_labels\napp=\"dapi-vol-pod\"\nrack=\"rack-101\"\nzone=\"east-china\"\n```\n\n', 9, 1, 0, '2020-12-15 00:10:01.166492', '2021-01-08 07:53:54.620223', 1, 1, 1, 1);
INSERT INTO `blog_section` VALUES (53, 'rook简介', '[TOC]\n\n# 一、容器的持久化存储\n\n1. 容器的持久化存储是保存容器存储状态的重要手段，存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。\n1. 由于 Kubernetes 本身的松耦合设计，绝大多数存储项目，比如 Ceph、GlusterFS、NFS 等，都可以为 Kubernetes 提供持久化存储能力。\n\n# 二、Rook\n\n## 1.  简介\n\n- Rook 是一个开源的cloud-native storage编排, 提供平台和框架；为各种存储解决方案提供平台、框架和支持，以便与云原生环境本地集成。\n- Rook 将存储软件转变为自我管理、自我扩展和自我修复的存储服务，它通过自动化部署、引导、配置、置备、扩展、升级、迁移、灾难恢复、监控和资源管理来实现此目的。\n- Rook 使用底层云本机容器管理、调度和编排平台提供的工具来实现它自身的功能。\n- Rook 目前支持Ceph、NFS、Minio、EdgeFS等\n\n## 2.  架构\n\n- Rook使用Kubernetes原语使Ceph存储系统能够在Kubernetes上运行。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606964282557-23f888f4-3de1-4fb6-8f10-6b83eb855164.png#align=left&display=inline&height=928&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=928&originWidth=1650&size=195732&status=done&style=none&width=1650)\n\n\n- 随着Rook在Kubernetes集群中运行，Kubernetes应用程序可以挂载由Rook管理的块设备和文件系统，或者可以使用S3 / Swift API提供对象存储。Rook oprerator自动配置存储组件并监控群集，以确保存储处于可用和健康状态。\n- Rook oprerator是一个简单的容器，具有引导和监视存储集群所需的全部功能。oprerator将启动并监控ceph monitor pods和OSDs的守护进程，它提供基本的RADOS存储。oprerator通过初始化运行服务所需的pod和其他组件来管理池，对象存储（S3 / Swift）和文件系统的CRD。\n- oprerator将监视存储后台驻留程序以确保群集正常运行。Ceph mons将在必要时启动或故障转移，并在群集增长或缩小时进行其他调整。oprerator还将监视api服务请求的所需状态更改并应用更改。\n- Rook oprerator还创建了Rook agent。这些agent是在每个Kubernetes节点上部署的pod。每个agent都配置一个Flexvolume插件，该插件与Kubernetes的volume controller集成在一起。处理节点上所需的所有存储操作，例如附加网络存储设备，安装卷和格式化文件系统。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606964589386-0780df85-541a-472f-836d-69a983a4e2fa.png#align=left&display=inline&height=834&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=834&originWidth=1652&size=197286&status=done&style=none&width=1652)\n\n', 31, 0, 0, '2020-12-15 23:09:56.423303', '2021-01-26 09:09:53.504581', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (54, 'ceph', '# 2.ceph\n\n# 一、ceph基本概念\n\n## 1.  功能\n\n- Ceph是一个可靠、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备和文件系统服务。\n\n## 2. 优点\n\n- 高性能：摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高，能够实现各类负载的副本放置规则，能够支持上千个存储节点的规模，支持TB到PB级的数据。\n- 高可用性：副本数可以灵活控制。支持故障域分隔，数据强一致性。多种故障场景自动进行修复自愈。没有单点故障，自动管理，自动扩充副本数。\n- 高可扩展性：去中心化。扩展灵活。随着节点增加而线性增长。\n- 特性丰富：支持三种存储接口：块存储（得到的是硬盘）、文件存储（目录）、对象存储（对接的是一个挂载的目录，后端会把数据打散，采用键值对形式存储）。支持自定义接口，支持多种语言驱动。\n\n## 3. Ceph核心组件\n\n ![1647888-20190617091304152-781044244.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606966757366-b570250e-8a3d-4b17-92d6-a875f4577344.png#align=left&display=inline&height=600&margin=%5Bobject%20Object%5D&name=1647888-20190617091304152-781044244.png&originHeight=600&originWidth=447&size=227545&status=done&style=none&width=447)\n\n- Monitors（管理服务）：监视器，维护集群状态的多种映射，同时提供认证和日志记录服务，包括有关monitor 节点端到端的信息，其中包括 Ceph 集群ID，监控主机名和IP以及端口。并且存储当前版本信息以及最新更改信息，通过 \"ceph mon dump\"查看 monitor map。\n- MDS（Metadata Server）：Ceph 元数据，主要保存的是Ceph文件系统的元数据。注意：ceph的块存储和ceph对象存储都不需要MDS。\n- OSD：即对象存储守护程序，但是它并非针对对象存储。是物理磁盘驱动器，将数据以对象的形式存储到集群中的每个节点的物理磁盘上。OSD负责存储数据、处理数据复制、恢复、回（Backfilling）、再平衡。完成存储数据的工作绝大多数是由 OSD daemon 进程实现。在构建 Ceph OSD的时候，建议采用SSD 磁盘以及xfs文件系统来格式化分区。此外OSD还对其它OSD进行心跳检测，检测结果汇报给Monitor\n- RADOS：Reliable Autonomic Distributed Object Store。RADOS是ceph存储集群的基础。在ceph中，所有数据都以对象的形式存储，并且无论什么数据类型，RADOS对象存储都将负责保存这些对象。RADOS层可以确保数据始终保持一致。\n- librados：librados库，为应用程度提供访问接口。同时也为块存储、对象存储、文件系统提供原生的接口。\n- RADOSGW：网关接口，提供对象存储服务。它使用librgw和librados来实现允许应用程序与Ceph对象存储建立连接。并且提供S3 和 Swift 兼容的RESTful API接口。\n- RBD：块设备，它能够自动精简配置并可调整大小，而且将数据分散存储在多个OSD上。\n- CephFS：Ceph文件系统，与POSIX兼容的文件系统，基于librados封装原生接口。\n\n## 4. Ceph 数据存储过程\n\n![11706687-69984801fd28704b.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606966824660-847e2f99-f80f-4478-ada2-acf79c94b84c.png#align=left&display=inline&height=705&margin=%5Bobject%20Object%5D&name=11706687-69984801fd28704b.png&originHeight=705&originWidth=614&size=140074&status=done&style=none&width=614)\n\n- File ： 用户需要存储或者访问的文件。当上层应用向RADOS存入size很大的file时，需要将file切分成统一大小的一系列object（最后一个的大小可以不同）进行存储。\n- Object ： 无论使用哪种存储方式（对象、块、文件系统），存储的数据都会被切分成Objects。每个对象都会有一个唯一的OID，由ino（文件的File ID，在全局唯一标识每一个文件）与ono（分片的编号，比如：一个文件FileID为A，它被切成了两个对象，那么这两个文件的oid则为A0与A1）生成。\n- pool是ceph存储数据时的逻辑分区，它起到namespace的作用。其他分布式存储系统。每个pool包含一定数量的PG，PG里的对象被映射到不同的OSD上，因此pool是分布到整个集群的。除了隔离数据，我们也可以分别对不同的POOL设置不同的优化策略，比如副本数、数据清洗次数、数据块及对象大小等。\n- PG（Placement Group）：对object的存储进行组织和位置映射。PG和object之间是“一对多”映射关系。PG和OSD之间是“多对多”映射关系。\n- OSD: 即object storage device，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。\n\n## 5. Ceph中的寻址映射过程\n\n- File -> object映射\n- Object -> PG映射，hash(oid) & mask -> pgid（哈希算法）\n- PG -> OSD映射，CRUSH算法\n\n## 6. RADOS分布式存储相较于传统分布式存储的优势\n\n- 将文件映射到object后，利用Cluster Map 通过CRUSH 计算而不是查找表方式定位文件数据存储到存储设备的具体位置。优化了传统文件到块的映射和Block MAp的管理。\n- RADOS充分利用OSD的智能特点，将部分任务授权给OSD，最大程度地实现可扩展\n\n## 7. cephfs与ceph rbd对比\n\n|          | CephFS 文件存储                                              | Ceph RBD块存储                                               |\n| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 优点:    | 读取延迟低,I/O带宽表现良好,尤其是block size较大一些的文件<br />灵活度高,支持k8s的所有接入模式 | 读取延迟低,I/O带宽表现良好,尤其是block size较大一些的文件I/O带宽表现良好<br />灵活度高,支持k8s的所有接入模式读写延迟都很低<br />支持镜像快照,镜像转储<br />支持k8s动态供给 |\n| 缺点     | 写入延迟相对较高且延迟时间不稳定，不支持动态供给             | 不支持多节点挂载                                             |\n| 适用场景 | 适用于要求灵活度高(支持k8s多节点挂载特性),对I/O延迟不甚敏感的文件读写操作,以及非海量的小文件存储支持.例如作为常用的应用/中间件挂载存储后端. | 对I/O带宽和延迟要求都较高,且无多个节点同时读写数据需求的应用,例如数据库 |\n\n\n\n# 二、部署ceph集群\n\n • github地址：[https://github.com/luckman666/deploy_ceph_cluster_luminous_docker](https://github.com/luckman666/deploy_ceph_cluster_luminous_docker)\n\n## 1. 环境准备\n\n| 主机名 | IP             | 配置 | 磁盘空间                                                     |\n| ------ | -------------- | ---- | ------------------------------------------------------------ |\n| ceph1  | 192.168.10.131 | 4C4G | sda（50G，安装系统）<br/>sdb（50G，不格式化）<br/>sdc（50G，不格式化） |\n| ceph2  | 192.168.10.132 | 4C4G | sda（50G，安装系统） <br/>sdb（50G，不格式化）<br/>sdc（50G，不格式化） |\n| ceph3  | 192.168.10.132 | 4C4G | sda（50G，安装系统） <br/>sdb（50G，不格式化）<br/>sdc（50G，不格式化） |\n\n## 2. clone项目脚本，赋予执行权限\n\n `# git clone https://github.com/luckman666/deploy_ceph_cluster_luminous_docker.git && chmod -R 755 deploy_ceph_cluster_luminous_docker` \n\n## 3. 配置base.config基本信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606967413383-a05329ef-b2f4-4676-b8b0-5d188e4b11f0.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=214&originWidth=440&size=197286&status=done&style=none&width=440)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606967437841-e75ae90f-a984-41a4-90a0-b738d44ac13d.png#align=left&display=inline&height=135&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=135&originWidth=283&size=197286&status=done&style=none&width=283)\n\n## 4. 执行安装脚本\n\n `# ./ceph_luminous.sh` \n\n## 5. 查看安装日志信息\n\n `# tail -f setup.log` \n\n## 6. 验证\n\n `# ceph -s` \n\n# 三、k8s连接ceph\n\n## 1. k8s所有节点安装ceph客户端\n\n- 添加阿里云repo源\n\n```bash\n# vim /etc/yum.repos.d/ceph.repo\n[ceph-nautilus]\nname=ceph-luminous\nbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/\ngpgcheck=0\n[ceph-noarch]\nname=cephnoarch\nbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/\ngpgcheck=0\n```\n\n## 2. 查看可用版本，与ceph服务端版本保持一致\n\n `# yum list ceph --showduplicates | sort -r` \n\n## 3. 安装指定版本ceph\n\n `# yum install -y ceph-common-14.2.6` \n\n## 4. 添加配置文件，验证连接\n\n- ceph进入到/data/ceph/etc目录将 ceph.client.admin.keyring和ceph.conf两个配置文件传送到ceph客户端服务器的/etc/ceph目录下\n\n  `# scp /data/ceph/etc/ceph.client.admin.keyring 192.168.10.101:/etc/ceph` \n  `# scp /data/ceph/etc/ceph.conf 192.168.10.101:/etc/ceph` \n\n- k8s查看ceph信息\n\n  `# ceph -s` \n\n', 13, 0, 0, '2020-12-15 23:21:43.994253', '2021-01-26 09:10:07.980996', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (55, 'rook部署', '[TOC]\n\n> 官方文档链接： [https://www.rook.io/docs/rook/v1.2/ceph-quickstart.html](https://www.rook.io/docs/rook/v1.2/ceph-quickstart.html)\n\n## 1. clone项目到本地\n\n `# git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git` \n `# cd cluster/examples/kubernetes/ceph` \n\n## 2. 创建rook-ceph名称空间\n\n `# kubectl create namespace rook-ceph` \n\n## 3. apply资源清单文件\n\n `# kubectl apply -f common.yaml` \n `# kubectl apply -f operator.yaml` \n \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606967783943-d83b935c-b063-4d99-bdf1-b6c219795b3a.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=582&size=197286&status=done&style=none&width=582)\n \n `# kubectl apply -f cluster.yaml（cluster至少需要三个work节点）` \n \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606967821586-52ec58a6-3e72-44bb-8202-162aaaf19f89.png#align=left&display=inline&height=634&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=634&originWidth=743&size=290912&status=done&style=none&width=743)\n\n## 4. 创建ceph客户端连接工具\n\n `# kubectl apply -f toolbox.yaml`\n\n   ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974297590-95051754-8dbb-4a37-af6a-2da0f0db0031.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=794&size=290912&status=done&style=none&width=794)\n\n\n- 进入容器bash环境\n\n  `# kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath=\'{.items[0].metadata.name}\') bash` \n\n- 查看集群状态\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974364594-fdd33c33-1ba2-4b1e-87c5-d5d883ec1e8b.png#align=left&display=inline&height=539&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=539&originWidth=869&size=290912&status=done&style=none&width=869)\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974402186-6825e64d-2c9a-42a9-9269-7df4a7834941.png#align=left&display=inline&height=362&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=362&originWidth=787&size=290912&status=done&style=none&width=787)\n\n- 使用完成后删除部署\n\n  `kubectl -n rook-ceph delete deployment rook-ceph-tools` \n\n## 5. dashboard部署\n\n- 查看svc信息，默认使用clusterip\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974481647-32499a4b-e6a1-4eb6-bb1f-6d48fd2d10bd.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=161&originWidth=950&size=290912&status=done&style=none&width=950)\n\n- 创建nodeport模式https代理\n\n  `# kubectl apply -f dashboard-external-https.yaml` \n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974517322-41e2b44d-9898-4bfc-b4ef-22455a196fe5.png#align=left&display=inline&height=182&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=182&originWidth=1104&size=290912&status=done&style=none&width=1104)\n\n- 访问测试\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974577186-62e80a91-7323-416c-a439-128a9e8a7435.png#align=left&display=inline&height=656&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=656&originWidth=922&size=290912&status=done&style=none&width=922)\n\n\n- 获取默认密码\n\n  `kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{[\'data\'][\'password\']}\" | base64 --decode && echo` \n\n## 6. 使用Prometheus监控\n\n> [https://www.rook.io/docs/rook/v1.2/ceph-monitoring.html](https://www.rook.io/docs/rook/v1.2/ceph-monitoring.html)\n\n\n\n', 9, 0, 0, '2020-12-15 23:24:42.167996', '2021-01-11 23:13:17.304104', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (56, 'rbd块存储服务', '[TOC]\n\n# 一、创建StorageClass和存储池\n\n • K8 1.13及更高版本使用CSI驱动程序驱动程序。之前版本使用Flex驱动程序\n\n## 1. 创建sc资源\n\n- 默认配置文件在rook/cluster/examples/kubernetes/ceph/csi/rbd下，默认配置无需修改\n\n  `# kubectl apply -f storageclass.yaml` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974840146-eb7a1bde-6dd5-457f-85d1-2e48aef38532.png#align=left&display=inline&height=709&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=709&originWidth=494&size=290912&status=done&style=none&width=494)\n\n## 2. 查看创建的storageclass:\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974867073-1d43958c-8897-42ef-ab7f-3a1655adf785.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=876&size=290912&status=done&style=none&width=876)\n\n## 3. 登录ceph dashboard查看创建的存储池\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606974900555-942150a6-8e14-4793-82f3-9fb5e4871751.png#align=left&display=inline&height=276&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=276&originWidth=902&size=290912&status=done&style=none&width=902)\n\n# 二、使用块存储示例\n\n- 示例文件地址：cluster/examples/kubernetes/Wordpress\n\n## 1. 查看并创建mysql.yaml资源\n\n- 创建pvc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975015717-410eacbe-bb76-401a-8745-4cc05e91e985.png#align=left&display=inline&height=299&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=299&originWidth=425&size=290912&status=done&style=none&width=425)\n\n\n- deployment挂载pvc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975046703-4c9e88f2-7edf-4d7e-9bab-9e2c7fae6392.png#align=left&display=inline&height=165&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=165&originWidth=372&size=290912&status=done&style=none&width=372)\n\n `# kubectl apply -f mysql.yaml` \n\n## 2. 查看并创建wordpress.yaml资源\n\n- 创建pvc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975097983-9c41aaeb-2971-4d5d-9f08-6cc7a6227a4e.png#align=left&display=inline&height=298&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=298&originWidth=426&size=290912&status=done&style=none&width=426)\n\n\n- deployment挂载pvc\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975166754-c8e2013e-7078-4d4d-b986-91d7d4ed362a.png#align=left&display=inline&height=157&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=157&originWidth=515&size=290912&status=done&style=none&width=515)\n\n `# kubectl apply -f wordpress.yaml` \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975207265-01455f83-b7f9-4428-8368-f7d89ad81598.png#align=left&display=inline&height=299&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=299&originWidth=425&size=290912&status=done&style=none&width=425)\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975236402-75a6713d-53d0-49ee-a76c-3b9e329396f0.png#align=left&display=inline&height=165&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=165&originWidth=372&size=290912&status=done&style=none&width=372)\n	\n\n## 3. 这两个应用都会创建一个块存储卷，并且挂载到各自的pod中，查看声明的pvc和pv：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975299900-27d03405-c490-4308-993a-664d9d0389db.png#align=left&display=inline&height=293&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=293&originWidth=1236&size=290912&status=done&style=none&width=1236)\n\n## 4. 登陆pod检查rbd设备：\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975334329-326cf2db-f82e-4bfe-b56c-b582848c974d.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=210&originWidth=867&size=290912&status=done&style=none&width=867)\n\n## 5. 登录ceph dashboard查看创建的images\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975367399-377e0f8a-b00a-48cf-b5cb-008568ac44d2.png#align=left&display=inline&height=316&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=316&originWidth=1024&size=290912&status=done&style=none&width=1024)\n\n## 6. 获取wordpress应用程序的集群IP并使用浏览器访问\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975399958-63decf98-5958-4cd4-9afa-527d3cbe37f2.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=821&size=290912&status=done&style=none&width=821)\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975426085-0f238ca3-d9d5-419d-8f38-f76b227a09cd.png#align=left&display=inline&height=913&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=913&originWidth=997&size=290912&status=done&style=none&width=997)\n	\n	\n	\n	\n	\n\n', 12, 0, 0, '2020-12-15 23:26:34.185690', '2021-01-11 23:12:08.321248', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (57, 'cephfs共享文件存储', '[TOC]\n\n# 一、创建CephFileSystem\n## 1. 创建资源对象\n> 使用默认/rook/cluster/examples/kubernetes/ceph/filesystem.yaml资源清单文件，无需修改\n\n `# kubectl apply -f filesystem.yaml`\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975772485-a8140616-005c-487c-aa98-e935b826ecd7.png#align=left&display=inline&height=682&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=682&originWidth=401&size=290912&status=done&style=none&width=401)\n## 2. 查看创建的cephfs资源\n\n- 查看cephfilesystem资源信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975823657-014ae792-0b73-4952-be01-64ec8a45f228.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=650&size=290912&status=done&style=none&width=650)\n\n\n- 查看dashboard MDS信息\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975847139-00950ca0-5c98-4f86-a5b4-68056965ccb9.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=650&size=290912&status=done&style=none&width=650)\n\n- 查看dashboard pool信息\n\n![Snipaste_2020-02-13_10-12-25.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606975973693-a0507d6e-9815-4553-aebf-022e23e38f32.png#align=left&display=inline&height=302&margin=%5Bobject%20Object%5D&name=Snipaste_2020-02-13_10-12-25.png&originHeight=302&originWidth=904&size=59189&status=done&style=none&width=904)\n\n# 二、创建Provision Storage\n • 在Rook开始配置存储之前，需要根据文件系统创建一个StorageClass。Kubernetes需要与CSI驱动程序进行互操作以创建持久卷。\n## 1. 创建资源\n\n- 使用/rook/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml资源清单文件，无需修改\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976035180-dbee3f1e-356b-4598-b4a8-9745e7be6c7c.png#align=left&display=inline&height=642&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=642&originWidth=908&size=238104&status=done&style=none&width=908)\n `# kubectl apply -f storageclass.yaml` \n## 2. 查看sc信息\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976076332-2a6c622d-322c-4baf-95a9-3cef5754b970.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=637&size=238104&status=done&style=none&width=637)\n# 三、使用cephfs示例：K8s注册表\n## 1. 创建资源对象\n> 使用rook/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml资源清单文件并创建\n\n- pvc信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976160102-0875af2a-afc5-439a-b155-bd876831aa5d.png#align=left&display=inline&height=278&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=278&originWidth=375&size=238104&status=done&style=none&width=375)\n\n- 创建deployment资源\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976191763-189b4a7a-cb94-4e85-a3f0-a713bb1397f2.png#align=left&display=inline&height=617&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=617&originWidth=539&size=238104&status=done&style=none&width=539)\n\n- 查看数据卷信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976220571-b4bbf8d8-d043-4a57-917f-0ec31c7c136f.png#align=left&display=inline&height=451&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=451&originWidth=470&size=238104&status=done&style=none&width=470)\n `# kubectl apply -f kube-registry.yaml` \n## 2. 查看pv和pvc信息\n    ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976344149-904a1016-fe3d-4c6c-978e-b8957185f62c.png#align=left&display=inline&height=179&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=179&originWidth=1227&size=238104&status=done&style=none&width=1227)\n## 3. 查看验证\n\n- 创建的3个pod绑定同一个存储资源\n\n ![Snipaste_2020-02-13_10-23-29.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976405961-fd545f0b-d01e-492d-88a4-866b6fe70c70.png#align=left&display=inline&height=383&margin=%5Bobject%20Object%5D&name=Snipaste_2020-02-13_10-23-29.png&originHeight=383&originWidth=951&size=155323&status=done&style=none&width=951)\n ![Snipaste_2020-02-13_10-22-28.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976431641-a191feda-8ebf-4a77-8683-5ff43052a83e.png#align=left&display=inline&height=387&margin=%5Bobject%20Object%5D&name=Snipaste_2020-02-13_10-22-28.png&originHeight=387&originWidth=960&size=155469&status=done&style=none&width=960)\n ![Snipaste_2020-02-13_10-21-37.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976452581-7abb52a2-05ca-4c0e-b640-ab59518b81d7.png#align=left&display=inline&height=390&margin=%5Bobject%20Object%5D&name=Snipaste_2020-02-13_10-21-37.png&originHeight=390&originWidth=955&size=156393&status=done&style=none&width=955)\n\n\n\n\n', 8, 0, 0, '2020-12-15 23:29:32.479044', '2021-01-16 21:36:05.197861', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (58, 'RGW对象存储服务', '[TOC]\n\n • Ceph 对象存储可以简称为 RGW，Ceph RGW 是基于 librados，为应用提供 RESTful 类型的对象存储接口，其接口方式支持 S3（兼容 Amazon S3 RESTful API） 和 Swift（兼容 OpenStack Swift API） 两种类型。下边演示如何基于 Rook 创建 Ceph 对象存储，并验证测试。\n\n# 一、创建对象存储\n\n## 1. 创建一个CephObjectStore使用S3 API在群集中启动RGW服务。\n\n- 此样本至少需要3个bluestore OSD，每个OSD位于不同的节点上。OSD必须位于不同的节点上，因为failureDomain设置为host，并且erasureCoded块设置至少需要3个不同的OSD（2 dataChunks+ 1 codingChunks）。\n- 查看/root/rook/cluster/examples/kubernetes/ceph/object.yaml资源清单文件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976635477-c0c6d817-3c9a-44c5-928b-f6d6a259155d.png#align=left&display=inline&height=686&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=686&originWidth=387&size=238104&status=done&style=none&width=387)\n\n	`# kubectl apply -f object.yaml` \n\n## 2. 查看创建的rgw信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976667642-23c441f8-9016-455c-bddf-44917a224200.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=844&size=238104&status=done&style=none&width=844)\n\n## 3. dashboard查看pool信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976787267-3edf68dc-6175-4b12-919c-76782fbae793.png#align=left&display=inline&height=516&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=516&originWidth=869&size=238104&status=done&style=none&width=869)\n\n# 二、创建Storage\n\n## 1. 创建一个Bucket\n\n- 客户端可以在其中读取和写入对象。可以通过定义存储类来创建Bucket，类似于块和文件存储所使用的模式。首先，定义将允许对象客户端创建Bucket的存储类。存储类定义对象存储系统，Bucket保留策略以及管理员所需的其他属性。\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976843472-8f423227-5c55-4093-973e-7ed4f448bb98.png#align=left&display=inline&height=276&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=276&originWidth=416&size=238104&status=done&style=none&width=416)\n  `# kubectl apply -f storageclass-bucket-delete.yaml` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976891830-f13c836b-f97b-4da5-bb50-5926bc434c5b.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=74&originWidth=668&size=238104&status=done&style=none&width=668)\n\n## 2. 创建存储类\n\n- 基于此存储类，对象客户端现在可以通过创建对象Bucket声明（OBC）来请求Bucket。创建OBC后，Rook-Ceph存储桶配置程序将创建一个Bucket。\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976963511-9672b455-2159-4a69-b60c-23dbf6a909db.png#align=left&display=inline&height=300&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=300&originWidth=522&size=238104&status=done&style=none&width=522)\n  `# kubectl apply -f object-bucket-claim-delete.yaml` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606976995531-48ca35a3-d159-4d8a-a9f8-578efc01f866.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=491&size=238104&status=done&style=none&width=491)\n\n## 3. 使用与OBC相同的名称和相同的名称空间创建一个secret和ConfigMap。\n\n- 包含应用程序pod用来访问Bucket的凭据。ConfigMap包含存储区终结点信息，并且也由Pod使用\n- 从secret和configmap中提取关键信息：\n\n```bash\nexport AWS_HOST=$(kubectl get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk \'{print $2}\')\nexport AWS_PORT=$(kubectl get cm ceph-delete-bucket -o yaml | grep BUCKET_PORT | awk \'{print $2}\')\nexport AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk \'{print $2}\' | base64 --decode)\nexport AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk \'{print $2}\' | base64 --decode)\n```\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977190177-a6fab6dd-3739-489c-861a-0131034eb2f6.png#align=left&display=inline&height=185&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=185&originWidth=516&size=238104&status=done&style=none&width=516)\n\n# 三、集群内使用对象存储\n\n## 1. rook-ceph名称空间创建busybox并设置环境变量\n\n- 进入centos容器\n\n  `# kubectl -n rook-ceph exec -it centos bash` \n\n- 设置环境变量\n\n```bash\nexport AWS_HOST=rook-ceph-rgw-my-store.rook-ceph\nexport AWS_ENDPORT=10.103.168.162:80\nexport AWS_ACCESS_KEY_ID=42DPXRE5OFRNJK8A3OFI\nexport AWS_SECRET_ACCESS_KEY=k0SxaIoQAbUYLKpLxdkwbOWuCZHfg9iJ77G28YY5\n```\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977382508-052358b8-148a-47d8-923a-ef461d655984.png#align=left&display=inline&height=182&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=182&originWidth=442&size=238104&status=done&style=none&width=442)\n • Host：在群集中找到rgw服务的DNS主机名。\n • Endpoint：rgw服务正在侦听的端点。运行kubectl -n rook-ceph get svc rook-ceph-rgw-my-store，然后组合clusterIP和端口。\n • Access key：access_key如上打印的用户\n • Secret key：secret_key如上打印的用户\n\n## 2. 安装s3cmd\n\n	`yum --assumeyes install s3cmd` \n\n## 3. 验证\n\n- 获取 Bucket 列表 \n\n`# s3cmd ls --no-ssl --host=${AWS_HOST}` \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977432920-af55ddb6-0f89-4425-a35b-f55174911e5b.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=747&size=238104&status=done&style=none&width=747)\n\n- 将文件上传到新创建的对象\n\n  `# echo \"Hello Rook\" > /tmp/rookObj` \n  `# s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://ceph-bkt-f139c5f5-1200-42fa-8c74-668ecb38bdda` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977478539-3fe9d510-e7d3-4393-800a-e5c34439a2e4.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=1226&size=238104&status=done&style=none&width=1226)\n\n- 从 Bucket 下载并验证文件\n\n  `# s3cmd get s3://ceph-bkt-f139c5f5-1200-42fa-8c74-668ecb38bdda/rookObj /tmp/rookObj-download --no-ssl --host=${AWS_HOST} --host-bucket=` \n  `# cat /tmp/rookObj-download` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977541205-247606ba-7f00-4668-a148-eb05f3d83e76.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=470&size=238104&status=done&style=none&width=470)\n\n# 四、集群外部访问\n\n • Rook设置了对象存储，因此Pod可以访问集群内部。如果应用程序在集群外部运行，则需要通过来设置外部服务NodePort。\n\n## 1. 将RGW公开给群集的服务。保留该服务不变，并为外部访问创建一个新服务。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977570745-82d06e33-f48d-4659-99fc-21d9d4260ece.png#align=left&display=inline&height=69&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=69&originWidth=816&size=238104&status=done&style=none&width=816)\n\n- 查看/rook/cluster/examples/kubernetes/ceph/rgw-external.yaml并创建nodeport服务\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977600931-f1468c6e-ac1b-4f91-815a-da87e05aa0e4.png#align=left&display=inline&height=484&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=484&originWidth=478&size=238104&status=done&style=none&width=478)\n  `# kubectl apply -f rgw-external.yaml` \n\n## 2. 查看正在运行的rgw服务，并注意外部服务在哪个端口上运行：\n\n `# kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external` \n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977646298-a4c4c63f-16c5-478c-9199-3ba3c6402091.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=1164&size=238104&status=done&style=none&width=1164)\n\n\n- 内部的rgw服务在port上运行80，外部端口为31106。现在可以CephObjectStore从任何地方访问了！您只需要集群中任何计算机的主机名，外部端口和用户凭据。\n\n## 3. 创建一个用户\n\n如果需要创建一组独立的用户凭据来访问S3端点，先创建一个CephObjectStoreUser。该用户将使用S3 API连接到群集中的RGW服务。用户将独立于先前创建的任何对象。\n\n- 查看object-user.yaml并创建用户\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977683230-857a8eca-efe1-4b46-96e5-f2265a73d7e9.png#align=left&display=inline&height=189&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=189&originWidth=399&size=238104&status=done&style=none&width=399)\n  `# kubectl apply -f object-user.yaml` \n\n## 4. 查看用户密钥信息\n\n `# kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml | grep AccessKey | awk \'{print $2}\' | base64 --decode` \n `# kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml | grep SecretKey | awk \'{print $2}\' | base64 --decode` \n\n\n\n', 5, 0, 0, '2020-12-15 23:31:40.655805', '2021-01-08 07:55:53.020565', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (59, '维护rook存储', '[TOC]\n\n# 一、节点增加osd磁盘\n\n## 1. work节点添加磁盘sdb\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977924655-8dca8533-f5bd-43df-9a4f-66f2a69ac2dc.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=440&size=238104&status=done&style=none&width=440)\n\n## 2. 修改集群配置\n\n `# kubectl -n rook-ceph edit cephclusters.ceph.rook.io` \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977945818-57041f92-186a-4a8a-a18f-7489c3a6cb25.png#align=left&display=inline&height=411&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=411&originWidth=327&size=238104&status=done&style=none&width=327)\n\n## 3. node节点查看磁盘信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606977979536-afdbd722-409a-4cae-a463-0c0250c20180.png#align=left&display=inline&height=101&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=101&originWidth=633&size=238104&status=done&style=none&width=633)\n\n## 4. ceph客户端连接查看，sdb磁盘100G已添加\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1606978002982-0c4e1d35-7b4d-4bd7-8e65-b27cec830b90.png#align=left&display=inline&height=255&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=255&originWidth=873&size=238104&status=done&style=none&width=873)\n\n# 二、将错误osd卸载\n\n## 1. 查看错误osd的id编号\n\n `# ceph osd tree` \n\n## 2. 将osd.1的状态设置为out\n\n `# ceph osd out osd.1` \n\n## 3. 从集群中删除该osd\n\n `# ceph osd rm osd.1` \n\n## 4. 从CRUSH中删除该osd\n\n `# ceph osd crush rm osd.1` \n\n## 5. 删除osd.1的认证信息\n\n `# ceph auth del osd.1` \n\n## 6. 卸载磁盘\n\n `# umount /dev/sdb1` \n\n\n\n\n\n\n\n', 8, 0, 0, '2020-12-15 23:32:55.492214', '2021-01-15 00:22:54.376269', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (60, '网络概述', '[TOC]\n\n# 一、概述\n\n1. k8s网络特征\n\n 每个POD 具有集群内唯一IP\n 所有POD 通过IP 直接访问其他POD，不管POD 是否在同一台物理机上\n POD 内的所有容器共享一个网络堆栈, POD 内的容器, 都可以使用localhost 来访问pod 内的其他容器。\n\n2. Kubernetes的网络通信主要分为以下几种情况：\n\n Pod内容器之间的通信\n Pod到Pod之间的通信\n Pod到Service之间的通信\n 集群外部与内部组件之间的通信\n\n# 二、KubeProxy的实现模式\n\n1. 当service account创建的时候，Service Controller和EndPoints Controller就会被触发更新一些资源，例如基于Service中配置的Pod的selector给每一个Pod创建一个EndPoint资源并存入etcd，kube-proxy还会更新iptables的chain规则生成基于Service的Cluster IP链路到对应Pod的链路规则。\n1. 接下来集群内的一个pod想访问某个服务，直接cluster ip:port即可基于iptables的链路将请求发送到对应的Pod，这一层有两种挑选pod的算法，轮询(Round Robin)和亲和度匹配(Session Affinity)。当然，除了这种iptabels的模式，还有一种比较原始的方式就是用户态的转发。\n1. Kube-Proxy 会为每个 Service 随机监听一个端口 (Proxy Port)，并增加一条IPtables 规则。从客户端到 ClusterIP:Port 的报文都会被重定向到 Proxy Port，Kube-Proxy 收到报文后，通过 Round Robin (轮询) 或者 Session Affinity（会话亲和力，即同一 Client IP 都走同一链路给同一 Pod 服务）分发给对应的 Pod。\n\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205669464-b8838320-edc4-4936-8e45-b642e51dd99a.png#align=left&display=inline&height=564&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=564&originWidth=723&size=393875&status=done&style=none&width=723)\n\n\n# 三、CNI插件\n\n1. Kubernetes设计了网络模型，但将其实现交给了网络插件。于是，各种解决方案不断涌现。为了规范及兼容各种解决方案，CoreOS和Google联合制定了CNI（Container Network Interface）标准，旨在定义容器网络模型规范。\n1. 它连接了两个组件：容器管理系统和网络插件。它们之间通过JSON格式的文件进行通信，以实现容器的网络功能。具体的工作均由插件来实现，包括创建容器netns、关联网络接口到对应的netns以及为网络接口分配IP等。\n1. CNI的基本思想是：容器运行时环境在创建容器时，先创建好网络名称空间（netns），然后调用CNI插件为这个netns配置网络，而后再启动容器内的进程。\n1. 常用插件\n\n- Flannel（简单、使用居多）：基于Vxlan技术（叠加网络+二层隧道），不支持网络策略\n- Calico（较复杂，使用率少于Flannel）：也可以支持隧道网络，但是是三层隧道（IPIP），支持网络策略\n- Calico项目既能够独立地为Kubernetes集群提供网络解决方案和网络策略，也能与flannel结合在一起，由flannel提供网络解决方案，而Calico此时仅用于提供网络策略，这时我们也可以将Calico称为Canal。\n\n', 8, 0, 0, '2020-12-15 23:38:49.171989', '2021-01-26 09:10:32.335646', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (61, '网络类型', '[TOC]\n\n# 一、Pod内容器之间的通信：localhost\n\n\n1. 在 Kubernetes 的世界里，IP 是以 Pod 为单位进行分配的。一个 Pod内部的所有容器共享一个网络堆栈（实际上就是一个网络命名空间，包括它们的 IP\n   地址、网络设备、配置等都是共享的）。\n1. 在同一个pod内由pause镜像启动的容器。所有运行于同一个Pod内的容器与同一主机上的多个进程类似，彼此之间可通过lo接口完成交互。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205916825-3c46387d-c2b9-45da-b985-d5fb87aa223b.png#align=left&display=inline&height=785&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=785&originWidth=624&size=393875&status=done&style=none&width=624)\n\n# 二、同一node上的Pod之间的通信：overlay network\n\n\n1. 同一个Node内的不同Pod之间可以直接采用对方Pod的IP地址通信，而且不需要使用其他发现机制，例如DNS、Consul或者etcd。\n1. Pod1和Pod2都是通信veth\n   pair连接到同一个docker0网桥上，它们的IP地址IP1、IP2都是从docker0网段上动态获取的，它们和网桥本身的IP3是同一个网段的。由于Pod1和Pod2处于同一局域网内，它们之间可以通过docker0作为路由量进行通信。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604205963977-0e136342-5cbd-483e-a8d4-a8ceb3e8bae1.png#align=left&display=inline&height=455&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=455&originWidth=678&size=393875&status=done&style=none&width=678)\n\n\n# 三、不同node上的Pod之间的通信：iptables(LVS)规则\n\n\n1. 在Kubernetes的网络世界中，Pod之间假设是通过访问对方的Pod IP进行通信的，而不同Node之间的通信只能通过Node的物理网卡进行，Pod的IP地址是由各Node上的docker0网桥动态分配的。我们想要实现跨Node的Pod之间的通信，至少需要满足下面三个条件：\n\n- 知道Pod IP 和Node IP之间的映射关系，通过Node IP转发到Pod IP；\n- 在整个Kubernetes集群中对Pod的IP分配不能出现冲突；\n- 从Pod中发出的数据包不应该进行NAT地址转换。\n\n2. Kubernetes会记录所有正在运行的Pod的IP分配信息，并将这些信息保存到etcd中（作为Service的Endpoint），这样我们就可以知道Pod\n   IP和Node IP之间的映射关系。\n2. 以Flannel为例，Flannel实现的容器的跨主机通信通过如下过程实现：\n\n- 每个主机上安装并运行etcd和flannel；\n- 在etcd中规划配置所有主机的docker0子网范围；\n- 每个主机上的flanneld根据etcd中的配置，为本主机的docker0分配子网，保证所有主机上的docker0网段不重复，并将结果（即本主机上的docker0子网信息和本主机IP的对应关系）存入etcd库中，这样etcd库中就保存了所有主机上的docker子网信息和本主机IP的对应关系；\n- 当需要与其他主机上的容器进行通信时，查找etcd数据库，找到目的容器的子网所对应的outip（目的宿主机的IP）；\n- 将原始数据包封装在VXLAN或UDP数据包中，IP层以outip为目的IP进行封装；\n- 由于目的IP是宿主机IP，因此路由是可达的；\n- VXLAN或UDP数据包到达目的宿主机解封装，解出原始数据包，最终到达目的容器。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206071139-bb287a80-c537-4d85-8f27-8877679a031a.png#align=left&display=inline&height=776&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=776&originWidth=1278&size=468500&status=done&style=none&width=1278)\n\n\n# 四、Service与Pod间的通信：iptables(LVS)规则\n\n\n1. 集群网络需要在启动kube-apiserver时经由“--service-cluster-ip-range”选项进行指定，如10.96.0.0/12，而每个Service对象在此网络中均拥一个称为Cluster-IP的固定地址。\n1. 管理员或用户对Service对象的创建或更改操作由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将其定义为相应节点上的iptables规则或ipvs规则，借此完成从Service的Cluster-IP与Pod-IP之间的报文转发\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206146014-8923ae48-0c8c-4974-b572-2562fb1891a6.png#align=left&display=inline&height=215&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=215&originWidth=553&size=468500&status=done&style=none&width=553)\n\n\n# 五、集群外部到Pod对象之间的通信\n\n\n1. 将集群外部的流量引入到Pod对象的方式有受限于Pod所在的工作节点范围的节点端口（nodePort）和主机网络（hostNetwork）两种，以及工作于集群级别的NodePort或LoadBalancer类型的Service对象。\n1. 即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。', 6, 0, 0, '2020-12-15 23:40:18.317208', '2021-01-26 09:12:05.170813', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (62, 'flannel网络插件', '[TOC]\n\n# 一、原理\n\n1. 为了解决docker主机默认使用同一个子网造成容器ip地址相同问题，预留使用一个网络，如10.244.0.0/16，而后自动为每个节点的Docker容器引擎分配一个子网，如10.244.1.0/24和10.244.2.0/24，并将其分配信息保存于etcd持久存储。\n1. 为了解决因为在网络中缺乏路由信息而无法准确送达。flannel有着多种不同的处理方法，每一种处理方法也可以称为一种网络模型，或者称为flannel使用的后端。\n\n- VxLAN：虚拟可扩展局域网，采用的是MAC in UDP封装方式，将虚拟网络的数据帧添加到VxLAN首部后，封装在物理网络的UDP报文中，然后以传统网络的通信方式传送该UDP报文，待其到达目的主机后，去掉物理网络报文的头部信息以及VxLAN首部，然后将报文交付给目的终端，\n- Host\n  GateWay：通过在节点上创建到达目标容器地址的路由直接完成报文转发，因此这种方式要求各节点本身必须在同一个二层网络中，故该方式不太适用于较大的网络规模（大二层网络除外）。host-gw有着较好的转发性能，且易于设定，推荐对报文转发性能要求较高的场景使用。\n- UDP：使用普通UDP报文封装完成隧道转发，其性能较前两种方式要低很多，仅应该在不支持前两种方式的环境中使用。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206265281-757ac079-145a-4ae9-bcb9-99d786613138.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=285&originWidth=917&size=468500&status=done&style=none&width=917)\n\n# 二、配置清单\n\n1. 参考地址：[https://github.com/coreos/flannel/blob/master/Documentation/configuration.md](https://github.com/coreos/flannel/blob/master/Documentation/configuration.md)\n1. flannel的网络地址是10.244.0.0/16,默认每个子网的掩码长度为24\n1. K8s节点之间（Node）通过Vxlan技术进行通信，根据node情况，会把flannel的16位网络地址拆分成多个24位网络地址，供各Node进行分配\n1. 每个Node节点按序占用一个C类地址，对应节点上面的Podip是在该C类地址中按规则分配的。\n\n# 三、配置参数\n\n> flannel的配置信息保存于etcd的键名/coreos.com/network/config之下，可以使用etcd服务的客户端工具来设定或修改其可用的相关配置。\n\n1. Network：flannel于全局使用的CIDR格式的IPv4网络，字符串格式，此为必选键，余下的均为可选。\n1. SubnetLen：将Network属性指定的IPv4网络基于指定位的掩码切割为供各节点使用的子网，此网络的掩码小于24时（如16），其切割子网时使用的掩码默认为24位。\n1. SubnetMin：可用作分配给节点使用的起始子网，默认为切分完成后的第一个子网；字符串格式。\n1. SubnetMax：可用作分配给节点使用的最大子网，默认为切分完成后最大的一个子网；字符串格式。\n1. Backend：flannel要使用的后端类型，以及后端的相关配置，字典格式；VxLAN、host-gw和UDP后端各有其相关的参数。\n\n# 四、配置实例\n\n1. 全局网络为“10.244.0.0/16”，切分子网时用到的掩码长度为24，将相应的子网10.244.0.0/24-10.244.255.0/24分别分配给每一个工作节点使用，选择VxLAN作为使用的后端类型，并监听于8472端口\n\n```json\n{\n    \"Network\": \"10.244.0.0/16\",\n    \"SubnetLen\": 24,\n    \"SubnetMin\": \"10.244.0.0\",\n    \"SubnetMax\": \"10.244.255.0\",\n    \"Backend\": {\n        \"Type\": \"VxLAN\",\n        \"Port\": 8472\n    }\n}\n```', 7, 0, 0, '2020-12-15 23:41:04.228797', '2021-01-26 09:13:30.763499', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (63, '网络策略', '[TOC]\n\n# 一、概述\n\n\n1. 用于控制分组的Pod资源彼此之间如何进行通信，以及分组的Pod资源如何与其他网络端点进行通信的规范。它用于为Kubernetes实现更为精细的流量控制，实现租户隔离机制。Kubernetes使用标准的资源对象“NetworkPolicy”供管理员按需定义网络访问控制策略\n1. 核心概念\n\n ![networkpolicy.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206460233-2a4e20f6-9577-4c40-9d91-1f7d3bbb5b36.png#align=left&display=inline&height=562&margin=%5Bobject%20Object%5D&name=networkpolicy.png&originHeight=562&originWidth=970&size=65787&status=done&style=none&width=970)\n\n- Ingress（类似安全组的内外网入）：能接受哪些客户端的访问（-from，ports：自己端口）\n- egress（类似安全组的内外网出）：能访问哪些目标（-to，ports：目标端口）\n- 每种方向的控制策略则包含“允许”和“禁止”两种。默认情况下，Pod处于非隔离状态，它们的流量可以自由来去。\n- 当对应区域有多条规则时，满足其一即可。\n\n# 二、配置网络策略\n\n\n## 1. 管理过程\n\n- 由namespaceSelector选定名称空间后，NetworkPolicy对象使用标签选择器由ipBlock或podSelector选择出一组Pod资源作为控制对象。\n- 一旦名称空间中有任何NetworkPolicy对象匹配了某特定的Pod对象，则该Pod将拒绝Network-Policy所不允许的一切连接请求，而那些未被任何NetworkPolicy对象匹配到的其他Pod对象仍可接受所有流量。\n- 一旦在spec.policyTypes中指定了生效的规则类型，却在networkpolicy.spec字段中嵌套定义了没有任何规则的Ingress或Egress字段时，则表示拒绝相关方向上的一切流量。\n\n## 2. 定义网络策略时常用到的术语及说明具体。\n\n- Pod组：由网络策略通过Pod选择器选定的一组Pod的集合，它们是规则生效的目标Pod；可由NetworkPolicy对象通过macthLabel或matchExpression选定。\n- Egress：出站流量，即由特定的Pod组发往其他网络端点的流量，通常由流量的目标网络端点（to）和端口（ports）来进行定义。\n- Ingress：入站流量，即由其他网络端点发往特定Pod组的流量，通常由流量发出的源站点（from）和流量的目标端口所定义。\n- 端口（ports）:TCP或UDP的端口号。\n- 端点（to,\n  from）：流量目标和流量源相关的组件，它可以是CIDR格式的IP地址块（ipBlock）、网络名称空间选择器（namespaceSelector）匹配的名称空间，或Pod选择器（podSelector）匹配的Pod组。\n  无论是Ingress还是Egress流量，与选定的某Pod组通信的另一方都可使用“网络端点”予以描述，它通常是某名称空间中的一个或一组Pod资源。\n\n## 3. 管控入站流量规则\n   在Ingress规则中嵌套from和ports字段即可匹配特定的入站流量，from字段的值是一个对象列表，它可嵌套使用ipBlock、namespaceSelector和podSelector字段来定义流量来源。\n\n| ipBlock           | Object | 根据IP地址或网络地址块选择流量源端点                         |\n| ----------------- | ------ | ------------------------------------------------------------ |\n| namespaceSelector | Object | 基于集群级别的标签挑选名称空间，它将匹配由此标签选择器选出的所有名称空间内的所有Pod对象；空值表示所有的名称空间，即源站点为所有名称空间内的所有Pod对象 |\n| podSelector       | Object | 于NetworkPolicy所在的当前名称空间内基于标签选择器挑选Pod资源，赋予字段以空值来表示挑选当前名称空间内的所有Pod对象 |\n| ports             | Object | 它嵌套port和protocol来定义流量的目标端口，即由NetworkPolicy匹配到的当前名称空间内的所有Pod资源上的端口 |\n| port              | string | 端口号或在Container上定义的端口名称，未定义时匹配所有端口    |\n| protocol          | string | 传输层协议的名称，TCP或UDP，默认为TCP                        |\n\n## 4. 管控出站流量规则\n   networkpolicy.spec中嵌套的Egress字段用于定义入站流量规则，就特定的Pod集合来说，出站流量一样默认处于放行状态，除非在所有入站策略中至少有一条规则能够明确匹配到它。Egress字段的值是一个字段列表，它主要由以下两个字段组成。\n\n| to    | Object | 由当前策略匹配到的Pod资源发起的出站流量的目标地址列表，多个项目之间为“或”（OR）关系；若未定义或字段值为空则意味着应用于所有目标地址（默认为不限制）；若明确给出了主机地址列表，则只有目标地址匹配列表中的主机地址的出站流量被放行 |\n| ----- | ------ | ------------------------------------------------------------ |\n| ports | Object | 出站流量的目标端口列表，多个端口之间为“或”（OR）关系；若未定义或字段值为空则意味着应用于所有端口（默认为不限制）；若明确给出了端口列表，则只有目标端口匹配列表中的端口的出站流量被放行 |\n\n\n\n# 三、Calico安装\n\n\n1. Calico官网：\n   [https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel](https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/flannel)\n1. 安装calico的canal插件：\n   curl [https://docs.projectcalico.org/v3.10/manifests/canal.yaml](https://docs.projectcalico.org/v3.10/manifests/canal.yaml) -O\n\n- 如果使用的是pod cidr 10.244.0.0/16，请跳到下一步。如果您使用的是不同的pod cidr，请使用以下命令来设置包含pod cidr的环境变量pod\n  cidr，并将清单中的10.244.0.0/16替换为pod cidr。\n\n    POD_CIDR=\"<your-pod-cidr>\"\n    sed -i -e \"s?10.244.0.0/16?$POD_CIDR?g\" canal.yaml\n\n3. 部署canal插件：\n   `kubectl apply -f canal.yaml` \n3. 使用kubectl get pods -n kube-system中查看安装进程。\n\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206605663-080f060c-6f71-4e6a-b409-c142f52ee640.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=742&size=468500&status=done&style=none&width=742)\n\n\n# 四、示例\n\n## 1. 管控入站流量-设置默认的Ingress策略\n\n- 通过policyTypes字段指明要生效Ingress类型的规则，但未定义任何Ingress字段，因此不能匹配到任一源端点，从而拒绝所有入站\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-ingress\nspec:\n  podSelector: {}\n  policyTypes: [\"Ingress\"]\n```\n\n## 2. 管控入站流量-放行特定的入站流量\n\n- 将default名称空间中拥有标签“app=myapp”的Pod资源的80/TCP端口开放给10.244.0.0/16网络内除10.244.3.0/24子网中的所有源端点，以及当前名称空间中拥有标签“app=myapp”的所有Pod资源访问，其他未匹配到的源端点的流量默认为允许访问。\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-myapp-ingress\nnamespace: default\nspec:\npodSelector:\n  matchLabels:\n    app: myapp\npolicyTypes: [\"Ingress\"]\ningress:\n- from:\n  - ipBlock:\n      cidr: 10.244.0.0/16\n      except:\n      -10.244.3.0/24\n  - podSelector:\n      matchLabels:\n        app: myapp\n  ports:\n  - protocol: TCP\n    port: 80\n```\n\n## 3. 管控出站流量-设置默认的Egress策略\n\n- 通过policyTypes字段指明要生效Egress类型的规则，但未定义任何Egress字段，因此不能匹配到任何目标端点，从而拒绝所有的入站流量\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-egress\nspec:\n  podSelector: {}\n  policyTypes: [\"Egress\"]\n```\n\n## 4. 管控出站流量-管控出站流量-设置默认的Egress策略\n\n- 对来自拥有“app=tomcat”的Pod对象的，到达标签为“app=nginx”的Pod对象的80端口，以及到达标签为“app=mysql”的Pod对象的3306端口的流量给予放行\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-tomcat-egress\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: tomcat\n  policyTypes: [\"Egress\"]\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: nginx\n    ports:\n    - protocol: TCP\n      port: 80\n  - to:\n    - podSelector:\n        matchLabels:\n          app: mysql\n    ports:\n    - protocol: TCP\n      port: 3306\n```\n\n## 5. 隔离名称空间\n\n- 为default名称空间定义了相关的规则，在出站和入站流量默认均为拒绝的情况下，它用于放行名称空间内部的各Pod对象之间的通信，以及与kube-system名称空间内各Pod间的通信\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-deny-all\n  namespace: default\nspec:\n  policyTypes: [\"Ingress\", \"Egress\"]\n  podSelector: {}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-\n  namespace: default\nspec:\n  policyTypes: [\"Ingress\", \"Egress\"]\n  podSelector: {}\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchExpressions:\n        - key: name\n          operator: In\n          values: [\"default\", \"kube-system\"]\n  egress:\n  - to:\n    - namespaceSelector:\n        matchExpressions:\n        - key: name\n          operator: In\n          values: [\"default\", \"kube-system\"]\n```', 8, 0, 0, '2020-12-15 23:43:37.931914', '2021-01-26 09:14:08.695827', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (64, '网络与策略实例', '[TOC]\n\n# 一、实现目标\n\n\n> 假设有名为testing的名称空间内运行着一组nginx Pod和一组myappPod。要求实现如下目标。\n\n1. myapp Pod仅允许来自nginx Pod的流量访问其80/TCP端口，但可以向nginx Pod的所有端口发出出站流量。\n1. nginx Pod允许任何源端点对其80/TCP端口的访问，并能够向任意端点发出出站流量。\n1. myapp Pod和nginx Pod都可与kube-system名称空间的任意Pod进行任何类型的通信，以便于可以使用由kube-dns提供的名称解析服务等。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206958257-269bd384-9409-4802-8f04-e890578c93fb.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=562&size=468500&status=done&style=none&width=562)\n\n# 二、创建基本环境\n\n\n1. 创建testing名称空间，为其添加标签“ns=kube-system”\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604206983917-5a1fdfde-693d-4241-97f8-ceae3bb057e0.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=268&size=468500&status=done&style=none&width=268)\n\n2. 使用deployment控制器在testing空间创建nginx pod\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207001046-3b98e111-2a85-40f8-89d3-f25a5eacbc81.png#align=left&display=inline&height=494&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=494&originWidth=356&size=468500&status=done&style=none&width=356)\n\n3. 使用deployment控制器在testing空间创建myapp pod\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207019847-f800c0a9-1c74-48e2-a805-cb39906c379c.png#align=left&display=inline&height=491&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=491&originWidth=441&size=468500&status=done&style=none&width=441)\n\n4. 启动一个终端，在default名称空间中创建一个用于测试的临时交互式客户端：\n\n `# kubectl run cirros-$RANDOM --namespace=default --rm -it --image=cirros -- sh`\n\n- 分别测试访问nginx和myapp的服务，名称空间的默认网络策略为准许访问，接下来确认其访问请求可正常通过：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207041995-469fdb8e-be8d-4f2c-8055-3df92c32c98c.png#align=left&display=inline&height=163&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=163&originWidth=577&size=468500&status=done&style=none&width=577)\n\n# 三、定义默认网络策略并测试\n\n\n1. 将testing名称空间的入站及出站的默认策略修改为拒绝访问\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207099586-3bfd1f4a-a3b1-4ddd-a42b-6736b432270d.png#align=left&display=inline&height=242&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=242&originWidth=417&size=468500&status=done&style=none&width=417)\n\n2. 查看对应的netpol生成情况\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207118851-bcc044bf-7fb9-4bff-83fa-f81c5c751ff9.png#align=left&display=inline&height=67&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=67&originWidth=536&size=468500&status=done&style=none&width=536)\n\n3. 再一次进行访问测试：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207133888-53d875d9-090f-48ba-9b87-2b9a7ea8be18.png#align=left&display=inline&height=88&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=88&originWidth=218&size=468500&status=done&style=none&width=218)\n\n\n# 四、定义流量放行规则\n\n\n1. 配置清单，允许任何源端点对其80/TCP端口的访问\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207149721-0a9c05c5-8003-4089-a267-124da41be849.png#align=left&display=inline&height=486&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=486&originWidth=402&size=468500&status=done&style=none&width=402)\n\n  测试客户端发起访问请求进行测试，nginx已经能够正常访问。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207166918-2b88010f-8ecd-412a-be75-a7be0a2cf9e4.png#align=left&display=inline&height=162&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=162&originWidth=364&size=468500&status=done&style=none&width=364)\n\n2. 放行testing名称空间中来自nginx Pod的发往myapp Pod的80/TCP的访问流量，以及myapp Pod发往nginx Pod的所有流量。允许myapp Pod与kube-system名称空间的任何Pod进行交互的所有流量：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207193630-415a5c1a-bedc-47d5-a849-cc62da82dc12.png#align=left&display=inline&height=729&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=729&originWidth=396&size=468500&status=done&style=none&width=396)\n\n  测试客户端发起访问请求进行测试，myapp拒绝访问。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207211548-74a65f6b-01fa-4d5f-92c9-e6468c782635.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=217&size=468500&status=done&style=none&width=217)\n\n  进入nginx空间访问myapp测试，允许访问。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207228730-3cea9023-3f87-4f38-b699-1002d3b3cfad.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=882&size=468500&status=done&style=none&width=882)', 9, 0, 0, '2020-12-15 23:50:57.056811', '2021-01-26 09:14:32.807664', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (65, '访问控制', '[TOC]\n\n# 一、机制说明\n\n\n1. Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。API Server是集群内部各个组件通信的中介，也是外部控制的入口。所以 Kubernetes的安全机制基本就是围绕保护 API Server 来设计的。Kubernetes使用了认证（Authentication）、鉴权（Authorization）、准入控制（AdmissionControl）三步来保证API Server的安全\n1. API Server处理请求的过程中，认证插件负责鉴定用户身份，授权插件用于操作权限许可鉴别，而准入控制则用于在资源对象的创建、删除、更新或连接（proxy）操作时实现更精细的许可检查。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207401772-5ae6ca0a-ff22-42b8-b294-0775db03523b.png#align=left&display=inline&height=353&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=353&originWidth=811&size=468500&status=done&style=none&width=811)\n\n\n# 二、k8s中的用户概念\n\n\n> 客户端访问API服务的途径通常有三种：kubectl、客户端库或者直接使用REST接口进行请求，而可以执行此类请求的主体也被Kubernetes分为两类：现实中的“人”和Pod对象，它们的用户身份分别对应于常规用户（User Account）和服务账号（Service Account）。\n\n1. User Account（用户账号）：一般是指由独立于Kubernetes之外的其他服务管理的用户账号，例如由管理员分发的密钥、Keystone一类的用户存储（账号库）、甚至是包含有用户名和密码列表的文件等。Kubernetes中不存在表示此类用户账号的对象，因此不能被直接添加进Kubernetes系统中。\n1. Service Account（服务账号）：是指由Kubernetes API管理的账号，用于为Pod之中的服务进程在访问Kubernetes API时提供身份标识（identity）。Service Account通常要绑定于特定的名称空间，它们由API Server创建，或者通过API调用手动创建，附带着一组存储为Secret的用于访问API Server的凭据。\n\n# 三、Service Account\n\n1. Pod中的容器访问API Server。因为Pod的创建、销毁是动态的，所以要为它手动生成证书就不可行了。Kubenetes使用了Service Account解决Pod 访问API Server的认证问题\n1. ServiceAccount 中用到包含三个部分：Token、ca.crt、namespace\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207600564-97813084-0e3d-4f28-b7cf-c123de3bd43b.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=993&size=468500&status=done&style=none&width=993)\n\n- token：保存了Service Account的认证token，容器中的进程使用它向API\n  Server发起连接请求，进而由认证插件完成用户认证并将其用户名传递给授权插件。\n- ca.crt：根证书。用于Client端验证API Server发送的证书\n- namespace：标识这个service-account-token的作用域名空间\n\n3. 默认情况下，每个 namespace 都会有一个 ServiceAccount，如果 Pod 在创建时没有指定 ServiceAccount，就会使用 Pod 所属的 namespace 的ServiceAccount', 8, 0, 0, '2020-12-15 23:58:42.142086', '2021-01-26 09:14:43.140789', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (66, '认证', '[TOC]\n\n# 一、认证\n\n1. 认证方式\n\n- HTTP Token 认证：用一个很长的特殊编码方式的并且难以被模仿的字符串 - Token来表达客户的一种方式。Token 是一个很长的很复杂的字符串，每一个 Token对应一个用户名存储在 API Server 能访问的文件中。当客户端发起 API调用请求时，需要在 HTTP Header 里放入 Token\n- HTTP Base 认证：用户名+密码用 BASE64 算法进行编码后的字符串放在 HTTP Request中的 HeatherAuthorization域里发送给服务端，服务端收到后进行编码，获取用户名及密码\n- HTTPS 证书认证：基于 CA 根证书签名的客户端身份认证方式\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604207985541-ea6e9dfc-cd71-4844-9b36-9f8ae890437d.png#align=left&display=inline&height=435&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=435&originWidth=702&size=468500&status=done&style=none&width=702)\n\n2. 安全性说明\n\n- Controller Manager、Scheduler 与 API Server 在同一台机器，所以直接使用 API Server 的非安全端口访问，--insecure-bind-address=127.0.0.1\n- kubectl、kubelet、kube-proxy 访问 API Server 就都需要证书进行 HTTPS 双向认证\n\n# 二、HTTPS认证\n\n\n1. 证书颁发\n\n- 手动签发：通过 k8s 集群的跟 ca 进行签发 HTTPS 证书\n- 自动签发：kubelet 首次访问 API Server 时，使用 token做认证，通过后，Controller Manager 会为kubelet生成一个证书，以后的访问都是用证书做认证了\n\n# 三、kubeconfig\n\n\n1. kubeconfig 文件包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context 信息（集群名称、用户名）。Kubenetes 组件通过启动时指定不同的 kubeconfig文件可以切换到不同的集群。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209072283-49a2e61e-7719-45e9-81c7-774fc3fbadea.png#align=left&display=inline&height=152&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=152&originWidth=493&size=468500&status=done&style=none&width=493)\n\n2. 查看kubeconfig文件内容\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209088933-ae7c8b7c-d82b-4999-aa09-e36f6bd876c8.png#align=left&display=inline&height=464&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=464&originWidth=498&size=468500&status=done&style=none&width=498)\n\n- clusters：集群列表，包含访问API Server的URL和所属集群的名称等。\n- users：用户列表，包含访问API Server时的用户名和认证信息。\n- contexts：kubelet的可用上下文列表，由用户列表中的某特定用户名称和集群列表中的某特定集群名称组合而成。\n- current-context：kubelet当前使用的上下文名称，即上下文列表中的某个特定项。\n\n3. kubectlconfig命令的常用操作\n\n- kubectl config view：打印kubeconfig文件内容。\n- kubectl config set-cluster：设置kubeconfig的clusters配置段。\n- kubectl config set-credentials：设置kubeconfig的users配置段。\n- kubectl config set-context：设置kubeconfig的contexts配置段。\n- kubectl config use-context：设置kubeconfig的current-context配置段。\n\n# 四、TLS bootstrapping机制\n\n\n1. 新的工作节点接入集群时，由kubelet自行生成私钥和证书签署请求，而后发送给集群上的证书签署进程（CA），由管理员验证请求后予以签署或直接控制器进程自动统一签署。这种方式即为Kubelet TLS Bootstrapping机制。\n\n# 五、总结\n\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209142277-0143f8fd-29b7-423f-9e4e-eadcd39465e1.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=181&originWidth=774&size=468500&status=done&style=none&width=774)\n\n', 10, 0, 0, '2020-12-15 23:59:41.107322', '2021-01-09 16:44:27.948499', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (67, '鉴权', '[TOC]\n\n# 一、RBAC 授权模式\n\n\n1. RBAC的基于“角色”（role）这一核心组件实现了权限指派，它为账号赋予一到多个角色从而让其具有角色之上的权限，其中的账号可以是用户账号、用户组、服务账号及其相关的组等，而同时关联至多个角色的账号所拥有的权限是多个角色之上的权限集合。\n1. RBAC具有如下优势\n\n- 对集群中的资源和非资源型URL的权限实现了完整覆盖。\n- 整个RBAC完全由少数几个API对象实现，而且同其他API对象一样可以用kubectl或API调用进行操作。\n- 支持权限的运行时调整，无须重新启动API Server。\n\n3. RBAC基本概念\n\n- Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。\n- Subject：被作用者，既可以是“人”，也可以是“机器”，也可以使你在 Kubernetes 里定义的“用户”。\n- RoleBinding：定义了“被作用者”和“角色”的绑定关系。\n\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209404236-d77d934e-9e7c-4ead-a78c-6a1fc35eff68.png#align=left&display=inline&height=461&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=461&originWidth=845&size=468500&status=done&style=none&width=845)\n\n\n4. RBAC 的 API 资源对象\n   RBAC 引入了 4个新的顶级资源对象：Role、ClusterRole、RoleBinding、ClusterRoleBinding，4种对象类型均可以通过 kubectl 与 API 操作\n\n- Role：作用于名称空间级别，用于定义名称空间内的资源权限集合\n- ClusterRole：用于集群级别的资源权限集合。常用于控制Role无法生效的资源类型，这包括集群级别的资源（如Nodes）、非资源类型的端点（如/healthz）和作用于所有名称空间的资源（例如，跨名称空间获取任何资源的权限）。\n- RoleBinding用于将Role上的许可权限绑定到一个或一组用户之上，它隶属于且仅能作用于一个名称空间。绑定时，可以引用同一名称中的Role，也可以引用集群级别的ClusterRole。\n- ClusterRoleBinding把ClusterRole中定义的许可权限绑定在一个或一组用户之上，它仅可以引用集群级别的ClusterRole。\n\n5. 支持的动作\n   create delete deletecollection get list patch update watch，bind等\n5. 支持的资源\n   “services”, “endpoints”, “pods“，\"deployments“ **“**jobs”，“configmaps”，“nodes”，“rolebindings”，“clusterroles”\n\n# 二、Role and ClusterRole\n\n1. Role表示一组规则权限，权限只会增加(累加权限)，不存在一个资源一开始就有很多权限而通过RBAC对其进行减少的操作\n1. Role 可以定义在一个 namespace 中，如果想要跨 namespace则可以创建ClusterRole，ClusterRole 具有与 Role相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的，ClusterRole可以用于:\n\n- 集群级别的资源控制( 例如 node 访问权限 )\n- 非资源型 endpoints( 例如/healthz访问 )\n- 所有命名空间资源控制(例如 pods )\n\n3. Role对象中的rules也称为PolicyRule，用于定义策略规则，不过它不包含规则应用的目标，其可以内嵌的字段包含如下几个。\n\n| apiGroups       | string | 包含了资源的API组的名称，支持列表格式指定的多个组，空串（\"\"）表示核心组。 |\n| --------------- | ------ | ------------------------------------------------------------ |\n| resourceNames   | string | 规则应用的目标资源名称列表，可选，缺省时意味着指定资源类型下的所有资源。 |\n| resources       | string | 规则应用的目标资源类型组成的列表，例如pods、deployments、daemonsets、roles等，ResourceAll表示所有资源 |\n| verbs           | string | 可应用至此规则匹配到的所有资源类型的操作列表，可用选项有get、list、create、update、patch、watch、proxy、redirect、delete和deletecollection；此为必选字段 |\n| nonResourceURLs | string | 用于定义用户应该有权限访问的网址列表，它并非名称空间级别的资源，因此只能应用于ClusterRole和ClusterRoleBinding，在Role中提供此字段的目的仅为与ClusterRole在格式上兼容。 |\n\n\n\n2. RoleBinding的配置中主要包含两个嵌套的字段subjects和roleRef，其中，subjects的值是一个对象列表，用于给出要绑定的主体，而roleRef的值是单个对象，用于指定要绑定的Role或ClusterRole资源。\n\n- subjects字段的可嵌套字段具体如下。\n\n| apiGroup  | string | 要引用的主体所属的API群组，对于ServiceAccount类的主体来说默认为\"\"，而User和Group类主体的默认值为\"rbac.authorization.k8s.io\" |\n| --------- | ------ | ------------------------------------------------------------ |\n| kind      | string | 要引用的资源对象（主体）所属的类别，可用值为\"User\" \"Group\"和\"ServiceAccount\"三个，必选字段 |\n| name      | string | 引用的主体的名称，必选字段                                   |\n| namespace | string | 引用的主体所属的名称空间，对于非名称空间类型的主体，如\"User\"和\"Group\"，其值必须为空，否则授权插件将返回错误信息 |\n\n\n\n- roleRef的可嵌套字段具体如下。\n\n| apiGroup | string | 引用的资源（Role或ClusterRole）所属的API群组，必选字段    |\n| -------- | ------ | --------------------------------------------------------- |\n| kind     | string | 引用的资源所属的类别，可用值为Role或ClusterRole，必选字段 |\n| name     | string | 引用的资源的名称                                          |\n\n\n\n3. Resources\n\n- Kubernetes 集群内一些资源一般以其名称字符串来表示，这些字符串一般会在 API 的\n  URL 地址中出现；同时某些资源也会包含子资源，例如 logs 资源就属于 pods\n  的子资源，它们的URL格式通常形如如下表示：GET\n  /api/v1/namespaces/{namespace}/pods/{name}/log\n- 在RBAC角色定义中，如果要引用这种类型的子资源，则需要使用“resource/subre-source”的格式\n\n# 三、RoleBinding and ClusterRoleBinding\n\n1. RoloBinding 可以将角色中定义的权限授予用户或用户组，RoleBinding包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users,groups, or service accounts)；\n1. RoloBinding 同样包含对被Bind 的 Role 引用；RoleBinding适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权\n1. RoleBinding 同样可以引用 ClusterRole 来对当前 namespace 内用户、用户组或ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的ClusterRole，然后在不同的 namespace 中使用RoleBinding 来引用\n1. to Subjects\n\n- RoleBinding 和 ClusterRoleBinding 可以将 Role 绑定到 Subjects；Subjects可以是 groups、users 或者service accounts\n- Subjects 中 Users 使用字符串表示，它可以是一个普通的名字字符串，如“alice”；也可以是 email 格式的邮箱地址，如“wangyanglinux@163.com”；甚至是一组字符串形式的数字 ID 。但是 Users 的前缀，system: 是系统保留的，集群管理员应该确保普通用户不会使用这个前缀格式\n- Groups 书写格式与 Users 相同，都为一个字符串，并且没有特定的格式要求；同样\n  system: 前缀为系统保留\n\n# 四、常用策略\n\n\n1. Role+RoleBinding\n   正常使用：某个名称空间级别的权限授权\n1. ClusterRole+ClusterRoleBinding\n   正常使用：k8s集群级别的权限授权\n1. ClusterRole+RoleBinding\n   交叉使用：其中ClusterRole策略会降级成rolebinding名称空间的策略，效果和role+rolebinding一样，好处就是在名称空间很多的时候，重复权限的配置文件会少一半，而且更灵活', 11, 0, 0, '2020-12-16 00:00:41.765314', '2021-01-13 23:39:47.096835', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (68, '准入控制', '[TOC]\n\n# 一、简介\n\n\n1. 在经由认证插件和授权插件分别完成身份认证和权限检查之后，准入控制器将拦截那些创建、更新和删除相关的操作请求以强制实现控制器中定义的功能，包括执行对象的语义验证、设置缺失字段的默认值、限制所有容器使用的镜像文件必须来自某个特定的Registry、检查Pod对象的资源需求是否超出了指定的限制范围等。\n1. 准入控制是API Server的插件集合，通过添加不同的插件，实现额外的准入控制规则。甚至于API Server的一些主要的功能都需要通过 Admission Controllers 实现，比如ServiceAccount\n1. 在具体运行时，准入控制可分为两个阶段，第一个阶段变更准入控制，用来修改请求的对象，第二个阶段验证准入控制，用于验证请求的对象，在此过程中，一旦任一阶段中的任何控制器拒绝请求，则立即拒绝整个请求，并向用户返回错误。\n\n# 二、常用控制器\n\n- AlwaysPullImages：总是拉取远端镜像；好处：可以避免本地镜像被恶意入侵而篡改\n- LimitRanger：此准入控制器将确保所有资源请求不会超过namespace的LimitRange（定义Pod级别的资源限额，如cpu、mem）\n- ResourceQuota：此准入控制器将观察传入请求并确保它不违反命名空间的ResourceQuota对象中列举的任何约束（定义名称空间级别的配额，如pod数量）\n- PodSecurityPolicy：此准入控制器用于创建和修改pod，并根据请求的安全上下文和可用的Pod安全策略确定是否应该允许它。\n- ServiceAccount：实现了自动化添加 ServiceAccount。\n\n# 三、LimitRange资源与LimitRanger准入控制器\n\n\n1. 使用LimitRange资源在每个名称空间中为每个容器指定最小及最大计算资源用量，甚至是设置默认的计算资源需求和计算资源限制。在名称空间上定义了LimitRange对象之后，客户端提交创建或修改的资源对象将受到LimitRanger控制器的检查，任何违反LimitRange对象定义的资源最大用量的请求将被直接拒绝。\n1. LimitRange资源支持限制容器、Pod、PersistentVolumeClaim的系统资源用量，其中Pod和容器主要用于定义可用的CPU和内存资源范围，而PersistentVolume-Claim则主要定义存储空间的限制范围。\n1. 以容器的CPU资源为例，default用于定义默认的资源限制，defaultRequest定义默认的资源需求，min定义最小的资源用量，而最大的资源用量既可以使用max给出固定值，也可以使用maxLimitRequestRatio设定为最小用量的指定倍数：\n1. 示例\n\n- 创建资源清单，设置资源限制\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209793377-511fe316-991c-4bd0-848e-bf8c9735ca06.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=398&originWidth=733&size=468500&status=done&style=none&width=733)\n\n- 创建测试正常pod\n  `kubectl run limit-pod1 --image=ikubernetes/myapp:v1 --restart=Never` \n- 查看pod默认信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209819115-a65c1bc5-9338-4838-b801-cd36dea1b01a.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=196&size=468500&status=done&style=none&width=196)\n\n- 创建pod对象设定的系统资源需求量大于LimitRange中的最大用量限制\n  `kubectl run limit-pod2 --image=ikubernetes/myapp:v1 --restart=Never --limits=\'cpu=3000m\'` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209839362-064cc9c3-16a2-4f7a-9386-8eecefa2ac52.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=44&originWidth=1254&size=468500&status=done&style=none&width=1254)\n\n\n# 四、ResourceQuota资源与准入控制器\n\n\n1. ResourceQuota资源用于定义名称空间的对象数量或系统资源配额，它支持限制每种资源类型的对象总数，以及所有对象所能消耗的计算资源及存储资源总量等。\n1. 管理员可为每个名称空间分别创建一个ResourceQuota对象，随后，用户在名称空间中创建资源对象，ResourceQuota准入控制器将跟踪使用情况以确保它不超过相应ResourceQuota对象中定义的系统资源限制。\n1. ResourceQuota对象可限制指定名称空间中非终止状态的所有Pod对象的计算资源需求及计算资源限制总量\n1. 示例\n\n- 编辑配置文件resoucequota-demo.yaml，并apply\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209860164-665f60d2-0729-44d8-b5ad-366f411b0c48.png#align=left&display=inline&height=331&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=331&originWidth=467&size=468500&status=done&style=none&width=467)\n\n- describe命令打印其限额的生效情况\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209883654-d3590e5e-e83f-48d6-85bc-003b74dd594a.png#align=left&display=inline&height=301&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=301&originWidth=561&size=468500&status=done&style=none&width=561)\n\n- 创建一个pod后查看信息\n  `kubectl run limit-pod1 --image=ikubernetes/myapp:v1 --restart=Never`\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604209917755-f03e0aad-27e8-4b54-a5b8-b43579ff5452.png#align=left&display=inline&height=301&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=301&originWidth=618&size=468500&status=done&style=none&width=618) ', 10, 0, 0, '2020-12-16 00:01:58.819194', '2021-01-09 16:44:38.423396', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (69, '示例', '[TOC]\n\n# 一、创建一个User Account\n\n\n1. 为billy用户创建私钥及证书文件\n\n- 生成私钥文件，并将文件放置于/etc/kubernetes/pki/专用目录中：\n  `[root@master  ~]# cd /etc/kubernetes/pki/ ` \n  `[root@master  pki]#(umask 077;openssl genrsa -out billy.key 2048) ` \n- 创建证书签署请求，-subj选项中CN的值将被kubeconfig作为用户名使用，O的值将被识别为用户组：\n  `[root@master  pki]# openssl req -new -key kube-dba.key -out kube-dba.csr  -subj \"/CN=billy O=kubeusers\"` \n- 基于kubeadm安装Kubernetes集群时生成的CA签署证书，这里设置其有效时长为3650天：\n  `[root@master  pki]# openssl x509 -req -in kube-dba.csr -CA ca.crt -CAkey  ca.key -CAcreateserial -out kube-dba.crt -days 3650` \n\n2. 创建配置文件\n\n- 设置新集群（k8s），指明apiserver地址，k8s证书路径和隐藏证书，并保存在（/root/billy.conf）\n  `[root@master  pki]# kubectl config set-cluster k8s  --server=https://192.168.10.100:6443 --certificate-authority=ca.crt--embed-certs=true --kubeconfig=/root/billy.conf` \n- 配置客户端证书及密钥，用户名信息会通过命令从证书Subject的CN值中自动提取，而组名则来自于“O=kubeusers”的定义：\n  `[root@master  pki]#kubectl config set-credentials billy  --client-certificate=billy.crt --client-key=billy.key --embed-certs=true --kubeconfig=/root/billy.conf` \n- 配置context，用来组合cluster和credentials，即访问的集群的上下文。\n  `[root@master  pki]# kubectl config set-context billy@k8s  --cluster=k8s  --user=billy --kubeconfig=/root/billy.conf` \n\n3. 创建系统用户及k8s验证文件\n\n- 创建系统用户并拷贝配置文件\n\n```bash\n[root@master  ~]# useradd billy \n[root@master  ~]# mkdir /home/billy/.kube \n[root@master  ~]# cp billy.conf /home/billy/.kube/config \n[root@master  ~]# chown billy.billy -R /home/billy/.kube/ \n[root@master  ~]# su - billy\n```\n\n- 测试访问集群资源，不过在启用RBAC的集群上执行命令时，billy并未获得集群资源的访问权限，因此会出现错误提示：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210160504-0f4f35c6-d983-46d7-8e7a-52f3d2e5ce8c.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=1211&size=468500&status=done&style=none&width=1211)\n\n- 切换至admin用户\n  `[root@master  pki]# kubectl config use-context kubernetes-admin@kubernetes ` \n\n\n\n# 二、授权billy访问default命名空间资源\n\n\n1. 创建Role\n\n- 设定读取、列出及监视Pod资源的许可权限：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210196176-6af23f42-a585-47b5-889b-ff457e9d2b4b.png#align=left&display=inline&height=194&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=194&originWidth=491&size=468500&status=done&style=none&width=491)\n\n2. 创建Rolebinding\n\n- 用户billy和role pods-reader的绑定\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210217798-fc26a9df-b115-41c6-8723-17e0cbb41aab.png#align=left&display=inline&height=284&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=284&originWidth=496&size=468500&status=done&style=none&width=496)\n\n3. 访问验证\n\n- 访问default空间的pod资源信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210244466-f5d324bc-3cfd-45bf-a2d2-67180394720e.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=453&size=468500&status=done&style=none&width=453)\n\n- 访问所有名称空间的pod资源信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210390404-c91717b3-09f3-428e-a3a3-2a7a033439eb.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=766&size=468500&status=done&style=none&width=766)\n\n- 访问default空间的svc资源信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210402647-e6978d67-cd3e-450e-9062-84091e508e02.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=873&size=468500&status=done&style=none&width=873)\n\n\n# 三、授权billy访问所有命名空间资源\n\n\n1. 创建ClusterRole\n\n- 设定读取、列出及监视Pod资源的许可权限\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210425289-189f8812-06f9-4841-9edf-4526f7c95644.png#align=left&display=inline&height=192&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=192&originWidth=491&size=468500&status=done&style=none&width=491)\n\n2. 创建Rolebinding\n\n- 用户billy和clusterrole cluster-reader的绑定\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210445481-40f806a7-cd91-4722-a0fb-eafebee310ff.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=285&originWidth=494&size=468500&status=done&style=none&width=494)\n\n3. 访问验证\n\n- 访问所有名称空间的pod资源信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210471318-fa53ef6a-e408-4bcd-8070-576cd6a2dbc5.png#align=left&display=inline&height=208&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=208&originWidth=699&size=468500&status=done&style=none&width=699)\n\n# 四、授权devuser用户只能访问database名称空间\n\n1. 清除前面授予的权限信息\n\n```bash\n[root@master  ~]# kubectl delete -f pod-reader.yaml \n[root@master  ~]# kubectl delete -f pod-reader-cluster.yaml \n[root@master  ~]# kubectl delete -f resources-reader.yaml \n[root@master  ~]# kubectl delete -f resources-reader-cluster.yaml\n```\n\n2. 创建database名称空间\n   `[root@master  pki]# kubectl create namespace database ` \n2. 创建Role，设定只能读取、列出及监视database下的Pod资源的许可权限\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210665966-fa721b47-84be-4421-b0cb-797237275c37.png#align=left&display=inline&height=216&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=216&originWidth=498&size=468500&status=done&style=none&width=498)\n\n4. 创建Rolebinding\n\n- 用户billy和role pods-reader的绑定\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210685707-552d5912-75bc-4843-ba1d-b8a9a7617ddc.png#align=left&display=inline&height=308&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=308&originWidth=494&size=468500&status=done&style=none&width=494)\n\n5. 访问验证\n\n- 访问database下的pod资源信息正常\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210700845-35829c73-448b-437a-bed4-3d970f744e72.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=507&size=468500&status=done&style=none&width=507)\n\n- 访问default下的pod资源信息报错\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210716481-5dd0731e-c9e8-4ebe-820f-8e67f70c2064.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=70&originWidth=758&size=468500&status=done&style=none&width=758)\n\n\n# 五、ServiceAccount授权\n\n1. 创建SA\n   `kubectl create sa billy-sa` \n1. 创建Role\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210742150-3b398cb0-2016-4348-8dd9-adf4c1402e00.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=501&size=468500&status=done&style=none&width=501)\n\n3. 创建Rolebinding，将billy-sa和billy-sa-role的绑定\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604210765114-e58266d2-7f4d-4e78-93b5-dbc7a3303d42.png#align=left&display=inline&height=288&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=288&originWidth=496&size=468500&status=done&style=none&width=496)\n\n4. 验证结果\n   创建完SA之后系统会自动创建一个secret，我们可以获取这个secret里面的token去登录dashboard，就可以看到相应有权限的资源。\n   `kubectl get secret billy-sa-token-9rc55 -o jsonpath={.data.token} |base64 -d` \n\n- 还可以在创建pod时在pod的spec里指定serviceAccountName，那么这个pod就拥有了对应的权限。', 8, 0, 0, '2020-12-16 00:03:58.299996', '2021-01-08 07:57:05.650341', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (70, '调度器概述', '[TOC]\n\n# 一、概述\n\n\n1. 目标：基于资源可用性将各Pod资源公平地分布于集群节点之上。\n1. 若预选后不存在任何一个满足条件的节点，则Pod被置于Pending状态，直到至少有一个节点可用为止\n1. 除了 kubernetes自带的调度器，你也可以编写自己的调度器。通过spec:schedulername参数指定调度器的名字，可以为pod 选择某个调度器进行调度。\n\n\n\n# 二、调度过程\n\n\n1. 调度步骤：节点预选（Predicate）、节点优先级排序（Priority） 、节点择优\n\n 节点预选：基于一系列预选规则对每个节点进行检查，将那些不符合条件的节点过滤掉从而完成节点预选。\n 节点优先排序：对预选出的节点进行优先级排序，以便选出最适合运行Pod对象的节点\n 节点择优：从优先级排序结果中挑出优先级最高的节点运行Pod对象，当此类节点多于一个时，则从中随机选择一个。\n\n2. 节点预选有一系列的算法可以使用：\n\n PodFitsResources：节点上剩余的资源是否大于 pod 请求的资源\n PodFitsHost：如果 pod 指定了 NodeName，检查节点名称是否和 NodeName匹配PodFitsHost\n Ports：节点上已经使用的 port 是否和 pod 申请的 port 冲突\n PodSelectorMatches：过滤掉和 pod 指定的 label 不匹配的节点\n NoDiskConflict：已经 mount 的 volume 和 pod 指定的 volume不冲突，除非它们都是只读\n\n3. priorities\n   过程：按照优先级大小对节点排序优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重（该项的重要性）。这些优先级选项包括：\n\n LeastRequestedPriority：通过计算 CPU 和 Memory的使用率来决定权重，使用率越低权重越高。换句话说，这个优先级指标倾向于资源使用比例更低的节点\n BalancedResourceAllocation：节点上 CPU 和 Memory使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用\n ImageLocalityPriority：倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高通过算法对所有的优先级项目和权重进行计算，得出最终的结果', 12, 0, 0, '2020-12-16 23:00:17.321604', '2021-01-26 09:14:50.057147', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (71, 'node亲和调度', '[TOC]\n\n# 一、概述\n\n\n1. 基于节点上的自定义标签和Pod对象上指定的标签选择器将pod调度至node节点\n1. 定义节点亲和性规则时有两种类型的节点亲和性规则：硬亲和性（required）和软亲和性（preferred）。\n\n 硬亲和性是强制性规则，它是Pod调度时必须要满足的规则，而在不存在满足规则的节点时，Pod对象会被置为Pending状态。\n 软亲和性是一种柔性调度限制，它倾向于将Pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但在无法满足调度需求时它将退而求其次地选择一个不匹配规则的节点。\n\n3. 定义节点亲和规则的关键点有两个，一是为node配置合乎需求的标签，另一个是为Pod对象定义合理的标签选择器，从而能够基于标签选择出符合期望的目标节点。\n3. 在Pod资源基于节点亲和性规则调度至某节点之后，节点标签发生了改变而不再符合此节点亲和性规则时，它仅对新建的Pod对象生效。调度器不会将Pod对象从此节点上移出。\n3. 键值运算关系\n\n| In           | label 的值在某个列表中   |\n| ------------ | ------------------------ |\n| NotIn        | label 的值不在某个列表中 |\n| Gt           | label 的值大于某个值     |\n| Lt           | label 的值小于某个值     |\n| Exists       | 某个 label 存在          |\n| DoesNotExist | 某个 label 不存在        |\n\n6. requiredDuringSchedulingIgnoredDuringExecution字段用于定义节点硬亲和性，由一到多个nodeSelectorTerm定义的对象组成，彼此间为“逻辑或”的关系。\n6. nodeSelectorTerm用于定义节点选择器条目，由一个或多个matchExpressions对象定义的匹配规则组成，多个规则彼此之间为“逻辑与”的关系。\n\n# 二、示例\n\n\n1. 使用节点硬亲和规则定义，将当前Pod对象调度至拥有zone标签且其值为foo的节点之上。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211230253-8f32c9c8-fbe3-4b60-80a2-fc02ffb775a8.png#align=left&display=inline&height=401&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=401&originWidth=638&size=29881&status=done&style=none&width=638)\n\n- 处于Pending阶段，这是由于强制型的节点亲和限制场景中不存在能够满足匹配条件的节点所致\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211288005-f03874d2-67ad-4f3f-95ae-ea4405901b56.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=72&originWidth=680&size=29881&status=done&style=none&width=680)\n\n- 为node2节点设置标签zone=foo，使其满足条件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211304203-172588db-4439-46e7-b8bc-e27edfa4dd41.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=47&originWidth=631&size=29881&status=done&style=none&width=631)\n\n- pod节点成功调度至node2\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211318820-2df35eac-7cb6-4f94-a6ea-8a0c82fbb81d.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=96&originWidth=934&size=29881&status=done&style=none&width=934)\n\n\n2. 定义了调度拥有两个标签选择器的节点挑选条目，zone=foo且设置ssd标签，两个标签选择器彼此之间为“逻辑与”的关系。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211341047-4f170a36-adfd-4fda-a738-fc1950c28617.png#align=left&display=inline&height=355&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=355&originWidth=723&size=34191&status=done&style=none&width=723)\n\n- 处于pending状态，由于当前node只有zone=foo标签，还未设置ssd标签\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211356882-eb8db408-32ad-406b-90dc-eccfeeb785b2.png#align=left&display=inline&height=69&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=69&originWidth=649&size=34191&status=done&style=none&width=649)\n\n- 给node2设置ssd标签后，成功调度至node2\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211371468-2c5eeae2-955b-49f2-b763-45f94cfe381b.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=210&originWidth=952&size=34191&status=done&style=none&width=952)\n\n\n3. 定义节点软亲和性以选择运行在拥有zone=foo和ssd标签（无论其值为何）的节点之上，其中zone=foo是更为重要的倾向性规则，它的权重为60，ssd标签就没有那么关键，它的权重为30。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211428583-8af05bf3-6ebf-433b-b382-2ee56d1b7f45.png#align=left&display=inline&height=681&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=681&originWidth=699&size=48453&status=done&style=none&width=699)\n\n- 将node2原来的zone=foo标签修改为aaa\n  `# kubectl label nodes node2 zone=\'aaa\' --overwrite` \n- apply配置清单后查看pod信息。即便没有zone=foo的node，依然能成功调度\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211464186-32310c72-a31a-4a20-adc5-bdeae1087988.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=744&size=48453&status=done&style=none&width=744)', 8, 0, 0, '2020-12-16 23:01:58.618728', '2021-01-26 09:15:08.529849', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (72, 'pod亲和调度', '[TOC]\n\n# 一、概述\n\n\n1. 把一些Pod对象组织在相近的位置，如某业务的前端Pod和后端Pod等，这些Pod对象间的关系为亲和性。出于安全或分布式等原因也有可能需要将一些Pod对象在其运行的位置上隔离开来，这些Pod对象间的关系为反亲和性。\n1. 常用实现方式是允许调度器把第一个Pod放置于任何位置，而后与其有亲和或反亲和关系的Pod据此动态完成位置编排。\n1. Pod的亲和性定义也存在“硬”（required）亲和性和“软”（preferred）亲和性的区别。\n1. 亲和性/反亲和性调度策略比较如下：\n\n| 调度策略        | 匹配标签 | 操作符                                 | 拓扑域支持 | 调度目标                   |\n| --------------- | -------- | -------------------------------------- | ---------- | -------------------------- |\n| nodeAffinity    | 主机     | In, NotIn, Exists,DoesNotExist, Gt, Lt | 否         | 指定主机                   |\n| podAffinity     | POD      | In, NotIn, Exists,DoesNotExist         | 是         | POD与指定POD同一拓扑域     |\n| podAnitAffinity | POD      | In, NotIn, Exists,DoesNotExist         | 是         | POD与指定POD不在同一拓扑域 |\n\n\n\n# 二、示例\n\n\n## 1. pod硬亲和调度\n\n- 创建带有标签“app=tomcat”的Deployment资源\n  `# kubectl run tomcat -l app=tomcat --image tomcat:alpine` \n- 定义Pod对象，通过labelSelector定义的标签选择器，使其调度到和app=tomcat的资源到同一个主机名的节点上。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211540002-48c8bd23-a8b9-47c2-b32b-df0fdf27f300.png#align=left&display=inline&height=410&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=410&originWidth=615&size=68601&status=done&style=none&width=615)\n\n- 查看pod信息，与“app=tomcat”的Deployment资源在同一node上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211554677-40f7c10b-3a85-4f6e-9096-5d4d80818ad9.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=856&size=68601&status=done&style=none&width=856)\n\n## 2. pod软亲和调度\n\n- 定义Pod对象，通过labelSelector定义的标签选择器，尽可能使其调度到和app=apache的资源到同一个主机名的节点上。（当前环境不存在app=apache的资源）\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211579185-8dad2d5b-a0c3-48ee-bd19-3c81c9288142.png#align=left&display=inline&height=627&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=627&originWidth=757&size=68601&status=done&style=none&width=757)\n\n- myapp被调度到了node1和node2节点上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211595690-c3b9b6ab-41a0-4596-a2b9-93750e92fd95.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=1185&size=68601&status=done&style=none&width=1185)\n\n\n## 3. pod反亲和调度\n\n- 定义Pod对象，通过labelSelector定义的标签选择器，使其不要调度到和app=apache的资源在同一个主机名的节点上。（当前环境不存在app=apache的资源）\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211627407-b2c6f291-0e8a-4bd4-9941-b7db581dbad4.png#align=left&display=inline&height=697&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=697&originWidth=691&size=68601&status=done&style=none&width=691)\n\n- myapp被调度到了node1节点上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211642793-5e43bfee-8eff-4b11-b9e4-9e9aa3200b9a.png#align=left&display=inline&height=249&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=249&originWidth=1098&size=68601&status=done&style=none&width=1098)', 9, 0, 0, '2020-12-16 23:03:20.277868', '2021-01-26 09:15:18.262695', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (73, '污点和容忍度', '[TOC]\n\n# 一、简介\n\n\n1. 污点（taints）是定义在node之上的键值型属性数据，用于让节点拒绝将Pod调度运行于其上，除非该Pod对象具有接纳节点污点的容忍度。\n1. 容忍度（tolerations）是定义在Pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将Pod对象调度至其能够容忍该节点污点的节点之上，\n1. 节点亲和性使得Pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定Pod对象的能力\n\n# 二、定义污点和容忍度\n\n\n1. 污点定义在节点的nodeSpec中，而容忍度则定义在Pod的podSpec中，它们都是键值型数据，但又都额外支持一个效果（effect）标记，语法格式为“key=value:effect”，其中key和value的用法及格式与资源注解信息相似，effect用于定义对Pod对象的排斥等级，它主要包含以下三种类型。\n\n NoSchedule： k8s 将不会将 Pod 调度到具有该污点的 Node 上。\n PreferNoSchedule: k8s 将尽量避免将 Pod 调度到具有该污点的 Node 上。\n NoExecute：k8s 将不会将 Pod 调度到具有该污点的 Node 上，同时会将 Node上已经存在的 Pod 驱逐出去\n\n2. 在Pod对象上定义容忍度时，它支持两种操作符：\n\n 一种是等值比较（Equal），表示容忍度与污点必须在key、value和effect三者之上完全匹配\n 另一种是存在性判断（Exists），表示二者的key和effect必须完全匹配，而容忍度中的value字段要使用空值。\n\n3. 一个节点可以配置使用多个污点，一个Pod对象也可以有多个容忍度，不过二者在进行匹配检查时应遵循如下逻辑。\n\n 首先处理每个有着与之匹配的容忍度的污点。\n 不能匹配到的污点上，如果存在一个污点使用了NoSchedule效用标识，则拒绝调度Pod对象至此节点。\n 不能匹配到的污点上，若没有任何一个使用了NoSchedule效用标识，但至少有一个使用了PreferNoScheduler，则应尽量避免将Pod对象调度至此节点。\n 如果至少有一个不匹配的污点使用了NoExecute效用标识，则节点将立即驱逐Pod对象，或者不予调度至给定节点；另外，即便容忍度可以匹配到使用了NoExecute效用标识的污点，若在定义容忍度时还同时使用tolerationSeconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n4. pod.spec.tolerations字段设置\n\n key, vaule, effect 要与 Node 上设置的 taint 保持一致\n operator 的值为 Exists 将会忽略 value 值\n tolerationSeconds 用于描述当 Pod 需要被驱逐时可以在 Pod 上继续保留运行的时间\n 不指定 key 值时，表示容忍所有的污点 key\n\n5. tolerations:\n\n operator: \"Exists\"\n\n- 不指定 effect 值时，表示容忍所有的污点作用\n  tolerations:\n- key: \"key\"\n  operator: \"Exists\"\n- 有多个 Master 存在时，防止资源浪费，可以如下设置\n  kubectl taint nodes Node-Name\n  node-role.kubernetes.io/master=:PreferNoSchedule\n\n# 三、污点的设置、查看和去除\n\n\n1. 设置污点\n   `kubectl taint nodes node1 key1=value1:NoSchedule` \n1. 节点说明中，查找 Taints 字段\n   `kubectl describe node node-name` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211757921-1d314136-e2bc-40c1-8b8d-60af2bf99f7b.png#align=left&display=inline&height=23&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=23&originWidth=681&size=68601&status=done&style=none&width=681)\n\n3. 去除污点\n   kubectl taint nodes node1 key1:NoSchedule-\n\n# 四、示例\n\n## 1. 使用NoExecute：将node1已经存在的Pod对象驱逐。\n\n- 创建deployment资源，系统自动调度至node1和node2上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211785718-b4b29820-b4ca-469e-96f8-adc989e2bb52.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=139&originWidth=953&size=68601&status=done&style=none&width=953)\n\n- 给node1设置NoExecute污点\n  `# kubectl taint node node1 status=check:NoExecute` \n- 查看pod信息，已全部调度至node2上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211804410-56de9439-00b2-49ea-bb7c-d7a6185ea554.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=977&size=68601&status=done&style=none&width=977)\n\n## 2. 设置容忍，容忍status=check:NoExecute的污点存在，60秒后失效\n\n- 配置deployment清单文件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211826882-69407c74-e9ce-461b-9518-dabfdb0cec29.png#align=left&display=inline&height=603&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=603&originWidth=445&size=68601&status=done&style=none&width=445)\n\n- 查看pod信息，此时node1和node2上均有pod\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211842180-78ce7443-fc92-49e7-b12f-e902fcaff350.png#align=left&display=inline&height=134&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=134&originWidth=958&size=68601&status=done&style=none&width=958)\n\n- 60秒后查看pod信息，pod已全部调度至node2上\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211865247-c1afa4db-2fa6-48a8-96da-0768bc6e67b2.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=191&originWidth=977&size=68601&status=done&style=none&width=977)\n\n', 7, 0, 0, '2020-12-16 23:05:08.761467', '2021-01-26 09:17:49.638207', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (74, '固定节点调度', '\n> Pod.spec.nodeName 将 Pod 直接调度到指定的 Node 节点上，会跳过 Scheduler的调度策略，该匹配规则是强制匹配\n\n1. 编写资源清单，指定deployment调度至node1节点\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211983459-55825f2c-0ef4-462c-a933-275bd4179a01.png#align=left&display=inline&height=480&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=480&originWidth=428&size=68601&status=done&style=none&width=428)\n\n2. 查看pod信息，全部调度至node1节点\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604211999715-2510aea1-0e57-46e0-8c92-84819e12448a.png#align=left&display=inline&height=138&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=138&originWidth=944&size=68601&status=done&style=none&width=944)\n\n3. Pod.spec.nodeSelector：通过 kubernetes 的 label-selector机制选择节点，由调度器调度策略匹配 label，而后调度 Pod到目标节点，该匹配规则属于强制约束\n\n 编写资源清单，指定deployment调度至disk=ssd标签的节点\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212022814-82ce9889-e7bf-450b-b617-0df55200debb.png#align=left&display=inline&height=508&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=508&originWidth=434&size=68601&status=done&style=none&width=434)\n\n- 给node2设置disk=ssd标签\n  `# kubectl label nodes node2 disk=ssd` \n- 查看pod调度状态信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212058298-a71f34bd-3f61-4141-959a-53434dfab3a8.png#align=left&display=inline&height=130&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=130&originWidth=935&size=68601&status=done&style=none&width=935)\n\n', 12, 0, 0, '2020-12-16 23:06:09.314978', '2021-01-26 09:18:52.149091', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (75, '自定义资源类型（CRD）', '[TOC]\n\n# 一、简介\n\n\n1. 扩展Kubernetes\n   API的常用方式有三种：使用CRD（CustomResourceDefinitions）自定义资源类型、开发自定义的API\n   Server并聚合至主API Server，以及定制扩展Kubernetes源码。\n1. CRD无须修改Kubernetes源代码就能扩展它支持使用API资源类型。CRD本身也是一种资源类型，隶属于集群级别，实例化出特定的对象之后，它会在API上注册生成GVR类型URL端点，并能够作为一种资源类型被使用并实例化相应的对象。自定义资源类型之前，选定其使用的API群组名称、版本及新建的资源类型名称，根据这些信息即可创建自定义资源类型，并创建自定义类型的资源对象。\n\n\n\n# 二、创建CRD对象\n\n\n1. 定义了一个名为users.auth.ilinux.io的CRD资源对象，它的群组名称为auth.ilinux.io，仅支持一个版本级别v1beta1，复数形式为users，隶属于名称空间级别。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212244105-4c35b1a9-cac4-43e9-bfe6-1bbac7ffed88.png#align=left&display=inline&height=512&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=512&originWidth=637&size=71092&status=done&style=none&width=637)\n\n2. 列出集群上的CRD对象\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212259550-dde07a1a-1d09-4c71-beb1-f14a670e39ca.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=74&originWidth=614&size=71092&status=done&style=none&width=614)\n\n3. users已经是一个名称空间级别的可用资源类型，用户可按需创建出任意数量的users类型的对象。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212274526-59606950-1466-4305-9e9b-1a3221c15588.png#align=left&display=inline&height=195&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=195&originWidth=430&size=71092&status=done&style=none&width=430)\n\n4. 获取user类型资源信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212290647-b0d6ba03-14a6-4aa6-8e65-9cc1fe2be9e3.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=397&size=71092&status=done&style=none&width=397)\n\n5. 删除自定义的students对象object-student\n   `# kubectl delete users admin` \n\n\n\n# 三、自定义资源格式验证\n\n\n1. 对象配置的变动在存入etcd之前还需要经由准入控制器的核验，尤其是验证型（validation）控制器使用OpenAPI模式声明验证规则，检查传入的对象格式是否符合有效格式。\n1. 为CRD对象users.auth.ilinux.io，在spec字段定义了userID、groups、email和password字段的数据类型，并指定了userID字段的取值范围，以及password字段的数据格式，而且指定userID和groups是必选字段\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212312408-5309852f-7444-4c5e-b6dd-402ca8bc43a4.png#align=left&display=inline&height=468&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=468&originWidth=485&size=71092&status=done&style=none&width=485)\n\n3. 使用原admin资源清单时报错\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212326632-46db8d5e-539d-4702-bb34-0e8718a54de3.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=512&size=71092&status=done&style=none&width=512)', 9, 0, 0, '2020-12-20 23:13:55.783949', '2021-01-15 00:30:17.427989', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (76, '自定义控制器', '\n\n> 控制器负责持续监视资源变动，根据资源的spec及status中的信息执行某些操作逻辑，并将执行结果更新至资源的status中。自定义控制器同样担负着类似的职责，只不过一个特定的控制器通常仅负责管理一部分特定的资源类型，并执行专有管理逻辑。\n\n1. 控制器包含两个重要组件：Informer和Workqueue，前者负责监视资源对象当前状态的更改，并将事件发送至后者，而后由处理函数进行处理\n1. Controller可以有一个或多个informer来跟踪某一个resource。Informter跟API\n   server保持通讯获取资源的最新状态并更新到本地的cache中，一旦跟踪的资源有变化，informer就会调用callback。把关心的变更的Object放到workqueue里面。然后woker执行真正的业务逻辑，计算和比较workerqueue里items的当前状态和期望状态的差别，然后通过client-go向API\n   server发送请求，直到驱动这个集群向用户要求的状态演化。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212407211-56c1e184-dc6d-44cf-826b-f800290fe3a6.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=92&originWidth=512&size=71092&status=done&style=none&width=512)\n\n3. 现在已经有了几类更加成熟、更易上手的工具可用，它们甚至已经可以被视作开发CRD和控制器的SDK或框架，其中，主流的项目主要有Kubebuilder、Operator SDK和Metacontroller三个。', 8, 0, 0, '2020-12-20 23:14:37.923613', '2021-01-13 20:19:34.972998', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (77, '资源监控及资源指标', '[TOC]\n\n# 一、资源监控内容\n\n\n1. 在集群监控层面，目标是监控整个Kubernetes集群的健康状况，包括集群中的所有工作节点是否运行正常、系统资源容量大小、每个工作节点上运行的容器化应用的数量以及整个集群的资源利用率等，它们通常可分为如下一些可衡量的指标。\n\n 节点资源状态：主要有网络带宽、磁盘空间、CPU和内存的利用率。\n \n 节点数量：集群中的可用节点数量。\n \n 运行的Pod对象：正在运行的Pod对象数量。\n\n2. Pod资源对象的监控需求大体上可以分为三类。\n\n Kubernetes指标：用于监视特定应用程序相关的Pod对象的部署过程、当前副本数量、期望的副本数量、部署过程进展状态、健康状态监测及网络服务器的可用性等，这些指标数据需要经由Kubernetes系统接口获取。\n \n 容器指标：容器的资源需求、资源限制以及CPU、内存、磁盘空间、网络带宽等资源的实际占用状况等。\n \n 应用程序指标：应用程序自身内建的指标，通常与其所处理的业务规则相关，例如，关系型数据库应用程序可能会内建用于暴露索引状态有关的指标，以及表和关系的统计信息等。\n\n# 二、资源监控工具\n\n1. kubelet程序中集成相关的工具程序cAdvisor，仅能收集单个节点及其相关Pod资源的相关指标数据，用于对节点上的资源及容器进行实时监控及指标数据采集，支持的相关指标包括CPU、内存使用情况、网络吞吐量及文件系统使用情况等，并可通过TCP的4194端口提供一个Web UI。\n1. Heapster是集群级别的监视和事件数据聚合工具。Heapster本身可作为集群中的一个Pod对象运行，它通过发现集群中的所有节点实现从每个节点kubelet内建的cAdvisor获取性能和指标数据，并通过标签将Pod对象及其相关的监控数据进行分级、聚合后推送到可配置的后端存储系统进行存储和可视化。\n1. 功能完备的Heapster监控系统流行的解决方案是由InfluxDB作为存储后端，Grafana为可视化接口，而Heapster从各节点的cAdvisor采集数据并存储于InfluxDB中，由Grafana进行展示。托管于Kubernetes集群中的InfluxDB、Grafana和Heapster运行为常规的Pod资源对象，它们彼此之间通过环境变量及服务发现功能自动协同。\n\n\n\n# 三、新一代监控架构\n\n\n1. Kubernetes具有灵活的可扩展性，它通过API聚合器为开发人员提供了轻松扩展API资源的能力，可以自定义指标API和资源指标API。新一代的Kubernetes监控系统架构主要由核心指标流水线和监控指标流水线协同组成。\n\n 核心指标流水线\n  由kubelet、metrics-server以及由API server提供的api组成；CPU累积使用率、内存的实时使用率、pod的资源占用率及容器的磁盘占用率。\n  \n 监控指标流水线\n  用于从系统收集各种指标数据并提供给终端用户、存储系统以及HPA，包含核心指标以及其他许多非核心指标。非核心指标本身不能被K8S所解析。所以需要k8s-prometheus-adapter将prometheus采集到的数据转化为k8s能理解的格式，为k8s所使用。\n\n2. HPAv2：能同时使用资源指标API和自定义指标API的组件，它实现了基于观察到的指标自动缩放Deployment或ReplicaSet类型控制器管控下的Pod副本数量。\n2. 目前，资源指标API的实现较主流是的metrics-server，而自定义指标API则以建构在监控系统Prometheus之上的k8s-prometheus-adapter最广为接受。', 11, 0, 0, '2020-12-20 23:19:07.668419', '2021-01-26 09:03:08.492202', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (78, '监控组件安装', '[TOC]\n\n# 一、部署metrics-server\n\n- 如果已安装prometheus监控服务，则无需安装metrics-server\n- [github地址](https://github.com/kubernetes-sigs/metrics-server)\n\n1. 克隆项目代码的仓库至本地node节点目录以获得其资源配置清单\n   `# git clone https://github.com/kubernetes-incubator/metrics-server.git` \n1. 需要修改metrics-server/deploy/1.8+/metrics-server-deployment.yaml清单文件，spec.containers下添加：\n\n```yaml\ncommand:\n    - /metrics-server\n    - --kubelet-insecure-tls\n    - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n```\n\n3. 为master节点添加label\n   `# kubectl label nodes master metrics=yes` \n3. 部署metrics-server\n   `# kubectl create -f metrics-server/deploy/1.8+/` \n3. 检验相应的API群组metrics.k8s.io是否出现在Kubernetes集群的API群组列表中\n   `# kubectl api-versions | grep metrics` \n3. 确认相关的Pod对象运行正常\n   `# kubectl get pods -n kube-system -l k8s-app=metrics-server` \n3. 使用kubectl top node查看结果\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212866497-9a5765fd-3b28-409f-9c60-0114495a1d3b.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=579&size=71092&status=done&style=none&width=579)', 6, 0, 0, '2020-12-20 23:22:09.113764', '2021-01-08 07:58:27.767988', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (79, '资源指标及其应用', '[TOC]\n\n# 一、metrics-server\n\n\n1. 通过API Server的URL路径/apis/metrics.k8s.io/进行存取，并提供同样级别的安全性、稳定性及可靠性保证。\n1. Metrics Server通过Kubernetes聚合器（kube-aggregator）注册到主API Server之上，而后基于kubelet的Summary API收集每个节点上的指标数据，并将它们存储于内存中然后以指标API格式提供，\n1. Metrics Server基于内存存储，重启后数据将全部丢失，而且它仅能留存最近收集到的指标数据，因此，如果用户期望访问历史数据，就不得不借助于第三方的监控系统（如Prometheus等），或者自行开发以实现其功能。\n\n\n\n# 二、kubectl top命令\n\n\n1. kubectl top命令可显示节点和Pod对象的资源使用信息，它依赖于集群中的资源指标API来收集各项指标数据。它包含有node和pod两个子命令，可分别用于显示Node对象和Pod对象的相关资源占用率。\n1. 列出Node资源占用率命令的语法格式为“kubectl top node [-l label | NAME]”，例如下面显示所有节点的资源占用状况的结果中显示了各节点累计CPU资源占用时长及百分比，以及内容空间占用量及占用比例，可以使用标签选择器进行节点过滤\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212944422-c572e3c3-6cb8-46d2-8b55-a1f741902750.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=113&originWidth=579&size=71092&status=done&style=none&width=579)\n\n3. 名称空间级别的Pod对象资源占用率，一般应该限定名称空间及使用标签选择器过滤出目标Pod对象。命令的语法格式为“kubectl top pod [NAME | -l label] [--all-namespaces] [--containers=false|true]”，例如，下面显示kube-system名称空间中标签为“k8s-app=kube-dns”的所有Pod资源及其容器的资源占用状态：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604212987133-cf0d61fa-5347-4356-9288-9a4844262104.png#align=left&display=inline&height=97&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=97&originWidth=979&size=71092&status=done&style=none&width=979)', 5, 0, 0, '2020-12-20 23:22:59.096864', '2021-01-08 07:58:29.992571', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (80, '自动弹性缩放', '[TOC]\n\n# 一、HPA\n\n\n1. HPA作为Kubernetes\n   API资源和控制器实现，它基于采集到的资源指标数据来调整控制器的行为，控制器会定期调整ReplicaSets或Deployment控制器对象中的副本数，以使得观察到的平均CPU利用率与用户指定的目标相匹配。\n1. HPA自身是一个控制循环（control loop）的实现，其周期由controller-manager的--horizontal-pod-autoscaler-sync-period选项来定义，默认为30秒。在每个周期内，controller-manager将根据每个HPA定义中指定的指标查询相应的资源利用率。controller-manager从资源指标API（针对每个Pod资源指标）或自定义指标API（针对所有其他指标）中获取指标数据。\n1. HPA控制器可以通过两种不同的方式获取指标：Heapster和REST客户端接口。使用直接Heapster获取指标数据时，HPA直接通过API服务器的服务代理子资源向Heapster发起查询请求，因此，Heapster需要事先部署在群集上并在kube-system名称空间中运行。使用REST客户端接口获取指标时，需要事先部署好资源指标API及其API Server，必要时，还应该部署好自定义指标API及其相关的API Server。\n1. HPA是Kubernetes\n   autoscalingAPI群组中的API资源，当前的稳定版本仅支持CPU自动缩放，它位于autoscaling/v1群组中。而测试版本包含对内存和自定义指标的扩展支持，测试版本位于API群组autoscaling/v2beta1之中。\n\n\n\n# 二、HPA（v1）控制器\n\n\n1. HPA也是标准的Kubernetes\n   API资源，其基于资源配置清单的管理方式同其他资源相同。它还有一个特别的“kubectl autoscale”命令用于快速创建HPA控制器\n1. 示例\n\n- 创建一个名为myapp-deploy的Deployment控制器\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213049314-e9c774dd-c3f1-4ed8-b978-de3b9363c4b2.png#align=left&display=inline&height=632&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=632&originWidth=443&size=71092&status=done&style=none&width=443)\n\n- 创建一个同名的HPA控制器自动管控其Pod副本规模：\n  `# kubectl autoscale deployment myapp-deploy --min=2 --max=5 --cpu-percent=60` \n- 查看HPA控制器信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213071074-03a818aa-e880-4c8f-8a4a-013ce1f1051b.png#align=left&display=inline&height=68&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=68&originWidth=878&size=71092&status=done&style=none&width=878)\n\n- 向myapp-svc的NodePort发起持续性的压力测试式访问请求，各Pod对象的CPU利用率将持续上升，直到超过目标利用率边界的60%，而后触发增加Pod对象副本数量。\n\n\n\n# 三、HPA（v2）控制器\n\n\n1. HPA（v2）控制器支持基于核心指标CPU和内存资源以及基于任意自定义指标资源占用状态实现应用规模的自动弹性伸缩，它从metrics-server中请求查看核心指标，从k8s-prometheus-adapter一类的自定义API中获取自定义指标数据。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213087084-5aa77bab-e024-45e0-8992-47a2eac4ae4d.png#align=left&display=inline&height=482&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=482&originWidth=463&size=73462&status=done&style=none&width=463)\n\n2. spec字段主要嵌套使用如下几个字段。\n\n| minReplicas    | integer | 自动伸缩可缩减至的Pod副本数下限                              |\n| -------------- | ------- | ------------------------------------------------------------ |\n| maxReplicas    | integer | 自动伸缩可扩展至的Pod副本数上限，其值不能低于min-Replicas属性值 |\n| scaleTargetRef | object  | 要缩放的目标资源，以及应该收集指标数据并更改其副本数量的Pod对象，引用目标对象时，主要会用到三个嵌套属性——apiVersion、kind和name |\n| metrics        | object  | 用于计算所需Pod副本数量的指标列表，每个指标单独计算其所需的副本数，将所有指标计算结果中的最大值作为最终采用的副本数量。 |\n\n\n\n3. metrics字段值是对象列表，它由要引用的各指标的数据源及其类型构成的对象组成。\n\n| external | 用于引用非附属于任何对象的全局指标，甚至可以基于集群之外的组件的指标数据，如消息队列的长度等 |\n| -------- | ------------------------------------------------------------ |\n| object   | 引用描述集群中某单一对象的特定指标，如Ingress对象上的hits-per-second等 |\n| pods     | 引用当前被弹性伸缩的Pod对象的特定指标，如transactions-processed-per-second等 |\n| resource | 引用资源指标，即当前被弹性伸缩的Pod对象中容器的requests和limits中定义的指标（CPU或内存资源） |\n| type     | 表示指标源的类型，其值可为Objects、Pods或Resource            |\n\n', 8, 0, 0, '2020-12-20 23:24:08.968711', '2021-01-26 09:19:51.336279', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (81, 'helm基础', '[TOC]\n\n# 一、helm功能\n\n\n1. Helm就是Kubernetes的应用程序包管理器，类似于Linux系统之上的yum或apt-get等，可用于实现帮助用户查找、分享及使用Kubernetes应用程序。它的核心打包功能组件称为chart，可以帮助用户创建、安装及升级复杂应用。Helm将Kubernetes的资源（如Deployments、Services或ConfigMap等）打包到一个Charts中，制作并测试完成的各个Charts将保存到Charts仓库进行存储和分发。另外，Helm实现了可配置的发布，它支持应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除和更新等操作。\n1. Helm是一个基于Kubernetes的程序包（资源包）管理器，它将一个应用的相关资源组织成为Charts，并通过Charts管理程序包，其使用优势可简单总结为如下几个方面：\n\n- 管理复杂应用：Charts能够描述哪怕是最复杂的程序结构，其提供了可重复使用的应用安装的定义。\n- 易于升级：使用就地升级和自定义钩子来解决更新的难题。\n- 简单分享：Charts易于通过公共或私有服务完成版本化、分享及主机构建。\n- 回滚：可使用“helm rollback”命令轻松实现快速回滚。\n\n\n\n# 二、helm核心术语\n\n\n1. Helm将Kubernetes应用的相关配置组织为Charts，并通过它完成应用的常规管理操作。使用Charts管理应用的流程包括：\n\n 从0开始创建Charts\n 将Charts及其相关的文件打包为归档格式\n 将Charts存储于仓库（repository）中并与之交互\n 在Kubernetes集群中安装或卸载Charts以及管理经Helm安装的应用的版本发行周期\n\n2. 对Helm来说，它具有以下几个关键概念。\n\n- Charts：即一个Helm程序包，它包含了运行一个Kubernetes应用所需要的镜像、依赖关系和资源定义等，必要时还会包含Service的定义；它类似于yum的rpm文件。\n- Repository：Charts仓库，用于集中存储和分发Charts，类似于Python的PyPI。\n- Config：应用程序实例化安装运行时使用的配置信息。\n- Release：应用程序实例化配置后运行于Kubernetes集群中的一个Charts实例；从V3开始，Release\n  不再是全局资源，而是存储在各自命名空间内。\n\n# 三、helm架构\n\n> Helm主要由Helm客户端、Tiller服务器和Charts仓库（repository）组成\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213320583-2a0039a5-e9c5-480f-a9c7-b9cdfb451318.png#align=left&display=inline&height=309&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=309&originWidth=720&size=73462&status=done&style=none&width=720)\n\n\n1. Helm客户端是命令行客户端工具，采用Go语言编写，基于gRPC协议与Tiller\n   server交互。它主要完成如下任务。\n\n 本地Charts开发。\n 管理Charts仓库。\n 与Tiller服务器交互：发送Charts以安装、查询Release的相关信息以及升级或卸载已有的Release。\n\n2. Tiller\n   server（V3已将Tiller的删除，通过ApiServer与k8s交互）是托管运行于Kubernetes集群之中的容器化服务应用，它接收来自Helm客户端的请求，并在必要时与Kubernetes API Server进行交互。它主要完成以下任务。\n\n 监听来自于Helm客户端的请求。\n 合并Charts和配置以构建一个Release。\n 向Kubernetes集群安装Charts并对相应的Release进行跟踪。\n 升级和卸载Charts。\n\n3. 通常，用户于Helm客户端本地遵循其格式编写Charts文件，而后即可部署于Kuber-netes集群之上运行为一个特定的Release。仅在有分发需求时，才应该将同一应用的Charts文件打包成归档压缩格式提交到特定的Charts仓库。\n\n\n\n# 四、Helm工作原理\n\n ![11553600-23077c664811da57.jpg](https://cdn.nlark.com/yuque/0/2020/jpeg/2308212/1604213358275-42b64ad0-71cd-40c7-bac2-210d95ac7773.jpeg#align=left&display=inline&height=580&margin=%5Bobject%20Object%5D&name=11553600-23077c664811da57.jpg&originHeight=580&originWidth=790&size=59724&status=done&style=none&width=790)\n\n\n1. Chart Install 过程：\n\n Helm从指定的目录或者tgz文件中解析出Chart结构信息\n Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller\n Tiller根据Chart和Values生成一个Release\n Tiller将Release发送给Kubernetes用于生成Release\n\n2. Chart Update过程：\n\n Helm从指定的目录或者tgz文件中解析出Chart结构信息\n Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller\n Tiller生成Release并更新指定名称的Release的History\n Tiller将Release发送给Kubernetes用于更新Release\n\n3. Chart Rollback过程：\n\n Helm将要回滚的Release的名称传递给Tiller\n Tiller根据Release的名称查找History\n Tiller从History中获取上一个Release\n Tiller将上一个Release发送给Kubernetes用于替换当前Release\n\n', 12, 0, 0, '2020-12-20 23:28:33.758877', '2021-01-13 13:56:23.112961', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (82, 'helm安装', '[TOC]\n\n# 一、安装Helm\n\n\n- 官方参考文档：[https://helm.sh/docs/intro/quickstart/](https://helm.sh/docs/intro/quickstart/)\n- Helm的安装方式有两种：预编译的二进制程序和源码编译安装。\n- Helm项目托管在GitHub之上，项目地址为[https://github.com/helm/helm/releases](https://github.com/helm/helm/releases)。\n- Helm的运行依赖于本地安装并配置完成的kubectl方能与运行于Kubernetes集群之上的Tiller服务器进行通信，因此，运行Helm的节点也应该是可以正常使用kubectl命令的主机，或者至少是有着可用kubeconfig配置文件的主机。\n\n1. 下载合用版本的压缩包并将其展开。\n   `wget https://get.helm.sh/helm-v3.0.2-linux-amd64.tar.gz` \n   `tar -xvf helm-v3.0.2-linux-amd64.tar.gz` \n1. 将其二进制程序文件复制或移动到系统PATH环境变量指向的目录中\n   `cp linux-amd64/helm /usr/local/bin/` \n1. 以添加自动完成的代码：\n   `source <(helm completion bash)` \n1. Helm客户端安装完成后，进行验证。\n\n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213646506-586de468-4a93-4848-9db8-d1117bc3a1c6.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=44&originWidth=370&size=73462&status=done&style=none&width=370)\n\n\n# 二、添加Helm的官方仓库\n\n\n1. 添加官方Charts仓库\n   `helm repo add stable https://kubernetes-charts.storage.googleapis.com/` \n1. 更新仓库信息\n   `helm repo update` \n1. 查看官方Charts仓库\n   `helm search repo stable` \n\n', 6, 0, 0, '2020-12-20 23:29:17.030566', '2021-01-08 07:58:40.802070', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (83, 'helm常用命令', '[TOC]\n\n# 一、helm管理命令\n\n\n1. 查看版本\n   `#helm version` \n1. 增加repo\n   `#helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts` \n   `#helm repo add --username admin --password password myharbor https://harbor.qing.cn/chartrepo/charts` \n1. 更新repo仓库资源\n   `#helm repo update` \n\n\n\n# 二、charts管理\n\n\n1. 查看当前安装的charts\n   `#helm list` \n1. 将helm search hub显示所有可用图表。\n   `#helm search hub redis` \n1. 使用helm search repo，您可以在已添加的存储库中找到charts的名称：\n   `#helm search repo redis` \n1. 打印出指定的Charts的详细信息\n   `#helm show chart stable/redis` \n1. 下载charts到本地\n   `#helm fetch redis` \n1. 安装charts\n   `#helm install redis stable/redis` \n1. 查看charts状态\n   `#helm status redis` \n1. 删除charts\n   `#helm uninstall redis` \n\n\n\n# 三、自定义charts\n\n\n1. 创建charts\n   `#helm create helm_charts` \n1. 检查chart语法正确性\n   `# helm lint myapp` \n1. 打包自定义的chart\n   `# helm package myapp` \n1. 查看生成的yaml文件\n   `#helm template myapp-1.tgz` \n1. 使用默认chart部署到k8s\n   `helm install myapp myapp-1.tgz` \n1. 使用包去做release部署\n   `helm install --name example2 helm-chart-0.1.0.tgz --set  service.type=NodePort` \n\n\n\n# 四、更新与回滚\n\n\n1. 查看当前chart信息\n   `#helm list` \n1. 更新images\n   `#helm upgrade myapp myapp-2.tgz` \n1. 查看版本信息\n   `#helm history myapp` \n1. 回滚指定版本\n   `#helm rollback myapp 1` ', 7, 0, 0, '2020-12-20 23:30:35.825636', '2021-01-11 22:49:09.286162', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (84, 'Helm Charts', '[TOC]\n\n# 一、Charts文件组织结构\n\n\n> 一个Charts就是按特定格式组织的目录结构，目录名即为Charts名，目录名称本身不包含版本信息。目录结构中除了charts/和templates/是目录之外，其他的都是文件。它们的基本功用如下。\n\n\n\n1. Chart.yaml：当前Charts的描述信息，yaml格式的文件。\n1. LICENSE：当前Charts的许可证信息，纯文本文件；此为可选文件。\n1. README.md：易读格式的README文件；可选。\n1. requirements.yaml：当前Charts的依赖关系描述文件；可选。\n1. values.yaml：当前Charts用到的默认配置值。\n1. charts/：目录，存放当前Charts依赖到的所有Charts文件。\n1. templates/：目录，存放当前Charts用到的模板文件，可应用于Charts生成有效的Kuber-netes清单文件。\n1. templates/NOTES.txt：纯文本文件，Templates简单使用注解\n\n- 尽管Charts和Templates目录均为可选，但至少应该存在一个Charts依赖文件或一个模板文件。另外，Helm保留使用charts/和templates/目录以及上面列出的文件名称，其他文件都将被忽略。\n\n\n\n# 二、Chart.yaml文件组织格式\n\n\n> Chart.yaml用于提供Charts相关的各种元数据，如名称、版本、关键词、维护者信息、使用的模板引擎等，它是一个Charts必备的核心文件，主要包含以下字段。\n\n\n\n1. name：当前Charts的名称，必选字段。\n1. version：遵循语义化版本规范第2版的版本号，必选字段。\n1. description：当前项目的单语句描述信息，可选字段。\n1. keywords：当前项目的关键词列表，可选字段。\n1. home：当前项目的主页URL，可选字段。\n1. sources：当前项目用到的源码的来源URL列表，可选字段。\n1. maintainers：项目维护者信息，主要嵌套name、email和URL几个属性组成；可选字段。\n1. engine：模板引擎的名称，默认为gotpl，即go模板。\n1. icon:URL，指向当前项目的图标，SVG或PNG格式的图片；可选字段。\n1. appVersion：本项目用到的应用程序的版本号，可选字段，且不必为语义化版本。\n1. tillerVersion：当前Charts依赖的Tiller版本号，可以是语义化版本号的范围，如“>2.4.0”；可选字段。\n\n\n\n# 三、Charts中的依赖关系\n\n\n> Helm中的一个Charts可能会依赖不止一个其他的Charts，这种依赖关系可经requirements.yaml进行动态链接，也可直接存储于charts/目录中进行手动管理。\n\n\n\n1. requirements.yaml文件\n   requirements.yaml文件本质上只是一个简单的依赖关系列表，可用字段具体如下。\n1. name：被依赖的Charts的名称。\n1. version：被依赖的Charts的版本。\n1. repository：被依赖的Charts所属的仓库及其URL；如果是非官方的仓库，则需要先用helm\n   repo add命令将其添加进本地可用仓库。\n1. alias：为被依赖的Charts创建一个别名，从而让当前Charts可以将所依赖的Charts对应到新名称，即别名；可选字段。\n1. tags：默认情况下所有的Charts都会被装载，若给定了tags，则仅装载那些匹配到的Charts。\n1. condition：类似于tags字段，但需要通过自定义的条件来指明要装载的charts。\n1. import-values：导入子Charts中的的值；被导入的值需要在子charts中导出。\n1. Charts目录\n\n- 若需要对依赖关系进行更多的控制，则所有被依赖到的Charts都能以手工方式直接复制到Charts目录中。一个被依赖到的Charts既可以是归档格式，也可以是展开的目录格式，不过，其名称不能以下划线（_）或点号（.）开头，此类文件会被Charts装载器自动忽略。\n  例如，Wordpress Charts依赖关系在其Charts目录中的反映类似如下所示：\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213846878-bc9f0d02-84f6-42e6-8f71-7ddb2cd2c7a7.png#align=left&display=inline&height=540&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=540&originWidth=410&size=73462&status=done&style=none&width=410)\n\n- Helm\n  Charts模板（template）遵循Go模板语言格式，并支持50种以上的来自Spring库的模板函数附件，以及为数不少的其他专用函数。所有的模板文件都存储于Templates目录中，在当前Charts被Helm引用时，此目录中的所有模板文件都会传递给模板引擎进行处理。模板文件中用到的值（value）有如下两种提供方式。□通过Charts的values.yaml文件提供，通常用于提供默认值。□在运行“helm\n  install”命令时传递包含所需要的自定义值的YAML文件；此处传递的值会覆盖默认值。', 6, 0, 0, '2020-12-20 23:31:27.982550', '2021-01-08 07:58:50.742098', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (85, '自定义Charts', '[TOC]\n\n# 一、创建Chart\n\n\n1. 执行命令helm create myapp，会创建一个myapp目录\n   `# helm create myapp` \n1. 查看myapp目录结构\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213908507-37c1f24c-d722-4b8d-89e9-726b4048e21e.png#align=left&display=inline&height=323&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=323&originWidth=357&size=73462&status=done&style=none&width=357)\n\n\n# 二、修改配置文件\n\n\n1. 编辑自描述文件 Chart.yaml , 修改version和appVersion信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213925534-5c591ba4-21ec-4f43-be6c-076a2a0fceba.png#align=left&display=inline&height=484&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=484&originWidth=333&size=88222&status=done&style=none&width=333)\n\n2. 编辑values.yaml配置文件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213941106-5641c73c-ca21-4764-8220-cdd31be40e09.png#align=left&display=inline&height=122&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=122&originWidth=432&size=88222&status=done&style=none&width=432)\n\n\n# 三、打包安装chart\n\n\n1. 检查chart语法正确性\n   `# helm lint myapp` \n1. 打包自定义的chart\n   `# helm package myapp` \n1. 安装chart\n   `# helm install myapp myapp-1.tgz` \n1. 验证\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213964472-76d4300b-58fb-4d79-bf0d-b314b58366c5.png#align=left&display=inline&height=273&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=273&originWidth=703&size=113444&status=done&style=none&width=703)\n\n\n# 四、更新\n\n\n1. 编辑自描述文件 Chart.yaml , 修改version和appVersion信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604213997516-bc867f00-45fc-451d-a556-c59a0ac0b832.png#align=left&display=inline&height=489&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=489&originWidth=240&size=113444&status=done&style=none&width=240)\n\n2. 重新打包charts\n\n- 检查chart语法正确性\n  `# helm lint myapp` \n- 打包自定义的chart\n  `# helm package myapp` \n\n3. 更新chart\n   `# helm upgrade myapp myapp-2.tgz` \n3. 验证\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214043045-708c97cc-38c9-4a0e-9fac-fb39ac394e30.png#align=left&display=inline&height=160&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=160&originWidth=701&size=113444&status=done&style=none&width=701)\n\n\n# 五、回滚\n\n\n1. 查看当前版本信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214065831-f67387cf-66a6-45e5-a53b-d256e15874b2.png#align=left&display=inline&height=101&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=101&originWidth=1167&size=113444&status=done&style=none&width=1167)\n\n2. 查看历史版本信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214087205-b7454028-6ff3-4849-81a4-3c889d4e81d6.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=817&size=113444&status=done&style=none&width=817)\n\n3. 回滚到指定版本\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214103535-470fd467-4396-4558-ac4a-89553272da98.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=463&size=113444&status=done&style=none&width=463)\n\n4. 验证\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214120007-8cdd1acb-02dc-4d99-8426-4af3db6596d7.png#align=left&display=inline&height=159&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=159&originWidth=706&size=113444&status=done&style=none&width=706)', 7, 0, 0, '2020-12-20 23:32:55.733335', '2021-01-08 07:59:05.341119', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (86, 'kubeadm高可用部署', '[TOC]\n\n# 一、部署环境\n\n| 主机名  | ip             | docker  | flannel | Keepalived | Haproxy | 主机配置 | 用途                    |\n| ------- | -------------- | ------- | ------- | ---------- | ------- | -------- | ----------------------- |\n| master1 | 192.168.10.138 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点1               |\n| master2 | 192.168.10.139 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点2               |\n| master3 | 192.168.10.140 | 19.03.0 | v0.11.0 | v1.3.5     | v1.5.18 | 4C4G     | 控制节点3               |\n| work1   | 192.168.10.141 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点1               |\n| work2   | 192.168.10.142 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点2               |\n| work3   | 192.168.10.143 | 19.03.0 | v0.11.0 | /          | /       | 4C4G     | 工作节点3               |\n| VIP     | 192.168.10.150 | /       | /       | v1.3.5     | v1.5.18 | /        | 虚拟IP在控制节点上浮动  |\n| client  | 192.168.10.145 | /       | /       | /          |         | 2C2G     | 客户端，连接管理K8S集群 |\n\n\n\n# 二、高可用架构\n\n\n![K8S_Ceph云平台架构.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214397461-37728718-19a0-4719-9c0b-a0c1ead7ee38.png#align=left&display=inline&height=1033&margin=%5Bobject%20Object%5D&name=K8S_Ceph%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84.png&originHeight=1033&originWidth=988&size=298518&status=done&style=none&width=988)\n\n\n1. 主备模式高可用架构说明：\n\n| 核心组件           | 高可用模式 | 高可用实现方式  |\n| ------------------ | ---------- | --------------- |\n| apiserver          | 主备       | keepalived      |\n| controller-manager | 主备       | leader election |\n| scheduler          | 主备       | leader election |\n| etcd               | 集群       | kubeadm         |\n\n\n\n- apiserver 通过haproxy+keepalived实现高可用，当某个节点故障时触发keepalived vip 转移；\n- controller-manager k8s内部通过选举方式产生领导者(由--leader-elect 选型控制，默认为true)，同一时刻集群内只有一个controller-manager组件运行；\n- scheduler k8s内部通过选举方式产生领导者(由--leader-elect 选型控制，默认为true)，同一时刻集群内只有一个scheduler组件运行；\n- etcd 通过运行kubeadm方式自动创建集群来实现高可用，部署的节点数为奇数。如果剩余可用节点数量超过半数，集群可以几乎没有影响的正常工作(3节点方式最多容忍一台机器宕机)\n\n# 三、安装准备工作（所有节点都执行）\n\n\n1. 修改主机名\n   `# hostnamectl set-hostname master1` \n1. 修改hosts文件\n   `# vim /etc/hosts `  \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214509642-858c8113-28b9-4a78-ab32-7cfd207a8061.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=307&size=33595&status=done&style=none&width=307)\n\n3. 验证mac地址uuid，保证各节点mac和uuid唯一\n   `# cat /sys/class/net/ens33/address` \n   `# cat /sys/class/dmi/id/product_uuid` \n3. 免密登录\n   配置master1到master2、master3免密登录，本步骤只在master01上执行。\n\n- 创建密钥\n  `# ssh-keygen -t rsa` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214539642-a6e00006-560e-481a-9553-b7ed439e525c.png#align=left&display=inline&height=487&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=487&originWidth=749&size=119303&status=done&style=none&width=749)\n\n- 将密钥同步至master2/master3\n  `[root@master01  ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@master2  `\n  `[root@master01  ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@master3 `\n- 免密登陆测试\n  `# ssh root@master2  `\n  `# ssh root@master3 `\n- 安装依赖包\n  `# yum -y install conntrack chrony bash-completion ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git iptables-services` \n\n5. 时间同步\n\n- master1节点设置\n  `# vim /etc/chrony.conf` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214570252-f2413383-066a-4340-bf09-977ee3cdd222.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=774&size=119303&status=done&style=none&width=774)\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214584872-5b85c70c-5662-417a-86aa-86bd0b025cfb.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=585&size=119303&status=done&style=none&width=585)\n`# systemctl start chronyd` \n`# systemctl enable chronyd` \n`# chronyc sources` \n![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214618022-7c33bb76-d23b-4d5a-bc7f-1439895cb881.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=129&originWidth=960&size=119303&status=done&style=none&width=960)\n\n- 其余节点配置\n  `# vim /etc/chrony.conf` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214632701-a16335b0-9c5f-4324-9faa-f338b1528879.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=401&size=119303&status=done&style=none&width=401)\n  `# systemctl start chronyd` \n  `# systemctl enable chronyd` \n  `# chronyc sources` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214648770-24194bbf-cfba-4509-8e03-deaed1b39bff.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=125&originWidth=468&size=119303&status=done&style=none&width=468)\n\n- 设置防火墙规则\n  `# systemctl stop firewalld` \n  `# systemctl disable firewalld` \n  `# systemctl start iptables` \n  `# systemctl enable iptables` \n  `# iptables -F` \n  `# service iptables save` \n\n6. 关闭selinux\n   `# setenforce 0` \n   `# sed -i \'s/^SELINUX=.*/SELINUX=disabled/\' /etc/selinux/config` \n6. 关闭swap分区\n   `# swapoff -a` \n   `# sed -i \'/ swap / s/^\\(.*\\)$/#\\1/g\' /etc/fstab` \n6. 升级内核（可选）\n\n- 载入公钥\n  `# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org` \n- 升级安装ELRepo\n  `# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm` \n- centos8使用如下命令\n  `# yum install https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm` \n- 载入elrepo-kernel元数据\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist` \n- 安装最新版本的kernel\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y` \n- 删除旧版本工具包\n  `# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y` \n- 安装新版本工具包\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y` \n- 查看内核插入顺序\n  `# awk -F \\\' \'$1==\"menuentry \" {print i++ \" : \" $2}\' /etc/grub2.cfg` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214685031-f8a86358-72bc-48d5-9804-8960eb74418a.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=823&size=119303&status=done&style=none&width=823)\n\n- 设置默认启动\n  `# grub2-set-default 0 // 0代表当前第一行，也就是5.5版本` \n  `# grub2-editenv list` \n- 重启验证\n  `# uname -a` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214700346-4f7688a8-46ea-436e-9cec-5b6350be6e6c.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=471&size=119303&status=done&style=none&width=471)\n\n- 修改内核iptables相关参数\n\n```bash\n# cat <<EOF > /etc/sysctl.d/kubernetes.conf\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n# sysctl -p /etc/sysctl.d/kubernetes.conf\n```\n\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214725442-867f089e-6fae-4b31-a54c-84c5fdce7b71.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=473&size=119303&status=done&style=none&width=473)\n\n\n# 四、kubernetes安装前设置（每个节点）\n\n\n1. kube-proxy开启ipvs的前置条件\n\n```bash\n# yum -y install ipset ipvsadm\n# cat > /etc/sysconfig/modules/ipvs.modules <<EOF\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash\n# /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack\n```\n\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214760297-066b2aef-f381-49d9-98fa-0038086bcb22.png#align=left&display=inline&height=201&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=201&originWidth=713&size=119303&status=done&style=none&width=713)\n\n- linux kernel 4.19版本已经将nf_conntrack_ipv4 更新为 nf_conntrack\n\n2. docker安装（所有节点）\n\n- 安装前源准备\n  `yum install -y yum-utils device-mapper-persistent-data lvm2` \n- 配置yum源\n  `yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo` \n- 查看可安装的docker版本\n  `yum list docker-ce --showduplicates | sort -r` \n- 安装19.03.0版本docker\n  `yum install -y docker-ce-19.03.0*` \n- 使用阿里云做镜像加速\n\n```bash\nmkdir -p /etc/docker\ntee /etc/docker/daemon.json <<-\'EOF\'\n{\n\"registry-mirrors\": [\"https://o2j0mc5x.mirror.aliyuncs.com\"]\n}\nEOF\nsystemctl daemon-reload\n```\n\n\n- 启动docker\n  `systemctl start docker` \n  `systemctl enable docker` \n- docker 1.13以上版本默认禁用iptables的forward调用链，因此需要执行开启命令：\n  `iptables -P FORWARD ACCEPT` \n- 修改docker cgroup driver为systemd\n  使用systemd作为docker的cgroup\n  driver可以确保服务器节点在资源紧张的情况更加稳定\n\n```bash\n[root@master  ~]#vim /etc/docker/daemon.json \n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\nsystemctl daemon-reload\nsystemctl restart docker\ndocker info | grep Cgroup\n```\n\n\n- 配置阿里云yum源\n\n```bash\n[root@master  ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo \n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttps://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n\n3. 安装命令补全\n   `# yum -y install bash-completion` \n   `# source /etc/profile.d/bash_completion.sh` \n3. 安装kubelet、kubeadm和kubectl组件（所有节点）\n\n- 安装软件包\n  `# yum install -y kubelet kubeadm kubectl` \n  kubelet 运行在集群所有节点上，用于启动Pod和容器等对象的工具\n  kubeadm 用于初始化集群，启动集群的命令工具\n  kubectl 用于和集群通信的命令行，通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件\n- 设置kubelet开机启动\n  `# systemctl enable kubelet` \n- kubectl命令补全\n  `# echo \"source <(kubectl completion bash)\" >> ~/.bash_profile` \n  `# source .bash_profile` \n\n\n\n# 五、部署keepalived（master节点）\n\n\n- 此处的keeplived的主要作用是为haproxy提供vip（192.168.10.150），在三个haproxy实例之间提供主备，降低当其中一个haproxy失效的时对服务的影响。\n- 系统配置\n\n```bash\n# cat >> /etc/sysctl.conf << EOF\nnet.ipv4.ip_forward = 1\nEOF\nsysctl -p\n```\n\n\n- 安装keepalived\n  `yum install -y keepalived` \n\n1. 配置keepalived，修改/etc/keepalived/keepalived.conf文件\n\n- master1配置\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214820644-b211e033-0614-4ed0-bb4c-16b8d7cfc944.png#align=left&display=inline&height=740&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=740&originWidth=441&size=134163&status=done&style=none&width=441)\n\n- master2配置\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214836650-69d4adfb-2cad-4755-a469-810273965fbf.png#align=left&display=inline&height=738&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=738&originWidth=402&size=134163&status=done&style=none&width=402)\n\n- master3配置\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214850883-af21ce4d-4e4c-4351-b5a4-f1baa31a2fff.png#align=left&display=inline&height=739&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=739&originWidth=414&size=134163&status=done&style=none&width=414)\n\n- 启动keepalived\n  `# systemctl start keepalived` \n  `# systemctl enable keepalived` \n\n2. VIP查看\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214868024-3f8bc14a-1813-4dcf-a913-f0f749fd878f.png#align=left&display=inline&height=372&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=372&originWidth=580&size=142912&status=done&style=none&width=580)\n\n\n# 六、部署haproxy（master节点）\n\n> haproxy为apiserver提供反向代理，haproxy将所有请求轮询转发到每个master节点上。相对于仅仅使用keepalived主备模式仅单个master节点承载流量，这种方式更加合理、健壮。\n\n1. 系统配置\n\n```bash\n# cat >> /etc/sysctl.conf << EOF\nnet.ipv4.ip_nonlocal_bind = 1\nEOF\n# sysctl -p\n```\n\n\n2. 安装haproxy\n   `# yum install -y haproxy` \n2. 配置haproxy（所有master节点一样）\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214920117-959e5c24-887e-41c0-9155-da3059e25ff6.png#align=left&display=inline&height=130&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=130&originWidth=596&size=142912&status=done&style=none&width=596)\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214936866-552df92a-4f60-4c35-ab7c-f6c6db304121.png#align=left&display=inline&height=327&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=327&originWidth=618&size=142912&status=done&style=none&width=618)\n\n4. 启动并检测服务\n   `# systemctl enable haproxy.service` \n   `# systemctl start haproxy.service` \n   `# ss -lntp | grep 6443` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604214954799-5b92803f-88d9-4e8d-a450-757356c74d04.png#align=left&display=inline&height=53&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=53&originWidth=611&size=142912&status=done&style=none&width=611)\n\n\n# 七、初始化master节点（master1节点操作）\n\n\n1. 创建集群配置文件kubeadm-config.yaml\n\n```yaml\napiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.17.2\nimageRepository: registry.aliyuncs.com/google_containers\napiServer:\n  certSANs:    \n  #填写所有kube-apiserver节点的hostname、IP、VIP\n  - master1\n  - master2\n  - master3\n  - work1\n  - work2\n  - work3\n  - 192.168.10.138\n  - 192.168.10.139\n  - 192.168.10.140\n  - 192.168.10.141\n  - 192.168.10.142  \n  - 192.168.10.143\n  - 192.168.10.150\ncontrolPlaneEndpoint: \"192.168.10.150:16443\"\n#VIP:port\nnetworking:\n  podSubnet: \"10.244.0.0/16\"\n```\n\n2. master1节点初始化\n   `# kubeadm init --config=kubeadm-config.yaml` \n2. 记录kubeadm\n   join的输出，后面需要这个命令将work节点和其他master节点加入集群中。\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215043155-d557c36d-f562-4674-90cc-0d34752ff1bf.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=183&originWidth=918&size=142912&status=done&style=none&width=918)\n\n- 如果初始化失败：\n  `# kubeadm reset` \n  `# rm -rf $HOME/.kube/config` \n- 根据提示配置环境变量\n  `# mkdir -p $HOME/.kube` \n  `# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config` \n  `# chown $(id -u):$(id -g) $HOME/.kube/config` \n\n5. 安装flannel网络\n\n- 下载配置文件\n  `# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml` \n- 启用flannel\n  `# kubectl apply -f kube-flannel.yml` \n- 如果镜像不能正常下载，所有节点需提前导入镜像\n\n\n\n# 八、master2、master3节点加入集群\n\n\n1. 证书分发\n\n- master2、master3创建证书目录\n  `# mkdir -p /etc/kubernetes/pki/etcd` \n- master1运行脚本cert-main-master.sh，将证书分发至master02和master03\n\n```bash\n#cert-main-master.sh \nUSER=root\nCONTROL_PLANE_IPS=\"master2 master3\"\nfor host in ${CONTROL_PLANE_IPS}; do\n    scp /etc/kubernetes/pki/ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/ca.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/sa.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/sa.pub \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/front-proxy-ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/front-proxy-ca.key \"${USER}\"@$host:/etc/kubernetes/pki/\n    scp /etc/kubernetes/pki/etcd/ca.crt \"${USER}\"@$host:/etc/kubernetes/pki/etcd/\n    scp /etc/kubernetes/pki/etcd/ca.key \"${USER}\"@$host:/etc/kubernetes/pki/etcd/\ndone\n```\n\n\n2. master2、master3加入集群\n\n- 执行加入节点命令\n  `kubeadm join 192.168.10.150:16443 --token uueq7e.qz5tt55dx5y8vyuf  --discovery-token-ca-cert-hash sha256:ff411ba37bc403c9c87cae64355d222a6d8775737a65ac79b411256f58666eb7 --control-plane` \n\n3. 根据提示配置环境变量\n   `# mkdir -p $HOME/.kube` \n   `# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config` \n   `# chown $(id -u):$(id -g) $HOME/.kube/config` \n3. 查看验证\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215152521-160428f7-e9c0-48ed-883e-07352c89d78a.png#align=left&display=inline&height=662&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=662&originWidth=758&size=285628&status=done&style=none&width=758)\n\n\n# 九、work节点加入集群（所有work节点执行）\n\n\n1. work1、2、3执行加入集群命令\n   `kubeadm join 192.168.10.150:16443 --token uueq7e.qz5tt55dx5y8vyuf  --discovery-token-ca-cert-hash sha256:ff411ba37bc403c9c87cae64355d222a6d8775737a65ac79b411256f58666eb7` \n1. master节点上查看所有节点状态\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215185744-89e0b31e-5e77-41c8-8d32-e49a169b4902.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=519&size=285628&status=done&style=none&width=519)\n\n\n# 十、client配置\n\n\n1. 设置kubernetes源\n\n```bash\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\nhttps://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n2. 安装kubectl\n   `# yum install -y kubectl` \n\n- 安装版本与集群版本保持一致\n\n3. 命令补全\n\n- 安装bash-completion\n  `# yum -y install bash-completion` \n- bash-completion\n  `# source /etc/profile.d/bash_completion.sh` \n\n4. 拷贝admin.conf\n   `# mkdir -p /etc/kubernetes` \n   `# scp 192.168.10.138:/etc/kubernetes/admin.conf /etc/kubernetes/` \n   `# echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" >> ~/.bash_profile`  \n   `# source .bash_profile` \n4. 加载环境变量\n   `# echo \"source <(kubectl completion bash)\" >> ~/.bash_profile` \n   `# source .bash_profile` \n4. kubectl测试\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215252016-c69542f0-7250-476b-8d3c-3f319949d484.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=321&originWidth=579&size=285628&status=done&style=none&width=579)\n\n\n# 十一、Dashboard搭建（client端操作）\n\n\n1. 因为自动生成的证书很多浏览器无法使用，所以自己创建证书\n1. 新建证书存放目录\n   `mkdir /etc/kubernetes/dashboard-certs` \n   `cd /etc/kubernetes/dashboard-certs/` \n1. 创建命名空间\n   `kubectl create namespace kubernetes-dashboard` \n1. 创建key文件\n   `openssl genrsa -out dashboard.key 2048` \n1. 证书请求\n   `openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj \'/CN=dashboard-cert\' ` \n1. 自签证书\n   `openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt` \n1. 创建kubernetes-dashboard-certs对象\n   `kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard` \n1. 下载并修改配置文件\n\n- 下载配置文件\n  `wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml` \n- 修改配置文件，增加直接访问端口\n  `[root@master  ~]# vim recommended.yaml ` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215287691-e6bd84d5-5b5a-4663-8aef-1e8d547ceae5.png#align=left&display=inline&height=200&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=200&originWidth=458&size=285628&status=done&style=none&width=458)\n\n- 修改配置文件，注释原kubernetes-dashboard-certs对象声明\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215302751-bdb856e7-4ba8-42d0-8953-c6b0ba08f94f.png#align=left&display=inline&height=256&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=256&originWidth=431&size=285628&status=done&style=none&width=431)\n\n- 运行dashboard\n  `[root@master  ~]# kubectl apply -f recommended.yaml ` \n\n9. 更新配置信息\n\n- 创建Dashboard管理员账号dashboard-admin.yaml，并apply\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n- 赋权dashboard-admin-bind-cluster-role.yaml，并apply\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-bind-cluster-role\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin\n  namespace: kubernetes-dashboard\n```\n\n- 获取token信息\n  `kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk \'{print $1}\')` \n\n10. 登录访问[https://192.168.10.150:30001](https://192.168.10.150:30001)\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215381513-3963f5df-dcd6-4d4c-8965-e03f0964e4d2.png#align=left&display=inline&height=690&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=690&originWidth=1258&size=285628&status=done&style=none&width=1258)\n\n\n# 十二、集群高可用测试（client节点）\n\n\n1. 组件所在节点查看\n1. 查看apiserver所在节点（当前位于master1）\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215414607-ed319661-1b57-4a4f-bc7b-519b766f4555.png#align=left&display=inline&height=104&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=104&originWidth=539&size=285628&status=done&style=none&width=539)\n\n3. 查看scheduler所在节点（当前位于master3）\n   `# kubectl get endpoints kube-controller-manager -n kube-system -o yaml |grep holderIdentity` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215434024-ebbc1b50-8f6a-4629-9412-cabd38f2fa39.png#align=left&display=inline&height=75&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=75&originWidth=868&size=285628&status=done&style=none&width=868)[ ](media/6630119cd28b15c13995018641aa4492.png)\n\n4. 查看controller-manager所在节点（当前位于master3）\n   `[root@client  ~]# kubectl get endpoints kube-scheduler -n kube-system  -o yaml |grep holderIdentity` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215452891-5232e489-bd6c-4173-b99f-7251656bfaa6.png#align=left&display=inline&height=71&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=71&originWidth=866&size=285628&status=done&style=none&width=866)\n\n5. 创建deployment和svc，模拟生产业务\n\n- deployment资源清单文件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215468725-ddd2884b-f027-4628-a987-05838dd11018.png#align=left&display=inline&height=500&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=500&originWidth=468&size=285628&status=done&style=none&width=468)\n\n- svc资源清单文件\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215485214-2295d42f-6e00-4a90-9d1e-db03c41a50c9.png#align=left&display=inline&height=278&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=278&originWidth=266&size=285628&status=done&style=none&width=266)\n\n- client节点访问测试\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215503065-0b6fac4d-086c-42ab-a2e7-59e75135a6a6.png#align=left&display=inline&height=369&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=369&originWidth=1038&size=285628&status=done&style=none&width=1038)\n\n3. master1节点关机，模拟宕机\n\n- 关闭master1，模拟宕机\n  `# init 0` \n- 各组件查看\n  apiserver位于master1\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215529784-8387132b-0fda-4550-8f2e-248a44b17a5e.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=539&size=285628&status=done&style=none&width=539)\n  controller-manager位于master3\n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215546032-e4b8fa3b-7736-430f-9985-709489609e93.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=74&originWidth=757&size=285628&status=done&style=none&width=757)\n  scheduler位于master3\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215561038-4610c096-801a-4f97-9ab1-5831b751f436.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=758&size=285628&status=done&style=none&width=758)\n\n- 集群节点信息查看\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215579145-23d2239a-6ff7-40c8-a4a9-2e77afb0e562.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=196&originWidth=518&size=285628&status=done&style=none&width=518)\n\n- 业务访问测试\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215594824-6bb85bc0-d8b5-42f0-b8e4-f4af13d202fd.png#align=left&display=inline&height=45&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=45&originWidth=767&size=285628&status=done&style=none&width=767)\n\n- 结论\n  当有一个master节点宕机时，VIP会发生漂移，集群各项功能不受影响。\n\n4. master3关机，模拟2台节点宕机\n\n- 关闭master3:\n  `# init 0` \n- 查看VIP：\n  `# ip a|grep 150` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215620990-41c12dba-53f1-4fd0-9d68-332b4dd25cf6.png#align=left&display=inline&height=51&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=51&originWidth=541&size=285628&status=done&style=none&width=541)\n    vip漂移至唯一的master2\n\n- 集群功能测试\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215638949-54d6021e-f694-49c0-b7db-c8bd813ca1de.png#align=left&display=inline&height=101&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=101&originWidth=763&size=285628&status=done&style=none&width=763)\n\n- 由于etcd集群崩溃，整个k8s集群也不能正常对外服务。\n\n', 10, 0, 0, '2020-12-25 11:34:22.252815', '2021-01-15 02:45:59.205779', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (87, '离线二进制部署k8s', '[TOC]\n\n# 一、服务器规划\n\n\n1. 集群架构\n\n![1581598557837-0cbb83ab-3f08-4c42-837a-50002c3ecaa7.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215753668-14e62f9d-0573-4ffe-96d3-9ce7871721ca.png#align=left&display=inline&height=664&margin=%5Bobject%20Object%5D&name=1581598557837-0cbb83ab-3f08-4c42-837a-50002c3ecaa7.png&originHeight=664&originWidth=836&size=258261&status=done&style=none&width=836)\n\n2. 服务器角色\n\n| 主机名                   | ip             | 主机配置 | 组件                                                         | 用途                   |\n| ------------------------ | -------------- | -------- | ------------------------------------------------------------ | ---------------------- |\n| master1                  | 192.168.10.11  | 4C4G     | kube-apiserver kube-controller-manager kube-scheduler etcd flannel | 控制节点1              |\n| master2                  | 192.168.10.12  | 4C4G     | kube-apiserver kube-controller-manager kube-scheduler etcd flannel | 控制节点2              |\n| master3                  | 192.168.10.140 | 4C4G     | kube-apiserver kube-controller-manager kube-scheduler etcd flannel | 控制节点3              |\n| work1                    | 192.168.10.141 | 4C4G     | kubelet kube-proxy docker                                    | 工作节点1              |\n| work2                    | 192.168.10.142 | 4C4G     | kubelet kube-proxy docker                                    | 工作节点2              |\n| work3                    | 192.168.10.143 | 4C4G     | kubelet kube-proxy docker                                    | 工作节点3              |\n| Load Balancer （Master） |                | 2C2G     | Nginx keepalived                                             | 负载均衡高可用         |\n| Load Balancer （Backup） |                | 2C2G     | Nginx keeplived                                              | 负载均衡高可用         |\n| VIP                      | 192.168.10.10  | /        |                                                              | 虚拟IP在控制节点上浮动 |\n\n\n\n2. 组件使用的证书\n\n ![1591265-20190727122109980-376539073.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215772771-643562b2-8441-4497-a0f1-5f3d48a2aeac.png#align=left&display=inline&height=354&margin=%5Bobject%20Object%5D&name=1591265-20190727122109980-376539073.png&originHeight=354&originWidth=865&size=32360&status=done&style=none&width=865)\n\n\n# 二、系统初始化（所有master work节点）\n\n\n1. 修改主机名\n   `# hostnamectl set-hostname master1` \n1. 修改hosts文件\n   `# vim /etc/hosts` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215803136-39a2b5c8-ba25-4901-abbd-8c72dc68b329.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=307&size=285628&status=done&style=none&width=307)\n\n3. 验证mac地址uuid，保证各节点mac和uuid唯一\n   `# cat /sys/class/net/ens160/address` \n   `# cat /sys/class/dmi/id/product_uuid` \n3. 免密登录\n   配置master1到master2、master3免密登录，本步骤只在master01上执行。\n\n- 创建密钥\n  `# ssh-keygen -t rsa` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215823912-20d67d63-49a3-41b0-b334-1879e0934611.png#align=left&display=inline&height=487&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=487&originWidth=749&size=285628&status=done&style=none&width=749)\n\n- 将密钥同步至master2/master3/work1/work2/work3\n  `[root@master01  ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@master2 ` \n- 免密登陆测试\n  `# ssh root@master ` \n- 安装依赖包\n  `# dnf -y install conntrack bash-completion ipvsadm ipset jq iptables sysstat libseccomp git iptables-services` \n\n5. 时间同步\n\n- master1节点设置\n  `# vim /etc/chrony.conf` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215856195-0a568752-ac35-4ece-a49c-0b3f94c08f2b.png#align=left&display=inline&height=144&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=144&originWidth=774&size=285628&status=done&style=none&width=774)\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215872281-dc7f498a-4e2b-4909-9129-772a10443ad1.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=585&size=285628&status=done&style=none&width=585)\n`# systemctl start chronyd` \n      `# systemctl enable chronyd` \n      `# chronyc sources` \n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215888509-2adf7365-02ce-4b71-b4f6-be1bf10a7b9a.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=129&originWidth=960&size=285628&status=done&style=none&width=960)\n\n- 其余节点配置\n  `# vim /etc/chrony.conf` \n  ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215911474-73b80986-24fa-4a81-a211-089b0adb48ac.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=81&originWidth=401&size=285628&status=done&style=none&width=401)\n  `# systemctl start chronyd` \n  `# systemctl enable chronyd` \n  `# chronyc sources` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215928034-f2f3e7da-2c4d-4701-9ed9-dc9dc8f04e4f.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=125&originWidth=468&size=285628&status=done&style=none&width=468)\n\n6. 设置防火墙规则\n   `# systemctl stop firewalld` \n   `# systemctl disable firewalld` \n6. 关闭selinux\n   `# setenforce 0` \n   `# sed -i \'s/^SELINUX=.*/SELINUX=disabled/\' /etc/selinux/config` \n6. 关闭swap分区\n   `# swapoff -a` \n   `# sed -i \'/ swap / s/^\\(.*\\)$/#\\1/g\' /etc/fstab` \n6. 升级内核（可选）\n\n- 载入公钥\n  `# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org` \n- 升级安装ELRepo\n  `# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm` \n  centos8使用如下命令\n  `# yum install https://www.elrepo.org/elrepo-release-8.0-2.el8.elrepo.noarch.rpm` \n- 载入elrepo-kernel元数据\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist` \n- 安装最新版本的kernel\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y` \n- 删除旧版本工具包\n  `# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y` \n- 安装新版本工具包\n  `# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y` \n- 查看内核插入顺序\n  `# awk -F \\\' \'$1==\"menuentry \" {print i++ \" : \" $2}\' /etc/grub2.cfg` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215959392-423a0186-fba9-442d-a6ab-bc377d00bd64.png#align=left&display=inline&height=149&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=149&originWidth=823&size=285628&status=done&style=none&width=823)\n\n- 设置默认启动\n  `# grub2-set-default 0`  // 0代表当前第一行，也就是5.5版本`\n  `# grub2-editenv list` \n- 重启验证\n  `# uname -a` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215975448-4abb0cd5-56a2-412e-9611-31e93a4b2719.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=49&originWidth=471&size=285628&status=done&style=none&width=471)\n\n- 修改内核iptables相关参数\n\n```bash\n# cat <<EOF > /etc/sysctl.d/kubernetes.conf\nvm.swappiness = 0\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n# sysctl -p /etc/sysctl.d/kubernetes.conf\n```\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604215991266-52ba33a7-bbcd-4943-9874-501d4014532b.png#align=left&display=inline&height=93&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=93&originWidth=473&size=285628&status=done&style=none&width=473)\n\n\n# 三、部署etcd集群（master节点操作）\n\n\n1. 下载cfssl工具\n   `# curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64` \n   `# curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64` \n   `# curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64` \n   `# chmod +x /bin/cfssl*` \n1. 生成etcd证书\n   `# mkdir -p /k8s/tls/etcd` \n\n- ca-config.json\n\n```json\n{\n	\"signing\": {\n		\"default\": {\n			\"expiry\": \"87600h\"\n		},\n		\"profiles\": {\n			\"www\": {\n				\"expiry\": \"87600h\",\n				\"usages\": [\n					\"signing\",\n					\"key encipherment\",\n					\"server auth\",\n					\"client auth\"\n				]\n			}\n		}\n	}\n}\n```\n\n- ca-csr.json\n\n```json\n{\n	\"CN\": \"etcd CA\",\n	\"key\": {\n		\"algo\": \"rsa\",\n		\"size\": 2048\n	},\n	\"names\": [{\n		\"C\": \"CN\",\n		\"L\": \"Beijing\",\n		\"ST\": \"Beijing\"\n	}]\n}\n```\n\n3. server-csr.json\n\n```json\n{\n	\"CN\": \"etcd\",\n	\"hosts\": [\n		\"192.168.10.35\",\n		\"192.168.10.36\",\n		\"192.168.10.37\"\n	],\n	\"key\": {\n		\"algo\": \"rsa\",\n		\"size\": 2048\n	},\n	\"names\": [{\n		\"C\": \"CN\",\n		\"L\": \"BeiJing\",\n		\"ST\": \"BeiJing\"\n	}]\n}\n```\n\n4. 自建CA并签发证书\n   `# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -` \n\n- 生成 ca.pem ca-key.pem\n  `# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server` \n- 生成 server.csr server.pem server-key.pem\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604216139859-53d20bc6-681f-4fdf-9095-af673698e7b5.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=50&originWidth=558&size=285628&status=done&style=none&width=558)\n\n5. 下载解压etcd二进制文件（所有master节点）\n\n- [下载地址](https://github.com/coreos/etcd/releases/)\n  `# mkdir -p /opt/etcd/{bin,cfg,ssl}` \n  `# tar -zxvf etcd-v3.4.7-linux-amd64.tar.gz` \n  `# mv etcd-v3.4.7-linux-amd64/{etcd,etcdctl} /opt/etcd/bin` \n- 拷贝证书文件\n  `# cp {ca,server,server-key}.pem /opt/etcd/ssl/` \n- 创建etcd配置文件（所有master节点）\n\n```bash\n# cat > /opt/etcd/cfg/etcd.conf << EOF\n#[Member]\nETCD_NAME=\"etcd1\"\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\nETCD_LISTEN_PEER_URLS=\"https://192.168.10.35:2380\"\nETCD_LISTEN_CLIENT_URLS=\"https://192.168.10.35:2379\"\n#[Clustering]\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://192.168.10.35:2380\"\nETCD_ADVERTISE_CLIENT_URLS=\"https://192.168.10.35:2379\"\nETCD_INITIAL_CLUSTER=\"etcd1=https://192.168.10.35:2380,etcd2=https://192.168.10.36:2380,etcd3=https://192.168.10.37:2380\"\nETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\nETCD_ENABLE_V2=\"true\"\nEOF\n```\n\n- 配置文件参数说明\n  ETCD_NAME 节点名称\n  ETCD_DATA_DIR 数据目录\n  ETCD_LISTEN_PEER_URLS 集群通信监听地址（节点间相互通信）\n  ETCD_LISTEN_CLIENT_URLS 客户端访问监听地址（客户端访问集群）\n  ETCD_INITIAL_ADVERTISE_PEER_URLS 集群通告地址\n  ETCD_ADVERTISE_CLIENT_URLS 客户端通告地址\n  ETCD_INITIAL_CLUSTER 集群节点地址\n  ETCD_INITIAL_CLUSTER_TOKEN 集群Token\n  ETCD_INITIAL_CLUSTER_STATE\n  加入集群的当前状态，new是新集群，existing表示加入已有集群\n  ETCD_ENABLE_V2=\"true\"\n  flannel操作etcd使用的是v2的API，而kubernetes操作etcd使用的v3的API，为了兼容flannel，将默认开启v2版本，故配置文件中设置\n  ETCD_ENABLE_V2=\"true\"\n- systemd管理etcd（所有master节点）\n\n```bash\n# vim /usr/lib/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Server\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n[Service]\nType=notify\nEnvironmentFile=/opt/etcd/cfg/etcd.conf\nExecStart=/opt/etcd/bin/etcd \\\n--cert-file=/opt/etcd/ssl/server.pem \\\n--key-file=/opt/etcd/ssl/server-key.pem \\\n--peer-cert-file=/opt/etcd/ssl/server.pem \\\n--peer-key-file=/opt/etcd/ssl/server-key.pem \\\n--trusted-ca-file=/opt/etcd/ssl/ca.pem \\\n--peer-trusted-ca-file=/opt/etcd/ssl/ca.pem\nRestart=on-failure\nLimitNOFILE=65536\n[Install]\nWantedBy=multi-user.target\n```\n\n6. 拷贝master1文件到其他master节点\n   `# scp -r etcd master2:/opt` \n   `# scp /usr/lib/systemd/system/etcd.service master2:/usr/lib/systemd/system` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604216218265-9c5c6617-7e37-47c3-8ed6-1d13eecb74d1.png#align=left&display=inline&height=280&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=280&originWidth=365&size=285628&status=done&style=none&width=365)\n\n7. 启动并设置开机启动（第一个节点启动会卡主，在搜索集群其他节点）\n   `# systemctl start etcd` \n   `# systemctl enable etcd` \n7. 检查一下集群的状态\n   `# ./etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=\"https://192.168.10.35:2379,https://192.168.10.36:2379,https://192.168.10.37:2379\" endpoint health` \n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604216245283-1d7e4cc6-0c6e-4ae9-9e7a-492b199cd94d.png#align=left&display=inline&height=77&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=77&originWidth=619&size=285628&status=done&style=none&width=619)\n\n\n# 四、安装docker（work节点操作）\n\n\n- kubernets1.18支持最新docker版本为19.03.8\n- 下载离线包并解压\n  `# wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.8.tgz` \n- 将解压出来的docker文件内容移动到 /usr/bin/ 目录下\n  `# cp docker/* /usr/bin/` \n\n1. 将docker注册为service\n\n```bash\n# vim /usr/lib/systemd/system/docker.service\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service\nWants=network-online.target\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTimeoutStartSec=0\nDelegate=yes\nKillMode=process\nRestart=on-failure\nStartLimitBurst=3\nStartLimitInterval=60s\n[Install]\nWantedBy=multi-user.target\n```\n\n2. 启动docker服务\n   `systemctl daemon-reload` \n   `systemctl start docker` \n   `systemctl enable docker.service` \n2. 验证\n   `systemctl status docker` \n2. 文件复制至其他节点并启动docker\n\n# 五、安装flannel网络（master节点）\n\n\n1. Flannel要用etcd存储自身一个子网信息，要保证能成功连接etcd，写入预定义子网段\n   `# /opt/etcd/bin/etcdctl  --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem  --endpoints=\"https://192.168.10.35:2379,https://192.168.10.36:2379,https://192.168.10.37:2379\" set /coreos.com/network/config \'{ \"Network\": \"172.17.76.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}\'` \n1. 查看etcd配置子网信息\n   `# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://192.168.10.35:2379,https://192.168.10.36:2379,https://192.168.10.37:2379\" get /coreos.com/network/config` \n1. 下载二进制包\n   `# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz` \n   `# tar -zxvf flannel-v0.9.1-linux-amd64.tar.gz -C ./` \n   `# mv flanneld mk-docker-opts.sh /opt/kubernetes/bin` \n1. 配置Flannel\n   `# vim /opt/kubernetes/cfg/flanneld` \n   FLANNEL_OPTIONS=\"--etcd-endpoints=https://192.168.10.35:2379,https://192.168.10.36:2379,https://192.168.10.37:2379 -etcd-cafile=/opt/etcd/ssl/ca.pem -etcd certfile=/opt/etcd/ssl/server.pem -etcd-keyfile=/opt/etcd/ssl/server-key.pem\"\n1. systemd管理Flannel\n\n```bash\n# vim /usr/lib/systemd/system/flanneld.service\n[Unit]\nDescription=Flanneld overlay address etcd agent\nAfter=network-online.target network.target\nBefore=docker.service\n[Service]\nType=notify\nEnvironmentFile=/opt/kubernetes/cfg/flanneld\nExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS\nExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k\nDOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n6. 配置Docker启动指定子网段\n\n```bash\n# vim /usr/lib/systemd/system/docker.service\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service\nWants=network-online.target\n[Service]\nType=notify\nEnvironmentFile=/run/flannel/subnet.env\nExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTimeoutStartSec=0\nDelegate=yes\nKillMode=process\nRestart=on-failure\nStartLimitBurst=3\nStartLimitInterval=60s\n[Install]\nWantedBy=multi-user.target\n```\n\n7. 重启Flannel和Docker\n   `# systemctl daemon-reload` \n   `# systemctl start flanneld` \n   `# systemctl enable flanneld` \n   `# systemctl restart docker` \n7. 检查是否成功\n   `#ifconfig` \n   docker0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500\n   inet 172.17.74.1 netmask 255.255.255.0 broadcast 0.0.0.0\n   flannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1450\n   inet 172.17.74.0 netmask 255.255.255.255 broadcast 0.0.0.0\n   确保docker0与flannel.1在同一个网段\n7. 为了测试不同node之间互通，在任意一个node上ping另一个node的docker0的ip\n\n- k8s_node1——>172.17.74.0/16\n- k8s_node2——>172.17.76.0/16\n\n# 六、master节点部署\n\n\n1. 生成master节点需要的证书文件\n   `# mkdir -p /k8s/tls/kubernetes` \n\n- `# cat ca-config.json`\n\n```json\n{\n	\"signing\": {\n		\"default\": {\n			\"expiry\": \"87600h\"\n		},\n		\"profiles\": {\n			\"kubernetes\": {\n				\"expiry\": \"87600h\",\n				\"usages\": [\n					\"signing\",\n					\"key encipherment\",\n					\"server auth\",\n					\"client auth\"\n				]\n			}\n		}\n	}\n}\n```\n\n- `# cat ca-csr.json` \n\n```json\n{\n	\"CN\": \"kubernetes\",\n	\"key\": {\n		\"algo\": \"rsa\",\n		\"size\": 2048\n	},\n	\"names\": [{\n		\"C\": \"CN\",\n		\"L\": \"Beijing\",\n		\"ST\": \"Beijing\",\n		\"O\": \"k8s\",\n		\"OU\": \"System\"\n	}]\n}\n```\n\n`# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -` \n\n- 生成apiserver证书\n`cat server-csr.json`\n```json\n{\n	\"CN\": \"kubernetes\",\n	\"hosts\": [\n		\"10.0.0.1\",\n		\"127.0.0.1\",\n		\"192.168.18.80\",\n		\"kubernetes\",\n		\"kubernetes.default\",\n		\"kubernetes.default.svc\",\n		\"kubernetes.default.svc.cluster\",\n		\"kubernetes.default.svc.cluster.local\"\n	],\n	\"key\": {\n		\"algo\": \"rsa\",\n		\"size\": 2048\n	},\n	\"names\": [{\n		\"C\": \"CN\",\n		\"L\": \"BeiJing\",\n		\"ST\": \"BeiJing\",\n		\"O\": \"k8s\",\n		\"OU\": \"System\"\n	}]\n}\n```\n\n`# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server` \n\n- 生成kube-proxy证书\n\n  `# cat kube-proxy-csr.json` \n\n```json\n{\n	\"CN\": \"system:kube-proxy\",\n	\"hosts\": [],\n	\"key\": {\n		\"algo\": \"rsa\",\n		\"size\": 2048\n	},\n	\"names\": [{\n		\"C\": \"CN\",\n		\"L\": \"BeiJing\",\n		\"ST\": \"BeiJing\",\n		\"O\": \"k8s\",\n		\"OU\": \"System\"\n	}]\n}\n```\n\n`# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy` \n\n2. 部署apiserver组件\n\n- 下载二进制包kubernetes-server-linux-amd64.tar.gz。\n  `# mkdir /opt/kubernetes/{bin,cfg,ssl} -p` \n  `# tar -zxvf kubernetes-server-linux-amd64.tar.gz ./` \n  `# cd kubernetes/server/bin` \n  `# cp kube-apiserver kube-scheduler kube-controller-manager kubectl /opt/kubernetes/bin` \n- 创建token文件，用途之后会有用到\n  `# cat /opt/kubernetes/cfg/token.csv` \n  674c457d4dcf2eefe4920d7dbb6b0ddc,\n  kubelet-bootstrap,\n  10001,\n  \"system:kubelet-bootstrap\"\n  第一列：随机字符串，自己可生成\n  第二列：用户名\n  第三列：UID\n  第四列：用户组\n- 创建apiserver配置文件\n\n```bash\n# cat /opt/kubernetes/cfg/kube-apiserver\nKUBE_APISERVER_OPTS=\"--logtostderr=true \\\n--v=4 \\\n--etcd-servers=\"https://192.168.10.35:2379,https://192.168.10.36:2379,https://192.168.10.37:2379\"\n\\\n--bind-address=192.168.18.80 \\\n--secure-port=6443 \\\n--advertise-address=192.168.18.80 \\\n--allow-privileged=true \\\n--service-cluster-ip-range=10.0.0.0/24 \\\n--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction\n\\\n--authorization-mode=RBAC,Node \\\n--enable-bootstrap-token-auth \\\n--token-auth-file=/opt/kubernetes/cfg/token.csv \\\n--service-node-port-range=30000-50000 \\\n--tls-cert-file=/opt/kubernetes/ssl/server.pem \\\n--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\\n--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\n--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\n--etcd-cafile=/opt/etcd/ssl/ca.pem \\\n--etcd-certfile=/opt/etcd/ssl/server.pem \\\n--etcd-keyfile=/opt/etcd/ssl/server-key.pem\"\n```\n\n- 参数说明：\n  –logtostderr 启用日志\n  —v 日志等级\n  –etcd-servers etcd集群地址\n  –bind-address 监听地址\n  –secure-port https安全端口\n  –advertise-address 集群通告地址\n  –allow-privileged 启用授权\n  –service-cluster-ip-range Service虚拟IP地址段\n  –enable-admission-plugins 准入控制模块\n  –authorization-mode 认证授权，启用RBAC授权和节点自管理\n  –enable-bootstrap-token-auth 启用TLS bootstrap功能，后面会讲到\n  –token-auth-file token文件\n  –service-node-port-range Service Node类型默认分配端口范围\n- systemd管理apiserver\n\n```bash\n# cat /usr/lib/systemd/system/kube-apiserver.service\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\n[Service]\nEnvironmentFile=-/opt/kubernetes/cfg/kube-apiserver\nExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n- 启动apiserver\n  `# systemctl daemon-reload` \n  `# systemctl enable kube-apiserver` \n  `# systemctl restart kube-apiserver` \n\n3. 部署scheduler组件\n\n- 创建scheduler组件\n\n```bash\n# cat /opt/kubernetes/cfg/kube-scheduler\nKUBE_SCHEDULER_OPTS=\"--logtostderr=true \\\n--v=4 \\\n--master=127.0.0.1:8080 \\\n--leader-elect\"\n```\n\n- 参数说明：\n  –master 连接本地apiserver\n  –leader-elect 当该组件启动多个时，自动选举（HA）\n- systemd管理scheduler组件\n\n```bash\n# cat /usr/lib/systemd/system/kube-scheduler.service\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\n[Service]\nEnvironmentFile=-/opt/kubernetes/cfg/kube-scheduler\nExecStart=/opt/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n- 启动scheduler\n  `# systemctl daemon-reload` \n  `# systemctl enable kube-scheduler` \n  `# systemctl restart kube-scheduler` \n\n4. 部署controller-manager组件\n\n- 创建controller-manager组件\n\n```bash\n# cat /opt/kubernetes/cfg/kube-controller-manager\nKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\\n--v=4 \\\n--master=127.0.0.1:8080 \\\n--leader-elect=true \\\n--address=127.0.0.1 \\\n--service-cluster-ip-range=10.0.0.0/24 \\\n--cluster-name=kubernetes \\\n--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\n--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\n--root-ca-file=/opt/kubernetes/ssl/ca.pem \\\n--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem\"\n```\n\n- systemd管理controller-manager组件\n\n```bash\n# cat /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\n[Service]\nEnvironmentFile=-/opt/kubernetes/cfg/kube-controller-manager\nExecStart=/opt/kubernetes/bin/kube-controller-manager\n$KUBE_CONTROLLER_MANAGER_OPTS\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n\n- 启动controller-manager\n  `# systemctl daemon-reload` \n  `# systemctl enable kube-controller-manager` \n  `# systemctl restart kube-controller-manager` \n\n5. 通过kubectl工具查看当前集群组件状态\n   `# kubectl get cs` \n\n# 七、Node节点部署组件\n\n\n## 1. Master\n\napiserver启用TLS认证后，Node节点kubelet组件想要加入集群，必须使用CA签发的有效证书才能与apiserver通信，当Node节点很多时，签署证书是一件很繁琐的事情，因此有了TLSBootstrapping机制，kubelet会以一个低权限用户自动向apiserver申请证书，kubelet的证书由apiserver动态签署。\n\n- 将kubelet-bootstrap用户绑定到系统集群角色\n  `kubectl create clusterrolebinding kubelet-bootstrap  --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap` \n- 创建kubeconfig文件\n  在生成kubernetes证书的目录下执行以下命令生成kubeconfig文件（/home/k8s），创建kubernetes.sh\n- 创建kubelet bootstrapping kubeconfig\n  BOOTSTRAP_TOKEN=674c457d4dcf2eefe4920d7dbb6b0ddc KUBE_APISERVER=\"[https://192.168.18.80:6443](https://192.168.18.80:6443)\"\n- 设置集群参数\n  `kubectl config set-cluster kubernetes  --certificate-authority=./ca.pem  --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=bootstrap.kubeconfig` \n- 设置客户端认证参数\n  `kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=bootstrap.kubeconfig` \n- 设置上下文参数\n  `kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=bootstrap.kubeconfig` \n- 设置默认上下文\n  `kubectl config use-context default --kubeconfig=bootstrap.kubeconfig` \n- 创建kube-proxy kubeconfig文件\n  `kubectl config set-cluster kubernetes --certificate-authority=./ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=kube-proxy.kubeconfig` \n  `kubectl config set-credentials kube-proxy --client-certificate=./kube-proxy.pem --client-key=./kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig` \n  `kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig` \n  `kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig` \n- 在写完上述的shell脚本后，执行./kubernetes.sh生成了bootstrap.kubeconfig和kube-proxy.kubeconfig两个文件，将这两个文件通过scp发送到所有Node节点的/opt/kubernetes/cfg目录下。\n- 将master节点上的kubectl通过scp发送到所有node的/opt/kubernetes/bin下，同时修改/etc/profile文件，添加环境变量\n\nexport PATH=$PATH:/opt/kubernetes/bin/，\n\n- node节点cd到放有kubelet.kubeconfig的目录下执行\n\n`kubectl et nodes –kubeconfig=kubelet.kubeconfig` \n\n## 2. 部署kubelet组件\n\n- 解压kubernetes-server-linux-amd64.tar.gz后，在/kubernetes/server/bin下找到kubelet和kube-proxy两个文件，将这两个文件拷贝到Node节点的/opt/kubernetes/bin目录下。\n- 创建kubelet配置文件\n\n```bash\n# cat /opt/kubernetes/cfg/kubelet\nKUBELET_OPTS=\"--logtostderr=true \\\n--v=4 \\\n--hostname-override=192.168.18.85 \\\n--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\n--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\n--config=/opt/kubernetes/cfg/kubelet.config \\\n--cert-dir=/opt/kubernetes/ssl \\\n--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\"\n```\n\n- 参数说明：\n  –hostname-override 在集群中显示的主机名\n  –kubeconfig 指定kubeconfig文件位置，会自动生成\n  –bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件\n  –cert-dir 颁发证书存放位置\n  –pod-infra-container-image 管理Pod网络的镜像\n- kubelet.config配置文件如下，位置位于Node节点的/opt/kubernetes/cfg/kubelet.config下\n\n```yaml\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\naddress: 192.168.18.85\nport: 10250\nreadOnlyPort: 10255\ncgroupDriver: cgroupfs\nclusterDNS: [\"10.0.0.2\"]\nclusterDomain: cluster.local.\nfailSwapOn: false\nauthentication:\nanonymous:\nenabled: true\n```\n\n\n- systemd管理kubelet组件\n\n```bash\n# cat /usr/lib/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nAfter=docker.service\nRequires=docker.service\n[Service]\nEnvironmentFile=/opt/kubernetes/cfg/kubelet\nExecStart=/opt/kubernetes/bin/kubelet $KUBELET_OPTS\nRestart=on-failure\nKillMode=process\n[Install]\nWantedBy=multi-user.target\n```\n\n\n- 启动kubelet\n  `# systemctl daemon-reload` \n  `# systemctl enable kubelet` \n  `# systemctl restart kubelet` \n- 上述操作在所有Node节点都要执行，在启动后还是没有加入到集群中，这时需要Master手动允许才可以。\n  `# kubectl get csr` \n  `# kubectl certificate approve XXXXID` \n  `# kubectl get node` \n- 如：kubectl get csr\n  `kubectl certificate approve` \n  `node-csr-PjBnNDUYsE1soevzLVunyjrCWDEOj_bMUo7ypVuQOXs` \n\n## 3. 部署kube-proxy组件\n\n- 创建kube-proxy配置文件\n\n```bash\n# cat /opt/kubernetes/cfg/kube-proxy\nKUBE_PROXY_OPTS=\"--logtostderr=true \\\n--v=4 \\\n--hostname-override=192.168.18.85 \\\n--cluster-cidr=10.0.0.0/24 \\\n--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig\"\n```\n\n\n- systemd管理Kube-proxy组件\n\n```bash\n# cat /usr/lib/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Proxy\nAfter=network.target\n[Service]\nEnvironmentFile=-/opt/kubernetes/cfg/kube-proxy\nExecStart=/opt/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS\nRestart=on-failure\n[Install]\nWantedBy=multi-user.target\n```\n\n\n- 启动kube-proxy\n  `# systemctl daemon-reload` \n  `# systemctl enable kube-proxy` \n  `# systemctl restart kube-proxy` \n  上述操作在每个Node节点都要执行\n- 查看集群状态\n  `[root@k8s_master1  bin]# kubectl get node ` \n  `[root@k8s_master1  bin]# kubectl get cs ` \n\n\n\n# 八、运行测试\n\n\n- 创建一个Nginx Web，测试集群是否正常工作\n  `# kubectl run nginx --image=nginx --replicas=3` \n  `# kubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort` \n- 查看pod，service\n  `[root@k8s_master1  bin]# kubectl get pods ` \n  `[root@k8s_master1  bin]# kubectl get svc ` \n\n\n\n# 九、master高可用部署\n\n\n- Master高可用\n\n```bash\nscp -r /opt/kubernetes/ root@k8s-master2:/opt/\nmkdir /opt/etcd \n#在 k8s-master2节点\nscp -r /opt/etcd/ssl root@k8s-master2:/opt/etcd\nscp /usr/lib/systemd/system/{kube-apiserver,kube-controller-manager,kube-scheduler}.service root@k8s-master2:/usr/lib/systemd/system/\nscp /usr/bin/kube* root@k8s-master2:/usr/bin\n```\n\n- 修改api-server配置文件,并启动服务\n\n```bash\n[root@k8s-master2  cfg]# egrep \'advertise|bind\' kube-apiserver.conf \n--bind-address=192.168.31.64 \\\n--advertise-address=192.168.31.64 \\\n```\n\n\n1. 重启服务\n\n```bash\nsystemctl start kube-apiserver\nsystemctl start kube-controller-manager\nsystemctl start kube-scheduler\nsystemctl enable kube-apiserver\nsystemctl enable kube-controller-manager\nsystemctl enable kube-scheduler\nsystemctl status kube-apiserver\nsystemctl status kube-controller-manager\nsystemctl status kube-scheduler\n```\n\n\n2. 部署负载均衡\n\n- loadbalance-master 和 loadbalance-slave\n  分别安装nginx，keepalived，通过nginx 反向代理两个master的 kube-apiserver 服务\n- keepalived 设置健康检查 判断nginx\n  是否存活，如果一个节点nginx挂了，就会将vip 192.168.31.88 漂移到另一个节点。\n- 安装nginx，keepalived\n  `yum install -y nginx` \n  `yum install -y keepalived` \n- 修改nginx配置文件/etc/nginx/nginx.conf\n\n```bash\n[root@loadbalancer1  keepalived]# cat /etc/nginx/nginx.conf \n\nuser nginx;\nworker_processes auto;\nerror_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\nevents {\n    worker_connections 1024;\n}\nstream {\n    log_format main \'$remote_addr $upstream_addr - [$time_local] $status\n    $upstream_bytes_sent\';\n    access_log /var/log/nginx/k8s-access.log main;\n    upstream k8s-apiserver {\n        server 192.168.31.63:6443;\n        server 192.168.31.64:6443;\n    }\n    server {\n        listen 6443;\n        proxy_pass k8s-apiserver;\n    }\n}\nhttp {\n    log_format main \'$remote_addr - $remote_user [$time_local]\n    \"$request\" \'\n    \'$status $body_bytes_sent \"$http_referer\" \'\n    \'\"$http_user_agent\" \"$http_x_forwarded_for\"\';\n    access_log /var/log/nginx/access.log main;\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    include /etc/nginx/conf.d/*.conf;\n    server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name _;\n    root /usr/share/nginx/html;\n    # Load configuration files for the default server block.\n    include /etc/nginx/default.d/*.conf;\n        location / {\n        }\n    error_page 404 /404.html;\n        location = /40x.html {\n        }\n    error_page 500 502 503 504 /50x.html;\n        location = /50x.html {\n        }\n    }\n}\n```\n\n- keepalived 主配置\n\n```bash\n[root@loadbalancer1  keepalived]# cat /etc/keepalived/keepalived.conf \n\nglobal_defs {\n    notification_email {\n    acassen@firewall.loc  \n    failover@firewall.loc  \n    sysadmin@firewall.loc  \n    }\n    notification_email_from Alexandre.Cassen@firewall.loc  \n    smtp_server 127.0.0.1\n    smtp_connect_timeout 30\n    router_id NGINX_MASTER\n}\nvrrp_script check_nginx {\n    script \"/etc/keepalived/check_nginx.sh\"\n}\nvrrp_instance VI_1 {\n    state MASTER\n    interface eth0\n    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的\n    priority 100 # 优先级，备服务器设置 90\n    advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        192.168.31.88/24\n    }\n    track_script {\n        check_nginx\n    }\n}\n```\n\n- keepalived 从配置\n\n```bash\n[root@loadbalancer2  nginx]# cat /etc/keepalived/keepalived.conf \n\nglobal_defs {\n    notification_email {\n        acassen@firewall.loc  \n        failover@firewall.loc  \n        sysadmin@firewall.loc  \n    }\n    notification_email_from Alexandre.Cassen@firewall.loc  \n    smtp_server 127.0.0.1\n    smtp_connect_timeout 30\n    router_id NGINX_BACKUP\n}\nvrrp_script check_nginx {\n    script \"/etc/keepalived/check_nginx.sh\"\n}\nvrrp_instance VI_1 {\n    state BACKUP\n    interface eth0\n    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的\n    priority 90 # 优先级，备服务器设置 90\n    advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        192.168.31.88/24\n    }\n    track_script {\n        check_nginx\n    }\n}\n```\n\n- 健康检查脚本check_nginx.sh,两个loadbalance节点都要有\n\n```bash\n[root@loadbalancer1  nginx]# cat /etc/keepalived/check_nginx.sh \n\n#!/bin/bash\ncount=$(ps -ef |grep nginx |egrep -cv \"grep|$$\")\nif [ \"$count\" -eq 0 ];then\n    exit 1\nelse\n    exit 0\nfi\n```\n\n- 启动设置服务\n  `systemctl start nginx keepalived`\n\n`systemctl enable nginx keepalived` \n`systemctl status nginx keepalived` \n\n- 验证高可用', 17, 0, 0, '2020-12-25 12:14:56.392548', '2021-01-17 12:06:54.140668', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (88, '其他高可用部署方式', '[TOC]\n\n# 一、命令行工具部署：sealos\n\n\n[https://github.com/fanux/sealos](https://github.com/fanux/sealos)\n\n\n# 二、图形界面部署：breeze\n\n\n[https://github.com/wise2c-devops/breeze](https://github.com/wise2c-devops/breeze)\n\n\n# 三、rancher平台部署\n\n\n[https://rancher.com/docs/rancher/v2.x/en/](https://rancher.com/docs/rancher/v2.x/en/)\n\n# 四、KubeSphere 容器平台\n\n\n[https://kubesphere.com.cn/docs/](', 11, 0, 0, '2020-12-25 12:30:00.814928', '2021-01-13 18:36:49.504087', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (89, '更改证书有效期', '[TOC]\n\n# 一、基础知识\n\n\n1. k8s使用https双向认证，使用kubeadm安装后证书默认有效期为1年\n1. k8s证书存放路径：/etc/kubernetes/pki/\n1. 查看证书信息：openssl x509 -in apiserver.crt -text -noout\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218118360-1e2d43cb-d470-49ca-ac00-3c68f8e412bb.png#align=left&display=inline&height=73&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=73&originWidth=459&size=285628&status=done&style=none&width=459)\n\n4. 修改方式：修改kubeadm源码，更改证书默认有效期\n\n# 二、go 环境部署\n\n\n1. 下载go安装包\n   wget [https://studygolang.com/dl/golang/go1.13.7.linux-amd64.tar.gz](https://studygolang.com/dl/golang/go1.13.7.linux-amd64.tar.gz)\n1. 解压到/usr/local下\n   tar -zxvf go1.13.7.linux-amd64.tar.gz -C /usr/local/\n1. 修改环境变量\n   vim /etc/profile\n   export PATH=$PATH:/usr/local/go/bin\n1. 加载环境变量配置\n   source /etc/profile\n1. 查看版本信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218137951-2f4e6264-6e00-49cb-b2ce-f82bf12558fe.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=48&originWidth=366&size=285628&status=done&style=none&width=366)\n\n\n# 三、git源码\n\n\n1. 下载源码至本地\n   git clone [https://github.com/kubernetes/kubernetes.git](https://github.com/kubernetes/kubernetes.git)\n1. 查看当前k8s版本\n   kubeadm version\n1. 切换git到指定版本\n   cd kubernetes\n   git checkout -b remotes/origin/release-1.17.0 v1.17.0\n\n\n\n# 四、修改源码更新证书策略\n\n\n1. 查看相关代码所在文件名称\n   kubernetes]# grep -r kubeadmconstants.CertificateValidity .\n1. 更改源代码\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218156767-a917821f-ed9b-4d9e-8e84-429c85bafa1c.png#align=left&display=inline&height=560&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=560&originWidth=734&size=285628&status=done&style=none&width=734)\n\n3. 编译代码kubeadm\n   kubernetes] #make WHAT=cmd/kubeadm GOFLAGS=-v\n3. 将编译成功的kubeadm复制到/root下\n   kubernetes] #cp _output/bin/kubeadm /root/kubeadm-new\n\n\n\n# 五、更新 kubeadm\n\n\n1. 备份原kubeadm文件\n   cp /usr/bin/kubeadm /usr/bin/kubeadm.old\n1. 替换新kubeadm文件\n   cp /root/kubeadm-new /usr/bin/kubeadm\n1. 赋予执行权限\n   chmod a+x /usr/bin/kubeadm\n\n\n\n# 六、更新master节点证书\n\n\n1. 备份原证书文件\n   cp -r /etc/kubernetes/pki /etc/kubernetes/pki.old\n1. 生成新证书\n   cd /etc/kubernetes/pki\n   kubeadm alpha certs renew all\n1. 查看证书有效期\n   openssl x509 -in apiserver.crt -text -noout | grep Not\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218178574-ee3aea38-bdb9-43ec-9305-51a78495d8dc.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=65&originWidth=794&size=285628&status=done&style=none&width=794)\n\n\n# 七、HA其余master节点证书更新\n\n```bash\n#!/bin/bash\nmasterNode=\"192.168.66.20 192.168.66.21\"\n#for host in ${masterNode}; do\n#    scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key}\"\n${USER}\"@$host:/etc/kubernetes/pki/\n#    scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} \"root\"@$host:/etc/kubernetes/pki/etcd\n#    scp /etc/kubernetes/admin.conf \"root\"@$host:/etc/kubernetes/\n#done\nfor host in${CONTROL_PLANE_IPS}; do\n    scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key}\"${USER}\"@$host:/root/pki/\n    scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} \"root\"@$host:/root/etcd   \n    scp /etc/kubernetes/admin.conf \"root\"@$host:/root/kubernetes/\ndone\n\n```\n\n\n\n', 9, 0, 0, '2020-12-25 12:46:25.041033', '2021-01-16 07:32:39.015223', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (90, 'k8s版本升级', '[TOC]\n\n# 一、准备工作\n\n\n1. 在master节点上查看此时的kubernetes的版本\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218267571-0336ccbf-4d1c-4b45-8b46-acb5109de78c.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=884&size=285628&status=done&style=none&width=884)\n\n2. 查询最新版本号\n   yum list kubeadm --showduplicates | sort -r\n\n\n\n# 二、升级操作\n\n\n1. master和node节点执行升级命令\n   yum update -y kubeadm kubectl kubelet\n1. master节点验证升级版本\n   kubeadm upgrade plan\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218285326-7a41d155-c92d-4043-a838-6b1577c15a41.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=80&originWidth=716&size=285628&status=done&style=none&width=716)\n\n3. 升级到指定版本\n   kubeadm upgrade apply v1.17.2\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218305417-9dd74318-4eb7-43e9-8e75-e59015006a0f.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=46&originWidth=829&size=285628&status=done&style=none&width=829)\n\n4. master和node重启kubelet\n   systemctl daemon-reload\n   systemctl restart kubelet\n\n\n\n# 三、验证\n\n\n1. 查看node信息\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218326559-05d1773f-72d6-4970-8a93-d740f6602595.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=114&originWidth=440&size=285628&status=done&style=none&width=440)\n\n', 4, 0, 0, '2020-12-25 12:47:09.213546', '2021-01-08 08:01:32.435281', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (91, '添加work节点', '\n\n1. work节点进行初始化操作\n1. master节点查询join命令\n`# kubeadm token create --print-join-command`\n\n ![未命名图片.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1604218421749-8e84c96b-7788-421b-81ce-ea902b84aa12.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png&originHeight=116&originWidth=879&size=285628&status=done&style=none&width=879)\n\n3. work节点执行kubeadm join命令\n3. master节点查看node信息\n\n', 6, 0, 0, '2020-12-25 12:48:35.850025', '2021-01-26 09:20:33.388235', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (92, '控制节点启用pod调度', '\n\n> 默认情况下，出于安全原因，您的群集不会在控制节点上调度Pod。如果您希望能够在控制平面节点上调度Pod，例如用于单机Kubernetes集群进行开发，请运行：\n\nkubectl taint nodes --all node-role.kubernetes.io/master-\n\n', 4, 0, 0, '2020-12-25 12:49:06.013876', '2021-01-08 08:01:42.842516', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (93, '集群以外节点控制k8s集群', '\n\n1. 为了使kubectl在其他计算机上与集群通信，需要将管理员kubeconfig文件从控制平面节点复制到计算机上\n`# scp root@<control-plane-host>:/etc/kubernetes/admin.conf .` \n`kubectl --kubeconfig ./admin.conf get nodes`\n\n2. 注意：\n\n- 上面的示例假定为root用户启用了SSH访问。如果不是这种情况，您可以复制admin.conf文件以供其他用户访问，而scp改用该其他用户。\n- 该admin.conf文件为用户提供了对集群的超级用户特权。该文件应谨慎使用。对于普通用户，建议生成一个唯一的凭据，将其特权列入白名单。您可以使用kubeadm alpha kubeconfig user --client-name <CN> 命令执行此操作。该命令会将KubeConfig文件打印到STDOUT，您应该将其保存到文件并分发给用户。之后，使用来将特权列入白名单kubectl create (cluster)rolebinding。', 6, 0, 0, '2020-12-25 12:49:56.204246', '2021-01-11 23:02:31.585472', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (94, '删除本地集群', '\n\n1. 使用 kubectl config delete-cluster删除对集群的本地引用。\n1. 如果要更干净地取消配置群集，则应首先排空该节点，并确保该节点为空，然后取消配置该节点。\n1. 删除节点\n   使用适当的凭证与控制平面节点通信，请运行：\n   kubectl drain <node name> --delete-local-data --force --ignore-daemonsets\n   kubectl delete node <node name>\n1. 然后，在要删除的节点上，重置所有kubeadm安装状态：\n   kubeadm reset\n1. 重置过程不会重置或清除iptables规则或IPVS表。如果您希望重置iptables，则必须手动进行：\n   iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n1. 如果要重置IPVS表，则必须运行以下命令：\n   ipvsadm -C\n1. 如果您想重新开始，只需运行kubeadm init或kubeadm join使用适当的参数即可。\n\n', 5, 0, 0, '2020-12-25 12:50:26.525964', '2021-01-15 17:22:12.605801', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (95, '日志排查', '[TOC]\n\n# 一、状态查看\n\n\n1. 查看 Pod 状态以及运行节点\n   kubectl get pods -o wide\n   kubectl -n kube-system get pods -o wide\n1. 查看 Pod 事件\n   kubectl describe pod <pod-name>\n1. 查看 Node 状态\n   kubectl get nodes\n   kubectl describe node <node-name>\n\n\n\n# 二、日志查看\n\n\n## 1. kube-apiserver 日志\n   PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-apiserver 是用 systemd 管理的，则需要登录到 master 节点上，然后使用\n  journalctl -u kube-apiserver 查看其日志。\n\n\n\n## 2. kube-controller-manager 日志\n   PODNAME=$(kubectl -n kube-system get pod -l\n   component=kube-controller-manager -o jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-controller-manager 是用 systemd 管理的，则需要登录到 master\n  节点上，然后使用 journalctl -u kube-controller-manager 查看其日志。\n\n\n\n## 3. kube-scheduler 日志\n   PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME --tail 100\n\n\n\n- 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果\n  kube-scheduler 是用 systemd 管理的，则需要登录到 master 节点上，然后使用\n  journalctl -u kube-scheduler 查看其日志。\n\n\n\n## 4. kube-dns 日志\n   kube-dns 通常以 Addon 的方式部署，每个 Pod 包含三个容器，最关键的是 kubedns\n   容器的日志：\n   PODNAME=$(kubectl -n kube-system get pod -l k8s-app=kube-dns -o\n   jsonpath=\'{.items[0].metadata.name}\')\n   kubectl -n kube-system logs $PODNAME -c kubedns\n## 4. Kubelet 日志\n   Kubelet 通常以 systemd 管理。查看 Kubelet 日志需要首先 SSH 登录到 Node\n   上，推荐使用 kubectl-node-shell而不是为每个节点分配公网 IP 地址。比如：··\n\n```bash\n[root@localhost ~]# cat kubectl-node_shell\n#!/bin/sh\nif [ -z \"$1\" ]; then\n  echo \"Please specify node name\"\n  exit 1\nfi\n\nNODE=\"$1\"\nIMAGE=\"alpine\"\nPOD=\"nsenter-$(env LC_CTYPE=C tr -dc a-z0-9 < /dev/urandom | head -c 6)\"\nNAMESPACE=\"\"\n\n# Check the node\nkubectl get node \"$NODE\" >/dev/null || exit 1\n\nOVERRIDES=\"$(cat <<EOT\n{\n  \"spec\": {\n    \"nodeName\": \"$NODE\",\n    \"hostPID\": true,\n    \"containers\": [\n      {\n        \"securityContext\": {\n          \"privileged\": true\n        },\n        \"image\": \"$IMAGE\",\n        \"name\": \"nsenter\",\n        \"stdin\": true,\n        \"stdinOnce\": true,\n        \"tty\": true,\n        \"command\": [ \"nsenter\", \"--target\", \"1\", \"--mount\", \"--uts\", \"--ipc\", \"--net\", \"--pid\", \"--\", \"bash\", \"-l\" ]\n      }\n    ]\n  }\n}\nEOT\n)\"\n\necho \"spawning \\\"$POD\\\" on \\\"$NODE\\\"\"\nkubectl run --namespace \"$NAMESPACE\" --rm --image alpine --overrides=\"$OVERRIDES\" --generator=run-pod/v1 -ti \"$POD\"\nchmod +x ./kubectl-node_shell\nsudo mv ./kubectl-node-shell /usr/local/bin/kubectl-node_shell\n[root@localhost ~]# ./kubectl-node_shell localhost.localdomain\nspawning \"nsenter-i71opm\" on \"localhost.localdomain\"\nIf you don\'t see a command prompt, try pressing enter.\n[root@localhost /]# journalctl -l -u kubelet\n```\n\n\n## 6. Kube-proxy 日志\n   Kube-proxy 通常以 DaemonSet 的方式部署，可以直接用 kubectl 查询其日志\n   $ kubectl -n kube-system get pod -l component=kube-proxy\n   NAME READY STATUS RESTARTS AGE\n   kube-proxy-42zpn 1/1 Running 0 1d\n   kube-proxy-7gd4p 1/1 Running 0 3d\n   kube-proxy-87dbs 1/1 Running 0 4d\n   $ kubectl -n kube-system logs kube-proxy-42zpn', 8, 1, 0, '2020-12-25 12:52:34.340115', '2021-01-09 16:48:35.865535', 1, 1, 1, 0);
INSERT INTO `blog_section` VALUES (96, 'docker基础', '[TOC]\n\n# 一、docker简介\n\nDocker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。\n\n# 二、docker基本概念\n\n1. Docker镜像就是一个只读的模板。比如，一个镜像可以包含一个完整的ubuntu操作系统环境，里面仅安装了apache或用户需要的其他应用程序。镜像可以用来创建Docker容器。另外Docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人哪里下载一个已经做好的镜像来直接使用。\n1. Docker利用容器来运行应用。容器是从镜像创建的运行实例，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的Linux      环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\n\n注：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。\n\n3. Docker仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，      并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。\n3. 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是Docker Hub， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括Docker Pool等，可以提供大陆用户更稳定快速的访问。当然，用户也可以在本地网络内创建一个私有仓库。当用户创建了自己的镜像之后就可以使用push命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上pull下来就可以了。\n\n# 三、docker容器与虚拟机区别\n\n**首先明确一点，大家都把docker比喻成轻量化的虚拟机，但他们有本质的区别，docker并不是虚拟机**\n\n## 1. 虚拟机运行多个相互隔离的应用时\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608886762951-5b5480a9-eda9-424f-8b1f-6aa5ccf6caeb.png#align=left&display=inline&height=465&margin=%5Bobject%20Object%5D&name=image.png&originHeight=465&originWidth=612&size=153250&status=done&style=none&width=612)\n\n- 基础设施（Infrastructure）。它可以是你的个人电脑，数据中心的服务器，或者是云主机。\n- 主操作系统（Host Operating System）。你的个人电脑之上，运行的可能是MacOS，Windows或者某个Linux发行版。\n- 虚拟机管理系统（Hypervisor）。利用Hypervisor，可以在主操作系统之上运行多个不同的从操作系统。类型1的Hypervisor有支持MacOS的HyperKit，支持Windows的Hyper-V以及支持Linux的KVM。类型2的Hypervisor有VirtualBox和VMWare。\n- 从操作系统（Guest Operating System）。假设你需要运行3个相互隔离的应用，则需要使用Hypervisor启动3个从操作系统，也就是3个虚拟机。这些虚拟机都非常大，也许有700MB，这就意味着它们将占用2.1GB的磁盘空间。更糟糕的是，它们还会消耗很多CPU和内存。\n- 各种依赖。每一个从操作系统都需要安装许多依赖。如果你的的应用需要连接PostgreSQL的话，则需要安装libpq-dev；如果你使用Ruby的话，应该需要安装gems；如果使用其他编程语言，比如Python或者Node.js，都会需要安装对应的依赖库。\n- 应用。安装依赖之后，就可以在各个从操作系统分别运行应用了，这样各个应用就是相互隔离的。\n\n## 2. Docker容器运行多个相互隔离的应用\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608886841032-72a03fc8-bfa0-4306-924d-c2a71ec10452.png#align=left&display=inline&height=283&margin=%5Bobject%20Object%5D&name=image.png&originHeight=283&originWidth=606&size=119281&status=done&style=none&width=606)\n\n- 主操作系统（Host Operating System）。所有主流的Linux发行版都可以运行Docker。对于MacOS和Windows，也有一些办法\"运行\"Docker。\n- Docker守护进程（Docker Daemon）。Docker守护进程取代了Hypervisor，它是运行在操作系统之上的后台进程，负责管理Docker容器。\n- 各种依赖。对于Docker，应用的所有依赖都打包在Docker镜像中，Docker容器是基于Docker镜像创建的。\n- 应用。应用的源代码与它的依赖都打包在Docker镜像中，不同的应用需要不同的Docker镜像。不同的应用运行在不同的Docker容器中，它们是相互隔离的。\n\n## 3. 对比虚拟机与Docker\n\nDocker守护进程可以直接与主操作系统进行通信，为各个Docker容器分配资源；它还可以将容器与主操作系统隔离，并将各个容器互相隔离。虚拟机启动需要数分钟，而Docker容器可以在数毫秒内启动。由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源。\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608886953360-4940ba83-377d-456e-a60e-7e89f412487a.png#align=left&display=inline&height=992&margin=%5Bobject%20Object%5D&name=image.png&originHeight=992&originWidth=1528&size=962723&status=done&style=none&width=1528)\n\n## 4. 使用场景。\n\n虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。\nDocker通常用于隔离不同的应用，例如前端，后端以及数据库。', 57, 0, 0, '2020-12-25 22:35:57.929709', '2021-01-26 18:03:11.180603', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (97, 'docker安装与卸载', '[TOC]\n\n# 一、docker安装\n\n- [官方文档链接](https://docs.docker.com/get-started/#test-docker-installation)\n\n1. 安装前源准备\n`yum install -y yum-utils device-mapper-persistent-data lvm2`\n`yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo` \n2. 安装docker\n  如果是centos8 先安装containerd\n`dnf -y install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm` \n`yum -y install docker-ce`\n\n3. 使用阿里云做镜像加速\n- [参考链接](https://cr.console.aliyun.com/cn-qingdao/mirrors)\n\n```bash\nmkdir -p /etc/docker\ntee /etc/docker/daemon.json <<-\'EOF\'\n{\n  \"registry-mirrors\": [\"https://o2j0mc5x.mirror.aliyuncs.com\"]\n}\nEOF\nsystemctl daemon-reload\nsystemctl enable docker\nsystemctl restart docker\n```\n\n# 二、常见异常处理\n\n1. Cenntos8安装过程报错\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608886036527-08ef5196-00b2-4a75-86d0-959dc737cacc.png#align=left&display=inline&height=376&margin=%5Bobject%20Object%5D&name=image.png&originHeight=376&originWidth=956&size=121675&status=done&style=none&width=956)\n\n\n- 安装最新版containerd.io-1.2.6-3.3.el7.x86_64.rpm\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608886036475-8394a278-5e32-4211-a298-7dae7447b0d1.png#align=left&display=inline&height=405&margin=%5Bobject%20Object%5D&name=image.png&originHeight=405&originWidth=817&size=53793&status=done&style=none&width=817)\n`dnf install -y  https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm`\n\n ** 再装剩下两个**\n\n `dnf install docker-ce docker-ce-cli` \n`systemctl start docker` \n`docker --version` \n\n# 三、docker卸载\n\n1. 卸载软件包\n`yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine`\n\n2. 删除服务注册文件\n`rm -rf /etc/systemd/system/docker.service.d`\n\n3. 删除docker库\n`rm -rf /var/lib/docker`\n\n4. 删除运行文件\n`rm -rf /var/run/docker`', 46, 0, 0, '2020-12-25 22:38:51.657225', '2021-01-26 13:03:12.421649', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (98, '镜像操作命令', '[TOC]\n\n# 一、常用镜像命令总结\n\n| images        | 显示镜像列表              |\n| ------------- | ------------------------- |\n| history       | 显示镜像构建历史          |\n| commit        | 从容器创建新镜像          |\n| build         | 从 Dockerfile 构建镜像    |\n| tag           | 给镜像打 tag              |\n| pull          | 从 registry 下载镜像      |\n| push          | 将 镜像 上传到 registry   |\n| rmi           | 删除 Docker host 中的镜像 |\n| search        | 搜索 Docker Hub 中的镜像  |\n| image   prune | 清理临时镜像              |\n\n\n\n', 47, 0, 0, '2020-12-25 22:45:11.235075', '2021-01-20 13:01:10.998300', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (99, '管理镜像', '\n1. 从docker hub下载镜像\n[root@bogon ~]# docker pull hello-world\n\n2. 查看镜像信息\n[root@bogon ~]# docker images hello-world\n\n3. 给镜像添加tag标签（起别名，创建链接）\n[root@localhost ~]# docker tag ubuntu:latest ubuntu:18.10\n\n4. 运行镜像\n[root@bogon ~]# docker run -it ubuntu bash\n\n5. 查看镜像创建历史\n[root@bogon ~]# docker history hello-world\n\n6. 搜索镜像\n[root@localhost ~]# docker search nginx\n\n7. 删除镜像\n[root@bogon ~]# docker image rm ubuntu:18.10\n[root@bogon ~]# docker rmi ubuntu:18.10\n[root@bogon ~]# docker image rm 9SDS3HF3S2\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608887120593-a195c222-083d-4103-bcff-8064af826f6a.png#align=left&display=inline&height=85&margin=%5Bobject%20Object%5D&name=image.png&originHeight=85&originWidth=1251&size=30210&status=done&style=none&width=1251)\n\n 出现上述情况因为后台存在退出状态的容器，依赖该镜像，可以使用docker ps -a查看，使用docker rm删除依赖的容器，然后才能删除该镜像\n \n rmi 只能删除 host 上的镜像，不会删除      registry 的镜像。\n \n 如果一个镜像对应了多个 tag，只有当最后一个 tag      被删除时，镜像才被真正删除。\n\n8. 清理临时镜像\n[root@localhost ~]# docker image prune', 47, 0, 0, '2020-12-25 22:47:08.203625', '2021-01-14 14:31:43.948785', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (100, '构建镜像', '[TOC]\n# 一、docker commit（基于已有镜像）\n\n1. 第一步：运行容器\n[root@bogon ~]# docker run -it centos\n\n2. 第二步：进行自定义操作（安装vim）\n[root@5db3417f4eb6 /]# yum -y install vim\n\n3. 第三步：在宿主机操作，查看容器名称\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608887261476-c378f11c-5e4d-4488-b99c-85b781a6bac1.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=image.png&originHeight=80&originWidth=1605&size=22125&status=done&style=none&width=1605)\n\n4. 第四步：将容器保存为镜像\n[root@bogon ~]# docker commit brave_austin centos-vim\n\n5. 或者根据容器id直接创建\n[root@docker ~]# docker commit db1f98bb8a55 docker-vim\n\n6. 第五步：查看新镜像属性\n[root@bogon ~]# docker images centos-vim\n\n7. 第六步：从新镜像启动容器，验证操作\n[root@bogon ~]# docker run -it centos-vim\n\n# 二、docker import（本地模板导入）\n\n1. 第一步：访问openvz模板网站，下载模板文件\nhttps://wiki.openvz.org/Download/template/precreated\n\n2. 第二步：导入到docker镜像中\n[root@docker ~]# cat ubuntu-18.04-x86_64-minimal.tar.gz | docker import - ubuntu:18.04\n\n3. 第三步：查看镜像列表\n[root@docker ~]# docker images\n\n# 三、Dockerfile 构建镜像\n\n1. 第一步：准备Dockerfile文件\n[root@bogon ~]# mkdir /docker\n[root@bogon ~]# touch  /docker/Dockerfile\n[root@bogon docker]# cat Dockerfile \nFROM centos\nRUN yum -y install vim\n\n2. 第二步：使用docker build 创建镜像\n[root@bogon docker]# docker build -t centos-vim-dockerfile .\n-t：创建镜像的标签\n. 表示Dockerfile文件在当前路径\n-f 参数指定 Dockerfile 的位置\n\n3. 第三步：查看centos-vim-dockerfile镜像信息\n[root@bogon docker]# docker images centos-vim-dockerfile\n\n4. 第四步：查看镜像构建历史，验证镜像\n[root@bogon docker]# docker history centos-vim-dockerfile', 65, 16, 0, '2020-12-25 22:49:58.489591', '2021-01-17 10:27:11.022084', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (101, '导出和导入镜像', '\n1. 导出镜像\n   docker save image_name > xx.tar\n   docker save -o xx.tar image_name\n1. 导入镜像\n   docker load < xx.tar\n   docker load -i xx.tar', 49, 1, 0, '2020-12-25 22:50:34.998499', '2021-01-23 06:27:16.510723', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (102, '容器操作命令总结', '\n\n| create                | 创建容器                                         |\n| --------------------- | ------------------------------------------------ |\n| start                 | 启动容器                                         |\n| run                   | 创建并启动容器                                   |\n| ps                    | 查看正在运行的容器                               |\n| logs                  | 查看容器输出信息                                 |\n| pause                 | 暂停容器                                         |\n| unpause               | 取消暂停继续运行容器                             |\n| stop                  | 发送 SIGTERM 停止容器                            |\n| kill                  | 发送 SIGKILL   快速停止容器                      |\n| start                 | 启动容器                                         |\n| restart               | 重启容器                                         |\n| attach                | attach   到容器启动进程的终端                    |\n| exec                  | 在容器中启动新进程，通常使用   \"-it\" 参数        |\n| logs                  | 显示容器启动进程的控制台输出，用   \"-f\" 持续打印 |\n| rm                    | 删除终止或退出的容器                             |\n| export                | 导出已创建的容器到文件                           |\n| import                | 导入文件成镜像                                   |\n| [container]   inspect | 查看容器具体信息                                 |\n| [container]   top     | 查看容器内进程                                   |\n| [container]   stats   | 查看容器统计信息                                 |\n| [container]   cp      | 复制文件                                         |\n| [container]   diff    | 查看文件系统变更                                 |\n| [container]   port    | 查看端口映射                                     |\n| [container]   update  | 更新运行时资源配置                               |\n\n', 6, 0, 0, '2020-12-29 23:02:37.349095', '2021-01-21 12:31:49.646815', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (103, '创建容器', '\n\n1. 新建容器\n   [root[@docker ](/docker ) ~]# docker create -it ubuntu:latest \n   新建的容器处于停止状态\n1. create命令选项\n   参考链接\n1. 启动容器\n   [root[@docker ](/docker ) ~]# docker start priceless_archimedes \n   启动容器名称通过docker ps -a查看，粘贴names或者id值启动容器\n1. 创建并启动容器\n\n- docker run命令，等价于先执行docker create命令，然后执行docker start命令\n  [root[@docker ](/docker ) ~]# docker run -it ubuntu /bin/bash \n- Docker中系统镜像的缺省命令是 bash，如果不加 -it bash 命令执行了自动会退出。这是因为如果没有衔接输入流，本身就会马上结束。加-it 后docker命令会为容器分配一个伪终端，并接管其stdin/stdout支持交互操作，这时候bash命令不会自动退出。\n- 服务类容器以 daemon 的形式运行，对外提供服务。比如 web server，数据库等。通过 -d 以后台方式启动这类容器是非常合适的。如果要排查问题，可以通过 exec -it 进入容器。\n- 工具类容器通常给能我们提供一个临时的工作环境，通常以 run -it 方式运行，执行 exit 退出终端，同时容器停止。\n\n| -d, --detach=false      | 指定容器运行于前台还是后台，默认为false                      |\n| ----------------------- | ------------------------------------------------------------ |\n| -i, --interactive=false | 打开STDIN，用于控制台交互                                    |\n| -t, --tty=false         | 分配tty设备，该可以支持终端登录，默认为false                 |\n| -u, --user=\"\"           | 指定容器的用户                                               |\n| -a, --attach=[]         | 登录容器（必须是以docker run -d启动的容器）                  |\n| -w, --workdir=\"\"        | 指定容器的工作目录                                           |\n| -c, --cpu-shares=0      | 设置容器CPU权重，在CPU共享场景使用                           |\n| -e, --env=[]            | 指定环境变量，容器中可以使用该环境变量                       |\n| -m, --memory=\"\"         | 指定容器的内存上限                                           |\n| -p, --publish-all=false | 指定容器暴露的端口                                           |\n| -h, --hostname=\"\"       | 指定容器的主机名                                             |\n| -v, --volume=[]         | 给容器挂载存储卷，挂载到容器的某个目录                       |\n| --volumes-from=[]       | 给容器挂载其他容器上的卷，挂载到容器的某个目录               |\n| --entrypoint=\"\"         | 覆盖image的入口点                                            |\n| --env-file=[]           | 指定环境变量文件，文件格式为每行一个环境变量                 |\n| --expose=[]             | 指定容器暴露的端口，即修改镜像的暴露端口                     |\n| --link=[]               | 指定容器间的关联，使用其他容器的IP、env等信息                |\n| --lxc-conf=[]           | 指定容器的配置文件，只有在指定--exec-driver=lxc时使用        |\n| --name=\"\"               | 指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字 |\n| --net=\"bridge\"          | 容器网络设置 <br />bridge ：使用docker daemon指定的网桥 <br />host：容器使用主机的网络 container:NAME_or_ID >//使用其他容器的网路，共享IP和PORT等网络资源 <br />none：容器使用自己的网络（类似--net=bridge），但是不进行配置 |\n| --restart=\"no\"          | 指定容器停止后的重启策略，默认容器退出时不重启 <br />on-failure：容器故障退出（返回值非零）时重启 <br />always：容器退出时总是重启 <br />--rm=false	指定容器停止后自动删除容器(不支持以docker run -d启动的容器) |\n\n\n\n5. 守护态后台运行\n   [root[@docker ](/docker ) ~]# docker run -d ubuntu /bin/sh -c \"while true; do echo hello; sleep 1; done\" \n5. 查看容器输出\n   [root[@docker ](/docker ) ~]# docker logs 6a590199c02a ', 7, 0, 0, '2020-12-29 23:12:34.122591', '2021-01-21 18:56:05.530939', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (104, '停止容器', '\n\n1. 暂停容器\n   [root[@docker ](/docker ) ~]# docker pause 6a590199c02a \n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608898883998-b2bdb287-5421-4f37-9050-cacce34a9b8c.png#align=left&display=inline&height=84&margin=%5Bobject%20Object%5D&name=image.png&originHeight=84&originWidth=306&size=6662&status=done&style=none&width=306)\n\n2. 取消暂停继续运行容器\n   [root[@docker ](/docker ) ~]# docker unpause 6a590199c02a \n   有时我们只是希望暂时让容器暂停工作一段时间，或者 dcoker host 需要使用 CPU，这时可以执行 docker pause。处于暂停状态的容器不会占用 CPU 资源，直到通过 docker unpause 恢复运行。\n2. 终止容器\n   [root[@docker ](/docker ) ~]# docker stop 6a590199c02a \n   使用docker kill可以强行终止容器\n   当docker容器中的应用终止时，容器也会自动终止\n\n', 5, 0, 0, '2020-12-30 09:42:41.148265', '2021-01-22 15:18:09.461243', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (105, '进入容器', '\n\n1. 使用attach命令进入容器\n   [root[@docker ](/docker ) ~]# docker attach boring_bell \n   当多个窗口同时attach到一个容器时，所有窗口都会同步显示，当某个窗口阻塞时，其他窗口也无法操作\n1. 使用exec命令进入容器\n   [root[@docker ](/docker ) ~]# docker exec -it 8e3b24402577 /bin/bash \n\n- attach 直接进入容器 启动命令 的终端，不会启动新的进程。\n- exec 则是在容器中打开新的终端，并且可以启动新的进程。\n- 如果想直接在终端中查看启动命令的输出，用 attach；其他情况使用 exec。\n\n| 选项              | 作用                                 |\n| ----------------- | ------------------------------------ |\n| --detach,   -d    | 后台运行模式，在后台执行命令相关命令 |\n| --detach-keys     | 覆盖容器后台运行的一些参数信息       |\n| --env, -e         | 设置环境变量                         |\n| --interactive, -i | 展示容器输入信息STDIN                |\n| --privileged      | 为命令提供一些扩展权限               |\n| --tty, -t         | 命令行交互模式                       |\n| --user, -u        | 设置用户名(format:   <name           |\n| --workdir, -w     | 指定容器内的目录                     |\n\n3. 启动容器并指定名称\n   [root[@admin ](/admin ) ~]#docker run -d ', 6, 0, 0, '2020-12-30 09:43:44.383229', '2021-01-22 12:29:08.923865', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (106, '删除容器', '\n\n1. 使用docker rm删除已终止或退出的容器\n\n[root@docker ~]# docker rm e4af2433d094\n\n| -f   | 通过SIGKILL信号强制删除一个运行中的容器 |\n| ---- | --------------------------------------- |\n| -l   | 移除容器间的网络连接，而非容器本身      |\n| -v   | 删除与容器关联的卷                      |\n\n- docker rm 是删除容器，而docker rmi 是删除镜像\n\n', 6, 1, 0, '2020-12-30 09:44:26.175947', '2021-01-21 12:31:03.994827', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (107, '导入和导出容器', '\n1. 导出容器\n\n 导出容器是指导出一个已经创建的容器到一个文件，不管此时该容器是否处于运行状态\n\n  [root[@docker ](/docker ) ~]# docker export -o centos.tar f0761a1df372 \n  [root[@docker ](/docker ) ~]# docker export -o  f0761a1df372 > centos.tar \n\n2. 导入文件成为镜像\n\n   [root[@docker ](/docker ) ~]# docker import centos.tar centos:test \n\n- docker import和docker load区别在于：load导入镜像存储文件，import导入容器快照文件。容器快照文件仅保存当前快照状态，丢弃所有历史记录和元数据信息，镜像存储文件将保存完整记录，体积更大。从容器快照文件导入时可以重新制定标签等元数据信息。', 7, 1, 0, '2020-12-30 09:45:18.691289', '2021-01-22 08:38:37.085119', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (108, '查看容器', '\n\n1. 查看容器详情信息\n   [root[@docker ](/docker ) ~]# docker inspect 8e3b24402577 \n1. 查看容器内进程\n   [root[@docker ](/docker ) ~]# docker top 8e3b24402577 \n1. 查看统计信息\n   [root[@docker ](/docker ) ~]# docker stats 8e3b24402577 \n\n', 7, 0, 0, '2020-12-30 09:46:08.281943', '2021-01-22 10:18:37.506545', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (109, '其他容器命令', '\n\n1. 复制文件\n   [root[@docker ](/docker ) ~]# docker cp centos.tar 8e3b24402577:/ \n1. 查看容器文件系统的变更\n   [root[@docker ](/docker ) ~]# docker diff 8e3b24402577 \n1. 查看端口映射\n   [root[@docker ](/docker ) ~]# docker port 8e3b24402577 \n1. 更新容器运行时的配置，主要是资源限制\n   [root[@docker ](/docker ) ~]# docker update --cpu-period 100000 8e3b24402577 \n1. 容器直接互联\n\n- 启动mysql容器\n  [root[@docker ](/docker ) ~]# docker run --name mysqldb -p 3306:3306  -e MYSQL_ROOT_PASSWORD=my-pwd -d mysql:latest \n- 启动httpd容器与mysql容器互联\n  [root[@docker ](/docker ) ~]# docker run -it --name web --link mysqldb:webdb httpd /bin/bash \n- 测试链接\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899122431-e0b5525f-c7fd-469a-a6cf-3a584b592945.png#align=left&display=inline&height=365&margin=%5Bobject%20Object%5D&name=image.png&originHeight=365&originWidth=691&size=40321&status=done&style=none&width=691)\n\n', 4, 1, 0, '2020-12-30 09:46:50.903599', '2021-01-21 12:31:30.278720', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (110, 'docker hub公共仓库', '\n\n1. 公共仓库地址：[https://hub.docker.com](https://hub.docker.com/?utm_source=docker4mac_3.0.3&utm_medium=account_create&utm_campaign=referral)，进入后点击sign up注册docker账号\n1. 登录docker hub\n\n `# docker login https://hub.docker.com` \n\n3. 搜索镜像\n\n `# docker search centos`\n\n4. 使用阿里云镜像加速\n\n 参考文档：[https://help.aliyun.com/document_detail/60750.html](', 4, 0, 0, '2020-12-30 10:12:29.562133', '2021-01-23 08:06:59.469605', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (111, 'registry私有仓库', '[TOC]\n\n## 1. 部署register私有仓库\n\n- 服务器规划\n\n| ip              | 角色         |\n| --------------- | ------------ |\n| 192.168.217.128 | 本地仓库     |\n| 192.168.217.130 | docker客户端 |\n\n- 使用registry镜像创建私有仓库\n\n `[root@docker ~]# docker run -d -p 5000:5000 registry:2` \n\n- 该命令自动下载官方提供的registry镜像来搭建本地私有仓库，默认情况下创建在容器的/var/lib/registry目录下，可以通过-v来指定路径\n\n `[root@docker ~]# docker run -d -p 5000:5000 -v  /registry:/var/lib/registry registry:2` \n\n- 查看仓库运行情况\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622410-edc0cbce-6aef-45ac-8406-9f2e66df1bb3.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=image.png&originHeight=94&originWidth=558&size=11816&status=done&style=none&width=558)\n\n- {\"repositories\":       []} 表示现在仓库中，没有镜像images\n\n## 2. 使用register私有仓库\n\n- 查看客户端已有的images\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622674-9fdfe53f-b0e0-4f8e-93b8-a03bce5ef477.png#align=left&display=inline&height=83&margin=%5Bobject%20Object%5D&name=image.png&originHeight=83&originWidth=1041&size=20763&status=done&style=none&width=1041)\n\n- 将该镜像修改tag\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622591-d5bcb30b-4ea0-4320-b209-8ac189233a69.png#align=left&display=inline&height=132&margin=%5Bobject%20Object%5D&name=image.png&originHeight=132&originWidth=1143&size=41983&status=done&style=none&width=1143)\n\n- 上传标记镜像\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622715-2f9b86fb-0bc2-4aa8-ba8f-6615db07a59d.png#align=left&display=inline&height=78&margin=%5Bobject%20Object%5D&name=image.png&originHeight=78&originWidth=1034&size=28718&status=done&style=none&width=1034)\n\n- 成功会出现上述提示，表示本地的仓库默认使用的是https进行上传，那行是latest是重新上传出现的。\n- 如果你在push镜像的时候出现问题,可能是因为我们启动的registry服务不是安全可信赖的\n- 修改配置文件\n\n vim /etc/docker/daemon.json,\n添加下面的内容:   \"\"insecure-registries\":[\"192.168.217.128:5000\"]\"， 再重启docker 服务\n\n- 仓库端查看镜像\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622662-f192454b-9834-406d-be58-1d9ae32b1ba8.png#align=left&display=inline&height=81&margin=%5Bobject%20Object%5D&name=image.png&originHeight=81&originWidth=557&size=11900&status=done&style=none&width=557)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622482-af0062c0-353a-4b41-9ccd-066865fa0d88.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=image.png&originHeight=80&originWidth=912&size=17057&status=done&style=none&width=912)\n\n- 删除客户端镜像，从本地仓库下载镜像\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899622924-057a7d4c-5faf-4226-8f9b-4217f1feb501.png#align=left&display=inline&height=378&margin=%5Bobject%20Object%5D&name=image.png&originHeight=378&originWidth=1138&size=117000&status=done&style=none&width=1138)\n\n## 3. 管理register私有仓库\n\n- 删除镜像仓库镜像\n\n 打开镜像的存储目录，如有-V操作打开挂载目录也可以，删除镜像文件夹\n `# docker exec <容器名>  rm -rf /var/lib/registry/docker/registry/v2/repositories/<镜像名>` \n\n- 执行垃圾回收操作，**注意2.4版本以上的registry才有此功能**\n\n `# docker exec registry  bin/registry garbage-collect /etc/docker/registry/config.yml` ', 6, 0, 0, '2020-12-30 10:14:14.184846', '2021-01-23 11:26:25.036242', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (112, 'Harbor私有镜像仓库', '[TOC]\n\n# 一、Harbor私有镜像仓库\n\n1. 安装docker\n1. 安装docker-compose\n1. 下载harbor离线安装包\n\n [参考链接](https://github.com/goharbor/harbor/releases)\n`[root@harbor ~] wget https://github.com/vmware/harbor/releases/download/v1.8.6/harbor-offline-installer-v1.8.6.tgz`\n`[root@harbor ~]# tar -xvf harbor-offline-installer-v1.8.6.tgz` \n\n4. 修改harbor.yml配置文件\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352628-6d4612fc-7a1c-4068-a062-66faa672f1a0.png#align=left&display=inline&height=127&margin=%5Bobject%20Object%5D&name=image.png&originHeight=127&originWidth=362&size=11715&status=done&style=none&width=362)\n\n 注释https相关配置\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352628-c927f524-7c19-418a-9389-dbd2bc363c0b.png#align=left&display=inline&height=155&margin=%5Bobject%20Object%5D&name=image.png&originHeight=155&originWidth=590&size=15896&status=done&style=none&width=590)\n\n5. 运行install.sh脚本\n\n `[root@harbor harbor]# ./install.sh`\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352898-404cf29e-f736-4b57-87d5-c9b6e50257e3.png#align=left&display=inline&height=105&margin=%5Bobject%20Object%5D&name=image.png&originHeight=105&originWidth=894&size=10961&status=done&style=none&width=894)\n\n6. 访问Harbor并登陆。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352936-c7fc71a5-9445-4016-bc95-692d4cdbc45e.png#align=left&display=inline&height=629&margin=%5Bobject%20Object%5D&name=image.png&originHeight=629&originWidth=669&size=30128&status=done&style=none&width=669)\n\n 初始用户名admin\n\n 初始密码Harbor12345\n\n7. 创建systemd服务管理脚本\n\n```bash\n[Unit]\nDescription=Harbor\nAfter=docker.service systemd-networkd.service systemd-resolved.service\nRequires=docker.service\nDocumentation=http://github.com/vmware/harbor\n \n[Service]\nType=simple\nRestart=on-failure\nRestartSec=5\nExecStart=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml up\nExecReload=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml restart\nExecStop=/usr/local/bin/docker-compose -f /opt/harbor/docker-compose.yml down\n \n[Install]\nWantedBy=multi-user.target\n```\n\n\n# 二、docker授权访问harbor仓库\n\n1. docker配置文件私有仓库设置\n\n `[root@master ~]# vim /etc/docker/daemon.json`\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352894-a7207010-5cbf-49cf-81be-29f53e5fa7e8.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=image.png&originHeight=110&originWidth=804&size=8882&status=done&style=none&width=804)\n\n2. 重启docker\n\n `systemctl daemon-reload` \n `systemctl restart docker` \n\n3. master节点登陆测试\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352748-586d3b10-f7ac-4392-a517-0f470e3f9e74.png#align=left&display=inline&height=197&margin=%5Bobject%20Object%5D&name=image.png&originHeight=197&originWidth=948&size=21711&status=done&style=none&width=948)\n\n4. 推送镜像测试\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608899352703-0ceb76e6-5f7f-4c2a-963f-9525dd134a57.png#align=left&display=inline&height=229&margin=%5Bobject%20Object%5D&name=image.png&originHeight=229&originWidth=513&size=18623&status=done&style=none&width=513)\n`[root@master ~]# docker tag hello-world 192.168.10.103/library/hello-world:v1` \n`[root@master ~]# docker push 192.168.10.103/library/hello` \n\n', 4, 0, 0, '2020-12-30 10:16:12.902030', '2021-01-26 10:34:47.392207', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (113, '基本结构', '\n\n1. 组成\n   • 基础镜像信息\n   • 维护者信息\n   • 镜像操作指令\n   • 容器启动时执行的指令\n1. 示例\n\n```dockerfile\n#Nginx dockerfile\n#Base images\nFROM centos\n#MAINTAINER 维护人\nMAINTAINER kehaojian\n#ADD 添加本地文件到镜像\nADD pcre-8.37.tar.gz /usr/local/src\nADD nginx-1.9.3.tar.gz /usr/local/src\n#RUN 在镜像中执行命令\nRUN yum install -y wget gcc_c++ make openssl-devel\nRUN useradd -s /sbin/nologin -M www\n#WORKDIR 镜像中切换到目录\nWORKDIR /usr/local/src/nginx-1.9.3\nRUN yum install -y gcc gcc-c++\nRUN ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_ssl_module --with-http_stub_status_module --with-pcre=/usr/local/src/pcre-8.37 && make && make install\nRUN echo \"daemon off;\">>/usr/local/nginx/conf/nginx.conf\n#配置环境变量\nENV PATH /usr/local/nginx/sbin:$PATH\nEXPOSE 80\n#命令\nCMD  [\"nginx\"]\n```\n\n', 9, 0, 0, '2020-12-30 22:29:27.740148', '2021-01-23 03:10:24.463256', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (114, '配置命令', '\n\ndockerfile指令包括配置指令（配置镜像信息）和操作指令（具体执行操作）两部分\n\n| 分类     | 指令        | 说明                               | 格式                                                         |\n| -------- | ----------- | ---------------------------------- | ------------------------------------------------------------ |\n| 配置命令 | ARG         | 定义创建镜像过程中使用的变量       | ARG <name>[=<default value>]                                 |\n|          | FROM        | 指定所创建的基础镜像               | FROM    <image>:<tag><br />FROM    <image>:<digest>          |\n|          | LABEL       | 为生成的镜像添加元数据标签信息     | LABEL    <key>=<value> <key>=<value>    <key>=<value> ...    |\n|          | EXPOSE      | 声明镜像内服务监听的端口           | EXPOSE <port> [<port>/<portocol>…]                           |\n|          | ENV         | 指定环境变量                       | ENV <key>    <value>ENV    <key>=<value> ...                 |\n|          | ENTRYPOINT  | 指定镜像的默认入口命令             | ENTRYOINT   [\"executable\",\"param1\",\"param2\"]:exec调用执行 ENTRYOINT command param1 param2:shell中执行 |\n|          | VOLUME      | 创建一个数据卷挂载点               | VOLUME [\"/data\"]                                             |\n|          | USER        | 指定运行容器时的用户名或UID        | USER daemon                                                  |\n|          | WORKDIR     | 配置工作目录                       | WORKDIR /path                                                |\n|          | ONBUILD     | 创建子镜像时指定自动执行的操作命令 | ONBUILD [INSTRUCTION]                                        |\n|          | STOPSIGNAL  | 指定退出的信号值                   | STOPSIGNAL signal                                            |\n|          | HEALTHCHECK | 配置所启动容器如何进行健康检查     | HEALTHCHECK    [OPTIONS] CMD command：根据所执行命令返回值是否为0来判断； HEALTHCHECK NONE：禁止基础镜像中的健康检查。 |\n|          | SHELL       | 指定默认shell类型                  | SHELL [\"executable\", \"parameters\"]                           |\n| 操作命令 | RUN         | 运行指定命令                       | RUN <command>或RUN [\"executable\", \"param1\",    \"param2\"]     |\n|          | CMD         | 启动容器时指定默认执行的命令       | CMD    [\"executable\", \"param1\", \"param2\"]：相当于执行executable param1 param2，推荐方式； CMD    command param1 param2：在默认的Shell中执行，提供给需要交互的应用； CMD [\"param1\", \"param2\"]：提供给ENTRYPOINT的默认参数。 |\n|          | ADD         | 添加内容到镜像                     | ADD <src> <dest>                                             |\n|          | COPY        | 复制内容到镜像                     | COPY <src> <dest>                                            |\n\n1. ARG：定义创建镜像过程中使用的变量\n\n **ARG <name>[=<default value>]**\n\n 在docker build创建镜像的时候，使用 -build-arg [=]来指定参数,当镜像编译成功后，ARG指定的变量将不存在（ENV指定的变量将在镜像中保留）\n\n2. FROM：指定基础镜像\n\n **FROM <image>:<tag>**\n\n FROM必须是第一条指令。如果在一个dockerfile中指定多个镜像时，使用多个FROM指令\n\n 如果不以任何镜像为基础，那么写法为：\n\n FROM  scratch。\nARG VERSION=9.3\nFROM debian:${VERSION}\n\n3. LABEL：为镜像指定标签\n\n **LABEL       <key>=<value> <key>=<value>       <key>=<value> ...**\n\n 一个Dockerfile种可以有多个LABEL，如下：\n\n LABEL  version=\"1.0\"\nLABEL  multi.label1=\"value1\" \\\nmulti.label2=\"value2\"  \\\nother=\"value3\"\n说明：LABEL会继承基础镜像种的LABEL，如遇到key相同，则值覆盖\n\n4. EXPOSE：声明镜像内服务监听的端口，\n\n **EXPOSE <port>       [<port>/<portocol>…]**\n\n 该命令只是声明，并不会自动完成端口映射，如果想使得容器与主机的端口有映射关系，必须在容器启动的时候加上  -P参数\n\n5. ENV：指定环境变量，后续会被RUN命令使用，在镜像启动的容器中也会存在。\n\n **ENV  <key> <value>**\n **ENV  <key>=<value> ...**\n\n 两者的区别就是第一种是一次设置一个，第二种是一次设置多个 \n指定的环境变量在运行时可以被覆盖掉，为一个环境变量多次赋值，也会更新\n\n6. ENTRYOINT：指定的镜像的默认入口命令\n\n 该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数\n\n **ENTRYOINT       [\"executable\",\"param1\",\"param2\"]:exec调用执行**\n **ENTRYOINT command       param1 param2:shell中执行** \n\n  此时，CMD指令指定值将作为根命令的参数\n 每个Dockerfile中只能有一个ENTRYPOINT，当指定多个时，只有最后一个起效。\n\n7. VOLUME：创建一个数据卷挂载点。\n\n **VOLUME  [\"/data\"]**\n\n 运行容器时可以从本地主机或其他容器挂载数据卷，一般用来存放数据库和需要保持的数据等。\n\n8. USER：指定运行容器时的用户名或UID\n\n **USER daemon**\n\n 当服务不需要管理员权限时，可以通过该命令指定运行用户，并且可以在Dockerfile中创建所需要的用户。\n例如：RUN groupadd -r postgres &&useradd --no-log-init -r -g postgres postgres\n要临时获取管理员权限可以使用gosu命令。\n\n9. WORKDIR：为后续的RUN、CMD、ENTRYPOINT指令配置工作目录。\n\n **WORKDIR  /path/to/workdir**\n\n 可以使用多个WORKDIR指令，后续命令如果参数是相对路径，则会基于之\n前命令指定的路径。例如：WORKDIR /aWORKDIR bWORKDIR cRUN pwd则最终路径为/a/b/c。因此，为了避免出错，推荐WORKDIR指令中只使用绝对路径。\n\n10. ONBUILD：指定当基于所生成镜像创建子镜像时，自动执行的操作指令。\n\n **ONBUILD  [INSTRUCTION]。**\n\n11. STOPSIGNAL\n\n 指定所创建镜像启动的容器接收退出的信号值：\n\n **STOPSIGNAL  signal**\n\n12. HEALTHCHECK\n\n 配置所启动容器如何进行健康检查（如何判断健康与否），自Docker 1.12开始支持。格式有两种：\n\n HEALTHCHECK [OPTIONS] CMD command：根据所执行命令返回值是否为0来判断；\n\n HEALTHCHECK NONE：禁止基础镜像中的健康检查。OPTION支持如下参数：\n -interval=DURATION (default: 30s)：过多久检查一次；\n -timeout=DURATION (default: 30s)：每次检查等待结果的超时；\n -retries=N (default: 3)：如果失败了，重试几次才最终确定失败。\n\n13. SHELL：指定其他命令使用shell时的默认shell类型\n\n **SHELL [\"executable\", \"parameters\"]**\n\n 默认值为[\"/bin/sh\", \"-c\"]。', 5, 0, 0, '2020-12-30 22:46:20.045387', '2021-01-21 12:31:55.743844', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (115, '操作命令', '\n1. RUN ：运行指定命令。\n\n 格式为RUN  <command>或RUN [\"executable\", \"param1\",  \"param2\"]。注意后者指令会被解析为JSON数组，因此必须用双引号。前者默认将在shell终端中运行命令，即/bin/sh  -c；后者则使用exec执行，不会启动shell环境。\n \n 指定使用其他终端类型可以通过第二种方式实现，例如RUN  [\"/bin/bash\", \"-c\",\"echo hello\"]。\n\n 每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像层。当命令较长时可以使用\\来换行。\n\n2. CMD：指定启动容器时默认执行的命令。支持三种格式：\n\n CMD [\"executable\", \"param1\",  \"param2\"]：相当于执行executable param1 param2，推荐方式；\n\n CMD command param1 param2：在默认的Shell中执行，提供给需要交互的应用；\n \n CMD [\"param1\", \"param2\"]：提供给ENTRYPOINT的默认参数。\n\n 每个Dockerfile只能有一条CMD命令。如果指定了多条命令，只有最后一条会被执行。\n\n 如果用户启动容器时候手动指定了运行的命令（作为run命令的参数），则会覆盖掉CMD指定的命令。\n\n3. ADD：添加内容到镜像\n\n 格式为ADD  <src> <dest>。\n\n 该命令将复制指定的<src>路径下内容到容器中的<dest>路径下。\n\n 其中<src>可以是Dockerfile所在目录的一个相对路径（文件或目录）；也可以是一个URL；还可以是一个tar文件（自动解压为目录）<dest>可以是镜像内绝对路径，或者相对于工作目录（WORKDIR）的相对路径。\n\n 路径支持正则格式，例如：\n\n ADD ＊.c /code/\n\n4. COPY：复制内容到镜像。\n\n 格式为COPY  <src> <dest>。\n\n 复制本地主机的<src>（为Dockerfile所在目录的相对路径，文件或目录）下内容到镜像中的<dest>。目标路径不存在时，会自动创建。\n\n 路径同样支持正则格式。', 4, 0, 0, '2020-12-30 22:48:16.659039', '2021-01-22 12:48:47.088487', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (116, '命令区别', '[TOC]\n\n# 一、CMD与ENTRYPOINT\n\n1. dockerfile中的 CMD\n\n 每个dockerfile中只能有一个CMD如果有多个那么只执行最后一个。\n\n CMD启动docker的时候执行的命令，如果执行docker run命令时添加指令，会覆盖cmd的命令选项，举个简单例子：\n\n docker run -itd --name test docker_image /bin/bash -c.\n\n 镜像名称后面跟了一个/bin/bash -c ，其实等价于在dockerfile中的CMD [\"/bin/bash\",\"-c\"]\n如果dockerfile中的CMD中有了CMD[\"/bin/bash\",\"-c\"],那么就不用在执行的时候再添加了，如果添加了参数的话那么就相当于要执行你添加的参数，默认的CMD中的参数就无效了。\n\n2. dockerfile中的ENTRYPOINT\n\n 一个dockerfile中ENTRYPOINT也只能存在一个，若存在多个那么只执行最后一个\n\n dockerfile中有ENTRYPOINT [\"tail\",\"-f\",\"/usr/local/aaa\"]这句，那么你启动的时候镜像就执行了这个里面的内容，如果你像上面带参数的话就相当于在这个执行的内容后面再加入参数\n\n 如果我们的dockerfile中有a中的这句话然后我们启动我们的docker:docker run -itd --name test docker_image /bin/bash -c.\n\n 此时就相当于我们启动docker的时候执行了：tail -f /usr/local/aaa /bin/bash -c\n\n3. CMD和ENTRYPOINT总结\n\n 一般还是会用entrypoint的中括号形式作为docker 容器启动以后的默认执行命令，里面放的是不变的部分，可变部分比如命令参数可以使用cmd的形式提供默认版本，也就是run里面没有任何参数时使用的默认参数。如果我们想用默认参数，就直接run，否则想用其他参数，就run 里面加参数。\n\n# 二、COPY和ADD\n\n1. 唯一区别在于是否支持从远程URL获取资源。COPY指令只能从执行docker      build所在的主机上读取资源并复制到镜像中。而ADD指令还支持通过URL从远程服务器读取资源并复制到镜像中。\n\n1. 满足同等功能的情况下，推荐使用COPY指令。ADD指令更擅长读取本地tar文件并解压缩。\n\n# 三、shell和exec格式\n\n1. Shell 格式\n\n <instruction> <command>\n\n 例如：\n\n RUN apt-get install python3  \nCMD echo \"Hello world\"  \nENTRYPOINT echo \"Hello world\" \n\n 当指令执行时，shell 格式底层会调用 /bin/sh -c      <command>。\n\n 例如下面的 Dockerfile 片段：\n\n ENV name xld  \nENTRYPOINT echo \"Hello, $name\" \n\n 执行 docker run <image> 将输出：\n\n Hello,xld\n 注意环境变量 name 已经被值 xld 替换。\n\n2. Exec 格式\n\n <instruction> [\"executable\", \"param1\", \"param2\", ...]\n\n 例如：\n\n RUN [\"apt-get\", \"install\", \"python3\"]  \nCMD [\"/bin/echo\", \"Hello world\"]  \nENTRYPOINT [\"/bin/echo\", \"Hello world\"]\n\n 当指令执行时，会直接调用 <command>，不会被      shell 解析。\n\n 例如下面的 Dockerfile 片段：\n\n ENV name xld  \nENTRYPOINT [\"/bin/echo\", \"Hello, $name\"]\n\n 运行容器将输出：\n\n Hello, $name\n\n 注意环境变量“name”没有被替换。\n\n 如果希望使用环境变量，照如下修改\n\n ENV name xld  \n\n ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo Hello, $name\"]\n\n 运行容器将输出：\n\n Hello, xld\n\n3. 使用场景\n\n- CMD 和 ENTRYPOINT 推荐使用 Exec 格式，因为指令可读性更强，更容易理解。RUN 则两种格式都可以。\n- 使用 RUN 指令安装应用和软件包，构建镜像。\n- 如果 Docker 镜像的用途是运行应用程序或服务，比如运行一个      MySQL，应该优先使用 Exec 格式的 ENTRYPOINT 指令。CMD 可为 ENTRYPOINT 提供额外的默认参数，同时可利用      docker run 命令行替换默认参数。\n- 如果想为容器设置默认的启动命令，可使用 CMD 指令。用户可在      docker run 命令行中替换此默认命令。', 7, 0, 0, '2020-12-30 22:50:47.152011', '2021-01-21 12:34:20.406128', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (117, '构建镜像', '[TOC]\n\n1. 基本格式\n   docker build [OPTIONS] PATH\n1. 命令选项\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608901338317-b27a282e-bd93-4650-b399-bd8002aaa145.png#align=left&display=inline&height=1000&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1000&originWidth=840&size=353207&status=done&style=none&width=840)\n\n3. 选择父镜像\n\n 用户可以选择两种镜像作为父镜像，一种是所谓的基础镜像（baseimage），另外一种是普通的镜像（往往由第三方创建，基于基础镜像）。\n\n Docker不同类型镜像之间的继承关系\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608901644277-18e597db-a3c8-4dc9-8692-3be734618c16.png#align=left&display=inline&height=341&margin=%5Bobject%20Object%5D&name=image.png&originHeight=341&originWidth=552&size=57360&status=done&style=none&width=552)\n\n4. 使用．dockerignore文件\n \n 将不检查的目录，文件写到同Dockerfile目录下的.dockerignore文件中，docker build命令将不再检查在.dockerignore文件中的目录，文件，在创建镜像时候不将无关数据发送到服务端。\n\n “*”表示任意多个字符；\n \n “? ”代表单个字符；\n \n “! ”表示不匹配（即不忽略指定的路径或文件）。\n\n5. 多步骤创建\n\n   对于需要编译的应用（如C、Go或Java语言等）来说，通常情况下至少需要准备两个环境的Docker镜像：\n\n- 编译环境镜像：包括完整的编译引擎、依赖库等，往往比较庞大。作用是编译应用为二进制文件；\n\n- 运行环境镜像：利用编译好的二进制文件，运行应用，由于不需要编译环境，体积比较小。\n  使用多步骤创建，可以在保证最终生成的运行环境镜像保持精简的情况下，使用单一的Dockerfile，降低维护复杂度。', 6, 0, 0, '2020-12-30 22:52:11.398275', '2021-01-21 12:28:08.263036', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (118, 'dockerfile编写注意事项', '\n1. 精简镜像用途：尽量让每个镜像的用途都比较集中单一，避免构造大而复杂、多功能的镜像；\n\n1. 选用合适的基础镜像：容器的核心是应用。选择过大的父镜像（如Ubuntu系统镜像）会造成最终生成应用镜像的臃肿，推荐选用瘦身过的应用镜像（如node:slim），或者较为小巧的系统镜像（如alpine、busybox或debian）；\n\n1. 提供注释和维护者信息：Dockerfile也是一种代码，需要考虑方便后续的扩展和他人的使用；\n\n1. 正确使用版本号：使用明确的版本号信息，如1.0,2.0，而非依赖于默认的latest。通过版本号可以避免环境不一致导致的问题；\n\n1. 减少镜像层数：如果希望所生成镜像的层数尽量少，则要尽量合并RUN、ADD和COPY指令。通常情况下，多个RUN指令可以合并为一条RUN指令；\n\n1. 恰当使用多步骤创建（17.05+版本支持）：通过多步骤创建，可以将编译和运行等过程分开，保证最终生成的镜像只包括运行应用所需要的最小化环境。当然，用户也可以通过分别构造编译镜像和运行镜像来达到类似的结果，但这种方式需要维护多个Dockerfile。\n\n1. 使用．dockerignore文件：使用它可以标记在执行docker      build时忽略的路径和文件，避免发送不必要的数据内容，从而加快整个镜像创建过程。\n\n1. 及时删除临时文件和缓存文件：特别是在执行apt-get指令后，/var/cache/apt下面会缓存了一些安装包；\n\n1. 提高生成速度：如合理使用cache，减少内容目录下的文件，或使用．dockerignore文件指定等；\n\n1. 调整合理的指令顺序：在开启cache的情况下，内容不变的指令尽量放在前面，这样可以尽量复用；\n\n1. 减少外部源的干扰：如果确实要从外部引入数据，需要指定持久的地址，并带版本信息等，让他人可以复用而不出错。\n\n', 7, 0, 0, '2020-12-30 22:53:23.898359', '2021-01-22 08:05:30.944132', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (119, '内存限制', '\n\n1. 与操作系统类似，容器可使用的内存包括两部分：物理内存和      swap。 \n1. Docker 通过下面两组参数来控制容器内存的使用量。\n\n -m 或 --memory：设置内存的使用限额，例如 100M, 2G。\n --memory-swap：设置 内存+swap 的使用限额。\n\n3. 示例：[root@docker      ~]# docker run -m 200M --memory-swap=300M ubuntu\n\n 其含义是允许该容器最多使用 200M 的内存和 100M 的      swap。\n\n4. 使用 progrium/stress      镜像来学习如何为容器分配内存。该镜像可用于对容器执行压力测试。执行如下命令：\n\n`docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 280M` \n\n  --vm 1：启动 1 个内存工作线程。\n  --vm-bytes 280M：每个线程分配 280M 内存。\n\n- 运行结果如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904127038-aceae808-e9ad-40f0-967c-2cffcd2be446.png#align=left&display=inline&height=331&margin=%5Bobject%20Object%5D&name=image.png&originHeight=331&originWidth=864&size=44847&status=done&style=none&width=864)\n\n- 因为 280M      在可分配的范围（300M）内，所以工作线程能够正常工作，其过程是：\n\n 分配 280M 内存。\n 释放 280M 内存。\n 再分配 280M 内存。\n 再释放 280M 内存。\n 一直循环......\n\n- 如果让工作线程分配的内存超过 300M，\n\n`docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 310M`\n\n--vm 1：启动 1 个内存工作线程。\n--vm-bytes 310M：每个线程分配310M 内存。\n\n- 结果如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904127288-5788d42a-5ca2-4275-a2b5-604eb969179c.png#align=left&display=inline&height=241&margin=%5Bobject%20Object%5D&name=image.png&originHeight=241&originWidth=863&size=32769&status=done&style=none&width=863)\n\n - 分配的内存超过限额，stress 线程报错，容器退出。\n\n5. 如果在启动容器时只指定 -m 而不指定      --memory-swap，那么 --memory-swap 默认为 -m 的两倍，比如：\n\n`docker run -it -m 200M ubuntu` \n 容器最多使用 200M 物理内存和 400M swap', 5, 0, 0, '2021-01-03 22:57:24.494637', '2021-01-23 23:59:52.967303', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (120, 'CPU限制', '\n\n1. 默认设置下，所有容器可以平等地使用 host CPU       资源并且没有限制。\n1. Docker 可以通过 -c 或       --cpu-shares 设置容器使用 CPU 的权重。如果不指定，默认值为 1024。\n\n 与内存限额不同，通过 -c 设置的 cpu  share 并不是 CPU 资源的绝对数量，而是一个相对的权重值。某个容器最终能分配到的 CPU 资源取决于它的 cpu share 占所有容器 cpu  share 总和的比例。换句话说：通过 cpu share 可以设置容器使用 CPU 的优先级。\n\n 比如在 host 中启动了两个容器：\n\n docker run --name  \"container_A\" -c 1024 ubuntu\n docker run --name  \"container_B\" -c 512 ubuntu\n container_A 的 cpu  share 1024，是 container_B 的两倍。当两个容器都需要 CPU 资源时，container_A 可以得到的 CPU 是  container_B 的两倍。\n\n 需要特别注意的是，这种按权重分配 CPU 只会发生在 CPU       资源紧张的情况下。如果 container_A 处于空闲状态，这时，为了充分利用 CPU 资源，container_B 也可以分配到全部可用的       CPU。\n\n3. 继续用 progrium/stress 做实验。\n\n- 启动 container_A，cpu share 为       1024： \n\n [root@docker  ~]# docker run --name container_A -it -c 1024 progrium/stress --cpu 1\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904509530-18984a00-8140-4814-b97f-1aa3468e1f39.png#align=left&display=inline&height=85&margin=%5Bobject%20Object%5D&name=image.png&originHeight=85&originWidth=853&size=10514&status=done&style=none&width=853)\n--cpu 用来设置工作线程的数量。因为当前  host 只有 1 颗 CPU，所以一个工作线程就能将 CPU 压满。如果 host 有多颗 CPU，则需要相应增加 --cpu 的数量。\n\n 启动 container_B，cpu share 为 512：       \n\n [root@docker  ~]# docker run --name container_B -it -c 512 progrium/stress --cpu 1\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904509548-b32c1db1-0acc-4a8d-b0d1-a9fd6caf0a31.png#align=left&display=inline&height=82&margin=%5Bobject%20Object%5D&name=image.png&originHeight=82&originWidth=851&size=10435&status=done&style=none&width=851)\n\n- 在 host 中执行 top，查看容器对       CPU 的使用情况： \n\n container_A 消耗的 CPU 是  container_B 的两倍。\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904509596-d71ba657-994f-4478-8a19-29f045ae35e8.png#align=left&display=inline&height=90&margin=%5Bobject%20Object%5D&name=image.png&originHeight=90&originWidth=1105&size=12200&status=done&style=none&width=1105)\n\n- 现在暂停 container_A： \n\n [root@docker  ~]# docker pause container_A\n\n- top 显示 container_B 在       container_A 空闲的情况下能够用满整颗 CPU： \n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904509605-c8f23c6d-612d-4261-8587-c191c5f056f0.png#align=left&display=inline&height=64&margin=%5Bobject%20Object%5D&name=image.png&originHeight=64&originWidth=1081&size=8294&status=done&style=none&width=1081) ', 3, 0, 0, '2021-01-03 22:58:36.309019', '2021-01-22 12:24:04.124224', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (121, '限制磁盘IO', '\n\n1. Block IO       是另一种可以限制容器使用的资源。Block IO 指的是磁盘的读写，docker 可通过设置权重、限制 bps 和 iops       的方式控制容器读写磁盘的带宽，下面分别讨论。\n1. 默认情况下，所有容器能平等地读写磁盘，可以通过设置       --blkio-weight 参数来改变容器 block IO 的优先级。\n\n --blkio-weight 与  --cpu-shares 类似，设置的是相对权重值，默认为 500。在下面的例子中，container_A 读写磁盘的带宽是 container_B  的两倍。\n[root@docker  ~]# docker run -it --name container_A --blkio-weight  600 ubuntu   \n[root@docker  ~]# docker run -it --name container_B --blkio-weight  300 ubuntu\n\n3. 限制 bps 和 iops\n\n bps 是 byte per  second，每秒读写的数据量。\n iops 是 io per  second，每秒 IO 的次数。\n\n- 可通过以下参数控制容器的 bps 和 iops：\n\n --device-read-bps，限制读某个设备的  bps。\n --device-write-bps，限制写某个设备的  bps。\n --device-read-iops，限制读某个设备的  iops。\n --device-write-iops，限制写某个设备的  iops。\n\n- 下面这个例子限制容器写 /dev/sda 的速率为 30       MB/s\n\n [root@docker  ~]# docker run -it --device-write-bps /dev/sda:30MB centos\n我们来看看实验结果：\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904388707-26e1cbcd-addd-4d27-9947-9532666a2cd4.png#align=left&display=inline&height=134&margin=%5Bobject%20Object%5D&name=image.png&originHeight=134&originWidth=950&size=15569&status=done&style=none&width=950)\n\n- 通过 dd 测试在容器中写磁盘的速度。因为容器的文件系统是在       host /dev/sda 上的，在容器中写文件相当于对 host /dev/sda 进行写操作。另外，oflag=direct 指定用       direct IO 方式写文件，这样 --device-write-bps 才能生效。\n- 如果不限速，结果如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1608904388725-71be8eba-bcc8-45d0-a3e1-d17dcda6044e.png#align=left&display=inline&height=164&margin=%5Bobject%20Object%5D&name=image.png&originHeight=164&originWidth=944&size=17967&status=done&style=none&width=944)', 5, 0, 0, '2021-01-03 22:59:25.549743', '2021-01-21 12:31:12.141875', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (122, '存储管理常用命令', '\n\n| volume   create  | 创建数据卷     |\n| ---------------- | -------------- |\n| volume   inspect | 查看数据卷详情 |\n| volume ls        | 列出数据卷     |\n| volume   prune   | 清理无用数据卷 |\n| volume rm        | 删除数据卷     |\n\n1. docker 为容器提供了两种存储资源：数据层和 Data      Volume。\n1. 数据层包括镜像层和容器层，由 storage driver      管理。\n1. Data Volume 有两种类型：bind mount 和      docker managed volume。\n1. bind mount 可实现容器与 host      之间，容器与容器之间共享数据。\n1. volume container      是一种具有更好移植性的容器间数据共享方案，特别是 data-packed volume container。\n1. 容器中的数据管理方式：\n\n- 数据卷：容器内数据直接映射到本地主机环境\n- 数据卷容器：使用特定的容器维护数据', 1, 0, 0, '2021-01-08 17:50:25.903399', '2021-01-21 12:31:43.856591', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (123, '数据卷', '\n\n- 数据卷是一个可供一个或多个容器使用的特殊目录，他将主机操作系统目录直接映射到容器，类似mount\n- 数据卷可以在容器之间共享和重用\n- 对数据卷的修改会立马生效\n- 对数据卷的更新，不会影响镜像\n- 数据卷默认会一直存在，即使容器被删除\n\n1. 创建数据卷\n\n [root@docker  ~]# docker volume create -d local test\n \n 数据卷路径：/var/lib/docker/volumes/\n\n2. 绑定创建的数据卷（在创建容器时将主机本地的路径挂载到容器内作为数据卷）\n\n 在用docker run命令时，使用-mount选项使用数据卷\n\n| volume | 普通数据卷，在/var/lib/docker/volume下 |\n| ------ | -------------------------------------- |\n| bind   | 绑定数据卷，映射到主机指定路径         |\n| tmpfs  | 临时数据卷，只存在内存中               |\n\n- 挂载刚刚创建的数据卷到容器中--mount source=要挂载的数据卷,target=挂载到容器的路径 镜像名\n\n [root@docker  ~]# docker run -d -P --name nginx --mount source=test,target=/webapp nginx\n\n- 默认挂载的镜像是读写权限，可以设置为只读\n\n [root@docker  ~]# docker run -d -P --name nginx --mount source=test,target=/webapp:ro nginx\n\n3. 直接挂载主机目录到容器\n\n docker run -it -v /宿主机绝对目录:/容器内目录 镜像名\n\n [root@docker  ~]# docker run -it -v /myDataVolume:/dataVolumerContainer centos', 2, 0, 0, '2021-01-08 17:52:22.820652', '2021-01-22 12:38:32.229438', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (124, '数据卷容器', '\n\n 数据卷容器本身就是一个容器，专门提供数据卷给其他容器，用于多个容器数据共享。\n\n1. 创建数据卷容器，并将一个数据卷挂载到此容器下\n\n [root@docker ~]# docker run -it -v /dbdata --name dbdata ubuntu\n\n2. 创建db1和db2都挂载到同一个数据卷/dbdata目录下\n\n [root@docker ~]# docker run -it --volumes-from dbdata --name db1 ubuntu\n\n [root@docker ~]# docker run -it --volumes-from dbdata --name db2 ubuntu\n\n 向三个容器中任意一个/dbdata目录下写入，其他容器都可以看到\n\n3. 可以使用--volumes-from参数来从多个容器挂载多个数据卷，也可以从其他已挂载的数据卷容器来挂载数据卷\n\n [root@docker ~]# docker run -d --name db3 --volumes-from db1 training/postgres\n\n 在/dbdata目录下也可以看到文件\n\n4. 如果删除了挂载卷的容器，数据卷并不会自动删除，如果要删除数据卷，必须在删除最后一个挂载着他的容器时使用docker      rm -v命令指定数据卷', 3, 0, 0, '2021-01-08 17:57:07.942585', '2021-01-23 09:46:42.140324', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (125, '利用数据卷容器进行数据迁移', '\n\n1. 备份\n\n `[root@docker ~]# docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata` \n\n| --name   worker ubuntu               | 使用ubuntu镜像创建名为worker的容器              |\n| ------------------------------------ | ----------------------------------------------- |\n| --volumes-from   dbdata              | 让worker容器挂载dbdata容器的数据卷              |\n| -v   $(pwd):/backup                  | 挂载本地的当前目录到worker容器的/backup目录     |\n| tar cvf   /backup/backup.tar /dbdata | 将/dbdata下内容备份为容器内的/backup/backup.tar |\n\n2. 恢复\n\n- 创建一个带有数据卷的容器dbdata2\n\n `[root@docker ~]# docker run -v /dbdata --name dbdata2 ubuntu /bin/bash` \n\n- 创建另一个新的容器，挂载dbdata2的容器，并解压备份文件到所挂载的容器卷中\n\n `[root@docker ~]# docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar` ', 1, 0, 0, '2021-01-08 17:57:45.836469', '2021-01-21 12:31:40.928274', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (126, '端口映射实现容器访问', '\n\n1. 随机映射端口从外部访问容器应用\n\n 使用docker -P，随机映射一个49000-49900的端口到内部容器\n\n [root@docker  ~]# docker run -d -P nginx\n\n 使用ip::端口，绑定本地的任意端口到容器内部端口\n\n [root@docker  ~]# docker run -d -p 127.0.0.1::5000 training/webapp python app.py\n\n2. 指定映射端口从外部访问容器应用\n\n 使用docker -p，指定一个端口映射到内部容器，例如将本机 8080 端口映射到容器的 80 端口\n\n [root@docker  ~]# docker run -d -p 8080:80 nginx\n\n3. 映射多个端口到容器\n\n 多次使用-p标记可以绑定多个端口\n\n [root@docker  ~]# docker run -d -p 5000:5000 -p 3000:80 training/webapp python app.py\n\n4. 映射指定地址的指定端口\n\n 使用-p ip：端口：容器端口，可以指定映射一个特定的地址。\n\n [root@docker  ~]# docker run -d -p 127.0.0.1:5000:5000  training/webapp python app.py\n\n5. 映射指定udp端口\n\n 使用-p ip：端口：容器端口/udp，可以指定映射一个udp地址。\n\n [root@docker  ~]# docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py\n\n6. 查看端口映射状态\n\n 使用docker port来查看当前端口映射状态\n\n [root@docker  ~]# docker port 36606dfc3e52', 4, 0, 0, '2021-01-08 18:07:48.832634', '2021-01-26 10:35:49.978907', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (127, '互联机制实现便捷互访', ' 容器互联可以让多个容器中的应用进行快速交互，他会在源和接收容器之间创建连接关系，接收容器可以通过容器名快速访问到源容器，而不用指定具体IP\n\n1. 自定义容器名称\n\n 使用--name可以为容器自定义名称，即使容器重启，名称也不会改变\n\n [root@docker  ~]# docker run -d -P --name web training/webapp python app.py\n\n2. 容器互联\n\n 使用--link name：别名，可以让容器互联\n\n 创建一个新的数据库容器\n\n [root@docker  ~]# docker run -d --name db training/postgres\n\n 删除之前创建的web容器\n\n [root@docker  ~]# docker rm -f web\n\n 创建一个新的web容器，并连接到db容器上\n\n [root@docker  ~]# docker run -d -P --name web --link db:db training/webapp python app.py\n\n names列出现web/db表明连接成功\n\n3. 使用env查看容器环境变量信息\n\n```bash\n[root@docker  ~]# docker run --rm --name web2 --link db:db training/webapp env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=97301899affb\nDB_PORT=tcp://172.17.0.3:5432\nDB_PORT_5432_TCP=tcp://172.17.0.3:5432\nDB_PORT_5432_TCP_ADDR=172.17.0.3\nDB_PORT_5432_TCP_PORT=5432\nDB_PORT_5432_TCP_PROTO=tcp\nDB_NAME=/web2/db\nDB_ENV_PG_VERSION=9.3\nHOME=/root\n```\n\n- DB_开头的环境变量是供web容器连接db容器使用\n\n4. 查看容器hosts文件信息\n\n```bash\n[root@docker  ~]# docker run -it --rm  --link db:db  training/webapp /bin/bash\nroot@0318563292e7:/opt/webapp#  cat /etc/hosts \n127.0.0.1        localhost\n::1        localhost  ip6-localhost ip6-loopback\nfe00::0        ip6-localnet\nff00::0        ip6-mcastprefix\nff02::1        ip6-allnodes\nff02::2        ip6-allrouters\n172.17.0.3        db  e94a815cfa73\n172.17.0.4        0318563292e7\n#使用ping命令测试跟db容器的互通\nroot@0318563292e7:/opt/webapp#  ping db\nPING db (172.17.0.3):  56 data bytes\n64 bytes from  172.17.0.3: icmp_seq=0 ttl=64 time=0.203 ms\n64 bytes from  172.17.0.3: icmp_seq=1 ttl=64 time=0.060 ms\n64 bytes from  172.17.0.3: icmp_seq=2 ttl=64 time=0.061 ms\n```\n\n', 1, 0, 0, '2021-01-08 18:21:27.125536', '2021-01-22 12:26:41.779741', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (128, '为镜像添加SSH服务', '[TOC]\n\n# 一、基于commit命令创建\n\n1. 准备工作\n\n 首先，获取centos镜像，并创建一个容器：\n\n [root@docker sshd_centos]# docker pull centos\n\n [root@docker sshd_centos]# docker run -it  centos:latest bash        \n\n2. 安装和配置SSH服务\n\n [root@7b44d4e2dc60 ~]# yum install passwd openssh-server -y\n\n3. 修改root密码\n\n [root@7b44d4e2dc60 ~]# passwd\n\n4. 生成秘钥\n\n [root@7b44d4e2dc60 ~]# ssh-keygen -t rsa -f  /etc/ssh/ssh_host_rsa_key\n\n [root@7b44d4e2dc60 ~]#   ssh-keygen -t rsa -f /etc/ssh/ssh_host_ecdsa_key\n\n [root@7b44d4e2dc60 ~]# ssh-keygen -t rsa -f  /etc/ssh/ssh_host_ed25519_key\n\n5. 修改配置文件\n\n [root@7b44d4e2dc60 ~]# vi /etc/ssh/sshd_config \n\n 禁用 PAM\n\n UsePAM no\n\n6. 编写服务启动脚本\n\n [root@7b44d4e2dc60 ~]# vi /run.sh\n\n [root@7b44d4e2dc60 ~]# chmod +x /run.sh \n\n 其中 /run.sh的内容为：\n\n `#! /bin/bash` \n `/usr/sbin/sshd  -D` \n\n7. 退出容器，保存镜像\n\n [root@9df7d2107aad /]# exit\n\n [root@docker ~]# docker commit 9df7d2107aad sshd:centos\n\n8. 查看镜像\n\n [root@docker ~]# docker images sshd\n\n9. 使用镜像，运行容器\n\n [root@docker ~]# docker run -p 10086:22 -d sshd:centos /run.sh\n\n10. ssh链接docker容器\n\n [root@docker ~]# ssh root@192.168.0.3 -p 10086\n\n# 二、使用Dockerfile创建\n\n1. 创建工作目录及相关文件\n\n [root@docker ~]# mkdir sshd_centos\n\n [root@docker ~]# touch  sshd_centos/Dockerfile run.sh\n\n2. 编写run.sh脚本，创建authorized_keys文件\n\n [root@docker ~]# vim run.sh\n `#!  /bin/bash`\n `/usr/sbin/sshd -D`\n\n 在宿主主机上生成SSH密钥对，并创建authorized_keys文件：\n\n [root@docker ~]# ssh-keygen -t rsa  \n\n [root@docker ~]# cat /root/.ssh/id_rsa.pub > authorized_keys\n\n3. 编写Dockerfile\n\n```dockerfile\n#设置继承镜像\nFROM centos:latest\n#提供一些作者的信息\nMAINTAINER docker_user  (user@docker.com)\n#安装 ssh 服务\nRUN yum install openssh-server -y \n#修改root用户密码\nRUN /bin/echo \"123.com\" | passwd --stdin  root\n#修改配置信息\nRUN  /bin/sed -i \'s/.*session.*required.*pam_loginuid.so.*/session optional  pam_loginuid.so/g\' /etc/pam.d/sshd \\\n    && /bin/sed -i \'s/UsePAM  yes/UsePAM no/g\' /etc/ssh/sshd_config \\\n    && /bin/sed -i  \"s/#UsePrivilegeSeparation.*/UsePrivilegeSeparation no/g\"  /etc/ssh/sshd_config\n#生成密钥\nRUN  ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key \\\n    && ssh-keygen -t rsa -f  /etc/ssh/ssh_host_ecdsa_key \\\n    && ssh-keygen -t rsa -f  /etc/ssh/ssh_host_ed25519_key\n#开放端口\nEXPOSE 22\n#设置自启动命令\nCMD [\"/usr/sbin/sshd\",\"-D\"]\n```\n\n4. 创建镜像\n\n 在sshd_ubuntu目录下，使用docker  build命令来创建镜像。这里用户需要注意在最后还有一个“.”，表示使用当前目录中的Dockerfile：\n\n [root@docker sshd_centos]# docker  build -t sshd:dockerfile .\n\n5. 查看生成的docker镜像\n\n [root@docker sshd_centos]# docker  images sshd:dockerfile \n\n6. 测试镜像，运行容器\n\n [root@docker sshd_centos]# docker run -it -p 10010:22  sshd:dockerfile', 1, 0, 0, '2021-01-08 18:34:51.669450', '2021-01-21 12:27:47.415113', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (129, 'docker网络管理', '[TOC]\n\n# 一、启动和配置参数\n\n1. 网路启动过程\n\n 在主机上自动创建一个docker0虚拟网桥，实际上是一个Linux网桥。网桥可以理解为一个软件交换机，负责挂载其上的接口之间进行包转发。\n\n 创建一对虚拟接口，分别放到本地主机和新容器的命名空间中；\n\n 本地主机一端的虚拟接口连接到默认的docker0网桥或指定网桥上，并具有一个以veth开头的唯一名字，如veth1234；另一端的虚拟接口将放到新创建的容器中，并修改名字作为eth0。这个接口只在容器的命名空间可见；\n\n 从网桥可用地址段中获取一个空闲地址分配给容器的eth0（例如172.17.0.2/16），并配置默认路由网关为docker0网卡的内部接口docker0的IP地址（例如172.17.42.1/16）。\n\n2. 网络相关参数\n\n 在Docker服务启动的时候才能配置，修改后重启生效\n\n| -b BRIDGE or --bridge=BRIDGE  | 指定容器挂载的网桥         |\n| ----------------------------- | -------------------------- |\n| --bip=CIDR                    | 定制docker0的掩码          |\n| -H SOCKET... or --host=SOCKET | Docker服务端接收命令的通道 |\n| --icc=true                    | false                      |\n| --ip-forward=true             | false                      |\n| --iptables=true               | false                      |\n| --mtu=BYTES                   | 容器网络中的MTU            |\n\n 既可以在启动服务时指定，也可以Docker容器启动（使用docker      [con-tainer] run命令）时候指定。在Docker服务启动的时候指定则会成为默认值，后续执行该命令时可以覆盖设置的默认值：\n\n| --dns=IP_ADDRESS    | 使用指定的DNS服务器 |\n| ------------------- | ------------------- |\n| --dns-opt=\"\"        | 指定DNS选项         |\n| --dns-search=DOMAIN | 指定DNS搜索域       |\n\n 只能在docker [container] run命令执行时使用，因为它针对容器的配置\n\n| -h HOSTNAME or --hostname=HOSTNAME | 配置容器主机名         |\n| ---------------------------------- | ---------------------- |\n| -ip                                | 指定容器内接口的IP地址 |\n| --link=CONTAINER_NAME:ALIAS        | 添加到另一个容器的连接 |\n| --net=bridge                       | none                   |\n| --network-alias                    | 容器在网络中的别名     |\n| -p SPEC or --publish=SPEC          | 映射容器端口到宿主主机 |\n| -P or --publish-all=true           | false                  |\n\n --net选项支持以下五种模式：\n\n| --net=bridge               | 默认配置。为容器创建独立的网络命名空间，分配网卡、IP地址等网络配置，并通过veth接口对将容器挂载到一个虚拟网桥（默认为docker0）上 |\n| -------------------------- | ------------------------------------------------------------ |\n| --net=none                 | 为容器创建独立的网络命名空间，但不进行网络配置，即容器内没有创建网卡、IP地址等 |\n| --net=container:NAME_or_ID | 新创建的容器共享指定的已存在容器的网络命名空间，两个容器内的网络配置共享，但其他资源（如进程空间、文件系统等）还是相互隔离的 |\n| --net=host                 | 不为容器创建独立的网络命名空间，容器内看到的网络配置（网卡信息、路由表、Iptables规则等）均与主机上的保持一致。注意其他资源还是与主机隔离的 |\n| --net=user_defined_network | 用户自行用network相关命令创建一个网络，同一个网络内的容器彼此可见，可以采用更多类型的网络插件。 |\n\n# 二、Docker网络命令\n\n| create     | 创建一个网络       |\n| ---------- | ------------------ |\n| connect    | 将容器接入到网络   |\n| disconnect | 把容器从网络上断开 |\n| inspect    | 查看网络的详细信息 |\n| ls         | 列出所有的网络     |\n| prune      | 清理无用的网络资源 |\n| rm         | 删除一个网络       |\n\n1. 创建网络\n\n creat命令用于创建一个新的容器网络。Docker内置了bridge（默认使用）和overlay两种驱动，分别支持单主机和多主机场景。Docker服务在启动后，会默认创建一个bridge类型的网桥bridge。不同网络之间默认相互隔离。\n\n 创建网络命令格式为docker      network create [OPTIONS] NETWORK。\n\n 支持参数包括：\n\n| -attachable[=false]    | 支持手动容器挂载                                             |\n| ---------------------- | ------------------------------------------------------------ |\n| -aux-address=map[]     | 辅助的IP地址                                                 |\n| -config-from=\"\"        | 从某个网络复制配置数据                                       |\n| -config-only[=false]   | 启用仅可配置模式                                             |\n| -d,   -driver=\"bridge\" | 网络驱动类型，如bridge或overlay                              |\n| -gateway=[]            | 网关地址                                                     |\n| -ingress[=false]       | 创建一个Swarm可路由的网状网络用于负载均衡，可将对某个服务的请求自动转发给一个合适的副本 |\n| -internal[=false]      | 内部模式，禁止外部对所创建网络的访问                         |\n| -ip-range=[]           | 指定分配IP地址范围                                           |\n| -ipam-driver=\"default\" | IP地址管理的插件类型                                         |\n| -ipam-opt=map[]        | IP地址管理插件的选项                                         |\n| -ipv6[=false]          | 支持IPv6地址                                                 |\n| -label   value         | 为网络添加元标签信息                                         |\n| -o,   -opt=map[]       | 网络驱动所支持的选项                                         |\n| -scope=\"\"              | 指定网络范围                                                 |\n| -subnet=[]             | 网络地址段，CIDR格式，如172.17.0.0/16。                      |\n\n2. 接入网络\n\n  connect命令将一个容器连接到一个已存在的网络上。连接到网络上的容器可以跟同一网络中其他容器互通，同一个容器可以同时接入多个网络。也可以在执行docker      run命令时候通过-net参数指定容器启动后自动接入的网络。\n\n 接入网络命令格式为docker      network connect [OPTIONS] NETWORK CONTAINER。\n\n 支持参数包括：\n\n| -alias=[]         | 为容器添加一个别名，此别名仅在所添加网络上可见； |\n| ----------------- | ------------------------------------------------ |\n| -ip=\"\"            | 指定IP地址，需要注意不能跟已接入的容器地址冲突； |\n| -ip6=\"\"           | 指定IPv6地址；                                   |\n| -link   value     | 添加链接到另外一个容器；                         |\n| -link-local-ip=[] | 为容器添加一个链接地址。                         |\n\n3. 断开网络\n\n disconnect命令将一个连接到网络上的容器从网络上断开连接。\n\n 命令格式为docker network disconnect [OPTIONS] NETWORK CONTAINER。\n\n 支持参数包括-f, -force：强制把容器从网络上移除。\n\n4. 查看网络信息\n\n inspect命令用于查看一个网络的具体信息（JSON格式），包括接入的容器、网络配置信息等。\n\n 命令格式为docker network      inspect [OPTIONS] NETWORK [NETWORK...]。\n\n 支持参数包括：\n\n| -f,   -format=\"\"       | 给定一个Golang模板字符串，对输出结果进行格式化，如只查看地址配置可以用-f \'{{.IPAM.Config}}\' |\n| ---------------------- | ------------------------------------------------------------ |\n| -v,   -verbose[=false] | 输出调试信息                                                 |\n\n5. 列出网络\n\n ls命令用于列出网络。命令格式为docker network ls [OPTIONS]，其中支持的选项主要有：\n\n| -f,   -filter=\"\"     | 指定输出过滤器，如driver=bridge；                |\n| -------------------- | ------------------------------------------------ |\n| -format=\"\"           | 给定一个golang模板字符串，对输出结果进行格式化； |\n| -no-trunc[=false]    | 不截断地输出内容；                               |\n| -q,   -quiet[=false] | 安静模式，只打印网络的ID。                       |\n\n6. 清理无用网络\n\n prune命令用于清理已经没有容器使用的网络。\n\n 命令格式为docker network      prune [OPTIONS] [flags]，支持参数包括：\n\n| -filter=\"\" | 指定选择过滤器 |\n| ---------- | -------------- |\n| -f, -force | 强制清理资源   |\n\n7. 删除网络\n\n rm命令用于删除指定的网络。当网络上没有容器连接上时，才会成功删除。\n命令格式为docker network rm NETWORK [NETWORK...]。\n\n', 2, 0, 0, '2021-01-08 18:37:37.818156', '2021-01-21 12:31:59.121301', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (130, '配置DNS和主机名', '[TOC]\n\n Docker服务启动后会默认启用一个内嵌的DNS服务，来自动解析同一个网络中的容器主机名和地址，如果无法解析，则通过容器内的DNS相关配置进行解析。用户可以通过命令选项自定义容器的主机名和DNS配置，下面分别介绍。\n\n# 一、相关配置文件\n\n1. 容器中主机名和DNS配置信息可以通过三个系统配置文件来管理：/etc/resolv.conf、/etc/hostname和/etc/hosts。\n\n1. 启动一个容器，在容器中使用mount命令可以看到这三个文件挂载信息：\n\n [root@docker ~]# docker run -it centos\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385308078-dcbbfcad-4483-4e8d-a93f-e039a10959d8.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=image.png&originHeight=184&originWidth=961&size=26800&status=done&style=none&width=961)\n\n 从宿主机上复制 /etc/resolv.conf文件，并删除掉其中无法连接到的DNS服务器\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385308119-8b22f804-8995-45f9-804f-d8c263c9f4b5.png#align=left&display=inline&height=65&margin=%5Bobject%20Object%5D&name=image.png&originHeight=65&originWidth=481&size=5613&status=done&style=none&width=481)\n\n /etc/hosts文件中默认只记录了容器自身的地址和名称：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385308188-73fa3e57-5ba0-492d-a18d-51d33859531d.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=image.png&originHeight=183&originWidth=490&size=14384&status=done&style=none&width=490)\n\n /etc/hostname文件则记录了容器的主机名\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385308267-6bd90ef1-b1a4-416e-bd5b-d430aad463d1.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=image.png&originHeight=46&originWidth=446&size=4020&status=done&style=none&width=446)\n\n# 二、容器内修改配置文件\n\n 容器运行时，可以在运行中的容器里直接编辑/etc/hosts、/etc/hostname和/etc/resolve. conf文件。但是这些修改是临时的，只在运行的容器中保留，容器终止或重启后并不会被保存下来，也不会被docker commit提交。\n\n# 三、通过参数指定\n\n1. 如果用户想要自定义容器的配置，可以在创建或启动容器时利用下面的参数指定，注意一般不推荐与-net=host一起使用，会破坏宿主机上的配置信息：\n\n 指定主机名-h      HOSTNAME或者--hostname=HOSTNAME：设定容器的主机名。容器主机名会被写到容器内的/etc/hostname和/etc/hosts。但这个主机名只有容器内能中看到，在容器外部则看不到，既不会在docker      ps中显示，也不会在其他容器的/etc/hosts中看到；\n- --link=CONTAINER_NAME:ALIAS：记录其他容器主机名。在创建容器的时候，添加一个所连接容器的主机名到容器内/etc/hosts文件中。这样，新建容器可以直接使用主机名与所连接容器通信；\n- --dns=IP_ADDRESS：指定DNS服务器。添加DNS服务器到容器的/etc/resolv.      conf中，容器会用指定的服务器来解析所有不在/etc/hosts中的主机名；\n- --dns-option      list：指定DNS相关的选项；\n- --dns-search=DOMAIN：指定DNS搜索域。设定容器的搜索域，当设定搜索域为．example.com时，在搜索一个名为host的主机时，DNS不仅搜索host，还会搜索host.example.com。', 3, 0, 0, '2021-01-08 18:44:55.105013', '2021-01-21 12:41:57.599667', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (131, '容器防火墙访问控制', '[TOC]\n\n容器的访问控制主要通过Linux上的iptables防火墙软件来进行管理和实现。\n\n# 一、容器访问外部网络\n\n1. 容器默认指定了网关为docker0网桥上的docker0内部接口。docker0内部接口同时也是宿主机的一个本地接口。因此，容器默认情况下可以访问到宿主机本地网络。如果容器要想通过宿主机访问到外部网络，则需要宿主机进行辅助转发。\n1. 在宿主机Linux系统中，检查转发是否打开，代码如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385173767-04e6eeda-a6ad-4f40-b6ce-8aefa3255c02.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=image.png&originHeight=46&originWidth=503&size=4578&status=done&style=none&width=503)\n\n 如果为0，说明没有开启转发，则需要手动打开：    \n\n [root@docker  ~]# sysctl -w net.ipv4.ip_forward=1\nDocker服务启动时会默认开启--ip-forward=true，自动配置宿主机系统的转发规则。\n\n# 二、容器之间访问\n\n1. 容器之间相互访问需要两方面的支持：\n\n 网络拓扑是否已经连通。默认情况下，所有容器都会连接到docker0网桥上，这意味着默认情况下拓扑是互通的；\n\n 本地系统的防火墙软件iptables是否允许访问通过。这取决于防火墙的默认规则是允许（大部分情况）还是禁止。\n\n2. 下面分两种情况介绍容器间的访问。\n\n* 访问所有端口\n\n 当启动Docker服务时候，默认会添加一条“允许”转发策略到iptables的FORWARD链上。通过配置--icc=true|false（默认值为true）参数可以控制默认的策略。\n为了安全考虑，可以在Docker配置文件中配置DOCKER_OPTS=--icc=false来默认禁止容器之间的相互访问。\n同时，如果启动Docker服务时手动指定--iptables=false参数，则不会修改宿主机系统上的iptables规则。\n\n* 访问指定端口\n\n 在通过-icc=false禁止容器间相互访问后，仍可以通过--link=CONTAINER_NAME:ALIAS选项来允许访问指定容器的开放端口。\n \n 例如，在启动Docker服务时，可以同时使用icc=false  --iptables=true参数来配置容器间禁止访问，并允许Docker自动修改系统中的iptables规则。此时，系统中的iptables规则可能是类似如下规则，禁止所有转发流量：\n  \n 启动容器时使用--link=CONTAINER_NAME:ALIAS选项。Docker会在iptable中为两个互联容器分别添加一条ACCEPT规则，允许相互访问开放的端口\n \n [root@docker  ~]# docker run -it --link=3782df38f864:centoslink centos', 2, 0, 0, '2021-01-08 18:46:31.913474', '2021-01-21 20:35:48.572471', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (132, 'docker网络分析', '[TOC]\n\n# 一、容器访问外部网络\n\n1. centos 位于 docker0 这个私有      bridge 网络中（172.17.0.0/16），当 centos 从容器向外 ping 时，数据包通过NAT地址转换到达 baidu.com \n1. 查看一下 docker host 上的 iptables 规则：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385039344-d2ff8e2a-ba42-4c22-b5d7-7c86ab073fcb.png#align=left&display=inline&height=228&margin=%5Bobject%20Object%5D&name=image.png&originHeight=228&originWidth=649&size=195263&status=done&style=none&width=649)\n\n 在 NAT 表中，有这么一条规则：\n\n -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n其含义是：如果网桥 docker0 收到来自 172.17.0.0/16 网段的外出包，把它交给 MASQUERADE 处理。而 MASQUERADE 的处理方式是将包的源地址替换成 host 的地址发送出去，即做了一次网络地址转换（NAT）。\n\n3. 通过 tcpdump 查看地址是如何转换的。先查看 docker      host 的路由表：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385032265-fc6875be-d2bb-4bdf-b567-4906dc54c222.png#align=left&display=inline&height=54&margin=%5Bobject%20Object%5D&name=image.png&originHeight=54&originWidth=828&size=59112&status=done&style=none&width=828)\n\n 默认路由通过 ens33 发出去，所以我们要同时监控 ens33      和 docker0 上的 icmp（ping）数据包。\n 当 centos ping baidu.com      时，tcpdump 输出如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385036954-540814c4-ff05-4174-b702-a0bd460bb7f4.png#align=left&display=inline&height=78&margin=%5Bobject%20Object%5D&name=image.png&originHeight=78&originWidth=828&size=101822&status=done&style=none&width=828)\n\n docker0 收到 centos 的 ping      包，源地址为容器 IP 172.17.0.2，这没问题，交给 MASQUERADE 处理。这时，在 ens33 上我们看到了变化：\n\n  ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385037469-ecfd3d2e-ade4-45da-9be2-c057b4e221e5.png#align=left&display=inline&height=74&margin=%5Bobject%20Object%5D&name=image.png&originHeight=74&originWidth=828&size=90033&status=done&style=none&width=828)\n\n ping 包的源地址变成了 ens33 的 IP      192.168.137.104\n\n4. 这就是 iptable NAT      规则处理的结果，从而保证数据包能够到达外网。下面用一张图来说明这个过程：\n\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385040190-06105220-5b49-4597-9d97-466873718b01.png#align=left&display=inline&height=388&margin=%5Bobject%20Object%5D&name=image.png&originHeight=388&originWidth=798&size=158627&status=done&style=none&width=798)\n\n centos 发送 ping 包：172.17.0.2 >      [www.baidu.com](http://www.baidu.com)。\n\n docker0 收到包，发现是发送到外网的，交给 NAT 处理。\n \n NAT 将源地址换成 ens33 的      IP：192.168.137.104 > [www.baidu.com](http://www.baidu.com)。\n \n ping 包从ens33 发送出去，到达 [www.baidu.com](http://www.baidu.com)。\n\n# 二、外部网络访问容器\n\n1. docker 可将容器对外提供服务的端口映射到 host      的某个端口，外网通过该端口访问容器。容器启动时通过-p参数映射端口：\n1. 容器启动后，可通过 docker ps 或者 docker      port 查看到 host 映射的端口。在上面的例子中，httpd 容器的 80 端口被映射到 host 32770 上，这样就可以通过      <host ip>:<32770> 访问容器的 web 服务了。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385038223-865ca1ad-ca79-45a0-8a78-5ff9bf89adf6.png#align=left&display=inline&height=174&margin=%5Bobject%20Object%5D&name=image.png&originHeight=174&originWidth=828&size=96010&status=done&style=none&width=828)\n\n 除了映射动态端口，也可在 -p 中指定映射到 host      某个特定端口，例如可将 80 端口映射到 host 的 8080 端口：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385037809-cbc45c3f-c851-453a-8945-9f3b1c256188.png#align=left&display=inline&height=62&margin=%5Bobject%20Object%5D&name=image.png&originHeight=62&originWidth=828&size=90898&status=done&style=none&width=828)\n\n3. 每一个映射的端口，host 都会启动一个      docker-proxy 进程来处理访问容器的流量：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385039070-d40f9eca-2bfc-4cd2-a189-ebc3f6c0342d.png#align=left&display=inline&height=82&margin=%5Bobject%20Object%5D&name=image.png&originHeight=82&originWidth=828&size=114052&status=done&style=none&width=828)\n\n4. 以 0.0.0.0:32770->80/tcp      为例分析整个过程：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609385039761-0547ee7e-e483-4969-a6b6-8900c3bc8527.png#align=left&display=inline&height=290&margin=%5Bobject%20Object%5D&name=image.png&originHeight=290&originWidth=604&size=147806&status=done&style=none&width=604)\n\n docker-proxy 监听 host 的 32770 端口。\n \n 当 curl 访问 10.0.2.15:32770      时，docker-proxy 转发给容器172.17.0.2:80。\n \n httpd 容器响应请求并返回结果。', 1, 0, 0, '2021-01-08 18:49:41.366850', '2021-01-21 12:28:18.751693', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (133, 'docker网络类型', '[TOC]\n\n\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384450834-eadbab3a-9a30-49d9-8076-2cecbed632fc.png#align=left&display=inline&height=132&margin=%5Bobject%20Object%5D&name=image.png&originHeight=132&originWidth=828&size=28283&status=done&style=none&width=828)\n\n# 一、none网络\n\n1.  none       网络就是什么都没有的网络。挂在这个网络下的容器除了 lo，没有其他任何网卡。\n3.  容器创建时，可以通过 --network=none 指定使用       none 网络。\n5.  一些对安全性要求高并且不需要联网的应用可以使用 none       网络。比如某个容器的唯一用途是生成随机密码，就可以放到 none 网络中避免密码被窃取。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384450880-2ba09808-9367-4ecf-a5a7-753e1598ed47.png#align=left&display=inline&height=141&margin=%5Bobject%20Object%5D&name=image.png&originHeight=141&originWidth=715&size=14366&status=done&style=none&width=715)\n\n# 二、host网络\n\n- 连接到 host 网络的容器共享 Docker host       的网络栈，容器的网络配置与 host 完全一样。\n- 通过 --network=host 指定使用 host 网络。\n- 直接使用 Docker host       的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择 host       网络。当然不便之处就是牺牲一些灵活性，比如要考虑端口冲突问题，Docker host 上已经使用的端口就不能再用了。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384451783-1e75cd4e-02ca-4b00-ad72-c170c4a24130.png#align=left&display=inline&height=479&margin=%5Bobject%20Object%5D&name=image.png&originHeight=479&originWidth=931&size=118557&status=done&style=none&width=931)\n\n# 三、bridge网络\n\n- Docker 安装时会创建一个命名为       docker0 的 linux bridge。如果不指定--network，创建的容器默认都会挂到 docker0 上。\n- 创建一个容器后，查看容器网络信息。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384451141-f48ff89f-ef10-4f75-930c-d5c11e53b76d.png#align=left&display=inline&height=233&margin=%5Bobject%20Object%5D&name=image.png&originHeight=233&originWidth=842&size=26020&status=done&style=none&width=842)\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384451206-98d77f9a-2b2e-4d83-81e6-1133eea3fab9.png#align=left&display=inline&height=88&margin=%5Bobject%20Object%5D&name=image.png&originHeight=88&originWidth=701&size=16199&status=done&style=none&width=701)\n\n- 查看服务器网络信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384451748-d517c765-c1a2-4b23-8766-466d73dce67b.png#align=left&display=inline&height=622&margin=%5Bobject%20Object%5D&name=image.png&originHeight=622&originWidth=962&size=70778&status=done&style=none&width=962)\n\n- 一个新的网络接口vethe38e866被挂到了 docker0 上，veth9bfd744就是新创建容器的虚拟网卡。eth0@if7和vethe38e866@if6是一对 veth pair。veth pair 是一种成对出现的特殊网络设备，可以把它们想象成由一根虚拟网线连接起来的一对网卡，网卡的一头（eth0@if57）在容器中，另一头（veth9bfd744）挂在网桥 docker0 上，其效果就是将 eth0@if59 也挂在了 docker0 上。\n- 每次创建一个新容器的时候，Docker从可用的地址段中选择一个空闲的IP地址分配给容器的eth0端口，并且使用本地主机上docker0接口的IP作为容器的默认网关\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609384451749-05e96ed4-161c-4d86-8557-3bf89ff0eb85.png#align=left&display=inline&height=413&margin=%5Bobject%20Object%5D&name=image.png&originHeight=413&originWidth=443&size=68346&status=done&style=none&width=443)', 1, 0, 0, '2021-01-08 18:51:08.344197', '2021-01-21 12:28:14.405116', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (134, '自定义网络', '[TOC]\n\n# 一、修改默认网桥\n\n1. 安装brctl软件包\n\n [root@docker ~]# yum -y install bridge-utils\n\n2. 停止docker服务并移除docker0网桥\n\n [root@docker ~]# systemctl stop docker\n [root@docker ~]# ip link set dev docker0 down\n [root@docker ~]# brctl delbr docker0\n [root@docker ~]# iptables -t nat -F POSTROUTING\n\n3. 创建自定义网桥\n\n [root@docker ~]# brctl addbr bridge0\n [root@docker ~]# ip addr add 192.168.0.0/24 dev bridge0\n [root@docker ~]# ip link set dev bridge0 up\n\n4. 查看网桥信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426324-fc027bae-f749-4ac9-a702-b5bc175e61a9.png#align=left&display=inline&height=166&margin=%5Bobject%20Object%5D&name=image.png&originHeight=166&originWidth=1063&size=20830&status=done&style=none&width=1063)\n\n5. 修改配置，设置docker默认使用新的网桥\n\n [root@docker ~]# echo \'DOCKER_OPTS=\"-b=bridge0\"\' >> /etc/sysconfig/docker\n\n6. 修改systemctl启动配置\n\n [root@docker ~]# vim /lib/systemd/system/docker.service\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426378-f20ec5a8-c08b-4438-9b60-2dca887b8b6c.png#align=left&display=inline&height=273&margin=%5Bobject%20Object%5D&name=image.png&originHeight=273&originWidth=933&size=31452&status=done&style=none&width=933)\n\n7. 重启docker\n\n [root@docker ~]# systemctl daemon-reload\n [root@docker ~]# systemctl start docker\n\n8. 运行容器，测试效果\n\n [root@docker ~]# docker run -it busybox\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426374-fab266b9-4a19-4f44-9108-243bfb784bd9.png#align=left&display=inline&height=187&margin=%5Bobject%20Object%5D&name=image.png&originHeight=187&originWidth=850&size=21334&status=done&style=none&width=850)\n\n# 二、使用bridge创建自定义网桥\n\n1. 创建自定义网桥\n\n [root@docker ~]# docker network create docker1 --subnet=192.168.0.0/16 -o com.docker.network.bridge.name=docker1\n\n- --subnet=192.168.0.0/16：指定IP网段\n- -o      com.docker.network.bridge.name=docker1：指定设备名称\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426383-3f86c6c3-f632-4439-a3a2-f8b07f163e3c.png#align=left&display=inline&height=141&margin=%5Bobject%20Object%5D&name=image.png&originHeight=141&originWidth=727&size=13146&status=done&style=none&width=727)\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426409-bdd001c7-325d-42ad-95b1-4714e93d6734.png#align=left&display=inline&height=119&margin=%5Bobject%20Object%5D&name=image.png&originHeight=119&originWidth=958&size=15439&status=done&style=none&width=958)\n\n- 运行容器，使用docker1网络\n\n [root@docker ~]# docker run -it --network docker1 busybox\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426415-6edb0ff9-caa0-48a6-bead-9af8305a1fa3.png#align=left&display=inline&height=184&margin=%5Bobject%20Object%5D&name=image.png&originHeight=184&originWidth=847&size=20828&status=done&style=none&width=847)\n\n- 运行容器，指定ip地址\n\n [root@docker ~]# docker run -it --network docker1 --ip 192.168.0.100 busybox\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426430-8b549783-602a-45bb-89bc-13f75c0fb7c7.png#align=left&display=inline&height=116&margin=%5Bobject%20Object%5D&name=image.png&originHeight=116&originWidth=871&size=14628&status=done&style=none&width=871)\n\n- 删除自定义网桥\n\n [root@docker ~]# docker network rm docker1\n\n2. 自定义网桥与默认docker0通信\n\n-  docker 在设计上就是要隔离不同的 netwrok。由于iptables DROP 掉了网桥 docker0  之间双向的流量。只能采取为 容器添加一块docker1的网卡来实现通信\n\n [root@docker docker]# docker network connect docker1 a186189\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426426-4d9b7d57-19b2-4d86-821d-02d0912e4885.png#align=left&display=inline&height=556&margin=%5Bobject%20Object%5D&name=image.png&originHeight=556&originWidth=804&size=60595&status=done&style=none&width=804)\n\n- 容器中增加了一个网卡 eth1，分配了docker1 的 IP 192.168.0.3。使用busybox访问测试\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426454-98a18dc5-0080-43b4-bb38-d7575e597c2a.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=image.png&originHeight=117&originWidth=592&size=13746&status=done&style=none&width=592)\n\n# 三、创建一个点到点连接\n\n 用户有时候需要两个容器之间可以直连通信，而不用通过主机网桥进行桥接。解决办法很简单：创建一对peer接口，分别放到两个容器中，配置成点到点链路类型即可。\n\n1. 启动两个容器\n\n [root@docker ~]# docker run -it --net=none --name=box1 busybox\n [root@docker ~]# docker run -it --net=none --name=box2 busybox\n\n2. 找到容器进程号，并创建网络命名空间的跟踪文件\n\n [root@docker ~]# docker inspect -f \'{{.State.Pid}}\' box1\n 2989\n [root@docker ~]# docker inspect -f \'{{.State.Pid}}\' box2\n 3004\n [root@docker ~]# mkdir -p /var/run/netns\n [root@docker ~]# ln -s /proc/2989/ns/net /var/run/netns/2989\n [root@docker ~]# ln -s /proc/3004/ns/net /var/run/netns/3004\n\n3. 创建一对peer接口\n\n [root@docker ~]# ip link add A type veth peer name B\n\n4. 添加IP地址和路由信息\n\n [root@docker ~]# ip link set A netns 2989\n [root@docker ~]# ip netns exec 2989 ip addr add 10.1.1.1/32 dev A\n [root@docker ~]# ip netns exec 2989 ip link set A up\n [root@docker ~]# ip netns exec 2989 ip route add 10.1.1.2/32 dev A\n [root@docker ~]# ip link set B netns 3004\n [root@docker ~]# ip netns exec 3004 ip addr add 10.1.1.2/32 dev B\n [root@docker ~]# ip netns exec 3004 ip link set B up\n [root@docker ~]# ip netns exec 3004 ip route add 10.1.1.1/32 dev B\n\n5. 访问测试连通性\n\n# 四、跨主机容器网络\n\n 使用libnetwork自带的Overlay类型驱动来轻松实现跨主机的网络通信。Overlay驱动默认采用VXLAN协议，在IP地址可以互相访问的多个主机之间搭建隧道，让容器可以互相访问。\n\n1. 实验环境准备\n\n 在 docker 主机node1（192.168.10.128）和node2（192.168.10.129）上实践各种跨主机网络方案，在 master（192.168.10.223）上部署支持的组件，比如 Consul。\n\n 配置网络信息管理数据库\n\n 要连通不同的主机，需要交换机或路由器（跨子网时需要）这样的互联设备。这些设备一方面是在物理上起到连接作用，但更重要的是起到了网络管理的功能。例如，主机位置在什么地方，地址是多少等信息，都需要网络管理平面来维护。\n\n 在管理节点安装Consul\n\n [root@master ~]# docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426478-bf446cd4-cc01-47e1-adfb-8090f0926d51.png#align=left&display=inline&height=272&margin=%5Bobject%20Object%5D&name=image.png&originHeight=272&originWidth=719&size=19489&status=done&style=none&width=719)\n\n 修改host1和host2配置文件\n 修改docker daemon 的配置文件。\n\n [root@node1 ~]# vim /lib/systemd/system/docker.service\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426474-ee5cf57e-e505-4ed0-be4d-33eacda8374f.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=image.png&originHeight=183&originWidth=934&size=21020&status=done&style=none&width=934)\n\n- --cluster-store 指定 consul 的地址。\n- --cluster-advertise 告知 consul      自己的连接地址。\n- 重启服务，consul查看信息\n\n[root@node1 ~]# systemctl daemon-reload  \n[root@node1 ~]# systemctl restart docker\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426516-dc2aaceb-0a48-49dd-b36c-c00415cb9288.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=image.png&originHeight=321&originWidth=731&size=26290&status=done&style=none&width=731)\n\n2. 创建overlay网络\n\n- node1主机创建overlay网络\n\n[root@node1 ~]# docker network create -d overlay ov_net1\n\n- -d      overlay 指定 driver 为 overlay。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426538-965ed4ab-44a6-4ea5-b25f-51bba39b142b.png#align=left&display=inline&height=146&margin=%5Bobject%20Object%5D&name=image.png&originHeight=146&originWidth=739&size=14559&status=done&style=none&width=739)\n\n- node2主机查看网络\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383426560-a0386a05-2b25-4ad5-95fa-675b15ddccc7.png#align=left&display=inline&height=141&margin=%5Bobject%20Object%5D&name=image.png&originHeight=141&originWidth=735&size=13894&status=done&style=none&width=735)\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383427727-f4b6ea9d-60b9-4ffd-a043-d4fdc13d2676.png#align=left&display=inline&height=714&margin=%5Bobject%20Object%5D&name=image.png&originHeight=714&originWidth=632&size=37808&status=done&style=none&width=632)\n\n3. 在overlay中运行容器\n\n- 在node1中创建一个busybox容器并连接到ov_net1\n\n [root@node1 ~]# docker run -it --name bbox1 --network ov_net1 busybox\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383428454-fabd7c8a-fdf6-4118-b47a-35f3b243c770.png#align=left&display=inline&height=391&margin=%5Bobject%20Object%5D&name=image.png&originHeight=391&originWidth=871&size=43451&status=done&style=none&width=871)\n\n- 运行容器后会在宿主机上生成一个overlay网桥\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383428484-cba1f2e5-68b5-47e1-bde6-c12e067754b9.png#align=left&display=inline&height=183&margin=%5Bobject%20Object%5D&name=image.png&originHeight=183&originWidth=916&size=22582&status=done&style=none&width=916)\n\n- 容器 bbox1 就可以通过     docker_gwbridge 访问外网\n\n4. overlay连通性\n\n- 在node2中创建一个busybox容器并连接到ov_net1\n\n [root@node2 ~]# docker run -it --name bbox2 --network ov_net1 busybox\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383428846-644d55ac-3cde-43af-9145-eb5cdd864b35.png#align=left&display=inline&height=394&margin=%5Bobject%20Object%5D&name=image.png&originHeight=394&originWidth=784&size=43021&status=done&style=none&width=784)\n\n- bbox2访问bbox1\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383428840-ec451bde-d24f-4f9d-9b1d-f2b5237d0aef.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=image.png&originHeight=117&originWidth=565&size=11747&status=done&style=none&width=565)\n\n5. 实现原理\n\n- docker      会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0， veth      pair 一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。\n- br0      除了连接所有的 veth pair，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。容器之间的数据就是通过这个      tunnel 通信的。逻辑网络拓扑结构如图所示：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383432151-18b77cc2-7e23-4e76-b4a7-6d801f61fa47.png#align=left&display=inline&height=548&margin=%5Bobject%20Object%5D&name=image.png&originHeight=548&originWidth=745&size=157879&status=done&style=none&width=745)\n\n6. overlay网络隔离\n\n- node1创建ov_net2网络，并运行bbox3\n\n [root@node1 ~]# docker network create -d overlay ov_net2\n\n- 启动容器bbox3\n\n [root@node2 ~]# docker run -itd --name bbox3 --network ov_net2 busybox\n\n- 查看bbox3网络\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383429809-d1a645cf-a9a1-44ac-a8c1-14574ccf84f6.png#align=left&display=inline&height=298&margin=%5Bobject%20Object%5D&name=image.png&originHeight=298&originWidth=866&size=36858&status=done&style=none&width=866)\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383429897-c0180c09-795f-4274-97a1-b9c16273a03a.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=92&originWidth=552&size=9506&status=done&style=none&width=552)\n\n- 测试与bbox1、bbox2网络连通性\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383430174-b5dd7657-7a00-472f-b33f-8777440f65d9.png#align=left&display=inline&height=255&margin=%5Bobject%20Object%5D&name=image.png&originHeight=255&originWidth=663&size=19007&status=done&style=none&width=663)\n\n- 网络拓扑\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609383435243-bd6d4138-0a4c-4994-a602-aaf6d2d326a0.png#align=left&display=inline&height=959&margin=%5Bobject%20Object%5D&name=image.png&originHeight=959&originWidth=1278&size=442359&status=done&style=none&width=1278)\n\n\n\n', 2, 0, 0, '2021-01-08 18:54:34.063486', '2021-01-21 12:31:31.926225', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (135, 'docker网络模型', '[TOC]\n\n1. 容器网络模型包括三种基本元素：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609382241288-d087ebee-292e-4ea5-aaf1-6e1d74e621de.png#align=left&display=inline&height=689&margin=%5Bobject%20Object%5D&name=image.png&originHeight=689&originWidth=1279&size=67554&status=done&style=none&width=1279)\n\n 沙盒（Sandbox）：代表一个容器（准确地说，是其网络命名空间）；\n 接入点（Endpoint）：代表网络上可以挂载容器的接口，会分配IP地址；\n 网络（Network）：可以连通多个接入点的一个子网。\n\n2. 对于使用CNM的容器管理系统来说，具体底下网络如何实现，不同子网彼此怎么隔离，有没有QoS，都不关心。只要插件能提供网络和接入点，只需把容器给接上或者拔下，剩下的都是插件驱动自己去实现，这样就解耦了容器和网络功能，十分灵活。\n2. CNM的典型生命周期\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609382241226-0ee97eac-c6ae-43f8-bdcd-f2992ee5f250.png#align=left&display=inline&height=658&margin=%5Bobject%20Object%5D&name=image.png&originHeight=658&originWidth=1281&size=198120&status=done&style=none&width=1281)\n 首先，驱动注册自己到网络控制器，网络控制器使用驱动类型，来创建网络；然后在创建的网络上创建接口；最后把容器连接到接口上即可。销毁过程则正好相反，先把容器从接入口上卸载，然后删除接入口和网络即可。\n\n4. CNM的典型生命周期目前CNM支持的驱动类型有四种：Null、Bridge、Overlay、Remote，简单介绍如下：\n\n Null：不提供网络服务，容器启动后无网络连接；\n Bridge：就是Docker传统上默认用Linux网桥和Iptables实现的单机网络；\n Overlay：是用vxlan隧道实现的跨主机容器网络；\n Remote：扩展类型，预留给其他外部实现的方案，比如有一套第三方的SDN方案（如OpenStack       Neutron）就可以接进来。\n\n5. 从位置上看，libnetwork往上提供容器支持，往下隐藏实现差异，自身处于十分关键的中间层。libnetwork可以类比为最核心的TCP/IP层。目前，已有大量的网络方案开始支持libnetwork。\n\n', 2, 0, 0, '2021-01-08 18:55:42.582303', '2021-01-23 01:17:17.892408', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (136, '基本架构', '\n\n Docker目前采用了标准的C/S架构，包括客户端、服务端两大核心组件，同时通过镜像仓库来存储镜像。客户端和服务端既可以运行在一个机器上，也可通过socket或者RESTful  API来进行通信\n\n- 服务端用来接受并处理来自客户端的请求，包括创建、运行、删除容器等。\n- 客户端用来为用户提供一系列可执行命令，使用这些命令可实现与Docker服务端交互。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388651232-fb50d87a-7906-4311-a049-1699b68944e5.png#align=left&display=inline&height=934&margin=%5Bobject%20Object%5D&name=image.png&originHeight=934&originWidth=854&size=203936&status=done&style=none&width=854)\n\n1. 服务端\n\n Docker服务端一般在宿主主机后台运行，dockerd作为服务端接受来自客户的请求，并通过containerd具体处理与容器相关的请求，包括创建、运行、删除容器等。服务端主要包括四个组件：\n\n dockerd：为客户端提供RESTful API，响应来自客户端的请求，采用模块化的架构，通过专门的Engine模块来分发管理各个来自客户端的任务。可以单独升级；\n\n docker-proxy：是dockerd的子进程，当需要进行容器端口映射时，docker-proxy完成网络映射配置；\n\n containerd：是dockerd的子进程，提供gRPC接口响应来自dockerd的请求，对下管理runC镜像和容器环境。可以单独升级；\n\n containerd-shim：是containerd的子进程，为runC容器提供支持，同时作为容器内进程的根进程。\n\n runC是Docker引擎，容器的创建，运行，销毁等等操作最终都将通过调用runC完成\n \n docker-proxy只有当启动容器并且使用端口映射时候才会执行，负责配置容器的端口映射规则：\n\n2. 客户端\n\n Docker客户端为用户提供一系列可执行命令，使用这些命令可实现与Docker服务端交互。客户端发送命令后，等待服务端返回；一旦收到返回后，客户端立刻执行结束并退出。用户执行新的命令，需要再次调用客户端命令。\n\n3. docker的远程访问\n\n Docker使用socket进行客户端和服务端的连接，提供了三种进行socket连接的模式\n\n unix:///var/run/docker.sock        Unix端口，默认的连接方式\n tcp://host:port\n fd://socketfd\n\n 默认情况下，Docker守护进程会生成一个socket（/var/run/docker.sock）文件来进行本地进程通信，而不会监听任何端口，因此只能在本地使用docker客户端或者使用Docker       API进行操作。如果想在其他主机上操作Docker主机，就需要让Docker守护进程监听一个端口，这样才能实现远程通信。\n\n 服务端修改docker守护进程启动选项\n\n 更改docker配置文件，在[Service]模块ExecStart行追加-H unix:///var/run/docker.sock -H  0.0.0.0:5555\n [root@dockerserver ~]# vim /lib/systemd/system/docker.service\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388650161-8f8176ac-0cf5-4f90-87ce-975bad9a96f9.png#align=left&display=inline&height=55&margin=%5Bobject%20Object%5D&name=image.png&originHeight=55&originWidth=923&size=14732&status=done&style=none&width=923)\n\n 修改完配置后重启服务\n\n [root@dockerserver ~]# systemctl  daemon-reload \n [root@dockerserver ~]# systemctl restart docker\n\n 客户端测试远程访问\n\n [root@dockerclient]# docker -H 192.168.217.132:5555 images\n\n4. 镜像仓库\n\n 镜像是使用容器的基础，Docker使用镜像仓库在大规模场景下存储和分发Docker镜像。镜像仓库提供了对不同存储后端的支持，存放镜像文件，并且支持RESTful  API，接收来自dockerd的命令，包括拉取、上传镜像等。\n 用户从镜像仓库拉取的镜像文件会存储在本地使用；用户同时也可以上传镜像到仓库，方便其他人获取。使用镜像仓库可以极大地简化镜像管理和分发的流程。', 6, 0, 0, '2021-01-08 19:22:23.040167', '2021-01-22 12:33:33.332894', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (137, '命名空间', '[TOC]\n\n 每个容器都可以拥有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统环境中一样。命名空间机制保证了容器之间彼此互不影响。\n在操作系统中，包括内核、文件系统、网络、进程号（Process ID, PID）、用户号（User ID, UID）、进程间通信（InterProcess Communication, IPC）等资源，所有的资源都是应用进程直接共享的。要想实现虚拟化，除了要实现对内存、CPU、网络IO、硬盘IO、存储空间等的限制外，还要实现文件系统、网络、PID、UID、IPC等的相互隔离。\n\n# 一、种类\n\n1. PID进程命名空间\n\n Linux通过进程命名空间管理进程号，对于同一进程（同一个task_struct），在不同的命名空间中，看到的进程号不相同。每个进程命名空间有一套自己的进程号管理方法。\n\n- 进程命名空间是一个父子关系的结构，子空间中的进程对于父空间是可见的。新fork出的一个进程，在父命名空间和子命名空间将分别对应不同的进程号\n- 例如，查看Docker服务主进程id号为1131\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388435732-20cd7d97-c985-4f47-b279-e7051246dda3.png#align=left&display=inline&height=211&margin=%5Bobject%20Object%5D&name=image.png&originHeight=211&originWidth=937&size=61478&status=done&style=none&width=937)\n\n- 新建一个centos容器，执行sleep命令，查看docker进程\n\n [root@docker ~]# docker run --name test -d centos sleep 9999\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388438784-14a4be0a-c651-4189-82bd-09594ba19c2d.png#align=left&display=inline&height=381&margin=%5Bobject%20Object%5D&name=image.png&originHeight=381&originWidth=937&size=109361&status=done&style=none&width=937)\n 每当启动一个容器后，相应的会启动一个containerd-shim进程进程号为1859，作为该容器内所有进程的根进程\n\n- 查看容器内部进程信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388436746-48a6e9c8-c61b-4bcb-9e1f-da6d4746ab86.png#align=left&display=inline&height=105&margin=%5Bobject%20Object%5D&name=image.png&originHeight=105&originWidth=846&size=24145&status=done&style=none&width=846)\n\n- 宿主机与容器内进程关系空间如果所示\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388439745-fb44c89e-fad1-41a6-8330-cca5925e8ec3.png#align=left&display=inline&height=719&margin=%5Bobject%20Object%5D&name=image.png&originHeight=719&originWidth=781&size=136438&status=done&style=none&width=781)\n\n2. IPC进程交互命名空间\n\n 容器中的进程交互还是采用了Linux常见的进程间交互方法，包括信号量、消息队列和共享内存等方式。PID命名空间和IPC命名空间可以组合起来一起使用，同一个IPC命名空间内的进程可以彼此可见，允许进行交互；不同空间的进程则无法交互。\n\n3. net网络命名空间\n\n 有了进程命名空间后，不同命名空间中的进程号可以相互隔离，但是网络端口还是共享本地系统的端口。\n\n- 通过网络命名空间，可以实现网络隔离。一个网络命名空间为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口、IPv4和IPv6协议栈、IP路由表、防火墙规则、sockets等，这样每个容器的网络就能隔离开来。\n- Docker采用虚拟网络设备（Virtual Network      Device,      VND）的方式，将不同命名空间的网络设备连接到一起。默认情况下，Docker在宿主机上创建多个虚机网桥（如默认的网桥docker0），容器中的虚拟网卡通过网桥进行连接\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388439288-8715b55f-3a39-45c4-852a-42a74d8e6e2b.png#align=left&display=inline&height=581&margin=%5Bobject%20Object%5D&name=image.png&originHeight=581&originWidth=507&size=83785&status=done&style=none&width=507)\n\n- 使用docker network      ls命令可以查看到当前系统中的网桥：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437708-2ca4cdd5-6110-4d4c-ac67-15f6fc1785e2.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=957&size=14268&status=done&style=none&width=957)\n\n- 使用brctl工具（需要安装bridge-utils工具包），还可以看到连接到网桥上的虚拟网口的信息。每个容器默认分配一个网桥上的虚拟网口，并将docker0的IP地址设置为默认的网关，容器发起的网络流量通过宿主机的iptables规则进行转发：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437302-e3375f62-4bd8-498d-9fca-a93b8fe14c5d.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=image.png&originHeight=80&originWidth=958&size=9092&status=done&style=none&width=958)\n\n4. mnt挂载命名空间\n\n- 类似于chroot，挂载（Mount,      MNT）命名空间可以将一个进程的根文件系统限制到一个特定的目录下。\n- 挂载命名空间允许不同命名空间的进程看到的本地文件位于宿主机中不同路径下，每个命名空间中的进程所看到的文件目录彼此是隔离的。例如，不同命名空间中的进程，都认为自己独占了一个完整的根文件系统（rootfs），但实际上，不同命名空间中的文件彼此隔离，不会造成相互影响，同时也无法影响宿主机文件系统中的其他路径。\n- 容器有自己的 / 目录，可以执行 mount 和 umount 命令。当然我们知道这些操作只在当前容器中生效，不会影响到 host 和其他容器。\n\n5. UTS主机名域名命名空间\n\n- UTS（UNIX Time-sharing      System）命名空间允许每个容器拥有独立的主机名和域名，从而可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。\n- 默认情况下，容器的 hostname 是它的短ID，可以通过 -h 或 --hostname 参数设置。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437332-c3d6b2e5-40f0-44b0-a013-e4e41421fe04.png#align=left&display=inline&height=86&margin=%5Bobject%20Object%5D&name=image.png&originHeight=86&originWidth=706&size=7638&status=done&style=none&width=706)\n6．user用户命名空间\n\n- 每个容器可以有不同的用户和组id，也就是说，可以在容器内使用特定的内部用户执行程序，而非本地系统上存在的用户。\n- 每个容器内部都可以有最高权限的root帐号，但跟宿主主机不在一个命名空间。通过使用隔离的用户命名空间，可以提高安全性，避免容器内的进程获取到额外的权限；同时通过使用不同用户也可以进一步在容器内控制权限。\n- user namespace 让容器能够管理自己的用户，host 不能看到容器中创建的用户。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437335-fc5ead01-c46f-40dc-8060-efdc1e2ce2f5.png#align=left&display=inline&height=58&margin=%5Bobject%20Object%5D&name=image.png&originHeight=58&originWidth=542&size=6348&status=done&style=none&width=542)\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437166-4d3867b4-5e8e-4b75-ab0b-bf57170e803f.png#align=left&display=inline&height=56&margin=%5Bobject%20Object%5D&name=image.png&originHeight=56&originWidth=415&size=4648&status=done&style=none&width=415)\n\n- 例如，下面的命令在容器内创建了test用户，只有普通权限，无法访问更高权限的资源：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388437706-b84d409f-e85a-4e32-8c12-9cdea5d2ec10.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=image.png&originHeight=115&originWidth=541&size=12067&status=done&style=none&width=541)\n\n# 二、实现原理\n\n1. Namespace它其实只是 Linux 创建新进程的一个可选参数。在 Linux 系统中创建线程的系统调用是 clone()，比如：int pid =      clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。\n1. 当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如：int pid =      clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。\n1. 之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。\n1. 跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。', 1, 0, 0, '2021-01-08 19:23:45.234722', '2021-01-21 12:30:57.220688', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (138, '控制组', '\n\n1. 控制组（CGroups）是Linux内核的一个特性，主要用来对共享资源进行隔离、限制、审计等。只有将分配到容器的资源进行控制，才能避免多个容器同时运行时对宿主机系统的资源竞争。每个控制组是一组对资源的限制，支持层级化结构。\n1. 控制组提供如下功能：\n\n- 资源限制（resource  limiting）：可将组设置一定的内存限制。比如：内存子系统可以为进程组设定一个内存使用上限，一旦进程组使用的内存达到限额再申请内存，就会出发Out of  Memory警告。\n- 优先级（prioritization）：通过优先级让一些组优先得到更多的CPU等资源。\n- 资源审计（accounting）：用来统计系统实际上把多少资源用到适合的目的上，可以使用cpuacct子系统记录某个进程组使用的CPU时间。\n- 隔离（isolation）：为组隔离命名空间，这样使得一个组不会看到另一个组的进程、网络连接和文件系统。\n- 控制（control）：执行挂起、恢复和重启动等操作。   \n\n3. 分析：\n\n- 运行容器，对cpu资源进行限制\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388113480-fe97fab4-c4b1-463a-80b2-651b56aa7aa0.png#align=left&display=inline&height=133&margin=%5Bobject%20Object%5D&name=image.png&originHeight=133&originWidth=930&size=15762&status=done&style=none&width=930)\n\n- 查看容器的 ID：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388101573-4a9d8586-caf9-4c29-82ed-00d49e1a79b7.png#align=left&display=inline&height=161&margin=%5Bobject%20Object%5D&name=image.png&originHeight=161&originWidth=932&size=15251&status=done&style=none&width=932)\n\n- 在 /sys/fs/cgroup/cpu/docker       目录中，Linux 会为每个容器创建一个 cgroup 目录，以容器长ID 命名：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609388101841-3be46a59-935e-4165-a0d4-bad8b7f424e5.png#align=left&display=inline&height=268&margin=%5Bobject%20Object%5D&name=image.png&originHeight=268&originWidth=929&size=35165&status=done&style=none&width=929)\n\n- 目录中包含所有与 cpu 相关的 cgroup 配置，文件       cpu.shares 保存的就是 --cpu-shares 的配置，值为 512。\n- 同样的，/sys/fs/cgroup/memory/docker       和 /sys/fs/cgroup/blkio/docker 中保存的是内存以及 Block IO 的 cgroup 配置。\n\n4. 原理\n\n- Linux Cgroups ，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。\n- 至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，\n- 比如这样一条命令：$ docker run -it       --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：\n\n $ cat       /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000\n $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000\n 这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。\n\n', 2, 0, 0, '2021-01-08 19:24:40.155831', '2021-01-21 12:30:47.571747', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (139, '联合文件系统', '[TOC]\n\n# 一、简介\n\n1. 联合文件系统（UnionFS）是一种轻量级的高性能分层文件系统，它支持将文件系统中的修改信息作为一次提交，并层层叠加，同时可以将不同目录挂载到同一个虚拟文件系统下，应用看到的是挂载的最终结果。联合文件系统是实现Docker镜像的技术基础。\n1. Docker镜像可以通过分层来进行继承。例如，用户基于基础镜像来制作各种不同的应用镜像。这些镜像共享同一个基础镜像层，提高了存储效率。此外，当用户改变了一个Docker镜像（比如升级程序到新的版本），则会创建一个新的层（layer）。因此，用户不用替换整个原镜像或者重新建立，只需要添加新层即可。用户分发镜像的时候，也只需要分发被改动的新层内容（增量部分）。这让Docker的镜像管理变得十分轻量和快速。\n\n# 二、Docker存储原理\n\n1. Docker目前通过插件化方式支持多种文件系统后端。Debian/Ubuntu上成熟的AUFS，就是一种联合文件系统实现。AUFS支持为每一个成员目录（类似Git的分支）设定只读（readonly）、读写（readwrite）或写出（whiteout-able）权限，同时AUFS里有一个类似分层的概念，对只读权限的分支可以逻辑上进行增量地修改（不影响只读部分的）。\n1. Docker镜像自身就是由多个文件层组成，每一层有基于内容的唯一的编号（层ID）。对于Docker镜像来说，这些层的内容都是不可修改的、只读的。而当Docker利用镜像启动一个容器时，将在镜像文件系统的最顶端再挂载一个新的可读写的层给容器。容器中的内容更新将会发生在可读写层。当所操作对象位于较深的某层时，需要先复制到最上层的可读写层。当数据对象较大时，往往意味着较差的IO性能。因此，对于IO敏感型应用，一般推荐将容器修改的数据通过volume方式挂载，而不是直接修改镜像内数据。\n\n# 三、Docker存储结构\n\n1. 所有的镜像和容器都存储都在Docker指定的存储目录下，以Ubuntu宿主系统为例，默认路径是/var/lib/docker。在这个目录下面，存储由Docker镜像和容器运行相关的文件和目录。\n1. 其中，如果使用AUFS存储后端，则最关键的就是aufs目录，保存Docker镜像和容器相关数据和信息。包括layers、diff和mnt三个子目录。\n\n- layers子目录包含层属性文件，用来保存各个镜像层的元数据：某镜像的某层下面包括哪些层。\n- diff子目录包含层内容子目录，用来保存所有镜像层的内容数据\n- mnt子目录下面的子目录是各个容器最终的挂载点，所有相关的AUFS层在这里挂载到一起，形成最终效果。一个运行中容器的根文件系统就挂载在这下面的子目录上\n\n# 四、分析\n\n1. 我们启动一个容器，比如：$ docker run -d ubuntu:latest sleep 3600\n\n- 这时候，Docker 就会从 Docker Hub 上拉取一个 Ubuntu 镜像到本地。这个所谓的“镜像”，实际上就是一个 Ubuntu 操作系统的 rootfs，它的内容是 Ubuntu 操作系统的所有文件和目录。不过，与之前我们讲述的 rootfs 稍微不同的是，Docker 镜像使用的 rootfs，往往由多个“层”组成：\n\n```bash\n$ docker image inspect ubuntu:latest\n... \n\"RootFS\": { \n  \"Type\": \"layers\", \n    \"Layers\": [ \n    \"sha256:f49017d4d5ce9c0f544c...\",  \"sha256:8f2b771487e9d6354080...\",  \"sha256:ccd4d61916aaa2159429...\",  \"sha256:c01d74f99de40e097c73...\",  \"sha256:268a067217b5fe78e000...\" \n    ] \n}\n```\n\n- 可以看到，这个 Ubuntu 镜像，实际上由五个层组成。这五个层就是五个增量 rootfs，每一层都是 Ubuntu 操作系统文件与目录的一部分；而在使用镜像时，Docker  会把这些增量联合挂载在一个统一的挂载点上。这个挂载点就是 /var/lib/docker/aufs/mnt/，比如：/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e\n- 不出意外的，这个目录里面正是一个完整的 Ubuntu 操作系统：\n\n```bash\n $ ls  /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin  srv sys tmp usr var\n```\n\n- 那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的 Ubuntu 文件系统的呢？\n\n 这个信息记录在 AuFS 的系统目录 /sys/fs/aufs 下面。\n\n- 首先，通过查看 AuFS 的挂载信息，我们可以找到这个目录对应的 AuFS 的内部 ID（也叫：si）：\n\n```bash\n$ cat /proc/mounts| grep aufsnone  /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs  rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0\n即，si=972c6d361e6b32ba。\n```\n\n- 然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息：\n\n```bash\n$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*\n/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw\n/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh\n/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh\n/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh\n/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh\n/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh\n/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+wh\n```\n\n- 从这些信息里，我们可以看到，镜像的层都放置在 /var/lib/docker/aufs/diff 目录下，然后被联合挂载在 /var/lib，而且，从这个结构可以看出来，这个容器的 rootfs 由如下图所示的三部分组成：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609386296672-71860eb0-f97b-45c4-a561-5ba00172b363.png#align=left&display=inline&height=1002&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1002&originWidth=1350&size=151086&status=done&style=none&width=1350)\n\n## 1. 第一部分，只读层。\n\n 它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的。\n 这时，我们可以分别查看一下这些层的内容：\n\n```bash\n$ ls /var/lib/docker/aufs/diff/72b0744e06247c7d0…\netc sbin usr var\n$ ls /var/lib/docker/aufs/diff/32e8e20064858c0f2…\nrun\n$ ls /var/lib/docker/aufs/diff/a524a729adadedb900…\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin  srv sys tmp usr var\n```\n\n 可以看到，这些层，都以增量的方式分别包含了 Ubuntu 操作系统的一部分。\n\n## 2.第二部分，可读写层。\n\n 它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。\n可是，你有没有想到这样一个问题：如果我现在要做的，是删除只读层里的一个文件呢？\n\n 为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。\n \n比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。我喜欢把 whiteout 形象地翻译为：“白障”。\n\n 所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。\n\n## 3.第三部分，Init层。\n\n 它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。\n \n 需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。\n \n 可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。\n \n 所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。\n \n最终，这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下，表现为一个完整的 Ubuntu 操作系统供容器使用。', 2, 0, 0, '2021-01-08 19:26:18.799475', '2021-01-21 12:32:07.931073', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (140, '网络虚拟化', '\n\n1. 基本原理\n\n Docker中的网络接口默认都是虚拟接口。虚拟接口的最大优势就是转发效率极高。这是因为Linux通过在内核中进行数据复制来实现虚拟接口之间的数据转发，即发送接口的发送缓存中的数据包将被直接复制到接收接口的接收缓存中，而无须通过外部物理网络设备进行交换。对于本地系统和容器内系统来看，虚拟接口跟一个正常的以太网卡相比并无区别，只是它的速度要快得多。\nDocker容器网络就很好地利用了Linux虚拟网络技术，它在本地主机和容器内分别创建一个虚拟接口veth，并连通\n\n2. 网络创建过程\n\n 一般情况下，Docker创建一个容器的时候，会具体执行如下操作：\n\n- 创建一对虚拟接口，分别放到本地主机和新容器的命名空间中；\n- 本地主机一端的虚拟接口连接到默认的docker0网桥或指定网桥上，并具有一个以veth开头的唯一名字，如veth1234；\n- 器一端的虚拟接口将放到新创建的容器中，并修改名字作为eth0。这个接口只在容器的命名空间可见；\n- 从网桥可用地址段中获取一个空闲地址分配给容器的eth0（例如172.17.0.2/16），并配置默认路由网关为docker0网卡的内部接口docker0的IP地址（例如172.17.42.1/16）。\n\n 完成这些之后，容器就可以使用它所能看到的eth0虚拟网卡来连接其他容器和访问外部网络。', 2, 0, 0, '2021-01-08 19:26:57.458530', '2021-01-22 12:39:17.022099', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (141, '命名空间隔离的安全', '\n\n1. Docker容器和LXC容器在实现上很相似，所提供的安全特性也基本一致。当用docker       [container]       run命令启动一个容器时，Docker将在后台为容器创建一个独立的命名空间。命名空间提供了最基础也是最直接的隔离，在容器中运行的进程不会被运行在本地主机上的进程和其他容器通过正常渠道发现和影响。\n1. 通过命名空间机制，每个容器都有自己独有的网络栈，意味着它们不能访问其他容器的套接字（socket）或接口。当然，容器默认可以与本地主机网络连通，如果主机系统上做了相应的交换设置，容器可以像跟主机交互一样的和其他容器交互。启动容器时，指定公共端口或使用连接系统，容器可以相互通信了（用户可以根据配置来限制通信的策略）。从网络架构的角度来看，所有的容器实际上是通过本地主机的网桥接口（docker0）进行相互通信，就像物理机器通过物理交换机通信一样。\n1. 与虚拟机方式相比，通过命名空间来实现的隔离并不是那么绝对。运行在容器中的应用可以直接访问系统内核和部分系统文件。因此，用户必须保证容器中应用是安全可信的（这跟保证运行在系统中的软件是可信的一个道理），否则本地系统将可能受到威胁，即必须保证镜像的来源和自身可靠。Docker自1.3.0版本起对镜像管理引入了签名系统，加强了对镜像安全性的防护，用户可以通过签名来验证镜像的完整性和正确性', 3, 0, 0, '2021-01-08 20:55:34.236629', '2021-01-22 18:38:24.387263', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (142, '控制组资源控制的安全', '\n\n1. 控制组是Linux容器机制中的另外一个关键组件，它负责实现资源的审计和限制。\n1. 当用户执行docker [container]       run命令启动一个Docker容器时，Docker将通过Linux相关的调用，在后台为容器创建一个独立的控制组策略集合，该集合将限制容器内应用对资源的消耗。控制组提供了很多有用的特性。它可以确保各个容器公平地分享主机的内存、CPU、磁盘IO等资源；当然，更重要的是，通过控制组可以限制容器对资源的占用，确保了当某个容器对资源消耗过大时，不会影响到本地主机系统和其他容器。\n1. 尽管控制组不负责隔离容器之间相互访问、处理数据和进程，但是它在防止恶意攻击特别是拒绝服务攻击（DDoS）方面是十分有效的。对于支持多用户的服务平台（比如公有的各种PaaS、容器云）上，控制组尤其重要。例如，当个别应用容器出现异常的时候，可以保证本地系统和其他容器正常运行而不受影响，从而避免引发“雪崩”灾难。', 2, 0, 0, '2021-01-08 20:56:24.110074', '2021-01-21 12:34:03.222414', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (143, '内核能力机制', '\n\n1. 能力机制（capability）是Linux内核一个强大的特性，可以提供细粒度的权限访问控制。传统的Unix系统对进程权限只有根权限（用户id为0，即为root用户）和非根权限（用户非root用户）两种粗粒度的区别。Linux内核自2.2版本起支持能力机制，将权限划分为更加细粒度的操作能力，既可以作用在进程上，也可以作用在文件上。例如，一个Web服务进程只需要绑定一个低于1024端口的权限，并不需要完整的root权限，那么给它授权net_bind_service能力即可。此外，还可以赋予很多其他类似能力来避免进程获取root权限。\n1. 默认情况下，Docker启动的容器有严格限制，只允许使用内核的一部分能力，包括chown、dac_override、fowner、kill、setgid、setuid、setpcap、net_bind_service、net_raw、sys_chroot、mknod、setfcap、audit_write，等等。使用能力机制对加强Docker容器的安全性有很多好处。通常，在服务器上会运行一堆特权进程，包括ssh、cron、syslogd、硬件管理工具模块（例如负载模块）、网络配置工具等。容器与这些进程是不同的，因为几乎所有的特权进程都由容器以外的支持系统来进行管理。例如：\n\n ssh访问由宿主主机上的ssh服务来管理；\n cron通常应该作为用户进程执行，权限交给使用它服务的应用来处理；\n 日志系统可由Docker或第三方服务管理；\n 硬件管理无关紧要，容器中也就无须执行udevd以及类似服务；\n 网络管理也都在主机上设置，除非特殊需求，容器不需要对网络进行配置。\n\n3. 大部分情况下，容器并不需要“真正的”root权限，容器只需要少数的能力即可。为了加强安全，容器可以禁用一些没必要的权限，包括：\n\n- 完全禁止任何文件挂载操作；\n- 禁止直接访问本地主机的套接字；\n- 禁止访问一些文件系统的操作，比如创建新的设备、修改文件属性等；\n- 禁止模块加载。\n\n 这样，就算攻击者在容器中取得了root权限，也不能获得本地主机的较高权限，能进行的破坏也有限。\n\n4. 默认情况下，Docker采用白名单机制，禁用了必需的一些能力之外的其他权限，目前支持CAP_CHOWN、CAP_DAC_OVERRIDE、CAP_FSETID、CAP_FOWNER、CAP_MKNOD、CAP_NET_RAW、CAP_SETGID、CAP_SETUID、CAP_SETFCAP、CAP_SETPCAP、CAP_NET_BIND_SERVICE、CAP_SYS_CHROOT、CAP_KILL、CAP_AUDIT_WRITE等。用户也可以根据自身需求为Docker容器启用额外的权限。\n\n', 2, 0, 0, '2021-01-08 20:57:17.741148', '2021-01-23 23:59:49.343597', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (144, 'Docker服务端的防护', '\n1. 使用Docker容器的核心是Docker服务端。Docker服务的运行目前还需要root权限的支持，因此服务端的安全性十分关键。\n1. 确保只有可信的用户才能访问到Docker服务。Docker允许用户在主机和容器间共享文件夹，同时不需要限制容器的访问权限，这就容易让容器突破资源限制。例如，恶意用户启动容器的时候将主机的根目录/映射到容器的/host目录中，那么容器理论上就可以对主机的文件系统进行任意修改了。因此，当提供容器创建服务时（例如通过一个Web服务器），要更加注意进行参数的安全检查，防止恶意用户用特定参数来创建一些破坏性的容器。\n1. 为了加强对服务端的保护，Docker的REST       API（客户端用来与服务端通信的接口）在0.5.2之后使用本地的Unix套接字机制替代了原先绑定在127.0.0.1上的TCP套接字，因为后者容易遭受跨站脚本攻击。现在用户使用Unix权限检查来加强套接字的访问安全。用户仍可以利用HTTP提供REST       API访问。建议使用安全机制，确保只有可信的网络或VPN网络，或证书保护机制（例如受保护的stunnel和ssl认证）下的访问可以进行。此外，还可以使用TLS证书来加强保护，可以进一步参考dockerd的tls相关参数。\n1. 最近改进的Linux命名空间机制将可以实现使用非root用户来运行全功能的容器。这将从根本上解决了容器和主机之间共享文件系统而引起的安全问题。目前，Docker自身改进安全防护的目标是实现以下两个重要安全特性：\n\n1. 将容器的root用户映射到本地主机上的非root用户，减轻容器和主机之间因权限提升而引起的安全问题；\n允许Docker服务端在非root权限下运行，利用安全可靠的子进程来代理执行需要特权权限的操作。这些子进程将只允许在限定范围内进行操作，例如仅仅负责虚拟网络设定或文件系统管理、配置操作等。', 1, 0, 0, '2021-01-08 20:57:56.129349', '2021-01-21 12:31:16.246654', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (145, '其他安全特性', '\n\n1. 除了默认启用的能力机制之外，还可以利用一些现有的安全软件或机制来增强Docker的安全性，例如GRSEC、AppArmor、SELinux等：\n\n 在内核中启用GRSEC和PAX，这将增加更多的编译和运行时的安全检查；并且通过地址随机化机制来避免恶意探测等。启用该特性不需要Docker进行任何配置；\n\n 使用一些增强安全特性的容器模板，比如带AppArmor的模板和RedHat带SELinux策略的模板。这些模板提供了额外的安全特性；\n\n 用户可以自定义更加严格的访问控制机制来定制安全策略。\n\n2. 此外，在将文件系统挂载到容器内部时候，可以通过配置只读（read-only）模式来避免容器内的应用通过文件系统破坏外部环境，特别是一些系统运行状态相关的目录，包括但不限于/proc/sys、/proc/irq、/proc/bus等。这样，容器内应用进程可以获取所需要的系统信息，但无法对它们进行修改。\n2. 同时，对于应用容器场景下，Docker内启动应用的用户都应为非特权用户（可以进一步禁用用户权限，如访问Shell），避免出现故障时对容器内其他资源造成损害。', 2, 0, 0, '2021-01-08 20:58:40.517629', '2021-01-08 21:12:54.233531', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (146, '安装Docker Machine', '[TOC]\n\n# 一、简介\n\n1. Docker      Machine是Docker官方三剑客项目之一，负责使用Docker容器的第一步：在多种平台上快速安装和维护Docker运行环境。它支持多种平台，让用户可以在很短时间内在本地或云环境中搭建一套Docker主机集群。\n1. 基本功能包括：\n\n 在指定节点或平台上安装Docker引擎，配置其为可使用的Docker环境；\n\n 集中管理（包括启动、查看等）所安装的Docker环境。\n\n3. Machine连接不同类型的操作平台是通过对应驱动来实现的，目前已经集成了包括AWS、IBM、Google，以及OpenStack、VirtualBox、vSphere等多种云平台的支持。\n\n# 二、安装Machine\n\n1. 环境准备\n\n| IP             | 主机名 | 角色            |\n| -------------- | ------ | --------------- |\n| 192.168.10.220 | admin  | Machine管理节点 |\n| 192.168.10.221 | host1  | host主机1       |\n| 192.168.10.222 | host2  | host主机2       |\n\n* 使用免密登录\n\n [root@admin ~]# ssh-keygen\n [root@admin ~]# ssh-copy-id 192.168.10.221\n [root@admin ~]# ssh-copy-id 192.168.10.222\n\n2. 安装docker-machine\n\n 官网文档\n\n [https://docs.docker.com/machine/install-machine/](https://docs.docker.com/machine/install-machine/)\n\n admin主机安装docker\n\n [root@admin ~]# base=https://github.com/docker/machine/releases/download/v0.16.0 && curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/tmp/docker-machine && mv /tmp/docker-machine /usr/local/bin/docker-machine && chmod +x /usr/local/bin/docker-machine\n\n 验证是否成功安装\n\n [root@admin ~]# docker-machine version\n\n4. 安装 bash completion      script，这样在 bash 能够通过 tab 键补全 docker-mahine 的子命令和参数\n\n 参考链接\n\n [https://github.com/docker/machine/tree/master/contrib/completion/bash](https://github.com/docker/machine/tree/master/contrib/completion/bash)\n[root@admin ~]# base=https://raw.githubusercontent.com/docker/machine/v0.16.0\n[root@admin ~]# for i in docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash\ndo\nwget \"$base/contrib/completion/bash/${i}\" -P /etc/bash_completion.d\ndone\n\n5. 修改环境变量\n\n [root@localhost ~]# vim ~/.bashrc\n\n 末尾添加如下内容\n\n PS1=\'[\\u@\\h \\W$(__docker_machine_ps1)]\\$\'\n\n6. 加载环境变量配置\n\n [root@localhost ~]# source /etc/bash_completion.d/docker-machine-prompt.bash\n\n# 三、创建Machine（在 host 上安装和部署docker）\n\n1. 验证确保admin主机能免密登录host1主机\n1. 为host1主机安装docker\n\n [root@admin ~]#docker-machine create --driver generic --generic-ip-address=192.168.10.221 host1\n\n 其他系统安装参考[https://docs.docker.com/machine/drivers/](https://docs.docker.com/machine/drivers/)\n\n --generic-ip-address 指定目标系统的 IP，并命名为 host1\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609419291952-43791705-01cf-450d-808d-7409c3720716.png#align=left&display=inline&height=385&margin=%5Bobject%20Object%5D&name=image.png&originHeight=385&originWidth=1323&size=51036&status=done&style=none&width=1323)\n① 通过 ssh 登录到远程主机。\n② 安装 docker。\n③ 拷贝证书。\n④ 配置 docker daemon。\n⑤ 启动 docker。\n\n 如果安装超时，执行命令重新安装\n\n [root@admin ~]# docker-machine rm host1\n\n3. 执行 docker-machine ls查看管理主机列表\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609419291986-b30fbd19-b8fd-49cd-8796-612eb9b81341.png#align=left&display=inline&height=83&margin=%5Bobject%20Object%5D&name=image.png&originHeight=83&originWidth=1270&size=11791&status=done&style=none&width=1270)\n\n- 登录到 host1 查看 docker daemon 的具体配置。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609419292082-169e607c-26a6-4a8f-883f-495b1d43fc9d.png#align=left&display=inline&height=245&margin=%5Bobject%20Object%5D&name=image.png&originHeight=245&originWidth=1020&size=25449&status=done&style=none&width=1020)\n-H tcp://0.0.0.0:2376 使 docker daemon 接受远程连接。\n--tls* 对远程连接启用安全认证和加密。\n\n- 主机名自动修改为host1\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609419292109-445caf7c-0b28-4c39-a778-6fedc78e2030.png#align=left&display=inline&height=54&margin=%5Bobject%20Object%5D&name=image.png&originHeight=54&originWidth=345&size=2844&status=done&style=none&width=345)', 2, 0, 0, '2021-01-08 21:09:19.655242', '2021-01-21 12:31:19.525006', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (147, '管理machine', '\n# 一、命令总结\n\n Machine提供了一系列的子命令，每个命令都带有一系列参数，可以通过如下命令查看具体用法：\n[root@admin ~]#docker-machine  <COMMAND> -h\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418843194-250a037d-8d41-4c72-9e3e-83e77811a9f6.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=image.png&originHeight=720&originWidth=913&size=248638&status=done&style=none&width=913)\n\n# 二、常用命令\n\n1. 切换到被管理主机\n\n [root@admin ~]#eval $(docker-machine env host1)\n\n2. 节点之间文件拷贝\n\n [root@admin ~]#docker-machine scp  host1:/tmp/a host2:/tmp/b\n\n3. 更新 machine 的 docker 到最新版本，可以批量执行\n\n [root@admin ~]#docker-machine upgrade host1 host2\n\n4. 查看 machine 的 docker daemon 配置\n\n [root@admin ~]#docker-machine config host1\n\n# 三、其他命令\n\n## 1. active——查看激活状态的主机\n\n- 默认为非激活状态\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842284-7953708d-c30f-4e2a-b0ab-048a9b556d2e.png#align=left&display=inline&height=115&margin=%5Bobject%20Object%5D&name=image.png&originHeight=115&originWidth=1147&size=16040&status=done&style=none&width=1147)\n\n- 查看主机信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842304-4f84d41b-a53b-4605-87e8-e0d44062ccf5.png#align=left&display=inline&height=189&margin=%5Bobject%20Object%5D&name=image.png&originHeight=189&originWidth=879&size=23956&status=done&style=none&width=879)\n\n- 激活主机\n\n [root@admin ~]#export DOCKER_HOST=\"tcp://192.168.10.221:2376\"\n\n- 查看激活状态的主机\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842305-04fb5b52-e63e-4c07-a361-33af9554afb9.png#align=left&display=inline&height=56&margin=%5Bobject%20Object%5D&name=image.png&originHeight=56&originWidth=512&size=3893&status=done&style=none&width=512)\n\n## 2. create\n\n 格式为docker-machine  create [OPTIONS] [arg...]。创建一个Docker主机环境。支持的选项包括：\n-driver, -d \"virtualbox\"：指定驱动类型；\n-engine-install-url \"[https://get.docker.com](https://get.docker.com)\"：配置Docker主机时的安装URL；\n-engine-opt option：以键值对格式指定所创建Docker引擎的参数；\n-engine-insecure-registry option：以键值对格式指定所创建Docker引擎允许访问的不支持认证的注册仓库服务；\n-engine-registry-mirror option：指定使用注册仓库镜像；\n-engine-label option：为所创建的Docker引擎添加标签；\n-engine-storage-driver：存储后端驱动类型；\n-engine-env option：指定环境变量；\n-swarm：配置Docker主机加入到Swarm集群中；\n-swarm-image \"swarm:latest\"：使用Swarm时候采用的镜像；\n-swarm-master：配置机器作为Swarm集群的master节点；\n-swarm-discovery:Swarm集群的服务发现机制参数；\n-swarm-strategy “spread”:Swarm默认调度策略；\n-swarm-opt option：任意传递给Swarm的参数；\n-swarm-host \"tcp://0.0.0.0:3376\"：指定地址将监听Swarm master节点请求；\n-swarm-addr：从指定地址发送广播加入Swarm集群服务。\n例如，通过如下命令可以创建一个Docker主机的虚拟机镜像：\n\n```bash\n[root@admin ~]#docker-machine create -d  virtualbox \\\n--engine-storage-driver overlay \\\n--engine-label name=testmachine \\\n--engine-label year=2018 \\\n--engine-opt dns=8.8.8.8 \\\n--engine-env  HTTP_PROXY=http://proxy.com:3128 \\\n--engine-insecure-registry  registry.private.com \\\nmydockermachine\n```\n\n所创建Docker主机虚拟机中的Docker引擎将：\n\n- 使用overlay类型的存储驱动；\n- 带有name=testmachine和year=2015两个标签；\n- 引擎采用8.8.8.8作为默认DNS；\n- 环境变量中指定HTTP代理服务[http://proxy.com:3128](http://proxy.com:3128)。\n- 允许使用不带验证的注册仓库服务registry.private.com。\n\n## 3. env\n\n格式为docker-machine  env [OPTIONS] [arg...]。\n显示连接到某个主机需要的环境变量。支持的选项包括：\n\n- -swarm：显示Swarm集群配置；\n- -shell：指定所面向的Shell环境，默认为当前自动探测；\n- -unset, -u：取消对应的环境变量；\n- -no-proxy：添加对象主机地址到NO_PROXY环境变量。\n\n例如，显示连接到host1主机所需要的环境变量：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842337-ea7da88c-7276-4b2b-b0fd-8d44727f3c04.png#align=left&display=inline&height=188&margin=%5Bobject%20Object%5D&name=image.png&originHeight=188&originWidth=877&size=23826&status=done&style=none&width=877)\n\n## 4. inspect\n\n格式为docker-machine  inspect [OPTIONS] [arg...]。\n以json格式输出指定Docker主机的详细信息。支持-format,  -f选项使用指定的Go模板格式化输出。例如：\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842338-d3407dc3-1c07-419d-8b60-6aef662fc6df.png#align=left&display=inline&height=483&margin=%5Bobject%20Object%5D&name=image.png&originHeight=483&originWidth=630&size=31914&status=done&style=none&width=630)\n\n## 5. ip\n\n获取指定Docker主机地址。例如，获取default主机的地址，可以用如下命令：\n![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609418842333-dc5ebee6-b350-4450-8609-4db10791c948.png#align=left&display=inline&height=54&margin=%5Bobject%20Object%5D&name=image.png&originHeight=54&originWidth=537&size=4780&status=done&style=none&width=537)\n\n## 6. kill\n\n- 直接杀死指定的Docker主机，指定Docker主机会强行停止。', 3, 0, 0, '2021-01-08 21:12:24.483639', '2021-01-22 12:47:00.014662', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (148, 'Docker Compose简介', '[TOC]\n\n# 一、简介\n\n1. Compose作为Docker官方编排工具，是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用，它可以让用户通过编写一个简单的模板文件，快速地创建和管理基于Docker容器的应用集群。\n1. Compose允许用户通过一个单独的      docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个服务栈。\n1. Compose 中的重要概念：\n\n 任务（task）：一个容器被称为一个任务，任务拥有独一无二的ID，在同一个服务中的多个任务序号依次递增。\n 服务      (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。\n 项目      (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。\n\n4. Compose      的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。\n4. Compose 项目由 Python      编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用      Compose 来进行编排管理。\n\n# 二、安装与卸载compose\n\n1. 二进制包安装\n\n [安装包地址](https://github.com/docker/compose/releases)\n\n [root@localhost ~]# curl -L [https://github.com/docker/compose/releases/download/1.25.5/docker-compose-`uname](https://github.com/docker/compose/releases/download/1.25.5/docker-compose-%60uname) -s`-`uname -m` -o /usr/local/bin/docker-compose\n [root@localhost ~]#chmod +x /usr/local/bin/docker-compose\n\n2. yum安装\n\n [root@localhost ~]# yum -y install docker-compose\n\n3. bash 补全命令\n\n [root@localhost ~]#curl -L [https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose](https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose)> /etc/bash_completion.d/docker-compose\n\n4. 卸载\n\n 如果是二进制包方式安装的，删除二进制文件即可。\n [root@localhost ~]#rm /usr/local/bin/docker-compose\n\n ', 8, 0, 0, '2021-01-08 22:17:08.993575', '2021-01-26 10:36:22.446286', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (149, 'compose模板文件', '[TOC]\n\n# 一、概述\n\nDocker Compose 的模板文件主要分为3 个区域\n\n- services服务：在它下面可以定义应用需要的一些服务，每个服务都有自己的名字、使用的镜像、挂载的数据卷、所属的网络、依赖哪些其他服务等等。\n- volumes数据卷：在它下面可以定义的数据卷（名字等等），然后挂载到不同的服务下去使用。\n- networks：应用的网络，在它下面可以定义应用的名字、使用的网络类型等等。\n\n# 二、常用模板文件主要命令\n\n| 命令             | 功能                                                         |\n| ---------------- | ------------------------------------------------------------ |\n| build            | 指定Dockerfile所在文件夹的路径                               |\n| cap_add,cap_drop | 指定容器的内核能力分配                                       |\n| command          | 覆盖容器启动后默认执行的命令。                               |\n| configs          | 仅用于 Swarm mode                                            |\n| cgroup_parent    | 指定父 cgroup 组，意味着将继承该组的资源限制。               |\n| container_name   | 指定容器名称。默认将会使用    项目名称_服务名称_序号 这样的格式。 |\n| domainname       | 指定容器中搜索域名                                           |\n| deploy           | 指定部署和运行时的容器相关配置，仅用于 Swarm mode            |\n| devices          | 指定设备映射关系。                                           |\n| depends_on       | 解决容器的依赖、启动先后的问题。                             |\n| dns              | 自定义 DNS 服务器。可以是一个值，也可以是一个列表。          |\n| dns_search       | 配置 DNS 搜索域。可以是一个值，也可以是一个列表。            |\n| dockerfile       | 指定额外的编译镜像的Dockerfile文件                           |\n| entrypoint       | 覆盖容器中默认的入口命令                                     |\n| env_file         | 从文件中获取环境变量，可以为单独的文件路径或列表。           |\n| environment      | 设置环境变量                                                 |\n| expose           | 暴露端口，但不映射到宿主机，只被连接的服务访问。仅可以指定内部端口为参数 |\n| extends          | 基于其他模板文件进行扩展                                     |\n| external_links   | 链接到 docker-compose.yml 外的容器，甚至并非 Compose 管理的外部容器。 |\n| extra_hosts      | 类似 Docker 中的   --add-host 参数，指定额外的 host 名称映射信息 |\n| hostname         | 指定容器中搜索主机名                                         |\n| healthcheck      | 通过命令检查容器是否健康运行。                               |\n| image            | 指定为镜像名称或镜像 ID。如果镜像在本地不存在，   Compose 将会尝试拉取这个镜像。 |\n| isolation        | 配置容器隔离的机制                                           |\n| labels           | 为容器添加 Docker 元数据（metadata） 信息。                  |\n| links            | 链接到其他服务中的容器                                       |\n| logging          | 配置日志选项。                                               |\n| mac_address      | 指定容器中搜索mac 地址                                       |\n| network_mode     | 设置网络模式。                                               |\n| networks         | 配置容器连接的网络。                                         |\n| pid              | 跟主机系统共享进程命名空间。                                 |\n| privileged       | 允许容器中运行一些特权命令。                                 |\n| ports            | 暴露端口信息。                                               |\n| read_only        | 以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。 |\n| restart          | 指定容器退出后的重启策略为始终重启                           |\n| secrets          | 存储敏感数据                                                 |\n| security_opt     | 指定容器模板标签（label） 机制的默认属性（用户、角色、类型、级别等） |\n| stop_signal      | 设置另一个信号来停止容器                                     |\n| stdin_open       | 打开标准输入，可以接受外部输入。                             |\n| sysctls          | 配置容器内核参数。                                           |\n| tty              | 模拟一个伪终端。                                             |\n| tmpfs            | 挂载一个 tmpfs 文件系统到容器。                              |\n| ulimits          | 指定容器的 ulimits 限制值。                                  |\n| user             | 指定容器中运行应用的用户名。                                 |\n| userns_mode      | 指定用户命名空间模式                                         |\n| volumes          | 数据卷所挂载路径设置                                         |\n| working_dir      | 指定容器中工作目录。                                         |\n\n1. build\n\n指定Dockerfile所在文件夹的路径（可以是绝对路径，或者相对docker-compose.yml文件的路径）。Compose将会利用它自动构建应用镜像，然后使用这个镜像，例如：\n\n```yaml\nservices:\n  app:\n  	build: /path/to/build/dir\n```\n\n- build指令还可以指定创建镜像的上下文、Dockerfile路径、标签、Shm大小、参数和缓存来源等，例如：\n\n```yaml\nservices:\n  app:\n    build:\n      context:  /path/to/build/dir\n      dockerfile: Dockerfile-app\n      labels:\n        version:  \"2.0\"\n        released:  \"true\"\n      shm_size: \'2gb\'\n      args:\n        key: value\n        name: myApp\n      cache_from:\n        - myApp:1.0\n```\n\n2. cap_add, cap_drop\n\n指定容器的内核能力（capacity）分配。\n\n```yaml\n# 例如，让容器拥有所有能力可以指定为：\ncap_add:\n	- ALL\n# 去掉NET_ADMIN能力可以指定为：\ncap_drop:\n	- NET_ADMIN\n```\n\n3. command\n\n 覆盖容器启动后默认执行的命令，可以为字符串格式或JSON数组格式。 例如：\ncommand: echo \"hello world\"\n\n 或者：\n\n  command: [\"bash\",  \"-c\", \"echo\", \"hello world\"]\n\n4. configs\n\n 在Docker  Swarm模式下，可以通过configs来管理和访问非敏感的配置信息。支持从文件读取或外部读取。例如：\n\n```yaml\nservices:\n  app:\n    image: myApp:1.0\n    deploy:\n      replicas: 1\n    configs:\n      - file_config\n      - external_config\nconfigs:\n  file_config:\n    file: ./config_file.cfg\n  external_config:\n    external: true\n```\n\n5. cgroup_parent\n\n 指定父cgroup组，意味着将继承该组的资源限制。目前不支持在Swarm模式中使用。例如，创建了一个cgroup组名称为cgroups_1：\n cgroup_parent: cgroups_1\n\n6. container_name\n\n 指定容器名称。默认将会使用“项目名称_服务名称_序号”这样的格式。目前不支持在Swarm模式中使用。例如：\n container_name: docker-web-container\n\n- 需要注意，指定容器名称后，该服务将无法进行扩展，因为Docker不允许多个容器实例重名。\n\n7. devices\n\n 指定设备映射关系，不支持Swarm模式。例如：\n\n```yaml\ndevices:\n	-  \"/dev/ttyUSB1:/dev/ttyUSB0\"\n```\n\n8. depends_on\n\n 指定多个服务之间的依赖关系。启动时，会先启动被依赖服务。例如，可以指定依赖于db服务：\n depends_on: db\n\n9. dns\n\n 自定义DNS服务器。可以是一个值，也可以是一个列表。例如：\n\n```yaml\ndns: 8.8.8.8\ndns:\n  -8.8.8.8\n  -9.9.9.9\n```\n\n10. dns_search\n\n 配置DNS搜索域。可以是一个值，也可以是一个列表。例如：\n\n```yaml\ndns_search: example.com\ndns_search:\n  - domain1.example.com\n  - domain2.example.com\n```\n\n11. dockerfile\n\n 如果需要，指定额外的编译镜像的Dockefile文件，可以通过该指令来指定。例如：\n dockerfile: Dockerfile-alternate\n 该指令不能跟image同时使用，否则Compose将不知道根据哪个指令来生成最终的服务镜像。\n\n12. entrypoint\n\n 覆盖容器中默认的入口命令。注意，也会取消掉镜像中指定的入口命令和默认启动命令。例如，覆盖为新的入口命令：\n entrypoint: python app.py\n\n13. env_file\n\n 从文件中获取环境变量，可以为单独的文件路径或列表。如果通过docker-compose-f  FILE方式来指定Compose模板文件，则env_file中变量的路径会基于模板文件路径。如果有变量名称与environment指令冲突，则按照惯例，以后者为准。例如：\n\n```yaml\nenv_file: .env\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n```\n\n- 环境变量文件中每一行必须符合格式，支持#开头的注释行，例如：\n\n common.env: Set development  environment\n\n PROG_ENV=development\n\n14. environment\n\n- 设置环境变量，可以使用数组或字典两种格式。只给定名称的变量会自动获取运行Compose主机上对应变量的值，可以用来防止泄露不必要的数据。\n\n```yaml\n# 例如\nenvironment:\n  RACK_ENV: development\n  SESSION_SECRET:\n# 或者：\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n```\n\n15. expose\n\n 暴露端口，但不映射到宿主机，只被连接的服务访问。仅可以指定内部端口为参数，如下所示：\n\n```yaml\nexpose:\n  - \"3000\"\n  - \"8000\"\n```\n\n16. extends\n\n 基于其他模板文件进行扩展。例如，我们已经有了一个webapp服务，定义一个基础模板文件为common.yml，如下所示：\n\n```yaml\n# common.yml\nwebapp:\n  build: ./webapp\n  environment:\n    - DEBUG=false\n    - SEND_EMAILS=false\n# 再编写一个新的development.yml文件，使用common.yml中的webapp服务进行扩展：\n# development.yml\nweb:\n  extends:\n    file: common.yml\n    service: webapp\n  ports:\n  	- \"8000:8000\"\n  links:\n  	- db\n  environment:\n  	- DEBUG=true\ndb:\n	image: postgres\n```\n\n- 后者会自动继承common.yml中的webapp服务及环境变量定义。\n\n17. external_links\n\n 链接到docker-compose.yml外部的容器，甚至并非Compose管理的外部容器。参数格式跟links类似。\n\n```yaml\nexternal_links:\n  - redis_1\n  - project_db_1:mysql\n  - project_db_1:postgresql\n```\n\n18. extra_hosts\n\n 类似Docker中的--add-host参数，指定额外的host名称映射信息。\n\n```yaml\nextra_hosts:\n        - \"googledns:8.8.8.8\"\n        - \"dockerhub:52.1.157.61\"\n# 会在启动后的服务容器中/etc/hosts文件中添加如下两条条目。\n8.8.8.8 googledns\n52.1.157.61 dockerhub\n```\n\n19. healthcheck\n\n- 指定检测应用健康状态的机制，包括检测方法（test）、间隔（interval）、超时（timeout）、重试次数（retries）、启动等待时间（start_period）等。\n- 例如，指定检测方法为访问8080端口，间隔为30秒，超时为15秒，重试3次，启动后等待30秒再做检查。\n\n```yaml\nhealthcheck:\n  test: [\"CMD\",  \"curl\", \"-f\", \"http://localhost:8080\"]\n  interval: 30s\n  timeout: 15s\n  retries: 3\n  start_period: 30s\n```\n\n20. image\n\n 指定为镜像名称或镜像ID。如果镜像在本地不存在，Compose将会尝试拉去这个镜像。\n\n```yaml\nimage: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n```\n\n21. isolation\n\n 配置容器隔离的机制，包括default、process和hyperv。\n\n22. labels\n\n 为容器添加Docker元数据（metadata）信息。例如可以为容器添加辅助说明信息。\n\n```yaml\nlabels:\n  com.startupteam.description:  \"webapp for a startup team\"\n  com.startupteam.department:  \"devops department\"\n  com.startupteam.release: \"rc3 for  v1.0\"\n```\n\n23. links\n\n 注意：links命令属于旧的用法，可能在后续版本中被移除。\n 链接到其他服务中的容器。使用服务名称（同时作为别名）或服务名称：服务别名（SERVICE:ALIAS）格式都可以。\n\n```yaml\nlinks:\n  - db\n  - db:database\n  - redis\n# 使用的别名将会自动在服务容器中的/etc/hosts里创建。例如：\n172.17.2.186   db\n172.17.2.186   database\n172.17.2.187   redis\n# 被链接容器中相应的环境变量也将被创建。\n```\n\n24. logging\n\n 跟日志相关的配置，包括一系列子配置。\n\n- logging.driver：类似于Docker中的--log-driver参数，指定日志驱动类型。目前支持三种日志驱动类型：\n\n driver: \"json-file\"\n driver: \"syslog\"\n driver: \"none\"\n\n- logging.options：日志驱动的相关参数。例如：\n\n```yaml\nlogging:\n  driver: \"syslog\"\n  options:\n  	syslog-address:  \"tcp://192.168.0.42:123\"\n或：\nlogging:\n  driver: \"json-file\"\n  options:\n    max-size: \"1000k\"\n    max-file: \"20\"\n```\n\n25. network_mode\n\n 设置网络模式。使用和docker  client的--net参数一样的值。\n network_mode: \"none\"\n network_mode: \"bridge\"\n network_mode: \"host\"\n network_mode: \"service:[service  name]\"\n network_mode: \"container:[name or  id]\"\n\n26. networks\n\n 所加入的网络。需要在顶级的networks字段中定义具体的网络信息。\n\n- 例如，指定web服务的网络为web_net，并添加服务在网络中别名为web_app。\n\n```yaml\nservices:\n  web:\n    networks:\n      web_net：\n        aliases: web_app\n      ipv4_address: 172.16.0.10\nnetworks:\n  web_net:\n    driver: bridge\n    enable_ipv6: true\n    ipam:\n      driver: default\n      config:\n        subnet: 172.16.0.0/24\n```\n\n27. pid\n\n 跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程ID来相互访问和操作。\n pid: \"host\"\n\n28. ports\n\n 暴露端口信息。\n 使用宿主：容器（HOST:CONTAINER）格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\n\n```yaml\nports:\n  - \"3000\"\n  - \"8000:8000\"\n  - \"49100:22\"\n  - \"127.0.0.1:8001:8001\"\n 或者：\nports:\n  - target: 80\n    published: 8080\n    protocol: tcp\n    mode: ingress\n```\n\n29. secrets\n\n 配置应用的秘密数据。\n 可以指定来源秘密、挂载后名称、权限等。\n\n```yaml\nservices:\n  web:\n    image: webapp:stable\n    deploy:\n      replicas: 2\n    secrets:\n      - source: web_secret\n        target: web_secret\n        uid: \'103\'\n        gid: \'103\'\n        mode: 0444\nsecrets:\n  web_secret:\n    file: ./web_secret.txt\n```\n\n30. security_opt\n\n 指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。\n 例如，配置标签的用户名和角色名：\n\n```yaml\nsecurity_opt:\n  - label:user:USER\n  - label:role:ROLE\n```\n\n31. stop_grace_period\n\n 指定应用停止时，容器的优雅停止期限。过期后则通过SIGKILL强制退出。默认值为10s。\n\n32. stop_signal\n\n 指定停止容器的信号，默认为SIGTERM。\n\n33. sysctls\n\n 配置容器内的内核参数。Swarm模式中不支持。\n例如，指定连接数为4096和开启TCP的syncookies：\n\n```yaml\nsysctls:\n  net.core.somaxconn: 4096\n  net.ipv4.tcp_syncookies: 1\n```\n\n34. ulimits\n\n 指定容器的ulimits限制值。\n 例如，指定最大进程数为65535，指定文件句柄数为20000（软限制，应用可以随时修改，不能超过硬限制）和40000（系统硬限制，只能root用户提高）。\n\n```yaml\nulimits:\n  nproc: 65535\n  nofile:\n    soft: 20000\n    hard: 40000\n```\n\n35. userns_mode\n\n 指定用户命名空间模式。Swarm模式中不支持。例如，使用主机上的用户命名空间：\nuserns_mode: \"host\"\n\n36. volumes\n\n 数据卷所挂载路径设置。可以设置宿主机路径（HOST:CONTAINER）或加上访问模式（HOST:CONTAINER:ro）。\n支持driver、driver_opts、external、labels、name等子配置。\n\n```yaml\n# 该指令中路径支持相对路径。例如\nvolumes:\n  - /var/lib/mysql\n  - cache/:/tmp/cache\n  - ~/configs:/etc/configs/:ro\n# 或者可以使用更详细的语法格式：\nvolumes:\n  - type: volume\n    source: mydata\n    target: /data\n    volume:\n    	nocopy: true\nvolumes:\n	mydata:\n```\n\n37. restart\n\n 指定重启策略，可以为no（不重启）、always（总是）、on-failure（失败时）、unless-stopped（除非停止）。\n注意Swarm模式下要使用restart_policy。在生产环境中推荐配置为always或者unless-stopped。\n  例如，配置除非停止：\n restart: unless-stopped\n\n38. deploy\n\n 指定部署和运行时的容器相关配置。该命令只在Swarm模式下生效，且只支持docker  stack deploy命令部署。\n\n```yaml\nversion: \'3\'\n  services:\n    redis:\n      image: web:stable\n      deploy:\n        replicas: 3\n        update_config:\n          parallelism: 2\n          delay: 10s\n        restart_policy:\n          condition: on-failure\n```\n\n deploy命令中包括endpoint_mode、labels、mode、placement、replicas、resources、restart_policy、update_config等配置项。\n\n- endpoint_mode\n\n 指定服务端点模式。包括两种类型：\nvip:Swarm分配一个前端的虚拟地址，客户端通过给地址访问服务，而无须关心后端的应用容器个数；\ndnsrr:Swarm分配一个域名给服务，用户访问域名时候回按照轮流顺序返回容器地址。\n\n```yaml\nversion: \'3\'\n  services:\n    redis:\n      image: web:stable\n      deploy:\n        mode: replicated\n        replicas: 3\n        endpoint_mode: vip\n```\n\n- labels\n\n 指定服务的标签。注意标签信息不会影响到服务内的容器。\n\n```yaml\nversion: \"3\"\n  services:\n    web:\n      image: web:stable\n      deploy:\n        labels:\n          description: \"This is  a web application service.\"\n```\n\n- mode\n\n 定义容器副本模式，可以为：\nglobal：每个Swarm节点上只有一个该应用容器；\nreplicated：整个集群中存在指定份数的应用容器副本，默认值。\n例如，指定集群中web应用保持3个副本：\n\n```yaml\nversion: \"3\"\n  services:\n    web:\n      image: web:stable\n      deploy:\n        mode: replicated\n        replicas: 3\n```\n\n- placement\n\n 定义容器放置的限制（constraints）和配置（preferences）。限制可以指定只有符合要求的节点上才能运行该应用容器；配置可以指定容器的分配策略。例如，指定集群中web应用容器只存在于高安全的节点上，并且在带有zone标签的节点上均匀分配。\n\n```yaml\nversion: \'3\'\n  services:\n    db:\n      image: web:stable\n      deploy:\n        placement:\n          constraints:\n            -  node.labels.security==high\n          preferences:\n            - spread:  node.labels.zone\n```\n\n- replicas\n\n 容器副本模式为默认的replicated时，指定副本的个数。\n\n- resources\n\n 指定使用资源的限制，包括CPU、内存资源等。例如，指定应用使用的CPU份额为10%～25%，内存为200  MB到500 MB。\n\n```yaml\nversion: \'3\'\n  services:\n    redis:\n      image: web:stable\n      deploy:\n        resources:\n          limits:\n            cpus: \'0.25\'\n            memory: 500M\n          reservations:\n            cpus: \'0.10\'\n            memory: 200M\n```\n\n- restart_policy\n\n 指定容器重启的策略。例如，指定重启策略为失败时重启，等待2s，重启最多尝试3次，检测状态的等待时间为10s。\n\n```yaml\nversion: \"3\"\n  services:\n    redis:\n      image: web:stable\n      deploy:\n        restart_policy:\n          condition: on-failure\n          delay: 2s\n          max_attempts: 3\n          window: 10s\n```\n\n- update_config\n\n 有些时候需要对容器内容进行更新，可以使用该配置指定升级的行为。包括每次升级多少个容器（parallelism）、升级的延迟（delay）、升级失败后的行动（failure_action）、检测升级后状态的等待时间（monitor）、升级后容忍的最大失败比例（max_failure_ratio）、升级顺序（order）等。例如，指定每次更新两个容器、更新等待10s、先停止旧容器再升级。\n\n```yaml\nversion: \"3.4\"\n  services:\n    redis:\n      image: web:stable\n      deploy:\n        replicas: 2\n        update_config:\n          parallelism: 2\n          delay: 10s\n          order: stop-first\n```\n\n\n# 三、其他指令\n\ndomainname、hostname、ipc、mac_address、privileged、read_only、shm_size、stdin_open、tty、user、working_dir等指令，基本跟docker-run中对应参数的功能一致。\n\n- 指定容器中工作目录：\n\n working_dir: /code\n\n- 指定容器中搜索域名、主机名、mac地址等：\n\n domainname: your_website.com    \n hostname: test\n mac_address: 08-00-27-00-0C-0A\n\n- 允许容器中运行一些特权命令：\n\n privileged: true', 3, 0, 0, '2021-01-08 22:24:23.654908', '2021-01-21 12:30:44.205232', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (150, 'compose命令', '\n\n1. 对于Compose来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。\n1. 执行docker-compose       [COMMAND] --help或者docker-compose help [COMMAND]可以查看具体某个命令的使用格式。\n1. Compose命令的基本的使用格式是：\n\n docker-compose  [-f=<arg>...] [options] [COMMAND] [ARGS...]\n\n4. 命令选项：\n\n| -f, --file    FILE           | 指定使用的Compose模板文件，默认为docker-compose.yml，可以多次指定； |\n| ---------------------------- | ------------------------------------------------------------ |\n| -p,    --project-name NAME   | 指定项目名称，默认将使用所在目录名称作为项目名；             |\n| --verbose                    | 输出更多调试信息；                                           |\n| -v,    --version             | 打印版本并退出；                                             |\n| -H, -host    HOST            | 指定所操作的Docker服务地址；                                 |\n| -tls                         | 启用TLS，如果指定-tlsverify则默认开启；                      |\n| -tlscacert    CA_PATH        | 信任的TLS CA的证书；                                         |\n| -tlscert    CLIENT_CERT_PATH | 客户端使用的TLS证书；                                        |\n| -tlskey    TLS_KEY_PATH      | TLS的私钥文件路径；                                          |\n| -tlsverify                   | 使用TLS校验连接对方；                                        |\n| -skip-hostname-check         | 不使用TLS证书校验对方的主机名；                              |\n| -project-directory    PATH   | 指定工作目录，默认为Compose文件所在路径。                    |\n\n5. 命令列表\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1609425604233-323ce38e-f405-4c43-bc25-c306a3f2aac5.png#align=left&display=inline&height=717&margin=%5Bobject%20Object%5D&name=image.png&originHeight=717&originWidth=699&size=193922&status=done&style=none&width=699)\n\n6. 使用说明\n\n- build\n\n 格式为docker-compose  build [options] [SERVICE...]。\n 构建（重新构建）项目中的服务容器。\n 服务容器一旦构建后，将会带上一个标记名，例如对于Web项目中的一个db容器，可能是web_db。\n 可以随时在项目目录下运行docker-compose  build来重新构建服务。\n 选项包括：\n --force-rm：强制删除构建过程中的临时容器；\n --no-cache：构建镜像过程中不使用cache（这将加长构建过程）；\n --pull：始终尝试通过pull来获取更新版本的镜像；\n -m, -memory MEM：指定创建服务所使用的内存限制；\n -build-arg key=val：指定服务创建时的参数。\n\n- bundle\n\n 格式为docker-compose  bundle [options]。\n 创建一个可分发（Distributed  Application Bundle, DAB）的配置包，包括整个服务栈的所有数据，他人可以利用该 文件启动服务栈。\n 支持选项包括：\n ❑ -push-images：自动推送镜像到仓库；\n ❑ -o, -output PATH：配置包的导出路径。\n\n- config\n\n 格式为docker-compose  config [options]。\n 校验和查看Compose文件的配置信息。\n 支持选项包括：\n ❑ -resolve-image-digests：为镜像添加对应的摘要信息；\n ❑ -q, -quiet：只检验格式正确与否，不输出内容；\n ❑ -services：打印出Compose中所有的服务信息；\n ❑ -volumes：打印出Compose中所有的挂载卷信息；\n\n- down\n\n 格式为docker-compose  down [options]。\n 停止服务栈，并删除相关资源，包括容器、挂载卷、网络、创建镜像等。\n 默认情况下只清除所创建的容器和网络资源。\n 支持选项包括：\n ❑ -rmi type：指定删除镜像的类型，包括all（所有镜像）, local（仅本地）；\n ❑ -v, -volumes：删除挂载数据卷；\n ❑ -remove-orphans：清除孤儿容器，即未在Compose服务中定义的容器；\n ❑ -t, -timeout TIMEOUT：指定超时时间，默认为10s。\n\n- events\n\n 格式为docker-compose  events [options] [SERVICE...]。\n 实时监控容器的事件信息。\n 支持选项包括-json：以Json对象流格式输出事件信息。\n\n- exec\n\n 格式为docker-compose  exec [options] [-e KEY=VAL...] SERVICE COMMAND [ARGS...]。\n 在一个运行中的容器内执行给定命令。\n 支持选项包括：\n ❑ -d：在后台运行命令；\n ❑ -privileged：以特权角色运行命令；\n ❑ -u, -user USER：以给定用户身份运行命令；\n ❑ -T：不分配TTY伪终端，默认情况下会打开；\n ❑ -index=index：当服务有多个容器实例时指定容器索引，默认为第一个；\n ❑ -e, -env KEY=VAL：设置环境变量。\n\n- help\n\n 获得一个命令的帮助。\n\n- images\n\n 格式为docker-compose  images [options] [SERVICE...]。\n 列出服务所创建的镜像。\n 支持选项为：-q：仅显示镜像的ID。\n\n- kill\n\n 格式为docker-compose  kill [options] [SERVICE...]。\n 通过发送SIGKILL信号来强制停止服务容器。\n 支持通过-s参数来指定发送的信号，例如通过如下指令发送SIGINT信号。\n $ docker-compose kill -s SIGINT\n\n- logs\n\n 格式为docker-compose  logs [options] [SERVICE...]。\n 查看服务容器的输出。默认情况下，docker-compose将对不同的服务输出使用不同的颜色来区分。可以通过--no- color来关闭颜色。\n 该命令在调试问题的时候十分有用。\n 支持选项为：\n ❑ -no-color：关闭彩色输出；\n ❑ -f, -follow：持续跟踪输出日志消息；\n ❑ -t, -timestamps：显示时间戳信息；\n ❑ -tail=\"all\"：仅显示指定行数的最新日志消息。\n\n- pause\n\n 格式为docker-compose  pause [SERVICE...]。\n 暂停一个服务容器。\n\n- port\n\n 格式为docker-compose  port [options] SERVICE PRIVATE_PORT。\n 打印某个容器端口所映射的公共端口。\n 选项：\n ❑ --protocol=proto：指定端口协议，tcp（默认值）或者udp；\n ❑ --index=index：如果同一服务存在多个容器，指定命令对象容器的序号（默认为1）。\n\n- ps\n\n 格式为docker-compose  ps [options] [SERVICE...]。\n 列出项目中目前的所有容器。\n 选项包括-q：只打印容器的ID信息。\n\n- pull\n\n 格式为docker-compose  pull [options] [SERVICE...]。\n 拉取服务依赖的镜像。\n 选项包括--ignore-pull-failures：忽略拉取镜像过程中的错误。\n\n- push\n\n 格式为docker-compose  push [options] [SERVICE...]。\n 推送服务创建的镜像到镜像仓库。\n 选项包括--ignore-push-failures：忽略推送镜像过程中的错误。\n\n- restart\n\n 格式为docker-compose  restart [options] [SERVICE...]。\n 重启项目中的服务。\n 选项包括-t,  --timeout TIMEOUT：指定重启前停止容器的超时（默认为10秒）。\n\n- rm\n\n 格式为docker-compose  rm [options] [SERVICE...]。\n 删除所有（停止状态的）服务容器。推荐先执行docker-compose  stop命令来停止容器。\n 选项：\n ❑ -f, --force：强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。\n ❑ -v：删除容器所挂载的数据卷。\n\n- run\n\n 格式为docker-compose  run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]。\n 在指定服务上执行一个命令。\n 例如：\n    $ docker-compose run ubuntu ping  docker.com\n 将会启动一个ubuntu服务容器，并执行ping  docker.com命令。\n 默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n 该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。\n 两个不同点：\n ❑ 给定命令将会覆盖原有的自动运行命令；\n ❑ 会自动创建端口，以避免冲突。\n 如果不希望自动启动关联的容器，可以使用--no-deps选项，例如\n $  docker-compose run --no-deps web python manage.py shell\n 将不会启动web容器所关联的其他容器。\n 选项：\n ❑ -d：后台运行容器；\n ❑ --name NAME：为容器指定一个名字；\n ❑ --entrypoint CMD：覆盖默认的容器启动指令；\n ❑ -e KEY=VAL：设置环境变量值，可多次使用选项来设置多个环境变量；\n ❑ -u, --user=\"\"：指定运行容器的用户名或者uid；\n ❑ --no-deps：不自动启动关联的服务容器；\n ❑ --rm：运行命令后自动删除容器，d模式下将忽略；\n ❑ -p, --publish=[]：映射容器端口到本地主机；\n ❑ --service-ports：配置服务端口并映射到本地主机；\n ❑ -T：不分配伪tty，意味着依赖tty的指令将无法运行。\n\n- scale\n\n 格式为docker-compose  scale [options] [SERVICE=NUM...]。\n 设置指定服务运行的容器个数。\n 通过service=num的参数来设置数量。例如：\n    $ docker-compose scale web=3 db=2\n 将启动3个容器运行web服务，2个容器运行db服务。\n 一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。\n 选项包括-t,  --timeout TIMEOUT：停止容器时候的超时（默认为10秒）。\n\n- start\n\n 格式为docker-compose  start [SERVICE...]。\n 启动已经存在的服务容器。\n\n-  stop\n\n 格式为docker-compose  stop [options] [SERVICE...]。\n 停止已经处于运行状态的容器，但不删除它。通过docker-compose  start可以再次启动这些容器。\n 选项包括-t,  --timeout TIMEOUT：停止容器时候的超时（默认为10秒）。\n\n- top\n\n 格式为docker-compose  top [SERVICE...]。\n 显示服务栈中正在运行的进程信息。\n\n- unpause\n\n 格式为docker-compose  unpause [SERVICE...]。\n 恢复处于暂停状态中的服务。\n\n- up\n\n 格式为docker-compose  up [options] [SERVICE...]。\n 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。\n 链接的服务都将会被自动启动，除非已经处于运行状态。\n 可以说，大部分时候都可以直接通过该命令来启动一个项目。\n 默认情况，docker-compose  up启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。\n 当通过Ctrl-C停止命令时，所有容器将会停止。\n 如果使用docker-compose  up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。\n默认情况，如果服务容器已经存在，docker-compose  up将会尝试停止容器，然后重新创建（保持使用volumes-from挂载的卷），以保证新启动的服务匹配docker-compose.yml文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用docker-compose  up --no-recreate。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用docker-compose  up--no-deps -d <SERVICE_NAME>来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。\n选项：\n ❑ -d：在后台运行服务容器；\n ❑ --no-color：不使用颜色来区分不同的服务的控制台输出；\n ❑ --no-deps：不启动服务所链接的容器；\n ❑ --force-recreate：强制重新创建容器，不能与--no-recreate同时使用；\n ❑ --no-recreate：如果容器已经存在了，则不重新创建，不能与--force-recreate同\n时使用；\n ❑ --no-build：不自动构建缺失的服务镜像；\n ❑ --abort-on-container-exit：当有容器停止时中止整个服务，与-d选项冲突。\n ❑ -t, --timeout TIMEOUT：停止容器时候的超时（默认为10秒），与-d选项冲突；\n ❑ --remove-orphans：删除服务中未定义的孤儿容器；\n ❑ --exit-code-from SERVICE：退出时返回指定服务容器的退出符；\n ❑ --scale SERVICE=NUM：扩展指定服务实例到指定数目。\n\n- version\n\n 格式为docker-compose  version。\n 打印版本信息。\n\n', 3, 0, 0, '2021-01-08 22:32:09.295616', '2021-01-21 12:31:07.833142', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (151, 'compose环境变量', '\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610077867388-48833903-5443-421c-9310-84a4e4238ecf.png#align=left&display=inline&height=711&margin=%5Bobject%20Object%5D&name=image.png&originHeight=711&originWidth=844&size=331780&status=done&style=none&width=844)\n\n- 以DOCKER_开头的变量和用来配置Docker命令行客户端的使用一样。', 5, 0, 0, '2021-01-08 22:32:40.171501', '2021-01-22 23:37:18.237466', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (152, 'compose案例', '[TOC]\n\n# 一、wordpress安装\n\n```bash\n[root@docker ~]# mkdir wordpress\n[root@docker ~]# cd wordpress\n[root@docker ~]# vim docker-compose.yml\n```\n\n- docker-compose.yml文件内容如下\n\n```yaml\nversion: \"3\"\nservices:\n  db:\n    image: 10.30.12.55/docker/mysql:5.6\n    volumes:\n      - /var/lib/mysql:/var/lib/mysql\n    #自动重启\n    restart: always\n    environment:\n      #指定MySQL的root账号初始密码\n      MYSQL_ROOT_PASSWORD: 123456\n      #指定容器启动后要创建的数据库\n      MYSQL_DATABASE: wordpress\n      #指定容器在启动后要创建的普通用户账号，该账号有远程登录的权限\n      MYSQL_USER: tom\n      #为用户设置密码\n      MYSQL_PASSWORD: 123456\n  wordpress:\n    depends_on:\n      - db\n    image: 10.30.12.55/docker/wordpress\n    ports:\n      - \"8000:80\"\n    restart: always\n    environment:\n      #指定要使用的数据库名\n      WORDPRESS_DB_NAME: wordpress\n      #指定要MySQL容器的ip和端口\n      WORDPRESS_DB_HOST: db:3306\n      #指定登录MySQL的账号\n      WORDPRESS_DB_USER: tom\n      #指定登录MySQL的密码\n      WORDPRESS_DB_PASSWORD: 123456\n```\n\n- 构建并运行项目\n\n`[root@docker ~]# docker-compose up -d` \n\n# 二、web负载均衡\n\n1. 目录结构\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610077928620-0cbf07a0-8018-40f0-9d07-2ae888e1e4ee.png#align=left&display=inline&height=301&margin=%5Bobject%20Object%5D&name=image.png&originHeight=301&originWidth=342&size=11518&status=done&style=none&width=342)\n\n2. 各个文件内容\n\n- docker-compose.yml\n\n```yaml\nweb1:\n    build: ./httpd\n    expose: \n       - 80\nweb2:\n    image: nginx:latest\n    volumes:\n        - ./nginx1:/usr/share/nginx/html\n    expose:\n        - 80\nweb3:\n    image: nginx\n    volumes:\n        - ./nginx2:/usr/share/nginx/html\nhaproxy:\n    image: haproxy\n    volumes:\n        - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro\n    links: \n        - web1\n        - web2\n        - web3\n    ports:\n       - \"8888:80\"\n    expose:\n        - 80\n```\n\n- haproxy.cfg\n\n```yaml\nglobal\n    log 127.0.0.1 local0\n    log 127.0.0.1 local1 notice\ndefaults\n    log global\n    mode http\n    option httplog\n    option dontlognull\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n    stats uri /status\nfrontend balancer\n    bind 0.0.0.0:80\n    mode http\n    default_backend web_backends\nbackend web_backends\n    mode http\n    option forwardfor\n    balance roundrobin\n    server weba web1:80 check\n    server webb web2:80 check\n    server webc web3:80 check\n    option httpchk GET /\n    http-check expect status 200\n```\n\n- Dockerfile\n\n```dockerfile\nFROM centos\nEXPOSE 80\nRUN rm -rf /etc/yum.repos.d/*\nCOPY docker.repo /etc/yum.repos.d/\nRUN yum install -y httpd\nADD index.html /var/www/html\nCMD [\"/usr/sbin/httpd\",\"-D\",\"FOREGROUND\"]\n```\n\n- index.html\n\n test\n\n3. 执行docker-compose up构建镜像\n\n `[root@docker haproxy]# docker-compose up -d` \n\n4. 查看docker进程信息\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610077928678-2225a24c-9d2c-4ad7-8362-9c5256bf0849.png#align=left&display=inline&height=254&margin=%5Bobject%20Object%5D&name=image.png&originHeight=254&originWidth=884&size=31164&status=done&style=none&width=884)\n\n5. 浏览器访问测试\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610077928698-22a6c5c9-bf84-42ec-82bc-01727771e6d7.png#align=left&display=inline&height=80&margin=%5Bobject%20Object%5D&name=image.png&originHeight=80&originWidth=368&size=4978&status=done&style=none&width=368)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610077928723-66b1020b-d3c9-435b-9f16-10f1a70205f0.png#align=left&display=inline&height=495&margin=%5Bobject%20Object%5D&name=image.png&originHeight=495&originWidth=568&size=53293&status=done&style=none&width=568)\n\n\n\n', 3, 0, 0, '2021-01-08 22:33:44.501981', '2021-01-21 12:30:29.371396', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (153, 'Docker Swarm简介', '\n\n# 一、简介\n\n1. Docker Swarm提供Docker容器集群服务，是Docker官方对容器云生态进行支持的核心方案。使用它，用户可以将多个Docker主机抽象为大规模的虚拟Docker服务，快速打造一套容器云平台。\n1. 作为容器集群管理器，Swarm最大的优势之一就是原生支持Docker      API，给用户使用带来极大的便利。各种基于标准API的工具比如Compose、Docker      SDK、各种管理软件，甚至Docker本身等都可以很容易的与Swarm进行集成。这大大方便了用户将原先基于单节点的系统移植到Swarm上。同时Swarm内置了对Docker网络插件的支持，用户可以很容易地部署跨主机的容器集群服务。\n1. Swarm也采用了典型的“主从”结构，通过Raft协议来在多个管理节点（Manager）中实现共识。工作节点（Worker）上运行agent接受管理节点的统一管理和任务分配。用户提交服务请求只需要发给管理节点即可，管理节点会按照调度策略在集群中分配节点来运行服务相关的任务。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078364142-76122cec-c37d-4d30-b45c-35c24ebce587.png#align=left&display=inline&height=563&margin=%5Bobject%20Object%5D&name=image.png&originHeight=563&originWidth=1283&size=309731&status=done&style=none&width=1283)\n\n4. 在Swarm      V2中，集群中会自动通过Raft协议分布式选举出Manager节点，无须额外的发现服务支持，避免了单点瓶颈。同时，V2中内置了基于DNS的负载均衡和对外部负载均衡机制的集成支持。\n\n# 二、Swarm 中重要的概念\n\n1. Swarm集群\n\n Swarm集群（Cluster）为一组被统一管理起来的Docker主机。集群是Swarm所管理的对象。这些主机通过Docker引擎的Swarm模式相互沟通，其中部分主机可能作为管理节点（manager）响应外部的管理请求，其他主机作为工作节点（worker）来实际运行Docker容器。当然，同一个主机也可以即作为管理节点，同时作为工作节点。\n当用户使用Swarm集群时，首先定义一个服务（指定状态、复制个数、网络、存储、暴露端口等），然后通过管理节点发出启动服务的指令，管理节点随后会按照指定的服务规则进行调度，在集群中启动起来整个服务，并确保它正常运行。\n\n2. 节点\n\n 节点（Node）是Swarm集群的最小资源单位。每个节点实际上都是一台Docker主机。\nSwarm集群中节点分为两种：\n\n 管理节点（manager node）：负责响应外部对集群的操作请求，并维持集群中资源，分发任务给工作节点。同时，多个管理节点之间通过Raft协议构成共识。一般推荐每个集群设置5个或7个管理节点；\n 工作节点（worker node）：负责执行管理节点安排的具体任务。默认情况下，管理节点自身也同时是工作节点。每个工作节点上运行代理（agent）来汇报任务完成情况。\n\n 用户可以通过docker node promote命令来提升一个工作节点为管理节点；或者通过docker node demote命令来将一个管理节点降级为工作节点。\n\n3. 服务\n\n 服务（Service）是Docker支持复杂多容器协作场景的利器。\n 一个服务可以由若干个任务组成，每个任务为某个具体的应用。服务还包括对应的存储、网络、端口映射、副本个数、访问配置、升级配置等附加参数。\n 一般来说，服务需要面向特定的场景，例如一个典型的Web服务可能包括前端应用、后端应用，以及数据库等。这些应用都属于该服务的管理范畴。\n Swarm集群中服务类型也分为两种（可以通过-mode指定）：\n\n 复制服务（replicated services）模式：默认模式，每个任务在集群中会存在若干副本，这些副本会被管理节点按照调度策略分发到集群中的工作节点上。此模式下可以使用-replicas参数设置副本数量；\n\n 全局服务（global services）模式：调度器将在每个可用节点都执行一个相同的任务。该模式适合运行节点的检查，如监控应用等。\n\n4. 任务\n\n 任务是Swarm集群中最小的调度单位，即一个指定的应用容器。例如仅仅运行前端业务的前端容器。任务从生命周期上将可能处于创建（NEW）、等待（PENDING）、分配（ASSIGNED）、接受（ACCEPTED）、准备（PREPARING）、开始（STARTING）、运行（RUNNING）、完成（COMPLETE）、失败（FAILED）、关闭（SHUTDOWN）、拒绝（REJECTED）、孤立（ORPHANED）等不同状态。\nSwarm集群中的管理节点会按照调度要求将任务分配到工作节点上。例如指定副本为2时，可能会被分配到两个不同的工作节点上。一旦当某个任务被分配到一个工作节点，将无法被转移到另外的工作节点，即Swarm中的任务不支持迁移。\n\n5. 服务的外部访问\n\n Swarm集群中的服务要被集群外部访问，必须要能允许任务的响应端口映射出来。Swarm中支持入口负载均衡（ingress load balancing）的映射模式。该模式下，每个服务都会被分配一个公开端口（PublishedPort），该端口在集群中任意节点上都可以访问到，并被保留给该服务。\n当有请求发送到任意节点的公开端口时，该节点若并没有实际执行服务相关的容器，则会通过路由机制将请求转发给实际执行了服务容器的工作节点。', 5, 0, 0, '2021-01-08 22:52:18.635422', '2021-01-21 12:34:00.051550', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (154, '部署swarm集群', '[TOC]\n\n# 一、创建使用Swarm集群\n\n 创建三节点的 swarm 集群。\n swarm-manager 是 manager node，swarm-worker1 和 swarm-worker2 是 worker node。\n 所有节点的 Docker 版本均不低于 v1.12。我们的实验环境 node 的操作系统为 centos7.4\n 在 swarm-manager 上执行如下命令创建 swarm。\n `[root@host2 ~]# docker swarm init --advertise-addr 192.168.137.104` \n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734761-d86f3bf0-4ca1-4090-8b81-43d3f612c14d.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=92&originWidth=691&size=59792&status=done&style=none&width=691)\n--advertise-addr 指定与其他 node 通信的地址。\ndocker swarm init 输出告诉我们：\n\n- swarm 创建成功，swarm-manager 成为 manager node。\n- 添加 worker node 需要执行的命令。\n\n`docker swarm join --token SWMTKN-1-5bob8gfqqopfxedxa09bm15fzi9p07vifinzw99c607yec5tvt-43uw9okdwssjtnib0ke1lqsst 192.168.137.104:2377` \n\n- 添加 manager node 需要执行的命令。\n\n`docker swarm join-token manager` \n执行 docker node ls 查看当前 swarm 的 node，目前只有一个 manager。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734792-e653e57c-0aa4-41c5-a078-6a71c8ffc5c0.png#align=left&display=inline&height=41&margin=%5Bobject%20Object%5D&name=image.png&originHeight=41&originWidth=692&size=16902&status=done&style=none&width=692)\n复制前面的 docker swarm join 命令，在 vm1 和 vm2 上执行，将它们添加到 swarm中。命令输出如下：\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734793-36868eae-2f5d-4c3e-a2b4-f8131bc702fc.png#align=left&display=inline&height=29&margin=%5Bobject%20Object%5D&name=image.png&originHeight=29&originWidth=691&size=15788&status=done&style=none&width=691)\ndocker node ls 可以看到两个 worker node 已经添加进来了。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734860-0d59f073-e66c-4c10-b74c-29add0b5ae4d.png#align=left&display=inline&height=58&margin=%5Bobject%20Object%5D&name=image.png&originHeight=58&originWidth=691&size=27424&status=done&style=none&width=691)\n如果当时没有记录下 docker swarm init 提示的添加 worker 的完整命令，可以通过 docker swarm join-token worker 查看。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734889-19ff1176-09b8-46f7-b0e2-8b3ed04abe0b.png#align=left&display=inline&height=50&margin=%5Bobject%20Object%5D&name=image.png&originHeight=50&originWidth=692&size=24659&status=done&style=none&width=692)\n注意：此命令只能在 manager node 上执行。\n至此，三节点的 swarm 集群就已经搭建好了，操作还是相当简单的。\n\n# 二、运行第一个Service \n\n现在部署一个运行 httpd 镜像的 service，执行如下命令：\ndocker service create --name web_server httpd\n部署 service 的命令形式与运行容器的 docker run 很相似，--name 为 service 命名，httpd 为镜像的名字。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078734974-243cb9a4-9dd7-4915-8c5e-25080e5b075a.png#align=left&display=inline&height=70&margin=%5Bobject%20Object%5D&name=image.png&originHeight=70&originWidth=691&size=52888&status=done&style=none&width=691)\n通过 docker service ls 可以查看当前 swarm 中的 service。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078735034-a2983af4-c952-41fe-a4ad-8e5a9cdabe60.png#align=left&display=inline&height=34&margin=%5Bobject%20Object%5D&name=image.png&originHeight=34&originWidth=692&size=16626&status=done&style=none&width=692)\nREPLICAS 如果显示副本信息，0/1 的意思是 web_server 这个 service 期望的容器副本数量为1，目前已经启动的副本数量为 0。也就是当前 service 还没有部署完成。命令 docker service ps 可以查看 service 每个副本的状态。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078735061-33ad2dab-2d79-4053-928d-00c8847c17f7.png#align=left&display=inline&height=44&margin=%5Bobject%20Object%5D&name=image.png&originHeight=44&originWidth=691&size=19298&status=done&style=none&width=691)\n可以看到 service 唯一的副本被分派到 vm2，当前的状态是 Running，如果看到状态为Preparing，说明还没达到期望的状态 Running，这个副本在 Preparing 什么呢？\n其实答案很简单，vm2 是在 pull 镜像，下载完成后，副本就会处于 Running 状态了。\nservice 的运行副本数也正常了。\n如果觉得不放心，还可以到 vm2 去确认 httpd 容器已经运行。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610078735065-a607ea40-678e-4e1f-9adb-8c5d53ab5e11.png#align=left&display=inline&height=53&margin=%5Bobject%20Object%5D&name=image.png&originHeight=53&originWidth=692&size=23996&status=done&style=none&width=692)\n当前 web_server 在 swarm 中的分布\n目前为止 Service 与普通的容器还没有太大的不同，下面我们就要学习容器编排引擎的强大功能了，首先从应用伸缩Scale Up/Down 开始。', 4, 0, 0, '2021-01-08 22:53:09.676152', '2021-01-22 13:37:50.026682', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (155, 'swarm高可用', '[TOC]\n\n# 一、实现 Service伸缩\n\nswarm 要实现运行多个实例。这样可以负载均衡，同时也能提供高可用，增加 service 的副本数就可以了。在swarm-manager 上执行如下命令：\ndocker service scale web_server=5\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079595167-e92fb337-806b-4eaf-8405-9aea8a971a66.png#align=left&display=inline&height=154&margin=%5Bobject%20Object%5D&name=image.png&originHeight=154&originWidth=692&size=112279&status=done&style=none&width=692)\n副本数增加到 5，通过 docker service ls 和 docker service ps 查看副本的详细信息。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079594603-01f8d67d-de12-4d7e-b1cd-575948726ffa.png#align=left&display=inline&height=159&margin=%5Bobject%20Object%5D&name=image.png&originHeight=159&originWidth=690&size=69378&status=done&style=none&width=690)\n5 个副本已经分布在 swarm 的所有三个节点上。\n默认配置下 manager node 也是 worker node，所以 swarm-manager 上也运行了副本。如果不希望在 manager 上运行 service，可以执行如下命令：\n`docker node update --availability drain host2` \n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079592711-03d52bd2-f176-4e70-8a75-fec770231912.png#align=left&display=inline&height=40&margin=%5Bobject%20Object%5D&name=image.png&originHeight=40&originWidth=691&size=27994&status=done&style=none&width=691)\n通过 docker node ls 查看各节点现在的状态：\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079592739-ac567b6a-f26f-45fb-a585-7a4d6a7746f6.png#align=left&display=inline&height=60&margin=%5Bobject%20Object%5D&name=image.png&originHeight=60&originWidth=691&size=31565&status=done&style=none&width=691)\nDrain 表示swarm-manager 已经不负责运行 service，之前 swarm-manager 运行的那个副本会如何处理呢？用 docker service ps 查看一下：\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079597565-e590c0d1-a4d9-4fa6-9f55-ae0b7e92e6b6.png#align=left&display=inline&height=165&margin=%5Bobject%20Object%5D&name=image.png&originHeight=165&originWidth=690&size=78644&status=done&style=none&width=690)\nhost2 上的副本 web_server.4 已经被 Shutdown 了，为了达到 5 个副本数的目标，在 vm1 上添加了副本 web_server.4。(web_server.5同理)\n前面我们的场景是 scale up，我们还可以 scale down，减少副本数，运行下面的命令：\n`docker service scale web_server=3` \n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079597552-f8ba4501-820f-4de3-ab6e-169e2f9eb158.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=image.png&originHeight=117&originWidth=691&size=70235&status=done&style=none&width=691)\n可以看到，web_server.4 和 web_server.5 这两个副本已经被删除了。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079597560-abaa9716-7846-4189-bc1b-fbab0a914ad6.png#align=left&display=inline&height=155&margin=%5Bobject%20Object%5D&name=image.png&originHeight=155&originWidth=691&size=72407&status=done&style=none&width=691)\nService 的伸缩就讨论到这里，下边学习故障切换 Failover。\n\n# 二、Swarm 实现 Failover\n\n故障是在所难免的，容器可能崩溃，Docker Host 可能宕机，不过幸运的是，Swarm 已经内置了 failover 策略。\n创建 service 的时候，我们没有告诉 swarm 发生故障时该如何处理，只是说明了我们期望的状态（比如运行3个副本），swarm 会尽最大的努力达成这个期望状态，无论发生什么状况。\n以[上一节我们部署的 Service ](http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653588026&idx=1&sn=d5dfbbe5d602154375dce2f566d21072&chksm=8d308223ba470b35682e0d77f2b1c4b8a3b9b6d3f2f0701cc30f59abca8efebc90de0847ebd1&scene=21#wechat_redirect)为例，当前 3 个副本分布在 vm1和 vm2 上。\n现在我们测试 swarm 的 failover 特性，关闭 vm1。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079596631-949c8f0b-9728-4649-9387-1c3650f6f9fc.png#align=left&display=inline&height=66&margin=%5Bobject%20Object%5D&name=image.png&originHeight=66&originWidth=562&size=48154&status=done&style=none&width=562)\nSwarm 会检测到 vm1 的故障，并标记为 Down。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079596195-18a579ff-a725-4dd6-91c8-e05236660372.png#align=left&display=inline&height=59&margin=%5Bobject%20Object%5D&name=image.png&originHeight=59&originWidth=690&size=32429&status=done&style=none&width=690)\nSwarm 会将 vm1 上的副本调度到其他可用节点。我们可以通过 docker service ps 观察这个 failover 过程。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079597565-13b6feed-90c9-4b37-91e7-ad361138759c.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=692&size=67852&status=done&style=none&width=692)\n可以看到，web_server.1 和 web_server.2 已经从 vm1 迁移到了 vm2，之前运行在故障节点 vm1 上的副本状态被标记为 Shutdown。\n\n', 2, 0, 0, '2021-01-08 22:53:52.256265', '2021-01-21 12:30:19.083307', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (156, 'service访问', '[TOC]\n\n# 一、如何访问Service？\n\n为了便于分析，我们重新部署web_server。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079748908-2a8d5cd0-8de2-485e-9b23-fc6405dda3f6.png#align=left&display=inline&height=154&margin=%5Bobject%20Object%5D&name=image.png&originHeight=154&originWidth=691&size=63395&status=done&style=none&width=691)\n① docker service rm 删除 web_server，service 的所有副本（容器）都会被删除。\n② 重新创建 service，这次直接用 --replicas=2 创建两个副本。\n③ 每个 worker node 上运行了一个副本。\n好了，现在 service 已经在那里了，我们如何访问呢？\n要访问 http 服务，最起码网络得通，服务的 IP 我们得知道，但这些信息目前我们都不清楚。不过至少我们知道每个副本都是一个运行的容器，先看看容器的网络配置。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079749274-b6d8451a-7d17-4474-ae58-8b23339a034e.png#align=left&display=inline&height=54&margin=%5Bobject%20Object%5D&name=image.png&originHeight=54&originWidth=691&size=30261&status=done&style=none&width=691)\n在 vm1 上运行了一个容器，是 web_server 的一个副本，容器监听了 80 端口，但并没有映射到 Docker Host，所以只能通过容器的 IP 访问。查看一下容器的 IP。\n[root@vm1 ~]# docker inspect web_server.1.n3tj9nfvyuatvylzom57qq9mc\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079754481-d8dbb7a3-6ba6-42a5-b11c-8e0474c310b4.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=image.png&originHeight=218&originWidth=691&size=80505&status=done&style=none&width=691)\n容器 IP 为 172.17.0.2，实际上连接的是Docker 默认 bridge 网络。\n我们可以直接在 vm1 上访问容器的 http 服务。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079757472-a7de79dd-1b1c-4dc2-841a-daeeeff4a26d.png#align=left&display=inline&height=53&margin=%5Bobject%20Object%5D&name=image.png&originHeight=53&originWidth=692&size=50722&status=done&style=none&width=692)\n但这样的访问也仅仅是容器层面的访问，服务并没有暴露给外部网络，只能在 Docker 主机上访问。换句话说，当前配置下，我们无法访问 service web_server。\n\n# 二、从外部访问service\n\n要将 service 暴露到外部，方法其实很简单，执行下面的命令：\ndocker service update --publish-add 8080:80 web_server\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079759658-9309d82c-b155-4806-93c1-8f7d8ffbcae5.png#align=left&display=inline&height=100&margin=%5Bobject%20Object%5D&name=image.png&originHeight=100&originWidth=691&size=72981&status=done&style=none&width=691)\n如果是新建 service，可以直接用使用 --publish 参数，比如：\ndocker service create --name web_server --publish 8080:80 --replicas=2 httpd\n\n容器在 80 端口上监听 http 请求，--publish-add 8080:80 将容器的 80 映射到主机的 8080 端口，这样外部网络就能访问到 service 了。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079763195-b83f672f-4b15-47f7-8aa1-307a2f984413.png#align=left&display=inline&height=106&margin=%5Bobject%20Object%5D&name=image.png&originHeight=106&originWidth=692&size=132816&status=done&style=none&width=692)\n大家可能会奇怪，为什么 curl 集群中任何一个节点的 8080 端口，都能够访问到 web_server？\n这实际上就是使用 swarm 的好处了，这个功能叫做 routing mesh，我们下一节重点讨论。\n**神奇的 routing mesh**\n当我们访问任何节点的 8080 端口时，swarm 内部的 load balancer 会将请求转发给 web_server 其中的一个副本。\n这就是 routing mesh 的作用。\n所以，无论访问哪个节点，即使该节点上没有运行service 的副本，最终都能访问到 service。\n**ingress 网络**\n当我们应用 --publish-add 8080:80 时，swarm 会重新配置 service，我们看看容器都发生了哪些重要变化。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079759488-a8144025-e7e9-47ee-9cd7-7410a06e95ad.png#align=left&display=inline&height=124&margin=%5Bobject%20Object%5D&name=image.png&originHeight=124&originWidth=691&size=70815&status=done&style=none&width=691)\n我们会发现，之前的所有副本都被Shutdown，然后启动了新的副本。我们查看一下新副本的容器网络配置。\n[root@vm2 ~]# docker exec -it web_server.1.4wlgb5raw8slxjhswm34y7ngw /bin/sh\n\n yum -y update\n\n yum install net-tools\n\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079765072-e068b26c-c327-4247-9745-d3812097a289.png#align=left&display=inline&height=267&margin=%5Bobject%20Object%5D&name=image.png&originHeight=267&originWidth=692&size=220254&status=done&style=none&width=692)\n容器的网络与 --publish-add 之前已经大不一样了，现在有两块网卡，每块网卡连接不同的 Docker 网络。\n实际上：\n\n   1. eth0 连接的是一个 overlay 类型的网络，名字为 ingress，其作用是让运行在不同主机上的容器可以相互通信。\n   1. eth1 连接的是一个 bridge 类型的网络，名字为 docker_gwbridge，其作用是让容器能够访问到外网。\n\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079761089-bb7cde37-cbcb-4591-ac77-b48289c9da66.png#align=left&display=inline&height=121&margin=%5Bobject%20Object%5D&name=image.png&originHeight=121&originWidth=692&size=91553&status=done&style=none&width=692)\ningress 网络是 swarm 创建时 Docker 为自动我们创建的，swarm 中的每个 node 都能使用 ingress。\n通过 overlay 网络，主机与容器、容器与容器之间可以相互访问；同时，routing mesh 将外部请求路由到不同主机的容器，从而实现了外部网络对 service 的访问。\n那么接下来，service 跟 service 如何通信呢？\n\n# 三、Service 之间如何通信\n\n微服务架构的应用由若干 service 组成。比如有运行 httpd 的 web 前端，有提供缓存的memcached，有存放数据的 mysql，每一层都是 swarm 的一个 service，每个 service 运行了若干容器。在这样的架构中，service 之间是必然要通信的。\n**服务发现**\n一种实现方法是将所有 service 都 publish 出去，然后通过 routing mesh 访问。但明显的缺点是把 memcached 和 mysql 也暴露到外网，增加了安全隐患。\n如果不 publish，那么 swarm 就要提供一种机制，能够：\n\n   1. 让 service 通过简单的方法访问到其他 service。\n   1. 当 service 副本的 IP 发生变化时，不会影响访问该 service 的其他 service。\n   1. 当 service 的副本数发生变化时，不会影响访问该 service 的其他 service。\n\n这其实就是服务发现（service discovery）。Docker Swarm 原生就提供了这项功能，通过服务发现，service 的使用者不需要知道 service 运行在哪里，IP 是多少，有多少个副本，就能与service 通信。下面我们开始实践。\n**创建 overlay 网络**\n要使用服务发现，需要相互通信的 service必须属于同一个 overlay 网络，所以我们先得创建一个新的 overlay 网络。\n[root@host2 ~]# docker network create --driver overlay myapp_net\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079763675-1ca2f898-f527-48bb-98a7-3700933f5eeb.png#align=left&display=inline&height=175&margin=%5Bobject%20Object%5D&name=image.png&originHeight=175&originWidth=692&size=142457&status=done&style=none&width=692)\n直接使用 ingress 不行，因为目前 ingress 没有提供服务发现，必须创建自己的overlay 网络。\n**部署 service 到 overlay**\n部署一个 web 服务，并将其挂到新创建的 overlay 网络。\n[root@host2 ~]# docker service create --name my_web --replicas=3 --network myapp_net httpd\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079758309-67e9996f-7534-4ce0-a0fb-945831668bb3.png#align=left&display=inline&height=91&margin=%5Bobject%20Object%5D&name=image.png&originHeight=91&originWidth=692&size=56084&status=done&style=none&width=692)\n部署一个 util 服务用于测试，挂载到同一个 overlay 网络。\n[root@host2 ~]# docker service create --name util --network myapp_net busybox sleep 10000000\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079755982-de2d40eb-9569-48aa-b982-c37b47194f4b.png#align=left&display=inline&height=64&margin=%5Bobject%20Object%5D&name=image.png&originHeight=64&originWidth=692&size=38076&status=done&style=none&width=692)\nsleep 10000000 的作用是保持 busybox容器处于运行的状态，我们才能够进入到容器中访问service my_web。\n**验证**\n通过 docker service ps util 确认 util 所在的节点为 swarm-worker1。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079753930-f1e46d59-4655-4518-af30-a583b3ff6e0f.png#align=left&display=inline&height=49&margin=%5Bobject%20Object%5D&name=image.png&originHeight=49&originWidth=691&size=22083&status=done&style=none&width=691)\n登录到 vm2，在容器 util.1 中 ping 服务 my_web。\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079758903-fffd779f-1485-4a5e-9480-6bc677b24bf8.png#align=left&display=inline&height=72&margin=%5Bobject%20Object%5D&name=image.png&originHeight=72&originWidth=691&size=61671&status=done&style=none&width=691)\n可以看到 my_web 的 IP 为 10.0.1.2，这是哪个副本的 IP 呢？\n其实哪个副本的 IP 都不是。10.0.1.2 是 my_web service的 VIP（Virtual IP），swarm 会将对 VIP 的访问负载均衡到每一个副本。\n我们可以执行下面的命令查看每个副本的 IP。\n[root@vm2 ~]# docker network inspect myapp_net\n![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610079764346-d0e12cfb-44f5-4886-9174-21b2dce78266.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=image.png&originHeight=285&originWidth=692&size=169175&status=done&style=none&width=692)\n10.0.1.5、10.0.1.4、10.0.1.9 才是各个副本自己的 IP。不过对于服务的使用者（这里是 util.1），根本不需要知道 my_web副本的 IP，也不需要知道 my_web 的 VIP，只需直接用 service 的名字 my_web 就能访问服务。\n\n', 1, 0, 0, '2021-01-08 22:54:47.319176', '2021-01-22 12:25:41.843501', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (157, 'Docker图形化管理', '[TOC]\n\n# 一、Portainer简介\n\nPortainer是一款轻量级的图形化管理工具，通过它我们可以轻松管理不同的docker环境。Portainer部署和使用都非常的简单，它由一个可以运行在任何docker引擎上的容器组成。Portainer提供管理docker的containers、  images、volumes、networks等等。它兼容独立的docker环境和swarm集群模式。基本满足中小型单位对docker容器的管理工作。\n\n# 二、快速部署\n\n1. 从仓库中查询Portainer相关镜像：\n\n [root@centos7 ~]# docker search portainer\n\n2. 拉取Portainer镜像到本地：\n\n [root@centos7 ~]# docker pull portainer/portainer\n\n3. 查看本地主机上的镜像：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080128881-e36a18b7-c0c4-4a5b-b65f-bfb19849e682.png#align=left&display=inline&height=442&margin=%5Bobject%20Object%5D&name=image.png&originHeight=442&originWidth=1640&size=269898&status=done&style=none&width=1640)\n\n4. 单机安装\n\n 如果只有一个docker宿主机，可以使用以下命令安装：\n [root@centos7 ~]# docker volume create portainer_data\n [root@centos7 ~]# docker run -d -p 9000:9000 --name portainer --restart always -v  /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer\n\n5. Swarm集群下安装\n\n 使用Portainer提供的代理安装程序在Swarm集群下部署：\n[root@centos7 ~]# curl -L [https://portainer.io/download/portainer-agent-stack.yml](https://portainer.io/download/portainer-agent-stack.yml)-o portainer-agent-stack.yml\n[root@centos7 ~]#docker stack deploy --compose-file=portainer-agent-stack.yml portainer\n\n- 更多场景下部署方法可以参考以下：[https://portainer.readthedocs.io/en/stable/index.html](https://portainer.readthedocs.io/en/stable/index.html)\n\n# 三、访问Portainer\n\n单机环境中安装完成之后，可以通过浏览机对9000端口进行访问。如：[http://192.168.39.185:9000](http://192.168.39.185:9000/)\n\n1. 首次登陆需要注册用户：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080117622-fa52aa17-dfd5-47d7-b056-239d56c9f299.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=203186&status=done&style=none&width=2854)\n\n2. 由于是单机环境下安装，选择local模式即可：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080139238-64aedbc3-c79d-468c-8b56-5a64dd3c3c6f.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=292663&status=done&style=none&width=2854)\n\n 注意：该页面上有提示需要挂载本地 /var/run/docker.socker与容器内的/var/run/docker.socker连接。因此，在启动时必须指定该挂载文件。\n\n3. 进入首页可以查看到本地环境下docker相关的信息如容器，镜像等等。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080149145-ad85907f-2325-46a7-a8be-9c742381ca58.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=319738&status=done&style=none&width=2854)\n\n4. 点击“local“，查看本地docker相关信息：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080152752-ec488d76-c522-44aa-beab-cc5fdc799d91.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=348717&status=done&style=none&width=2854)\n\n5. 点击“Containers”查看容器相关信息：\n\n 可以进行创建容器、启动、停止、删除等等操作非常的方便。\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080162498-13be63a6-bd30-4f6e-844a-fe153ca3c151.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=542432&status=done&style=none&width=2854)\n\n6. 点击“Name”，可以查看相关容器详细信息：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080158185-00e2dd08-38d5-4798-9dec-a926c48def44.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=393540&status=done&style=none&width=2854)\n\n7. 在容器列表页面下，点击“Add      Containers”可以进行容器的创建：\n\n 输入相关信息点击“deploy the container”，可以根据已有的镜像快速创建容器：\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080161889-34ce4a91-8246-4950-a890-4084568d6dc8.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=472416&status=done&style=none&width=2854)\n\n8. 浏览器打开刚才创建的nginx容器：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080130852-4f71bd52-39b0-49e9-be63-4054cd423831.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2854&size=219096&status=done&style=none&width=2854)\n\n Portainer中还有很多对容器有意思的操作管理，诸如提供容器状态监控视图、容器状态日志、通过console登陆容器等等。\n Portainer中还有一些很多其他的操作，比如stacks管理、镜像管理、卷管理、权限管理、网络管理等等，可以安装上进行了解学习。\n 如镜像管理，可以查看到所有的镜像相关列表，在镜像列表可以直接pull一个镜像，可以从远程pull，也可以从私有库中pull。从私有库中pull，需要将私有库的地址提前进行配置。\n\n# 四、Portainer多docker宿主机环境下管理\n\n1. 添加docker节点\n\n 首先在docker个节点中需要开启远程管理端口：2375或2376（支持ca认证，此端口较安全）\n 开启2375端口，可能导致安全漏洞的出现，以下有详细介绍：[https://blog.csdn.net/ghostcloud2016/article/details/51539837](https://blog.csdn.net/ghostcloud2016/article/details/51539837)\n 开启2376端口比较复杂，可以浏览以下：[https://docs.docker.com/engine/security/https/](https://docs.docker.com/engine/security/https/)\n 编辑/etc/docker/daemon.json,添加如下：\n {\n  \"hosts\": [\"tcp://192.168.39.100:2375\", \"unix:///var/run/docker.sock\"]\n }\n 注意：192.168.39.100为docker所在宿主机ip\n\n- 重启docker服务\n\n systemctl restart docker\n\n Warning: docker.service changed on disk. Run \'systemctl daemon-reload\' to reload units.\n可以查看到端口已经开放：\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080130433-a71cf7fa-9fd3-47f5-a315-97b662bdfb41.png#align=left&display=inline&height=398&margin=%5Bobject%20Object%5D&name=image.png&originHeight=398&originWidth=1792&size=217966&status=done&style=none&width=1792)\n\n- 在docker其他节点可以通过docker -H      192.168.39.100:2375 info命令查看到该节点docker相关信息。\n- 点击Home页面下“add      endportainer”添加该节点docker信息：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080151353-79835148-6c34-44a0-8eec-3ac519d729ee.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2848&size=333508&status=done&style=none&width=2848)\n\n\n- 输入以下相关信息，点击“add      endpoartainer”添加信息即可：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080159908-af46cda0-8763-4832-8a3c-d3784f9f896a.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2848&size=420205&status=done&style=none&width=2848)\n\n\n- 可以看到添加docker-node1节点成功：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080153901-5723b923-0374-4a19-8da6-f18395524216.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2848&size=352269&status=done&style=none&width=2848)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080159916-b5752354-0aed-4006-9595-490798a68098.png#align=left&display=inline&height=1656&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1656&originWidth=2848&size=417638&status=done&style=none&width=2848)\n\n- 添加各个docker节点完成，这样可以通过portainer对每个节点下的docker环境进行相关的管理操作。\n\n2. Portanier管理Swarm集群\n\n Portainer管理swarm集群时，确保环境下已经设置好Swarm集群\n\n- Master节点下安装Portainer服务：\n\n [root@centos7 ~]# docker service create --name portainer --publish 9000:9000 --constraint \'node.role == manager\' --mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock portainer/portainer -H unix:///var/run/docker.sock\n j6mzodyhsilp6i266w7d0i4ty\n overall progress: 1 out of 1 tasks \n 1/1: running  [==================================================>] \n verify: Service converged\n\n- 查看Portainer服务：\n\n```bash\n[root@centos7 ~]# docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE                       PORTS\nj6mzodyhsilp        portainer           replicated          1/1                portainer/portainer:latest  *:9000->9000/tcp\n```\n\n- 安装完成之后，打开浏览器输入http://{ip}:9000同样会出现登陆密码的设置界：\n- 设置完帐号和密码，登陆主界面，可以查看到swarm集群信息：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080154136-ee19e409-d4b9-406d-a54b-9c58e123cfdb.png#align=left&display=inline&height=1578&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1578&originWidth=2872&size=357718&status=done&style=none&width=2872)\n\n- 点击“Swarm”，可以查看到个集群节点信息：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080159162-0266b463-d2ab-48aa-9f20-60c3e2d0a4f1.png#align=left&display=inline&height=1572&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1572&originWidth=2876&size=406269&status=done&style=none&width=2876)\n\n- 点击“Image”,拉取一个nginx镜像：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080158076-3c3fecf1-584c-4376-8a89-beeccda48cee.png#align=left&display=inline&height=1562&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1562&originWidth=2878&size=389680&status=done&style=none&width=2878)\n\n- 在Services下创建nginx服务，这里创建4个副本，并将80端口映射出去，最后点击“Create      Service”创建服务:\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080161875-7be138da-eba1-415d-b002-721e23e03f02.png#align=left&display=inline&height=1564&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1564&originWidth=2878&size=464460&status=done&style=none&width=2878)\n\n- 查看service list      列表会发现，nginx服务已经创建成功：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080151598-6eba5799-e351-4971-82c6-4ee65a084ae0.png#align=left&display=inline&height=1194&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1194&originWidth=2876&size=330521&status=done&style=none&width=2876)\n\n- 浏览器中输入个节点ip地址，都可以看到nginx页面如下：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080119839-5ba9a8c6-116a-4c3f-a64a-5d29527601e6.png#align=left&display=inline&height=960&margin=%5Bobject%20Object%5D&name=image.png&originHeight=960&originWidth=2868&size=151167&status=done&style=none&width=2868)\n\n- 通过命令可以查看到nginx服务运行状态：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610080153972-72f955cb-d7d6-4986-9eb1-85cb50fa11f8.png#align=left&display=inline&height=440&margin=%5Bobject%20Object%5D&name=image.png&originHeight=440&originWidth=2764&size=350887&status=done&style=none&width=2764) ', 2, 0, 0, '2021-01-08 23:01:58.578601', '2021-01-21 12:31:23.815075', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (158, 'docker system命令', '\n\n1. 查看docker 空间使用情况,包括镜像、容器和（本地）volume。\n\n```yaml\n[root@worker1 ~]# docker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              3                   0                   319.9MB             319.9MB (100%)\nContainers          0                   0                   0B                  0B\nLocal Volumes       1                   0                   156.5kB             156.5kB (100%)\nBuild Cache         0                   0                   0B                  0B\n```\n\n2. 查看实时事件（例如容器创建，删除等均会实时显示）\n\n `[root@worker1 ~]# docker system events`\n\n3. 查看docker 系统信息（同docker info）\n\n `[root@worker1 ~]# docker system info` \n\n4. docker清理(清理停止的容器，没用容器使用的网络，镜像，缓存)\n\n```bash\n[root@worker1 ~]# docker system prune\nWARNING! This will remove:\n  - all stopped containers\n  - all networks not used by at least one container\n  - all dangling images\n  - all dangling build cache\nAre you sure you want to continue? [y/N] y\nTotal reclaimed space: 0B\n```\n\n', 1, 0, 0, '2021-01-08 23:24:16.387400', '2021-01-21 17:16:22.482802', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (159, 'docker镜像常见问题', '\n\n1. 如何备份系统中所有的镜像？\n\n- 首先，备份镜像列表可以使用docker images|awk      \'NR>1{print $1\":\"$2}\'|sort > images.list。\n\n 导出所有镜像为当前目录下文件，可以使用如下命令：\n    while read img; do\n        echo $img\n        file=\"${img/\\//-}\"\n        sudo docker save --output $file.tar $img\n    done < images.list\n将本地镜像文件导入为Docker镜像：\n    while read img; do\n        echo $img\n        file=\"${img/\\//-}\"\n        docker load < $file.tar\n    done < images.list\n\n2. 如何批量清理临时镜像文件？\n\n 可以使用docker rmi $(docker images      -q -f dangling=true)命令。\n\n3. 如何删除所有本地的镜像？\n\n 可以使用docker rmi -f $(docker      images -q)命令。\n\n4. 如何清理Docker系统中的无用数据？\n\n 可以使用docker system prune      --volumes -f命令，这个命令会自动清理处于停止状态的容器、无用的网络和挂载卷、临时镜像和创建镜像缓存。\n\n5. 如何查看镜像内的环境变量？\n\n 可以使用docker run IMAGE env命令。\n\n6. 本地的镜像文件都存放在哪里？\n\n 与Docker相关的本地资源（包括镜像、容器）默认存放在/var/lib/docker/目录下。以aufs文件系统为例，其中container目录存放容器信息，graph目录存放镜像信息，aufs目录下存放具体的镜像层文件。\n\n7. 构建Docker镜像应该遵循哪些原则？\n\n 整体原则上，尽量保持镜像功能的明确和内容的精简，避免添加额外文件和操作步骤，要点包括：\n\n- 尽量选取满足需求但较小的基础系统镜像，例如大部分时候可以选择debian:wheezy或debian:jessie镜像，仅有不足百兆大小；\n- 清理编译生成文件、安装包的缓存等临时文件；\n- 安装各个软件时候要指定准确的版本号，并避免引入不需要的依赖；\n- 从安全角度考虑，应用要尽量使用系统的库和依赖；\n- 如果安装应用时候需要配置一些特殊的环境变量，在安装后要还原不需要保持的变量值；\n- 使用Dockerfile创建镜像时候要添加．dockerignore文件或使用干净的工作目录；\n- 区分编译环境容器和运行时环境容器，使用多阶段镜像创建。\n\n8. 碰到网络问题，无法pull镜像，命令行指定http_proxy无效，怎么办？\n\n- 在Docker配置文件中添加export       http_proxy=\"http://<PROXY_HOST>:<PROXY_PORT>\"，之后重启Docker服务即可。\n\n', 4, 0, 0, '2021-01-08 23:27:30.389632', '2021-01-22 16:59:03.891136', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (160, 'docker容器常见问题', '\n1. docker容器自启动\n\n 运行docker容器时可以加如下参数来保证每次docker服务重启后容器也自动重启：\n ocker  run --restart=always\n 果已经启动了则可以使用如下命令：\n ocker  update --restart=always <CONTAINER ID>\n\n2. 容器退出后，通过docker ps命令查看不到，数据会丢失么？\n\n 器退出后会处于终止（exited）状态，此时可以通过docker       ps -a查看。其中的数据也不会丢失，还可以通过 ocker [container] start命令来启动它。只有删除掉容器才会清除所有数据。\n\n3. 如何停止所有正在运行的容器？\n\n 以使用docker [container] stop       $(docker ps -q)命令。\n\n4. 如何批量清理所有的容器，包括处于运行状态和停止状态的？\n\n 以使用docker rm -f $(docker ps       -qa)命令。\n\n5. 如何获取某个容器的PID信息？\n\n 以使用docker [container] inspect       --format \'{{ .State.Pid }}\'<CONTAINER ID or NAME>命令。\n\n6. 如何获取某个容器的IP地址？\n\n 以使用docker [container] inspect       --format \'{{ .NetworkSettings. IPAddress }}\' <CONTAINER ID or       NAME>命令。\n\n7. 如何给容器指定一个固定IP地址，而不是每次重启容器时IP地址都会变？\n\n 前Docker并没有提供直接的对容器IP地址的管理支持，用户可以参考本书第三部的第20章“高级网络配置”中介绍的创建点对点连接例子，来手动配置容器的静态IP。或者在启动容器后，再手动进行修改（参考后面“其他类”的问题“如何进入Docker容器的网络命名空间？”）。\n\n8. 如何临时退出一个正在交互的容器的终端，而不终止它？\n\n Ctrl-p       Ctrl-q。如果按Ctrl-c往往会让容器内应用进程终止，进而会终止容器。\n\n9. 可以在一个容器中同时运行多个应用进程么？\n\n 般并不推荐在同一个容器内运行多个应用进程。如果有类似需求，可以通过一些额外的进程管理机制，比如supervisord，来管理所运行的进程。可以参考[https://docs.docker.com/articles/using_supervisord/](https://docs.docker.com/articles/using_supervisord/)。\n\n10. 如何控制容器占用系统资源（CPU、内存）的份额？\n\n 使用docker [container]       create命令创建容器或使用docker [con-tainer]       run创建并启动容器的时候，可以使用-c|-cpu-shares[=0]参数来调整容器使用CPU的权重；使用-m|-memory[=MEMORY]参数来调整容器使用内存的大小。', 3, 0, 0, '2021-01-08 23:29:19.356428', '2021-01-21 12:28:23.225268', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (161, 'docker仓库常见问题', '\n\n1. 仓库（Repository）、注册服务器（Registry）、注册索引（Index）有何关系？\n\n 仓库是存放一组关联镜像的集合，比如同一个应用的不同版本的镜像。注册服务器是存放实际的镜像文件的地方。注册索引则负责维护用户的账号、权限、搜索、标签等的管理。因此，注册服务器利用注册索引来实现认证等管理。\n\n2. 从非官方仓库（例如non-official-repo.com）下载镜像时候，有时候会提示“Error:Invalid       registry endpoint [https://non-official-repo.com/v1/……”，怎么办](https://non-official-repo.com/v1/……”，怎么办)？\n\n- Docker自1.3.0版本往后，加强了对镜像安全性的验证，需要添加私有仓库证书，或者手动添加对非官方仓库的信任。编辑Docker配置文件，在其中添加：DOCKER_OPTS=\"--insecure-registry       non-official-repo\"之后，重启Docker服务即可。\n\n', 2, 0, 0, '2021-01-08 23:29:53.847166', '2021-01-23 04:47:33.964455', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (162, 'docker配置常见问题', '\n\n1. Docker的配置文件放在哪里，如何修改配置？\n\n- 使用upstart的系统（如Ubuntu       16.04）的配置文件在/etc/default/docker，使用systemd的系统（如Ubuntu       16.04、Centos等）的配置文件在/etc/systemd/system/docker. service.d/docker.conf。\n\n Ubuntu下面的配置文件内容如下，读者可以参考配置（如果出现该文件不存在的情况，重启或者自己新建一个文件都可以解决）：\n\n``` bash\n# Customize location of Docker binary  (especially for development testing).\n\n#DOCKERD=\"/usr/local/bin/dockerd\"\n\n# Use DOCKER_OPTS to modify the daemon  startup options.\n\n#DOCKER_OPTS=\"--dns 8.8.8.8--dns  8.8.4.4\"\n\n# If you need Docker to use an HTTP proxy,  it can also be specified here.\n\n#export  http_proxy=\"http://127.0.0.1:3128/\"\n\n# This is also a handy place to tweak  where Docker\'s temporary files go.\n\n#export  TMPDIR=\"/mnt/bigdrive/docker-tmp\"\n```\n\n2. 如何更改Docker的默认存储位置？\n\n Docker的默认存储位置是/var/lib/docker，如果希望将Docker的本地文件存储到其他分区，可以使用Linux软连接的方式来完成，或者在启动daemon时通过-g参数指定。\n\n- 例如，如下操作将默认存储位置迁移到/storage/docker：\n\n```bash\n[root@s26~]#  df -h\nFilesystem                         Size   Used Avail Use% Mounted on\n/dev/mapper/VolGroup-lv_root    50G    5.3G    42G   12% /\ntmpfs                                 48G   228K     48G    1% /dev/shm\n/dev/sda1                          485M    40M    420M    9% /boot\n/dev/mapper/VolGroup-lv_home   222G    188M   210G    1% /home\n/dev/sdb2                          2.7T   323G    2.3T   13% /storage\n[root@s26~]# service docker stop\n[root@s26~]# cd /var/lib/\n[root@s26 lib]# mv docker /storage/\n[root@s26 lib]# ln -s /storage/docker/  docker\n[root@s26 lib]# ls -la docker\nlrwxrwxrwx. 1 root root 15 11月 17 13:43  docker -> /storage/docker\n[root@s26 lib]# service docker start\n```\n\n3. 使用内存和swap限制启动容器时候报警告：“WARNING:       Your kernel does not support cgroup swap limit. WARNING: Your kernel does       not support swap limit capabilities. Limitation discarded.”，怎么办？\n\n- 这是因为系统默认没有开启对内存和swap使用的统计功能，引入该功能会带来性能的下降。要开启该功能，可以采取如下操作：\n- 编辑/etc/default/grub文件（Ubuntu系统为例），配置GRUB_CMDLINE_LINUX=\"cgroup_enable=memory  swapaccount=1\"；\n- 更新grub:$  sudo update-grub；\n- 重启系统即可。', 4, 0, 0, '2021-01-08 23:31:22.087330', '2021-01-23 13:06:08.473200', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (163, 'docker其他问题', '\n\n1. Docker能在非Linux平台（比如macOS或Windows）上运行么？\n\n 可以。macOS目前需要使用docker for      mac等软件创建一个轻量级的Linux虚拟机层。由于成熟度不高，暂时不推荐在Windows环境中使用Docker。\n\n2. 如何将一台宿主主机的Docker环境迁移到另外一台宿主主机？\n\n 停止Docker服务。将整个Docker存储文件夹（如默认的/var/lib/docker）复制到另外一台宿主主机，然后调整另外一台宿主主机的配置即可。\n\n3. 如何进入Docker容器的网络命名空间？\n\n Docker在创建容器后，删除了宿主主机上/var/run/netns目录中的相关的网络命名空间文件。因此，在宿主主机上是无法看到或访问容器的网络命名空间的。用户可以通过如下方法来手动恢复它：\n\n- 使用下面的命令查看容器进程信息，比如这里的1234：\n\n     $ docker [container] inspect --format=\'{{. State.Pid}} \'     $container_id\n     1234\n\n- 在/proc目录下，把对应的网络命名空间文件链接到/var/run/netns目录：\n\n     $ sudo ln -s /proc/1234/ns/net /var/run/netns/\n\n- 在宿主主机上就可以看到容器的网络命名空间信息。例如：\n\n      $ sudo ip netns show\n      1234\n此时，用户可以通过正常的系统命令来查看或操作容器的命名空间了。例如修改容器的IP地址信息为172.17.0.100/16：\n      $ sudo ip netns exec 1234 ifconfig eth0172.17.0.100/16\n\n', 2, 0, 0, '2021-01-08 23:31:58.452860', '2021-01-21 12:33:56.838730', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (164, 'docker命令总结', '[TOC]\n\n# 一、基本语法\n\n1. Docker命令有两大类：客户端命令和服务端命令，前者是主要的操作接口，后者用来启动Docker服务。\n\n- 客户端命令：基本命令格式为docker [OPTIONS] COMMAND [arg...]；\n- 服务端命令：基本命令格式为dockerd [OPTIONS]。\n\n2. 可以通过man docker或docker      help来查看这些命令，通过man docker-COMMAND或docker help COMMAND来查看这些命令的具体用法和支持的参数。\n\n# 二、客户端命令\n\n1. 命令选项\n\n 客户端命令负责操作接口，支持如下命令选项：\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082682564-1d753d55-9c74-4a80-ab31-32b2bfe6c049.png#align=left&display=inline&height=674&margin=%5Bobject%20Object%5D&name=image.png&originHeight=674&originWidth=1280&size=332392&status=done&style=none&width=1280)\n\n2. 客户端管理命令\n\n Docker客户端单独提供了一组管理命令，对某个资源集中进行管理，包括快照、配置、容器、镜像、网络、节点、插件、秘密、服务、服务栈、集群、系统、密钥和挂载卷等，如下表所示。\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082698534-d9f1e41a-619a-4d50-bbaf-2df60a62a366.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=image.png&originHeight=720&originWidth=695&size=333521&status=done&style=none&width=695)\n\n3. 客户端常用命令\n\n 除了针对某个资源的管理命令外，Docker也兼容了之前版本的做法，为一些常见操作提供了快捷命令，如下表所示。\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082713709-11036e84-3505-4060-ad45-159a1444ceae.png#align=left&display=inline&height=710&margin=%5Bobject%20Object%5D&name=image.png&originHeight=710&originWidth=639&size=200187&status=done&style=none&width=639)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082722605-c0a4fbdd-2d68-46ea-99a8-13a06dcf1aaa.png#align=left&display=inline&height=707&margin=%5Bobject%20Object%5D&name=image.png&originHeight=707&originWidth=1087&size=264855&status=done&style=none&width=1087)\n\n# 三、服务端命令选项\n\n dockerd命令负责启动服务端主进程，支持的命令选项如下表所示。\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082754960-096b267b-9507-4a3f-95a8-037f7cd8386d.png#align=left&display=inline&height=720&margin=%5Bobject%20Object%5D&name=image.png&originHeight=720&originWidth=1280&size=372557&status=done&style=none&width=1280)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082764195-066f56d6-efdd-4795-812a-886eccdce590.png#align=left&display=inline&height=714&margin=%5Bobject%20Object%5D&name=image.png&originHeight=714&originWidth=541&size=200883&status=done&style=none&width=541)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082775231-de606c82-c98d-4da7-81e2-fe8720ea9fff.png#align=left&display=inline&height=717&margin=%5Bobject%20Object%5D&name=image.png&originHeight=717&originWidth=542&size=210838&status=done&style=none&width=542)\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082799490-bad549e8-20da-4e22-80dc-2cdd4d1152a7.png#align=left&display=inline&height=717&margin=%5Bobject%20Object%5D&name=image.png&originHeight=717&originWidth=542&size=210838&status=done&style=none&width=542)\n\n# 四、一张图总结Docker命令\n\n ![image.png](https://cdn.nlark.com/yuque/0/2021/png/2308212/1610082800066-b9db97f2-f93a-45d8-a4af-017bf0116f71.png#align=left&display=inline&height=1688&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1688&originWidth=937&size=244612&status=done&style=none&width=937)\n\n ', 6, 0, 0, '2021-01-08 23:33:01.109001', '2021-01-21 12:33:55.311616', 1, 1, 2, 0);
INSERT INTO `blog_section` VALUES (165, 'prometheus简介', '[TOC]\n\n# 一、prometheus优势\n1. 易于管理\n1. 监控服务的内部运行状态\n1. 强大的数据模型\n1. 强大的查询语言promQL\n1. 高效\n1. 可扩展\n1. 易于集成\n1. 可视化\n1. 开放性\n\n# 二、prometheus组件\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607591405090-2a02f2d0-d3c6-49cc-8dac-1eaf9fc32fc2.png#align=left&display=inline&height=459&margin=%5Bobject%20Object%5D&name=image.png&originHeight=459&originWidth=690&size=42626&status=done&style=none&width=690)\n\n1. Prometheus Server\n\n Prometheus Server是Prometheus组件中的核心部分，负责实现对监控数据的获取，存储以及查询。\n\n Prometheus Server可以通过静态配置管理监控目标，也可以配合使用Service Discovery的方式动态管理监控目标，并从这些监控目标中获取数据。其次Prometheus Server需要对采集到的监控数据进行存储，Prometheus Server本身就是一个时序数据库，将采集到的监控数据按照时间序列的方式存储在本地磁盘当中。最后Prometheus Server对外提供了自定义的PromQL语言，实现对数据的查询以及分析。\n\n Prometheus Server内置的Express Browser UI，通过这个UI可以直接通过PromQL实现数据的查询以及可视化。\n\n Prometheus Server的联邦集群能力可以使其从其他的Prometheus Server实例中获取数据，因此在大规模监控的情况下，可以通过联邦集群以及功能分区的方式对Prometheus Server进行扩展。\n2. Exporters\n\n Exporter将监控数据采集的端点通过HTTP服务的形式暴露给Prometheus Server，Prometheus Server通过访问该Exporter提供的Endpoint端点，即可获取到需要采集的监控数据。一般来说可以将Exporter分为2类：\n\n 直接采集：这一类Exporter直接内置了对Prometheus监控的支持，比如cAdvisor，Kubernetes，Etcd，Gokit等，都直接内置了用于向Prometheus暴露监控数据的端点。\n\n 间接采集：间接采集，原有监控目标并不直接支持Prometheus，因此我们需要通过Prometheus提供的Client Library编写该监控目标的监控采集程序。例如： Mysql Exporter，JMX Exporter，Consul Exporter等。\n3. AlertManager\n\n 在Prometheus Server中支持基于PromQL创建告警规则，如果满足PromQL定义的规则，则会产生一条告警，而告警的后续处理流程则由AlertManager进行管理。在AlertManager中我们可以与邮件，Slack等等内置的通知方式进行集成，也可以通过Webhook自定义告警处理方式。AlertManager即Prometheus体系中的告警处理中心。\n\n4. PushGateway\n\n 由于Prometheus数据采集基于Pull模型进行设计，因此在网络环境的配置上必须要让Prometheus Server能够直接与Exporter进行通信。 当这种网络需求无法直接满足时，就可以利用PushGateway来进行中转。可以通过PushGateway将内部网络的监控数据主动Push到Gateway当中。而Prometheus Server则可以采用同样Pull的方式从PushGateway中获取到监控数据。\n \n \n', 1, 0, 0, '2021-01-26 19:30:55.512035', '2021-01-26 19:40:27.111934', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (166, '部署prometheus', '[TOC]\n\n# 一、安装go\n\n- [下载链接](https://studygolang.com/dl)\n\n1. 解压安装\n\n `# tar -C /usr/local/ -xvf go1.14.1.linux-amd64.tar.gz`\n\n2. 配置环境变量\n\n```bash\n# vim /etc/profile\nexport PATH=$PATH:/usr/local/go/bin\n# source /etc/profile\n```\n\n3. 验证\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592442694-b4aa0345-3e8e-455e-aa19-61651a032f10.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=image.png&originHeight=47&originWidth=350&size=4302&status=done&style=none&width=350)\n\n# 二、安装prometheus\n\n- [下载链接](https://prometheus.io/download/)\n\n1. 安装\n\n `# tar -C /usr/local/ -xvf prometheus-2.17.1.linux-amd64.tar.gz`\n `# mv /usr/local/prometheus-2.6.0.linux-amd64/ /usr/local/Prometheus`\n\n2. 创建服务脚本\n\n```bash\n# cd /usr/lib/systemd/system\n# vim prometheus.service\n[Unit]\nDescription=Prometheus\nDocumentation=https://prometheus.io/\nAfter=network.target\n \n[Service]\n# Type设置为notify时，服务会不断重启\nType=simple\nUser=root\nGroup=root\n# --storage.tsdb.path是可选项，默认数据目录在运行目录的./dada目录中\nExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml \nRestart=on-failure\n \n[Install]\nWantedBy=multi-user.target\n```\n\n3. 启动prometheus\n\n `# systemctl daemon-reload  `\n `# systemctl enable prometheus.service `\n `# systemctl start prometheus.service `\n `# systemctl status prometheus  ` \n\n4. 验证\n\n 浏览器打开IP:9090端口即可打开普罗米修斯自带的监控页面\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592793608-d6c33e26-1ad3-4d82-956a-d4d6c5b1b4d1.png#align=left&display=inline&height=254&margin=%5Bobject%20Object%5D&name=image.png&originHeight=507&originWidth=823&size=25800&status=done&style=none&width=411.5)\n\n# 三、安装grafana-server\n\n- [下载链接](https://grafana.com/grafana/download)\n\n1. 安装\n\n `# rpm -ivh grafana-6.7.2-1.x86_64.rpm` \n\n2. 启动\n\n `# systemctl enable grafana-server.service`\n `# systemctl start grafana-server.service`\n\n3. 访问grafana\n\n 浏览器访问IP:3000端口，即可打开grafana页面，默认用户名密码都是admin，初次登录会要求修改默认的登录密码\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592879332-d0c26be5-ae0d-433e-989c-f22577e8267e.png#align=left&display=inline&height=296&margin=%5Bobject%20Object%5D&name=image.png&originHeight=592&originWidth=944&size=53854&status=done&style=none&width=472)\n\n4. 添加prometheus数据源\n\n- 点击主界面的“Add data source”\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592906314-83b80b06-eb1d-46ab-bc3d-ba288c7e416d.png#align=left&display=inline&height=219&margin=%5Bobject%20Object%5D&name=image.png&originHeight=437&originWidth=1015&size=53781&status=done&style=none&width=507.5)\n\n- 选择prometheus\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592920195-7f40dca1-8787-4ff5-9346-2e7e60107289.png#align=left&display=inline&height=304&margin=%5Bobject%20Object%5D&name=image.png&originHeight=608&originWidth=1143&size=115113&status=done&style=none&width=571.5)\n\n- 添加url地址并保存\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592942321-b77690f5-0cec-49bc-ae5e-568f4b2aba40.png#align=left&display=inline&height=221&margin=%5Bobject%20Object%5D&name=image.png&originHeight=441&originWidth=852&size=78270&status=done&style=none&width=426)\n\n- 添加dashboard\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592956146-5bef1aad-ac94-4e21-ad7d-dfb025ba31af.png#align=left&display=inline&height=172&margin=%5Bobject%20Object%5D&name=image.png&originHeight=344&originWidth=1137&size=96180&status=done&style=none&width=568.5)\n\n- 查看仪表盘\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607592967301-60c72136-f0d2-4ea1-8922-dd4839e00ca7.png#align=left&display=inline&height=283&margin=%5Bobject%20Object%5D&name=image.png&originHeight=565&originWidth=971&size=77583&status=done&style=none&width=485.5)\n\n# 四、使用node-exporter监控linux主机\n\n1. 被监控的机器安装node-exporter\n\n `# tar -zxvf node_exporter-0.18.1.linux-amd64.tar.gz` \n `# mv node_exporter-0.18.1.linux-amd64 /usr/local/node_exporter` \n\n2. 创建服务脚本\n\n```bash\n# cd /usr/lib/systemd/system\n# vim node_exporter.service\n[Unit]\nDescription=node_exporter\nAfter=network.target\n \n[Service]\nType=simple\nUser=root\nExecStart=/usr/local/node_exporter/node_exporter\nRestart=on-failure\n \n[Install]\nWantedBy=multi-user.target\n```\n\n3. 启动node_exporter\n\n `# systemctl daemon-reload  `\n `# systemctl enable node_exporter.service`\n `# systemctl start node_exporter.service`\n `# systemctl status node_exporter.service`\n\n4. prometheus服务端添加监控项，然后重启prometheus\n\n `# vim /usr/local/Prometheus/prometheus.yml`\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593229797-181fa747-6f82-497a-93d0-ca6912ca6145.png#align=left&display=inline&height=139&margin=%5Bobject%20Object%5D&name=image.png&originHeight=277&originWidth=529&size=26104&status=done&style=none&width=264.5)\n\n5. 查看prometheus web界面——status——targets\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593250917-3cac8c1d-eb0f-4813-aa17-d3ccf80f410a.png#align=left&display=inline&height=267&margin=%5Bobject%20Object%5D&name=image.png&originHeight=533&originWidth=986&size=51614&status=done&style=none&width=493)\n\n6. grafana导入自定义的dashboard\n\n- [参考链接](https://grafana.com/grafana/dashboards/8919)\n- 创建自定义dashboard，选择导入\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593268081-f2fd39a8-8a50-4f31-8ef3-618152652d36.png#align=left&display=inline&height=282&margin=%5Bobject%20Object%5D&name=image.png&originHeight=564&originWidth=1146&size=93328&status=done&style=none&width=573)\n\n- 配置数据源，dashboard名称\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593279559-eafe8106-171f-4a8e-adec-bee77e4091be.png#align=left&display=inline&height=279&margin=%5Bobject%20Object%5D&name=image.png&originHeight=557&originWidth=1147&size=101385&status=done&style=none&width=573.5)\n\n- 查看dashboard\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593291746-8089da27-2438-4777-bd5f-5145e34aedde.png#align=left&display=inline&height=321&margin=%5Bobject%20Object%5D&name=image.png&originHeight=642&originWidth=886&size=78588&status=done&style=none&width=443)\n\n# 五、监控windows机器（wmi-exporter）\n\n- [下载地址](https://github.com/martinlindhe/wmi_exporter/releases)（下载msi格式安装包）\n\n1. 被监控windows机器安装wmi-exporter，会自动创建一个开机自启的服务\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593309380-e7db5788-6570-4054-9560-cc8c65f21948.png#align=left&display=inline&height=230&margin=%5Bobject%20Object%5D&name=image.png&originHeight=459&originWidth=471&size=29527&status=done&style=none&width=235.5)\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593322117-6c03d395-4f4c-43c3-b302-f47479c1d51b.png#align=left&display=inline&height=208&margin=%5Bobject%20Object%5D&name=image.png&originHeight=416&originWidth=609&size=31495&status=done&style=none&width=304.5)\n\n2. prometheus服务端添加监控项，然后重启prometheus\n\n `# vim /usr/local/Prometheus/prometheus.yml`\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593616178-38661675-474b-4d9d-bd19-13309568afe1.png#align=left&display=inline&height=36&margin=%5Bobject%20Object%5D&name=image.png&originHeight=72&originWidth=455&size=7483&status=done&style=none&width=227.5)\n\n3. 查看prometheus web界面——status——targets\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593633182-d45801b4-d521-4d40-9906-7ff2c658faa3.png#align=left&display=inline&height=82&margin=%5Bobject%20Object%5D&name=image.png&originHeight=164&originWidth=918&size=18160&status=done&style=none&width=459)\n\n4. grafana导入自定义的dashboard\n\n- [参考链接](https://grafana.com/grafana/dashboards/10467)', 1, 0, 0, '2021-01-26 19:33:18.000969', '2021-01-26 19:40:31.959233', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (167, '数据模型', '[TOC]\n\n>Prometheus从根本上将所有数据存储为时间序列：带有时间戳值的数据流属于同一度量标准和同一组标签维度。除了存储的时间序列，Prometheus可能会生成临时的导出时间序列作为查询的结果。\n\n# 一、指标定义\n\n1. 监控指标格式：\n\n `<metric name>{<label name>=<label value,……>}` \n\n2. 指标名称（metric name）\n\n 用于说明指标的含义，必须由字母、数值下画线或者冒号组成，其中的冒号指标不能用于exporter。\n\n3. 标签（label）\n\n 标签可体现指标的维度特征，用于过滤和聚合。它通过标签名（label name）和标签值（label value）这种键值对的形式，形成多种维度。\n\n# 二、示例\n\n1. 给定度量标准名称和一组标签，通常使用以下表示法标识时间序列：\n\n `<metric name>{<label name>=<label value>, ...}` \n\n2. 例如，度量标准名称api_http_requests_total和标签      method=\"POST\" 和 handler=\"/messages\"的时间序列可以这样写：`api_http_requests_total{method=\"POST\", handler=\"/messages\"}`\n\n# 三、数据样本\n\n1. Prometheus会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。time-series是按照时间戳和值的序列顺序存放的，我们称之为向量(vector). 每条time-series通过指标名称(metrics name)和一组标签集(labelset)命名。如下所示，可以将time-series理解为一个以时间为Y轴的数字矩阵：\n\n ![image.png](https://cdn.nlark.com/yuque/0/2020/png/2308212/1607593731892-2b77d379-e1b4-4a15-aa71-ebee6624e377.png#align=left&display=inline&height=296&margin=%5Bobject%20Object%5D&name=image.png&originHeight=296&originWidth=610&size=78607&status=done&style=none&width=610)\n\n2. 在time-series中的每一个点称为一个样本（sample），样本由以下三部分组成：\n\n- 指标(metric)：metric name和描述当前样本特征的labelsets;\n- 时间戳(timestamp)：一个精确到毫秒的时间戳;\n- 样本值(value)：一个float64的浮点型数据表示当前样本的值。\n\n ', 1, 0, 0, '2021-01-26 19:34:19.921328', '2021-01-26 19:40:35.006295', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (168, '指标类型', '[TOC]\n\n# 一、Counter\n\n1. counter是一个累积指标，代表一个单调递增的计数器，其值只能增加或在重新启动时重置为零。      例如，您可以使用counter来表示已服务请求，已完成任务或错误的数量。\n1. 不要使用计数器来显示会减小的值。例如，请勿对当前正在运行的进程数使用计数器； 而是使用 gauge。\n\n# 二、Gauge\n\n1. Gauge是一种可以表示任意上下浮动的单个数值的度量指标。\n1. Gauge通常用于测量值，例如温度或当前内存使用情况，还用于可能上升和下降的“计数”，例如并发请求数。比如磁盘容量、内存使用量就必须使用      gauge来度量\n\n# 三、histogram\n\n1. histogram是柱状图，在Prometheus系统中的查询语言中，有三种作用：\n\n- 对每个采样点进行统计（并不是一段时间的统计），打到各个桶(bucket)中\n- 对每个采样点值累计和(sum)\n- 对采样点的次数累计和(count)\n\n2. 基本度量指标名称为<basename>的histogram在抓取期间显示多个时间序列：  \n\n- 观察桶的累积计数器，显示为     <basename>_bucket{le=\"<upper inclusive bound>\"}\n- 所有观察值的总和，显示为<basename>_sum\n- 观察到的事件数，显示为<basename>_count（与 <basename>_bucket{le=\"+Inf\"}相同）\n\n3. 使用histogram_quantile()函数从直方图甚至直方图的聚合中计算分位数。 直方图也适合计算Apdex分数。 在存储桶上操作时，请记住直方图是累积的。      有关直方图用法的详细信息以及与摘要的差异，请参见直方图和摘要。 客户端库使用情况的直方图文档：\n\n# 四、summary\n\n1. summary是采样点分位图统计。 它也有三种作用：\n\n- 在客户端对于一段时间内（默认是10分钟）的每个采样点进行统计，采样点分位图统计，用于得到数据的分布情况（例如，在要统计的班级中，有90%学生的成绩低于93分，有95%学生的成绩低于96分，则采用Summary能够更好地展示数据的分布情况。）\n- 统计班上所有同学的总成绩(sum)\n- 统计班上同学的考试总人数(count)\n\n2. 基本指标名称为<basename>的summary在抓取期间显示多个时间序列：\n\n- 流观察到的事件的φ分位数（0≤φ≤1），显示为<basename>      {quantile=\"<φ>\"\n- 所有观察值的total      sum，显示为<basename>_sum\n- 观察到的事件count，显示为<basename>_count', 1, 0, 0, '2021-01-26 19:35:01.438166', '2021-01-26 20:08:19.622508', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (169, 'Jobs和Instances', '\n\n1. 当我们需要采集不同的监控指标(例如：主机、MySQL、Nginx)时，我们只需要运行相应的监控采集程序，并且让Prometheus       Server知道这些Exporter实例的访问地址。在Prometheus中，每一个暴露监控样本数据的HTTP服务称为一个实例。例如在当前主机上运行的node       exporter可以被称为一个实例(Instance)。\n1. 而一组用于相同采集目的的实例，或者同一个采集进程的多个副本则通过一个一个任务(Job)进行管理。\n\n job: node\n instance 2: 1.2.3.4:9100\n instance 4: 5.6.7.8:9100\n\n3. 当前在每一个Job中主要使用了静态配置(static_configs)的方式定义监控目标。除了静态配置每一个Job的采集Instance地址以外，Prometheus还支持与DNS、Consul、E2C、Kubernetes等进行集成实现自动发现Instance实例，并从这些Instance上获取监控数据。', 0, 0, 0, '2021-01-26 19:35:38.753683', '2021-01-26 19:35:38.753720', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (170, '查询表达式', '[TOC]\n\n# 一、表达式语言类型\n\nPrometheus表达式或子表达式可以评估为一下四种类型之一：\n\n1. 即时向量（Instant vector）- 包含每个时间序列单个样品的一组时间序列，共享相同的时间戳\n1. 范围向量（Range vector）- 包含一个范围内数据点的一组时间序列\n1. 标量（Scalar）- 一个简单的数字浮点值\n1. 字符串（String）- 一个简单的字符串值；当前未使用\n\n# 二、即时向量选择\n\n1. 即时向量选择器允许选择一组时间序列，或者某个给定的时间戳的样本数据。\n\n- 选择具有http_requests_total的时间序列：\n\n `prometheus_http_requests_total` \n 等同于：\n `prometheus_http_requests_total{}`\n\n2. PromQL还支持用户根据时间序列的标签匹配模式来对时间序列进行过滤，\n\n- PromQL支持使用=和!=两种完全匹配模式：\n\n =：选择正好相等的字符串标签\n !=：选择不相等的字符串标签\n\n- 选择有http_requests_total名称的、有prometheus工作标签的、code为200标签的时间序列：\n\n`prometheus_http_requests_total{job=\"prometheus\",code=\"200\"}`\n\n3. PromQL还支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离：\n\n =~：选择匹配正则表达式的标签（或子标签）\n !=：选择不匹配正则表达式的标签（或子标签）\n\n- 选择staging、testing、development环境下的，GET之外的HTTP方法的http_requests_total的时间序列：`http_requests_total{environment=~\"staging|testing|development\",method!=\"GET\"}` \n\n# 三、范围向量选择\n\n1. 范围向量表达式返回从当前时刻开始的一定时间范围的时间序列集合回来。\n1. 语法是，在一个向量表达式之后添加[]来表示时间范围，持续时间用数字表示，后接下面单元之一：\n\n- s：seconds\n- m：minutes\n- h：hours\n- d：days\n- w：weeks\n- y：years\n\n 选择最后5分钟的记录，metric名称为http_requests_total、作业标签为prometheus的时间序列的所有值：\n\n `http_requests_total{job=\"prometheus\"}[5m]` \n\n# 四、时间位移操作\n\n1. 如果我们想查询5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据时，可以使用位移操作，位移操作的关键字为offset。\n1. 可以使用offset时间位移操作：\n\n- http_request_total{} offset 5m\n- http_request_total{}[1d] offset      1d\n\n ', 1, 0, 0, '2021-01-26 19:42:11.645984', '2021-01-26 20:24:45.480837', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (171, '操作符', '[TOC]\n\n# 一、数学运算符\n\nPromQL支持的所有数学运算符如下所示：\n\n1. + (加法)\n1. - (减法)\n1. * (乘法)\n1. / (除法)\n1. % (求余)\n1. ^ (幂运算)\n\n- 获取当前主机可用的内存空间大小，其样本单位为Bytes。换算成MB：\n\n `node_memory_free_bytes_total / (1024 * 1024)` \n\n# 二、使用比较运算过滤时间序列\n\nPrometheus支持以下布尔运算符如下：\n\n1. == (相等)\n\n1. != (不相等)\n\n1. `> (大于)\n\n1. < (小于)\n\n1. `>= (大于等于)\n\n1. <= (小于等于)\n\n- 获取当前内存使用率超过95%的主机：\n\n `(node_memory_bytes_total - node_memory_free_bytes_total) / node_memory_bytes_total > 0.95` \n\n# 三、布尔运算符依次比较过滤瞬时向量\n\n1. 查询当前模块的HTTP请求量是否>=1000，如果大于等于1000则返回1（true）否则返回0（false）。\n\n `http_requests_total > bool 1000` \n\n2. 如果是在两个标量之间使用布尔运算，则必须使用bool修饰符\n\n `2 == bool 2`  # 结果为1\n\n# 四、集合运算符\n\n1. Prometheus支持and (并且)、or (或者)、unless (排除)以下集合运算符：\n\n- vector1 and vector2：产生的新向量包含vector1和vector2中的公共元素。\n- vector1 or vector2：产生的新向量包含vector1和verctor2中的所有元素\n- vector1 unless vector2：产生的新向量包含除了verctor1和verctor2中所有元素。\n\n# 五、操作符优先级\n\n1. 在PromQL操作符中优先级由高到低依次为：\n\n ^\n *, /, %\n +, -\n ==, !=, <=, <, >=, >\n and, unless\n or\n\n', 1, 0, 0, '2021-01-26 19:43:33.559307', '2021-01-26 20:22:33.721582', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (172, '匹配模式', '[TOC]\n\n- 向量与向量之间进行运算操作时会基于默认的匹配规则：依次找到与左边向量元素匹配（标签完全一致）的右边向量元素进行运算，如果没找到匹配元素，则直接丢弃。\n\n# 一、一对一匹配\n\n1. 从操作符两边表达式获取的瞬时向量依次比较并找到唯一匹配(标签完全一致)的样本值。\n\n 默认情况下，使用表达式：\n\n  `vector1 <operator> vector2` \n\n2. 在操作符两边表达式标签不一致的情况下，可以使用on(label list)或者ignoring(label list）来修改便签的匹配行为。\n\n- 使用ignoreing可以在匹配时忽略某些便签。on则用于将匹配行为限定在某些便签之内。\n- 查询过去5分钟内，HTTP请求状态码为500的在所有请求中的比例。如果没有使用ignoring(code)，操作符两边表达式返回的瞬时向量中将找不到任何一个标签完全相同的匹配项。\n\n method_code: http_errors:rate5m{code=\"500\"} / ignoring(code) method: http_requests:rate5m \n\n# 二、多对一和一对多\n\n1. 多对一和一对多两种匹配模式指的是“一”侧的每一个向量元素可以与\"多\"侧的多个元素匹配的情况。\n1. 在这种情况下，必须使用group修饰符：group_left或者group_right来确定哪一个向量具有更高的基数（充当“多”的角色）。\n1. 多对一和一对多两种模式一定是出现在操作符两侧表达式返回的向量标签不一致的情况。因此需要使用ignoring和on修饰符来排除或者限定匹配的标签列表。\n\n 例如,使用表达式：\n\n method_code: http_errors  : rate5m / ignoring(code) group_left method: http_requests:rate5m`\n该表达式中，左向量method_code: http_errors:rate5m包含两个标签method和code。而右向量method: http_requests:rate5m中只包含一个标签method，因此匹配时需要使用ignoring限定匹配的标签为code。 在限定匹配标签后，右向量中的元素可能匹配到多个左向量中的元素 因此该表达式的匹配模式为多对一，需要使用group修饰符group_left指定左向量具有更好的基数。\n\n4. group修饰符只能在比较和数学运算符中使用。在逻辑运算and,unless和or才注意操作中默认与右向量中的所有元素进行匹配。', 0, 0, 0, '2021-01-26 19:50:14.895262', '2021-01-26 19:50:14.895306', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (173, '聚合操作', '[TOC]\n\n1. prometheus内置的聚合操作符可以将瞬时表达式返回的样本数据进行聚合，形成一个新的时间序列。\n\n- sum  (求和)\n- min  (最小值)\n- max  (最大值)\n- avg  (平均值)\n- stddev  (标准差)\n- stdvar  (标准方差)\n- count  (计数)\n- count_values  (对value进行计数)\n- bottomk  (后n条时序)\n- topk  (前n条时序)\n- quantile  (分位数)\n\n2. 使用聚合操作的语法如下：\n\n <aggr-op>([parameter,]  <vector expression>) [without|by (<label list>)]\n\n 其中只有count_values, quantile,       topk, bottomk支持参数(parameter)。\n\n3. without用于从计算结果中移除列举的标签，而保留其它标签。by则正好相反，结果向量中只保留列出的标签，其余标签则移除。通过without和by可以按照样本的问题对数据进行聚合。\n\n- 例如：\n sum(http_requests_total)  without (instance)\n\n 等价于\n\n sum(http_requests_total)  by (code,handler,job,method)\n- 如果只需要计算整个应用的HTTP请求总量，可以直接使用表达式：\n\n sum(http_requests_total)\n\n4. count_values用于时间序列中每一个样本值出现的次数。count_values会为每一个唯一的样本值输出一个时间序列，并且每一个时间序列包含一个额外的标签。\n\n- 例如：\n\n count_values(\"count\",  http_requests_total)\n\n5. topk和bottomk则用于对样本值进行排序，返回当前样本值前n位，或者后n位的时间序列。\n\n- 获取HTTP请求数前5位的时序样本数据，可以使用表达式：\n- topk(5,  http_requests_total)\n- quantile用于计算当前样本数据值的分布情况quantile(φ,  express)其中0 ≤ φ ≤ 1。\n- 例如，当φ为0.5时，即表示找到当前样本数据中的中位数：\n- quantile(0.5, http_requests_total)', 1, 0, 0, '2021-01-26 19:51:34.935696', '2021-01-26 20:19:30.831315', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (174, '内置函数', '[TOC]\n\n# 一、计算Counter指标增长率\n\n1. Counter类型的监控指标其特点是只增不减，在没有发生重置（如服务器重启，应用重启）的情况下其样本值应该是不断增大的。为了能够更直观的表示样本数据的变化剧烈情况，需要计算样本的增长速率。\n1. increase(v      range-vector)函数。其中参数v是一个区间向量，increase函数获取区间向量中的第一个后最后一个样本并返回其增长量。\n\n 例如：获取时间序列最近两分钟平均CPU使用率\n\n increase(node_cpu[2m]) / 120\n通过node_cpu[2m]获取时间序列最近两分钟的所有样本，increase计算出最近两分钟的增长量，最后除以时间120秒得到node_cpu样本在最近两分钟的平均增长率。并且这个值也近似于主机节点最近两分钟内的平均CPU使用率。\n\n1. PromQL中还直接内置了rate(v      range-vector)函数，rate函数可以直接计算区间向量v在时间窗口内平均增长速率。因此，通过以下表达式可以得到与increase函数相同的结果：\n\n rate(node_cpu[2m])\n\n- 需要注意的是使用rate或者increase函数去计算样本的平均增长速率，无法反应在时间窗口内样本数据的突发变化。      为了解决该问题，PromQL提供了另外一个灵敏度更高的函数irate(v range-vector)。\n\n1. irate同样用于计算区间向量的计算率，但是其反应出的是瞬时增长率。irate函数是通过区间向量中最后两个样本数据来计算区间向量的增长速率。并且体现出更好的灵敏度，通过irate函数绘制的图标能够更好的反应样本数据的瞬时变化状态。\n\n irate(node_cpu[2m])\n\n- irate函数相比于rate函数提供了更高的灵敏度，不过当需要分析长期趋势或者在告警规则中，irate的这种灵敏度反而容易造成干扰。因此在长期趋势分析或者告警中更推荐使用rate函数。\n\n# 二、预测Gauge指标变化趋势\n\n1. PromQL中内置的predict_linear(v      range-vector, t scalar)      函数可以预测时间序列v在t秒后的值。它基于简单线性回归的方式，对时间窗口内的样本数据进行统计，从而可以对时间序列的变化趋势做出预测。\n\n- 例如，基于2小时的样本数据，来预测主机可用磁盘空间的是否在4个小时候被占满，可以使用如下表达式：\n\n predict_linear(node_filesystem_free{job=\"node\"}[2h], 4 * 3600) < 0\n\n# 三、统计Histogram指标的分位数\n\nHistogram和Summary都可以用于统计和分析数据的分布情况。区别在于Summary是直接在客户端计算了数据分布的分位数情况。而Histogram的分位数计算需要通过histogram_quantile(φ float, b instant-vector)函数进行计算。其中φ（0<φ<1）表示需要计算的分位数，如果需要计算中位数φ取值为0.5，以此类推即可。\n以指标http_request_duration_seconds_bucket为例：\n\n `# HELP http_request_duration_seconds request duration histogram`\n\n `# TYPE http_request_duration_seconds histogram`\n\n http_request_duration_seconds_bucket{le=\"0.5\"} 0\n http_request_duration_seconds_bucket{le=\"1\"} 1\n http_request_duration_seconds_bucket{le=\"2\"} 2\n http_request_duration_seconds_bucket{le=\"3\"} 3\n http_request_duration_seconds_bucket{le=\"5\"} 3\n http_request_duration_seconds_bucket{le=\"+Inf\"} 3\n http_request_duration_seconds_sum 6\n http_request_duration_seconds_count 3\n\n# 四、动态标签替换\n\n1. 使用PromQL查询到时间序列后，可视化工具会根据时间序列的标签来渲染图表，为了能够让客户端的图标更具有可读性，可以通过label_replace标签为时间序列添加额外的标签。\n1. label_replace的具体参数如下：\n\n label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)\n\n1. 该函数会依次对v中的每一条时间序列进行处理，通过regex匹配src_label的值，并将匹配部分relacement写入到dst_label标签中。\n\n 如下所示：\n\n label_replace(up, \"host\", \"$1\", \"instance\",  \"(.*):.*\")\n 函数处理后，时间序列将包含一个host标签，host标签的值为Exporter实例的IP地址：\n up{host=\"localhost\",instance=\"localhost:8080\",job=\"cadvisor\"}    1\n up{host=\"localhost\",instance=\"localhost:9090\",job=\"prometheus\"}    1\n up{host=\"localhost\",instance=\"localhost:9100\",job=\"node\"} 1\n\n1. 除了label_replace以外，Prometheus还提供了label_join函数，该函数可以将时间序列中v多个标签src_label的值，通过separator作为连接符写入到一个新的标签dst_label中:\n\n label_join(v instant-vector, dst_label string, separator string, src_label_1 string, src_label_2 string, ...)\n\n1. label_replace和label_join函数提供了对时间序列标签的自定义能力，从而能够更好的于客户端或者可视化工具配合。\n\n \n\n', 0, 0, 0, '2021-01-26 19:53:06.233234', '2021-01-26 19:53:06.233268', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (175, '在HTTP API中使用PromQL', '[TOC]\n\n# 一、API响应格式\n\n1. Prometheus API使用了JSON格式的响应内容。      当API调用成功后将会返回2xx的HTTP状态码。\n1. 反之，当API调用失败时可能返回以下几种不同的HTTP状态码：\n\n- 404 Bad Request：当参数错误或者缺失时。\n- 422 Unprocessable Entity      当表达式无法执行时。\n- 503 Service Unavailiable      当请求超时或者被中断时。\n\n# 二、瞬时数据查询\n\n1. 通过使用QUERY      API我们可以查询PromQL在特定时间点下的计算结果。\n\n GET /api/v1/query\n\n1. URL请求参数：\n\n query=：PromQL表达式。\n time=：用于指定用于计算PromQL的时间戳。可选参数，默认情况下使用当前系统时间。\n timeout=：超时设置。可选参数，默认情况下使用-query,timeout的全局设置。\n\n- 例如使用以下表达式查询表达式up在时间点2015-07-01T20:10:51.781Z的计算结果：\n\n curl \'[http://localhost:9090/api/v1/query?query=up&time=2015-07-01T20:10:51.781Z](http://localhost:9090/api/v1/query?query=up&time=2015-07-01T20:10:51.781Z)\'\n\n# 三、响应数据类型\n\n1. 当API调用成功后，Prometheus会返回JSON格式的响应内容\n1. PromQL表达式可能返回多种数据类型，在响应内容中使用resultType表示当前返回的数据类型，包括：\n\n- 瞬时向量：vector\n- 区间向量：matrix\n- 标量：scalar\n- 字符串：string\n\n# 四、区间数据查询\n\n1. 使用QUERY_RANGE      API我们则可以直接查询PromQL表达式在一段时间返回内的计算结果。\n\n GET /api/v1/query_range\n\n1. URL请求参数：\n\n- query=: PromQL表达式。\n- start=: 起始时间。\n- end=: 结束时间。\n- step=: 查询步长。\n- timeout=:      超时设置。可选参数，默认情况下使用-query,timeout的全局设置。\n\n2. 例如使用以下表达式查询表达式up在30秒范围内以15秒为间隔计算PromQL表达式的结果。\n\n curl \'[http://localhost:9090/api/v1/query_range?query=up&start=2015-07-01T20:10:30.781Z&end=2015-07-01T20:11:00.781Z&step=15s](http://localhost:9090/api/v1/query_range?query=up&start=2015-07-01T20:10:30.781Z&end=2015-07-01T20:11:00.781Z&step=15s)\'', 0, 0, 0, '2021-01-26 19:53:56.185447', '2021-01-26 19:53:56.185492', 1, 1, 3, 0);
INSERT INTO `blog_section` VALUES (176, '最佳实践', '[TOC]\n\n# 一、监控维度\n\n| 级别              | 监控什么                                                   | Exporter                          |\n| ----------------- | ---------------------------------------------------------- | --------------------------------- |\n| 网络              | 网络协议：http、dns、tcp、icmp；网络硬件：路由器，交换机等 | BlackBox   Exporter;SNMP Exporter |\n| 主机              | 资源用量                                                   | node   exporter                   |\n| 容器              | 资源用量                                                   | cAdvisor                          |\n| 应用(包括Library) | 延迟，错误，QPS，内部状态等                                | 代码中集成Prmometheus Client      |\n| 中间件状态        | 资源用量，以及服务状态                                     | 代码中集成Prmometheus Client      |\n| 编排工具          | 集群资源用量，调度等                                       | Kubernetes   Components           |\n\n# 二、监控指标\n\n1. 延迟：服务请求所需时间。\n\n 记录用户所有请求所需的时间，重点是要区分成功请求的延迟时间和失败请求的延迟时间。 例如在数据库或者其他关键祸端服务异常触发HTTP 500的情况下，用户也可能会很快得到请求失败的响应内容，如果不加区分计算这些请求的延迟，可能导致计算结果与实际结果产生巨大的差异。\n\n1. 通讯量：监控当前系统的流量，用于衡量服务的容量需求。\n\n 流量对于不同类型的系统而言可能代表不同的含义。例如，在HTTP REST API中, 流量通常是每秒HTTP请求数；\n\n1. 错误：监控当前系统所有发生的错误请求，衡量当前系统错误发生的速率。\n\n 对于失败而言有些是显式的(比如, HTTP 500错误)，而有些是隐式(比如，HTTP响应200，但实际业务流程依然是失败的)。\n 对于一些显式的错误如HTTP 500可以通过在负载均衡器(如Nginx)上进行捕获，而对于一些系统内部的异常，则可能需要直接从服务中添加钩子统计并进行获取。\n\n1. 饱和度：衡量当前服务的饱和度。\n\n 主要强调最能影响服务状态的受限制的资源。 例如，如果系统主要受内存影响，那就主要关注系统的内存状态，如果系统主要受限与磁盘I/O，那就主要观测磁盘I/O的状态。因为通常情况下，当这些资源达到饱和后，服务的性能会明显下降。同时还可以利用饱和度对系统做出预测，比如，“磁盘是否可能在4个小时候就满了”。\n\n# 三、RED方法\n\n1. RED方法是Weave      Cloud在基于Google的“4个黄金指标”的原则下结合Prometheus以及Kubernetes容器实践，主要关注以下三种关键指标：\n\n- (请求)速率：服务每秒接收的请求数。\n- (请求)错误：每秒失败的请求数。\n- (请求)耗时：每个请求的耗时。\n\n2. 在“4大黄金信号”的原则下，RED方法可以有效的帮助用户衡量云原生以及微服务应用下的用户体验问题。\n\n# 四、USE方法\n\n1. USE方法全称\"Utilization      Saturation and Errors Method\"，主要用于分析系统性能问题，可以指导用户快速识别资源瓶颈以及错误的方法。\n\n- 使用率：关注系统资源的使用情况。      这里的资源主要包括但不限于：CPU，内存，网络，磁盘等等。100%的使用率通常是系统性能瓶颈的标志。\n- 饱和度：例如CPU的平均运行排队长度，这里主要是针对资源的饱和度(注意，不同于4大黄金信号)。任何资源在某种程度上的饱和都可能导致系统性能的下降。\n- 错误：错误计数。例如：“网卡在数据包传输过程中检测到的以太网网络冲突了14次”。', 0, 0, 0, '2021-01-26 19:54:41.462431', '2021-01-26 19:54:41.462467', 1, 1, 3, 0);
COMMIT;

-- ----------------------------
-- Table structure for blog_tag
-- ----------------------------
DROP TABLE IF EXISTS `blog_tag`;
CREATE TABLE `blog_tag` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of blog_tag
-- ----------------------------
BEGIN;
INSERT INTO `blog_tag` VALUES (1, '入门简介');
INSERT INTO `blog_tag` VALUES (2, '实验案例');
INSERT INTO `blog_tag` VALUES (3, '理论基础');
INSERT INTO `blog_tag` VALUES (4, '容器');
INSERT INTO `blog_tag` VALUES (5, '数据库');
INSERT INTO `blog_tag` VALUES (6, '服务部署');
INSERT INTO `blog_tag` VALUES (7, '虚拟化');
INSERT INTO `blog_tag` VALUES (8, '编程开发');
INSERT INTO `blog_tag` VALUES (9, '经验总结');
COMMIT;

-- ----------------------------
-- Table structure for captcha_captchastore
-- ----------------------------
DROP TABLE IF EXISTS `captcha_captchastore`;
CREATE TABLE `captcha_captchastore` (
  `id` int NOT NULL AUTO_INCREMENT,
  `challenge` varchar(32) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `response` varchar(32) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `hashkey` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `expiration` datetime(6) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `hashkey` (`hashkey`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=5232 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of captcha_captchastore
-- ----------------------------
BEGIN;
INSERT INTO `captcha_captchastore` VALUES (5222, 'HOCS', 'hocs', '5ad456b5853a0fca686de05fcb0ea25ffa0903c3', '2021-01-17 00:32:34.106305');
INSERT INTO `captcha_captchastore` VALUES (5223, 'IHQT', 'ihqt', '8828cea530e5697461979aace027ecdf412617e2', '2021-01-17 06:28:44.076061');
INSERT INTO `captcha_captchastore` VALUES (5224, 'VEXH', 'vexh', '09026175f96c84cda3dfbc5ce22b49a44a54e6f4', '2021-01-17 14:18:46.105071');
INSERT INTO `captcha_captchastore` VALUES (5225, 'WIGQ', 'wigq', '52f94528f3b3c79279507d0b54371352ae065287', '2021-01-19 09:29:28.967688');
INSERT INTO `captcha_captchastore` VALUES (5226, 'RZOL', 'rzol', '884975923b348e3e6ac12d9c3a108c5d14b33155', '2021-01-21 04:21:15.759985');
INSERT INTO `captcha_captchastore` VALUES (5227, 'HSJZ', 'hsjz', '17cfecc713f052333db7b2f9e978a114c27e90af', '2021-01-21 12:50:15.320728');
INSERT INTO `captcha_captchastore` VALUES (5228, 'ACVU', 'acvu', '66b1cd29a1e447e23610e16dcddcbc9b8b9ade45', '2021-01-21 14:17:59.880395');
INSERT INTO `captcha_captchastore` VALUES (5229, 'FHLE', 'fhle', 'ea8e6778ae868cb07f14d6998eee71a00961ef61', '2021-01-23 07:13:44.832379');
INSERT INTO `captcha_captchastore` VALUES (5230, 'QMQL', 'qmql', '5430e6474860dbd005b628eeb0ed1f3413d2a182', '2021-01-26 02:42:29.438897');
INSERT INTO `captcha_captchastore` VALUES (5231, 'EXRI', 'exri', '345583c36449498f16a450cf7017e59843c0369a', '2021-01-27 09:24:19.148789');
COMMIT;

-- ----------------------------
-- Table structure for django_admin_log
-- ----------------------------
DROP TABLE IF EXISTS `django_admin_log`;
CREATE TABLE `django_admin_log` (
  `id` int NOT NULL AUTO_INCREMENT,
  `action_time` datetime(6) NOT NULL,
  `object_id` longtext CHARACTER SET utf8 COLLATE utf8_general_ci,
  `object_repr` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `action_flag` smallint unsigned NOT NULL,
  `change_message` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `content_type_id` int DEFAULT NULL,
  `user_id` int NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  KEY `django_admin_log_content_type_id_c4bce8eb_fk_django_co` (`content_type_id`) USING BTREE,
  KEY `django_admin_log_user_id_c564eba6_fk_auth_user_id` (`user_id`) USING BTREE,
  CONSTRAINT `django_admin_log_content_type_id_c4bce8eb_fk_django_co` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `django_admin_log_user_id_c564eba6_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=445 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of django_admin_log
-- ----------------------------
BEGIN;
INSERT INTO `django_admin_log` VALUES (1, '1900-01-20 03:04:54.354329', '1', 'Linux', 1, '[{\"added\": {}}]', 8, 1);
INSERT INTO `django_admin_log` VALUES (2, '1900-01-20 03:04:58.685156', '2', 'MySQL', 1, '[{\"added\": {}}]', 8, 1);
INSERT INTO `django_admin_log` VALUES (3, '1900-01-20 03:05:03.516733', '3', 'Python', 1, '[{\"added\": {}}]', 8, 1);
INSERT INTO `django_admin_log` VALUES (4, '1900-01-20 03:05:07.749885', '4', 'Django', 1, '[{\"added\": {}}]', 8, 1);
INSERT INTO `django_admin_log` VALUES (5, '1900-01-20 03:05:12.375609', '5', 'docker', 1, '[{\"added\": {}}]', 8, 1);
INSERT INTO `django_admin_log` VALUES (6, '1900-01-20 03:05:21.423237', '1', '入门简介', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (7, '1900-01-20 03:05:29.647371', '2', '实验案例', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (8, '1900-01-20 03:05:35.850018', '3', '理论基础', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (9, '1900-01-20 03:06:37.686440', '1', '三字经', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (10, '1900-01-20 03:07:08.563856', '2', '百家姓', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (11, '1900-01-20 03:16:13.816038', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (12, '1900-01-20 03:16:32.610071', '2', '百家姓', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u7b7e\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (13, '1900-01-20 07:00:45.749527', '4', '绕口令', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (14, '1900-01-20 07:00:54.412350', '5', '笑话', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (15, '1900-01-20 07:01:01.889103', '6', 'ftp', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (16, '1900-01-20 07:01:08.141513', '7', '虚拟化', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (17, '1900-01-20 07:01:13.380415', '8', '监控', 1, '[{\"added\": {}}]', 10, 1);
INSERT INTO `django_admin_log` VALUES (18, '1900-01-20 07:01:19.546021', '3', '报菜名', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (19, '1900-01-20 07:01:36.097640', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u9605\\u8bfb\\u91cf\", \"\\u70b9\\u8d5e\\u6570\", \"\\u6587\\u7ae0\\u8bc4\\u5206\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (20, '1900-01-20 07:01:50.102753', '2', '百家姓', 2, '[{\"changed\": {\"fields\": [\"\\u9605\\u8bfb\\u91cf\", \"\\u70b9\\u8d5e\\u6570\", \"\\u6587\\u7ae0\\u8bc4\\u5206\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (21, '1900-01-20 07:05:52.178259', '4', '木兰诗', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (22, '1900-01-20 07:06:30.889194', '5', '七子之歌', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (23, '1900-01-20 07:07:25.462316', '6', '明月几时有', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (24, '1900-01-20 15:20:51.675688', '7', '金色花', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (25, '1900-01-20 15:26:50.262087', '8', '望庐山瀑布', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (26, '1900-01-20 15:26:59.685892', '4', '木兰诗', 2, '[{\"changed\": {\"fields\": [\"\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (27, '1900-01-20 15:27:32.999200', '9', '秋思', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (28, '1900-01-20 15:28:08.546596', '10', '悯农', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (29, '1900-01-20 15:37:34.890838', '10', '悯农', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (30, '1900-01-20 15:37:50.324197', '9', '秋思', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (31, '1900-01-20 15:38:22.244789', '8', '望庐山瀑布', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (32, '1900-01-20 15:38:33.811497', '7', '金色花', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (33, '1900-01-20 15:38:42.808225', '5', '七子之歌', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (34, '1900-01-20 15:39:34.877060', '4', '木兰诗', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (35, '1900-01-20 15:39:44.348051', '3', '报菜名', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (36, '1900-01-20 15:39:51.949927', '6', '明月几时有', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (37, '1900-01-20 15:40:00.667544', '2', '百家姓', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (38, '1900-01-20 15:40:09.377037', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\\u56fe\\u7247\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (39, '1900-01-20 16:44:37.004198', '11', '陈情表', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (40, '1900-01-20 17:04:53.084891', '5', '七子之歌', 2, '[{\"changed\": {\"fields\": [\"\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (41, '1900-01-20 17:05:06.616766', '2', '百家姓', 2, '[{\"changed\": {\"fields\": [\"\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (42, '1900-01-20 11:50:45.817027', '11', '陈情表', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (43, '1900-01-20 11:51:04.970437', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (44, '1900-01-20 11:51:10.193428', '2', '百家姓', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (45, '1900-01-20 11:51:18.324359', '3', '报菜名', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (46, '1900-01-20 11:51:31.179963', '4', '木兰诗', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (47, '1900-01-20 11:51:39.933963', '7', '金色花', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (48, '1900-01-20 23:00:20.050150', '1', 'user:admin', 1, '[{\"added\": {}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (49, '1900-01-20 23:00:45.989106', '9', '112233', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (50, '1900-01-20 23:00:46.004093', '17', '1122332211', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (51, '1900-01-20 23:00:46.016963', '15', '1122334455', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (52, '1900-01-20 23:00:46.033394', '4', '123', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (53, '1900-01-20 23:00:46.045936', '10', '123123124', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (54, '1900-01-20 23:00:46.058354', '16', '123321', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (55, '1900-01-20 23:00:46.071435', '8', '123445', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (56, '1900-01-20 23:00:46.083956', '5', 'adfasd', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (57, '1900-01-20 23:00:46.096702', '6', 'adsfsadf', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (58, '1900-01-20 23:00:46.109323', '7', 'asdfsdafasdfsda', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (59, '1900-01-20 23:01:30.179413', '18', 'cuiliang', 1, '[{\"added\": {}}]', 4, 1);
INSERT INTO `django_admin_log` VALUES (60, '1900-01-20 23:01:57.703036', '18', 'cuiliang', 2, '[{\"changed\": {\"fields\": [\"Email address\"]}}]', 4, 1);
INSERT INTO `django_admin_log` VALUES (61, '1900-01-20 23:03:03.161072', '2', 'user:cuiliang', 1, '[{\"added\": {}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (62, '1900-01-20 23:56:52.416385', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (63, '1900-01-20 23:57:04.468841', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (64, '1900-01-20 00:06:33.462807', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (65, '1900-01-20 00:06:44.862255', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (66, '1900-01-20 00:07:14.366233', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (67, '1900-01-20 00:07:46.694598', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (68, '1900-01-20 00:07:56.029233', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u5934\\u50cf\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (69, '1900-01-20 12:54:18.356347', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (70, '1900-01-20 13:00:00.162724', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (71, '1900-01-20 13:00:07.352526', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (72, '1900-01-20 13:00:15.384872', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (73, '1900-01-20 13:00:21.686227', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (74, '1900-01-20 13:01:15.826767', '11', '陈情表', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (75, '1900-01-20 13:01:34.108586', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (76, '1900-01-20 13:01:45.171620', '5', '七子之歌', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (77, '1900-01-20 13:01:52.520664', '8', '望庐山瀑布', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (78, '1900-01-20 13:02:03.393927', '9', '秋思', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (79, '1900-01-20 13:02:15.230683', '6', '明月几时有', 2, '[{\"changed\": {\"fields\": [\"\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (80, '1900-01-20 20:38:53.679020', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6027\\u522b\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (81, '1900-01-20 20:39:00.298397', '2', 'user:cuiliang', 2, '[]', 14, 1);
INSERT INTO `django_admin_log` VALUES (82, '1900-01-20 22:29:11.736999', '1', 'user:admin', 2, '[{\"changed\": {\"fields\": [\"\\u4e2a\\u4eba\\u7f51\\u7ad9\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (83, '1900-01-20 22:29:31.441764', '2', 'user:cuiliang', 2, '[{\"changed\": {\"fields\": [\"\\u4e2a\\u4eba\\u7f51\\u7ad9\"]}}]', 14, 1);
INSERT INTO `django_admin_log` VALUES (84, '1900-01-20 09:41:23.565815', '24', '123123123', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (85, '1900-01-20 09:41:23.575937', '29', '123123asd', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (86, '1900-01-20 09:41:23.581958', '32', '123123qw', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (87, '1900-01-20 09:41:23.590933', '25', '123123qwe', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (88, '1900-01-20 09:41:23.595428', '28', '123123qwer', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (89, '1900-01-20 09:41:23.599454', '31', '123123rty', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (90, '1900-01-20 09:41:23.604454', '30', '123123zxc', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (91, '1900-01-20 09:41:23.619835', '21', '123412', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (92, '1900-01-20 09:41:23.633798', '27', '1q2w3e', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (93, '1900-01-20 09:41:23.639688', '20', 'admin123', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (94, '1900-01-20 09:41:23.645367', '22', 'asdfsadf', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (95, '1900-01-20 09:41:23.650896', '18', 'cuiliang', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (96, '1900-01-20 09:41:23.656883', '26', 'wang', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (97, '1900-01-20 09:41:23.661261', '23', '崔亮哦', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (98, '1900-01-20 23:11:41.241324', '36', '123445', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (99, '1900-01-20 23:11:41.252316', '35', 'admin123', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (100, '1900-01-20 23:11:41.268308', '33', 'cuiliang', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (101, '1900-01-20 23:11:41.281301', '34', '王艳萍', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (102, '1900-01-20 10:53:55.635777', '37', 'admin123', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (103, '1900-01-20 13:27:51.624698', '38', 'admin1', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (104, '1900-01-20 13:27:51.629647', '40', 'admin11223', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (105, '1900-01-20 13:50:16.273468', '41', 'admin1', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (106, '1900-01-20 15:13:34.210141', '43', 'admin12', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (107, '1900-01-20 15:13:34.215131', '42', 'admin1234', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (108, '1900-01-20 22:17:51.734664', '12', 'django测试页1', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (109, '1900-01-20 14:26:28.340486', '27', 'django测试页17', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u7b7e\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (110, '1900-01-20 11:45:07.086241', '28', '百家姓', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (111, '1900-01-20 14:27:33.828237', '29', '测试内容', 1, '[{\"added\": {}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (112, '1900-01-20 22:21:37.861847', '19', 'django测试页9', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u7b7e\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (113, '1900-01-20 16:48:04.692233', '29', '测试内容', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (114, '1900-01-20 16:48:31.711596', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u9898\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (115, '1900-01-20 16:56:32.686674', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (116, '1900-01-20 16:59:16.657424', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"\\u6458\\u8981\", \"\\u6807\\u7b7e\", \"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (117, '1900-01-20 17:00:02.887420', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (118, '1900-01-20 17:04:25.718269', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (119, '1900-01-20 17:04:47.867530', '29', 'markdown', 2, '[]', 11, 1);
INSERT INTO `django_admin_log` VALUES (120, '1900-01-20 17:09:24.874329', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (121, '1900-01-20 17:10:40.509326', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (122, '1900-01-20 17:36:43.030852', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (123, '1900-01-20 17:37:01.530745', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (124, '1900-01-20 17:37:13.996730', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (125, '1900-01-20 18:15:26.715249', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (126, '1900-01-20 21:49:52.885845', '28', 'markdown2', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u9898\", \"\\u6458\\u8981\", \"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (127, '1900-01-20 22:12:14.420171', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (128, '1900-01-20 23:22:01.942192', '28', 'markdown2', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (129, '1900-01-20 23:22:41.853887', '28', 'markdown2', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (130, '1900-01-20 23:23:06.031980', '28', 'markdown2', 2, '[]', 11, 1);
INSERT INTO `django_admin_log` VALUES (131, '1900-01-20 10:33:12.480607', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (132, '1900-01-20 14:42:32.325362', '29', 'markdown', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (133, '1900-01-20 23:39:08.990126', '45', 'admin1', 2, '[{\"changed\": {\"fields\": [\"Email address\", \"Last login\"]}}]', 4, 1);
INSERT INTO `django_admin_log` VALUES (134, '1900-01-20 23:49:47.208832', '44', 'admin12', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (135, '1900-01-20 17:36:34.592903', '1', 'article:markdown,username:admin1', 1, '[{\"added\": {}}]', 16, 1);
INSERT INTO `django_admin_log` VALUES (136, '1900-01-20 17:36:42.638967', '2', 'article:markdown2,username:admin', 1, '[{\"added\": {}}]', 16, 1);
INSERT INTO `django_admin_log` VALUES (137, '1900-01-20 17:39:28.005027', '1', 'article:markdown,username:admin', 1, '[{\"added\": {}}]', 16, 1);
INSERT INTO `django_admin_log` VALUES (138, '1900-01-20 22:05:35.432757', '1', 'content:你说的真对啊！,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (139, '1900-01-20 22:06:43.949691', '2', 'content:嗯呢，我也觉得是,username:admin1', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (140, '1900-01-20 22:07:44.960951', '3', 'content:那必须的啊,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (141, '1900-01-20 22:08:28.268738', '4', 'content:好好学习天天向上,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (142, '1900-01-20 22:09:07.919721', '5', 'content:支持支持，鼎力支持！,username:admin1', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (143, '1900-01-20 09:59:21.335468', '2', 'content:嗯呢，我也觉得是,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (144, '1900-01-20 09:59:55.266051', '6', 'content:明月几时有,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (145, '1900-01-20 10:00:07.418029', '6', 'content:明月几时有,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (146, '1900-01-20 10:00:21.166914', '3', 'content:那必须的啊,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7528\\u6237\\u540d\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (147, '1900-01-20 10:00:28.103299', '3', 'content:那必须的啊,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (148, '1900-01-20 11:00:48.919417', '4', 'content:好好学习天天向上,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\", \"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (149, '1900-01-20 11:01:01.804646', '5', 'content:支持支持，鼎力支持！,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\", \"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (150, '1900-01-20 11:09:49.770073', '2', 'content:嗯呢，我也觉得是,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (151, '1900-01-20 11:09:55.819335', '6', 'content:明月几时有,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (152, '1900-01-20 11:10:00.554250', '4', 'content:好好学习天天向上,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (153, '1900-01-20 11:10:04.864659', '5', 'content:支持支持，鼎力支持！,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (154, '1900-01-20 11:10:09.821673', '3', 'content:那必须的啊,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (155, '1900-01-20 11:11:25.908841', '7', 'content:留言测试2,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (156, '1900-01-20 11:11:58.980328', '8', 'content:,username:admin1', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (157, '1900-01-20 11:12:30.926687', '7', 'content:留言测试1,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u5185\\u5bb9\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (158, '1900-01-20 11:13:10.901906', '8', 'content:留言测试2,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u5185\\u5bb9\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (159, '1900-01-20 11:13:22.810997', '8', 'content:留言测试2,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (160, '1900-01-20 11:13:51.719778', '9', 'content:留言测试2-1,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (161, '1900-01-20 11:13:58.777878', '8', 'content:留言测试2-2,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u5185\\u5bb9\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (162, '1900-01-20 11:14:18.856900', '9', 'content:留言测试2-1,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\", \"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (163, '1900-01-20 11:14:35.766664', '9', 'content:留言测试2-1,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (164, '1900-01-20 11:14:42.334583', '9', 'content:留言测试2-1,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7528\\u6237\\u540d\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (165, '1900-01-20 11:15:23.156165', '10', 'content:留言测试3,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (166, '1900-01-20 11:15:34.837148', '10', 'content:留言测试3,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u56de\\u590d\\u7559\\u8a00ID\", \"\\u6839\\u7559\\u8a00ID\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (167, '1900-01-20 11:15:57.181686', '10', 'content:留言测试3,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (168, '1900-01-20 15:18:00.508779', '1', 'content:你说的真对啊！,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u70b9\\u8d5e\\u6570\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (169, '1900-01-20 15:18:30.513670', '11', 'content:大家都来留言啊,username:admin', 1, '[{\"added\": {}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (170, '1900-01-20 15:52:48.381989', '11', 'content:大家都来留言啊,username:admin1', 2, '[{\"changed\": {\"fields\": [\"\\u7528\\u6237\\u540d\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (171, '1900-01-20 15:42:23.015659', '19', 'content:那必须的啊,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (172, '1900-01-20 15:44:54.991542', '20', 'content:测试3-1,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u7559\\u8a00\\u7b49\\u7ea7\"]}}]', 17, 1);
INSERT INTO `django_admin_log` VALUES (173, '1900-01-20 16:10:30.994445', '1', 'content:真是好文章啊,username:admin', 1, '[{\"added\": {}}]', 18, 1);
INSERT INTO `django_admin_log` VALUES (174, '1900-01-20 16:10:45.015848', '2', 'content:写的非常好,username:admin1', 1, '[{\"added\": {}}]', 18, 1);
INSERT INTO `django_admin_log` VALUES (175, '1900-01-20 16:35:55.031588', '28', 'markdown2', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (176, '1900-01-20 16:36:40.697804', '28', 'markdown2', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (177, '1900-01-20 16:57:12.113764', '3', 'content:<img src=\"http://127.0.0.1:8000/static/layui/images/face/39.gif\" alt=\"[鼓掌]\">写的太棒了<img src=\"media/comment/2020_09_30_16_55_48_539576.jpg\" alt=\"undefined\" style=\"display:inline-block;max-width:5', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\"]}}]', 18, 1);
INSERT INTO `django_admin_log` VALUES (178, '1900-01-20 17:07:50.677130', '4', 'content:123,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6587\\u7ae0\"]}}]', 18, 1);
INSERT INTO `django_admin_log` VALUES (179, '1900-01-20 23:00:53.832781', '1', 'About object (1)', 1, '[{\"added\": {}}]', 19, 1);
INSERT INTO `django_admin_log` VALUES (180, '1900-01-20 23:15:43.563851', '1', 'About object (1)', 2, '[{\"changed\": {\"fields\": [\"Body\"]}}]', 19, 1);
INSERT INTO `django_admin_log` VALUES (181, '1900-01-20 15:10:02.765585', '32', '百家姓123', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (182, '1900-01-20 15:10:02.771797', '31', '百家姓123', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (183, '1900-01-20 15:10:02.776813', '30', '百家姓123', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (184, '1900-01-20 15:29:04.054414', '1', '三字经', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (185, '1900-01-20 15:29:09.529483', '6', '明月几时有', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (186, '1900-01-20 15:29:14.712526', '9', '秋思', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (187, '1900-01-20 15:29:19.168041', '9', '秋思', 2, '[]', 11, 1);
INSERT INTO `django_admin_log` VALUES (188, '1900-01-20 15:29:24.860335', '11', '陈情表', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (189, '1900-01-20 15:29:31.223606', '4', '木兰诗', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (190, '1900-01-20 15:29:36.750769', '7', '金色花', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (191, '1900-01-20 15:29:48.042014', '10', '悯农', 2, '[{\"changed\": {\"fields\": [\"\\u662f\\u5426\\u63a8\\u8350\\u663e\\u793a\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (192, '1900-01-20 15:32:57.374524', '31', '新的123', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (193, '1900-01-20 15:32:57.381126', '30', '123', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (194, '1900-01-20 15:18:26.024030', '134', 'article:None,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (195, '1900-01-20 15:18:48.996886', '136', 'article:金色花,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (196, '1900-01-20 15:18:49.011886', '20', 'article:悯农,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (197, '1900-01-20 15:18:49.025891', '60', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (198, '1900-01-20 15:18:49.037884', '135', 'article:测试123,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (199, '1900-01-20 15:18:49.050884', '133', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (200, '1900-01-20 15:18:49.064886', '132', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (201, '1900-01-20 15:18:49.083889', '131', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (202, '1900-01-20 15:18:49.099886', '130', 'article:木兰诗,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (203, '1900-01-20 15:18:49.110884', '129', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (204, '1900-01-20 15:18:49.121887', '128', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (205, '1900-01-20 15:18:49.139887', '127', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (206, '1900-01-20 15:18:49.150886', '126', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (207, '1900-01-20 15:18:49.160886', '125', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (208, '1900-01-20 15:18:49.168888', '124', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (209, '1900-01-20 15:18:49.175885', '123', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (210, '1900-01-20 15:18:49.181886', '122', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (211, '1900-01-20 15:18:49.193888', '121', 'article:三字经,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (212, '1900-01-20 15:18:49.200886', '120', 'article:三字经,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (213, '1900-01-20 15:18:49.211887', '119', 'article:三字经,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (214, '1900-01-20 15:18:49.220891', '118', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (215, '1900-01-20 15:18:49.236887', '117', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (216, '1900-01-20 15:18:49.262889', '116', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (217, '1900-01-20 15:18:49.275885', '115', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (218, '1900-01-20 15:18:49.286893', '114', 'article:markdown2,username:admin1', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (219, '1900-01-20 15:18:49.301886', '113', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (220, '1900-01-20 15:18:49.315889', '112', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (221, '1900-01-20 15:18:49.329886', '111', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (222, '1900-01-20 15:18:49.343888', '110', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (223, '1900-01-20 15:18:49.358885', '109', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (224, '1900-01-20 15:18:49.374886', '108', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (225, '1900-01-20 15:18:49.413883', '107', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (226, '1900-01-20 15:18:49.438886', '106', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (227, '1900-01-20 15:18:49.453888', '105', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (228, '1900-01-20 15:18:49.470886', '104', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (229, '1900-01-20 15:18:49.485887', '103', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (230, '1900-01-20 15:18:49.499887', '102', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (231, '1900-01-20 15:18:49.512887', '101', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (232, '1900-01-20 15:18:49.525884', '100', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (233, '1900-01-20 15:18:49.539886', '99', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (234, '1900-01-20 15:18:49.553887', '98', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (235, '1900-01-20 15:18:49.567886', '97', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (236, '1900-01-20 15:18:49.580883', '96', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (237, '1900-01-20 15:18:49.593884', '95', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (238, '1900-01-20 15:18:49.607886', '94', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (239, '1900-01-20 15:18:49.623887', '93', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (240, '1900-01-20 15:18:49.636885', '92', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (241, '1900-01-20 15:18:49.661893', '91', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (242, '1900-01-20 15:18:49.675886', '90', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (243, '1900-01-20 15:18:49.692888', '89', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (244, '1900-01-20 15:18:49.708886', '88', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (245, '1900-01-20 15:18:49.722886', '87', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (246, '1900-01-20 15:18:49.739888', '86', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (247, '1900-01-20 15:18:49.753886', '85', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (248, '1900-01-20 15:18:49.774889', '84', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (249, '1900-01-20 15:18:49.788886', '83', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (250, '1900-01-20 15:18:49.803883', '82', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (251, '1900-01-20 15:18:49.818884', '81', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (252, '1900-01-20 15:18:49.831887', '80', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (253, '1900-01-20 15:18:49.844884', '79', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (254, '1900-01-20 15:18:49.857886', '78', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (255, '1900-01-20 15:18:49.873928', '77', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (256, '1900-01-20 15:18:49.887885', '76', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (257, '1900-01-20 15:18:49.901883', '75', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (258, '1900-01-20 15:18:49.914884', '74', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (259, '1900-01-20 15:18:49.927884', '73', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (260, '1900-01-20 15:18:49.941886', '72', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (261, '1900-01-20 15:18:49.954885', '71', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (262, '1900-01-20 15:18:49.967884', '70', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (263, '1900-01-20 15:18:49.980885', '69', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (264, '1900-01-20 15:18:49.994961', '68', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (265, '1900-01-20 15:18:50.009885', '67', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (266, '1900-01-20 15:18:50.021899', '66', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (267, '1900-01-20 15:18:50.035886', '65', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (268, '1900-01-20 15:18:50.048884', '64', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (269, '1900-01-20 15:18:50.061885', '63', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (270, '1900-01-20 15:18:50.075886', '62', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (271, '1900-01-20 15:18:50.090886', '61', 'article:markdown2,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (272, '1900-01-20 15:18:50.104886', '59', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (273, '1900-01-20 15:18:50.118886', '58', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (274, '1900-01-20 15:18:50.131883', '57', 'article:django测试页6,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (275, '1900-01-20 15:18:50.144888', '56', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (276, '1900-01-20 15:18:50.157888', '55', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (277, '1900-01-20 15:18:50.171884', '54', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (278, '1900-01-20 15:18:50.185888', '53', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (279, '1900-01-20 15:18:50.198883', '52', 'article:望庐山瀑布,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (280, '1900-01-20 15:18:50.212887', '51', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (281, '1900-01-20 15:18:50.228885', '50', 'article:七子之歌,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (282, '1900-01-20 15:18:50.240887', '49', 'article:望庐山瀑布,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (283, '1900-01-20 15:18:50.255885', '48', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (284, '1900-01-20 15:18:50.269885', '47', 'article:陈情表,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (285, '1900-01-20 15:18:50.284886', '46', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (286, '1900-01-20 15:18:50.299885', '45', 'article:报菜名,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (287, '1900-01-20 15:18:50.315886', '44', 'article:markdown,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (288, '1900-01-20 15:18:50.328886', '43', 'article:三字经,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (289, '1900-01-20 15:18:50.342887', '42', 'article:django测试页1,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (290, '1900-01-20 15:18:50.361886', '41', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (291, '1900-01-20 15:18:50.374886', '40', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (292, '1900-01-20 15:18:50.387885', '39', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (293, '1900-01-20 15:18:50.401885', '38', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (294, '1900-01-20 15:18:50.414885', '37', 'article:秋思,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (295, '1900-01-20 15:18:59.907958', '36', 'article:七子之歌,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (296, '1900-01-20 15:18:59.923957', '35', 'article:七子之歌,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (297, '1900-01-20 15:18:59.937956', '34', 'article:七子之歌,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (298, '1900-01-20 15:18:59.951958', '33', 'article:七子之歌,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (299, '1900-01-20 15:18:59.966958', '32', 'article:望庐山瀑布,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (300, '1900-01-20 15:18:59.983956', '31', 'article:望庐山瀑布,username:admin', 3, '', 16, 1);
INSERT INTO `django_admin_log` VALUES (349, '2020-11-10 10:22:08.434462', '14', '测试1', 1, '[{\"added\": {}}]', 9, 1);
INSERT INTO `django_admin_log` VALUES (350, '2020-12-20 23:01:22.360557', '199', 'article:使用阿里云SDK监控主机状态,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (351, '2020-12-20 23:01:41.164322', '197', 'article:Django项目使用CDN加速,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (352, '2020-12-20 23:01:52.134678', '198', 'article:文章测试,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (353, '2020-12-20 23:02:01.905488', '196', 'article:Django使用百度统计进行流量分析,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (354, '2020-12-20 23:02:10.789910', '195', 'article:运维工程师面试总结(含答案),username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (355, '2020-12-20 23:02:19.888165', '194', 'article:linux运维工程师面试题总结,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u5206\\u7c7b\"]}}]', 11, 1);
INSERT INTO `django_admin_log` VALUES (356, '2020-12-20 23:02:48.520755', '12', 'content:pod常用命令,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (357, '2020-12-20 23:02:56.784135', '42', 'content:rook简介,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (358, '2020-12-20 23:03:03.856096', '41', 'content:rbd块存储服务,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (359, '2020-12-20 23:03:11.548675', '40', 'content:rook部署,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (360, '2020-12-20 23:03:20.464895', '39', 'content:ceph,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (361, '2020-12-20 23:03:30.493394', '38', 'content:固定节点调度,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (362, '2020-12-20 23:03:44.077997', '37', 'content:示例,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (363, '2020-12-20 23:03:54.295554', '36', 'content:网络策略,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (364, '2020-12-20 23:04:08.381075', '35', 'content:网络与策略实例,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (365, '2020-12-20 23:04:17.582328', '34', 'content:downwardAPI存储卷,username:admin', 2, '[{\"changed\": {\"fields\": [\"\\u6240\\u5c5e\\u7b14\\u8bb0\"]}}]', 26, 1);
INSERT INTO `django_admin_log` VALUES (366, '2020-12-25 22:28:59.077587', '33', 'content:配置集合ConfigMap,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (367, '2020-12-25 22:28:59.083410', '32', 'content:Ingress案例,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (368, '2020-12-25 22:28:59.088050', '31', 'content:Service类型,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (369, '2020-12-25 22:28:59.092613', '30', 'content:服务发现,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (370, '2020-12-25 22:28:59.096305', '29', 'content:Service资源及模型,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (371, '2020-12-25 22:28:59.100568', '19', 'content:k8s yaml字段大全,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (372, '2020-12-25 22:28:59.106000', '23', 'content:部署ingress控制器,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (373, '2020-12-25 22:28:59.114942', '28', 'content:标签与标签选择器,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (374, '2020-12-25 22:28:59.127144', '1', 'content:Kubernetes特性,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (375, '2020-12-25 22:28:59.131759', '22', 'content:部署helm,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (376, '2020-12-25 22:28:59.135693', '27', 'content:PDB中断预算,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (377, '2020-12-25 22:28:59.139735', '20', 'content:管理Namespace资源,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (378, '2020-12-25 22:28:59.145870', '2', 'content:概念和术语,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (379, '2020-12-25 22:28:59.151155', '26', 'content:Job控制器,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (380, '2020-12-25 22:28:59.154847', '17', 'content:K8S中的资源对象,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (381, '2020-12-25 22:28:59.158236', '10', 'content:命令格式,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (382, '2020-12-25 22:28:59.161631', '24', 'content:部署calico网络组件,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (383, '2020-12-25 22:28:59.165326', '25', 'content:部署dashboard,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (384, '2020-12-25 22:28:59.169713', '21', 'content:Pod生命周期,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (385, '2020-12-25 22:28:59.179291', '5', 'content:前期准备,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (386, '2020-12-25 22:28:59.184583', '9', 'content:部署kubernets,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (387, '2020-12-25 22:28:59.190671', '18', 'content:yuml文件,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (388, '2020-12-25 22:28:59.196031', '16', 'content:日常命令总结,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (389, '2020-12-25 22:28:59.201549', '15', 'content:service常用命令,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (390, '2020-12-25 22:28:59.205542', '14', 'content:存储常用命令,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (391, '2020-12-25 22:28:59.209918', '13', 'content:控制器常用命令,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (392, '2020-12-25 22:28:59.213839', '11', 'content:node常用命令,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (393, '2020-12-25 22:28:59.217740', '3', 'content:集群组件,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (394, '2020-12-25 22:28:59.223894', '8', 'content:抽象对象,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (395, '2020-12-25 22:28:59.229411', '4', 'content:通过阿里云获取gcr.io镜像文件,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (396, '2020-12-25 22:28:59.233210', '7', 'content:部署Harbor私有镜像仓库,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (397, '2020-12-25 22:28:59.237061', '6', 'content:部署elk日志收集,username:admin', 3, '', 26, 1);
INSERT INTO `django_admin_log` VALUES (398, '2020-12-25 22:29:54.293398', '193', 'article:k8s维护-k8s版本升级,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (399, '2020-12-25 22:29:54.298317', '192', 'article:k8s高可用部署-其他高可用部署方式,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (400, '2020-12-25 22:29:54.307540', '186', 'article:k8s维护-添加work节点,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (401, '2020-12-25 22:29:54.314537', '169', 'article:资源对象-Pod生命周期,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (402, '2020-12-25 22:29:54.323517', '191', 'article:kubectl命令-存储常用命令,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (403, '2020-12-25 22:29:54.327787', '181', 'article:Service和Ingress-Ingress案例,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (404, '2020-12-25 22:29:54.331440', '190', 'article:安全-认证,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (405, '2020-12-25 22:29:54.337245', '189', 'article:控制器-Deployment控制器,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (406, '2020-12-25 22:29:54.341356', '188', 'article:k8s部署-部署metrics-server监控组件,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (407, '2020-12-25 22:29:54.346758', '187', 'article:helm-helm常用命令,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (408, '2020-12-25 22:29:54.351897', '182', 'article:k8s维护-日常错误排错,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (409, '2020-12-25 22:29:54.356515', '174', 'article:k8s基础-Kubernetes特性,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (410, '2020-12-25 22:29:54.360302', '167', 'article:资源对象-K8S中的资源对象,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (411, '2020-12-25 22:29:54.370040', '185', 'article:k8s基础-抽象对象,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (412, '2020-12-25 22:29:54.374737', '184', 'article:k8s高可用部署-源码部署,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (413, '2020-12-25 22:29:54.378008', '183', 'article:网络-网络类型,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (414, '2020-12-25 22:29:54.381351', '170', 'article:资源对象-Pod资源对象,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (415, '2020-12-25 22:29:54.384402', '180', 'article:Service和Ingress-Service类型,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (416, '2020-12-25 22:29:54.387410', '179', 'article:Service和Ingress-Service资源及模型,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (417, '2020-12-25 22:29:54.390495', '160', 'article:kubectl命令-service常用命令,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (418, '2020-12-25 22:29:54.393653', '178', 'article:kubectl命令-控制器常用命令,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (419, '2020-12-25 22:29:54.396682', '153', 'article:k8s部署-部署ingress控制器,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (420, '2020-12-25 22:29:54.399682', '177', 'article:k8s部署-部署Prometheus+Grafana,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (421, '2020-12-25 22:29:54.402787', '176', 'article:k8s部署-部署calico网络组件,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (422, '2020-12-25 22:29:54.405911', '175', 'article:k8s部署-部署dashboard,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (423, '2020-12-25 22:29:54.409418', '154', 'article:Jenkins-Jenkins部署,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (424, '2020-12-25 22:29:54.414654', '173', 'article:k8s基础-集群组件,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (425, '2020-12-25 22:29:54.417927', '159', 'article:k8s基础-概念和术语,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (426, '2020-12-25 22:29:54.420993', '156', 'article:k8s部署-helm安装,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (427, '2020-12-25 22:29:54.424119', '171', 'article:资源对象-Pod服务质量（优先级）,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (428, '2020-12-25 22:29:54.427744', '168', 'article:资源对象-资源需求与限制,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (429, '2020-12-25 22:29:54.431289', '166', 'article:kubectl命令-service常用命令,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (430, '2020-12-25 22:29:54.435157', '165', 'article:k8s基础-阿里云获取镜像文件,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (431, '2020-12-25 22:29:54.439233', '164', 'article:Jenkins-Jenkins部署,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (432, '2020-12-25 22:29:54.443939', '163', 'article:k8s部署-部署kubernets,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (433, '2020-12-25 22:29:54.447572', '162', 'article:k8s部署-helm安装,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (434, '2020-12-25 22:29:54.455121', '161', 'article:k8s部署-部署ingress控制器,username:admin1', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (435, '2020-12-25 22:29:54.458554', '157', 'article:k8s部署-部署kubernets,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (436, '2020-12-25 22:29:54.461866', '155', 'article:k8s基础-阿里云获取镜像文件,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (437, '2020-12-25 22:29:54.466349', '158', 'article:资源对象-标签与标签选择器,username:admin', 3, '', 11, 1);
INSERT INTO `django_admin_log` VALUES (438, '2020-12-26 17:10:50.601052', '12', 'cuiliang0302', 3, '', 31, 1);
INSERT INTO `django_admin_log` VALUES (439, '2020-12-26 17:48:03.225375', '57', 'cuiliang0302', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (440, '2020-12-26 17:48:03.230557', '59', 'cuiliang0302d4c944ae49254e4b', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (441, '2020-12-26 18:55:05.258752', '61', '~', 3, '', 4, 1);
INSERT INTO `django_admin_log` VALUES (442, '2020-12-30 22:57:34.146912', '11', 'k8s部署-部署ingress控制器', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\", \"\\u70b9\\u8d5e\\u6570\", \"\\u6536\\u85cf\\u6570\"]}}]', 9, 1);
INSERT INTO `django_admin_log` VALUES (443, '2020-12-30 22:58:30.169960', '4', 'k8s基础-阿里云获取镜像文件', 2, '[{\"changed\": {\"fields\": [\"Body\", \"\\u9605\\u8bfb\\u91cf\", \"\\u6536\\u85cf\\u6570\"]}}]', 9, 1);
INSERT INTO `django_admin_log` VALUES (444, '2021-01-08 23:15:07.060059', '186', 'docker其他与总结', 2, '[{\"changed\": {\"fields\": [\"\\u6807\\u9898\\u540d\\u79f0\"]}}]', 27, 1);
COMMIT;

-- ----------------------------
-- Table structure for django_content_type
-- ----------------------------
DROP TABLE IF EXISTS `django_content_type`;
CREATE TABLE `django_content_type` (
  `id` int NOT NULL AUTO_INCREMENT,
  `app_label` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `model` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE KEY `django_content_type_app_label_model_76bd3d3b_uniq` (`app_label`,`model`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=33 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of django_content_type
-- ----------------------------
BEGIN;
INSERT INTO `django_content_type` VALUES (11, 'account', 'articleviewhistory');
INSERT INTO `django_content_type` VALUES (13, 'account', 'commentmessage');
INSERT INTO `django_content_type` VALUES (12, 'account', 'leavemessage');
INSERT INTO `django_content_type` VALUES (25, 'account', 'sectionmessage');
INSERT INTO `django_content_type` VALUES (26, 'account', 'sectionviewhistory');
INSERT INTO `django_content_type` VALUES (10, 'account', 'userinfo');
INSERT INTO `django_content_type` VALUES (1, 'admin', 'logentry');
INSERT INTO `django_content_type` VALUES (3, 'auth', 'group');
INSERT INTO `django_content_type` VALUES (2, 'auth', 'permission');
INSERT INTO `django_content_type` VALUES (4, 'auth', 'user');
INSERT INTO `django_content_type` VALUES (9, 'blog', 'article');
INSERT INTO `django_content_type` VALUES (27, 'blog', 'catalogue');
INSERT INTO `django_content_type` VALUES (7, 'blog', 'category');
INSERT INTO `django_content_type` VALUES (22, 'blog', 'firstcatalogue');
INSERT INTO `django_content_type` VALUES (21, 'blog', 'note');
INSERT INTO `django_content_type` VALUES (23, 'blog', 'secondcatalogue');
INSERT INTO `django_content_type` VALUES (24, 'blog', 'section');
INSERT INTO `django_content_type` VALUES (8, 'blog', 'tag');
INSERT INTO `django_content_type` VALUES (20, 'captcha', 'captchastore');
INSERT INTO `django_content_type` VALUES (5, 'contenttypes', 'contenttype');
INSERT INTO `django_content_type` VALUES (16, 'management', 'about');
INSERT INTO `django_content_type` VALUES (14, 'management', 'carousel');
INSERT INTO `django_content_type` VALUES (18, 'management', 'imagesconfig');
INSERT INTO `django_content_type` VALUES (19, 'management', 'info');
INSERT INTO `django_content_type` VALUES (15, 'management', 'link');
INSERT INTO `django_content_type` VALUES (17, 'management', 'websiteconfig');
INSERT INTO `django_content_type` VALUES (6, 'sessions', 'session');
INSERT INTO `django_content_type` VALUES (28, 'social_django', 'association');
INSERT INTO `django_content_type` VALUES (29, 'social_django', 'code');
INSERT INTO `django_content_type` VALUES (30, 'social_django', 'nonce');
INSERT INTO `django_content_type` VALUES (32, 'social_django', 'partial');
INSERT INTO `django_content_type` VALUES (31, 'social_django', 'usersocialauth');
COMMIT;

-- ----------------------------
-- Table structure for django_migrations
-- ----------------------------
DROP TABLE IF EXISTS `django_migrations`;
CREATE TABLE `django_migrations` (
  `id` int NOT NULL AUTO_INCREMENT,
  `app` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `applied` datetime(6) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=104 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of django_migrations
-- ----------------------------
BEGIN;
INSERT INTO `django_migrations` VALUES (1, 'contenttypes', '0001_initial', '2020-11-09 14:08:25.915173');
INSERT INTO `django_migrations` VALUES (2, 'auth', '0001_initial', '2020-11-09 14:08:27.238408');
INSERT INTO `django_migrations` VALUES (3, 'blog', '0001_initial', '2020-11-09 14:08:29.216343');
INSERT INTO `django_migrations` VALUES (4, 'blog', '0002_auto_20200924_1659', '2020-11-09 14:08:29.729685');
INSERT INTO `django_migrations` VALUES (5, 'blog', '0003_article_comment', '2020-11-09 14:08:29.987023');
INSERT INTO `django_migrations` VALUES (6, 'blog', '0004_article_is_release', '2020-11-09 14:08:30.219616');
INSERT INTO `django_migrations` VALUES (7, 'blog', '0005_auto_20201010_1520', '2020-11-09 14:08:30.579929');
INSERT INTO `django_migrations` VALUES (8, 'blog', '0006_auto_20201101_2147', '2020-11-09 14:08:30.772300');
INSERT INTO `django_migrations` VALUES (9, 'account', '0001_initial', '2020-11-09 14:08:31.175971');
INSERT INTO `django_migrations` VALUES (10, 'account', '0002_articleview', '2020-11-09 14:08:31.558846');
INSERT INTO `django_migrations` VALUES (11, 'account', '0003_auto_20200927_1725', '2020-11-09 14:08:32.743170');
INSERT INTO `django_migrations` VALUES (12, 'account', '0004_auto_20200927_1727', '2020-11-09 14:08:33.102581');
INSERT INTO `django_migrations` VALUES (13, 'account', '0005_auto_20200927_1738', '2020-11-09 14:08:33.312110');
INSERT INTO `django_migrations` VALUES (14, 'account', '0006_auto_20200927_2229', '2020-11-09 14:08:33.403601');
INSERT INTO `django_migrations` VALUES (15, 'account', '0007_leavemessage', '2020-11-09 14:08:33.752575');
INSERT INTO `django_migrations` VALUES (16, 'account', '0008_auto_20200929_0958', '2020-11-09 14:08:34.277225');
INSERT INTO `django_migrations` VALUES (17, 'account', '0009_auto_20200929_1109', '2020-11-09 14:08:34.471309');
INSERT INTO `django_migrations` VALUES (18, 'account', '0010_auto_20200929_1634', '2020-11-09 14:08:34.664875');
INSERT INTO `django_migrations` VALUES (19, 'account', '0011_remove_leavemessage_comment_img', '2020-11-09 14:08:34.818465');
INSERT INTO `django_migrations` VALUES (20, 'account', '0012_auto_20200930_1558', '2020-11-09 14:08:35.246479');
INSERT INTO `django_migrations` VALUES (21, 'account', '0013_commentmessage_article', '2020-11-09 14:08:35.575924');
INSERT INTO `django_migrations` VALUES (22, 'account', '0014_auto_20201015_1112', '2020-11-09 14:08:36.841748');
INSERT INTO `django_migrations` VALUES (23, 'account', '0015_auto_20201015_1116', '2020-11-09 14:08:37.361765');
INSERT INTO `django_migrations` VALUES (24, 'account', '0016_auto_20201021_0953', '2020-11-09 14:08:37.441763');
INSERT INTO `django_migrations` VALUES (25, 'account', '0017_auto_20201101_2147', '2020-11-09 14:08:38.274509');
INSERT INTO `django_migrations` VALUES (26, 'account', '0018_auto_20201109_1408', '2020-11-09 14:08:38.766549');
INSERT INTO `django_migrations` VALUES (27, 'admin', '0001_initial', '2020-11-09 14:08:39.182344');
INSERT INTO `django_migrations` VALUES (28, 'admin', '0002_logentry_remove_auto_add', '2020-11-09 14:08:39.472962');
INSERT INTO `django_migrations` VALUES (29, 'admin', '0003_logentry_add_action_flag_choices', '2020-11-09 14:08:39.556308');
INSERT INTO `django_migrations` VALUES (30, 'contenttypes', '0002_remove_content_type_name', '2020-11-09 14:08:39.932493');
INSERT INTO `django_migrations` VALUES (31, 'auth', '0002_alter_permission_name_max_length', '2020-11-09 14:08:40.127063');
INSERT INTO `django_migrations` VALUES (32, 'auth', '0003_alter_user_email_max_length', '2020-11-09 14:08:40.341235');
INSERT INTO `django_migrations` VALUES (33, 'auth', '0004_alter_user_username_opts', '2020-11-09 14:08:40.420390');
INSERT INTO `django_migrations` VALUES (34, 'auth', '0005_alter_user_last_login_null', '2020-11-09 14:08:40.614982');
INSERT INTO `django_migrations` VALUES (35, 'auth', '0006_require_contenttypes_0002', '2020-11-09 14:08:40.687381');
INSERT INTO `django_migrations` VALUES (36, 'auth', '0007_alter_validators_add_error_messages', '2020-11-09 14:08:40.831092');
INSERT INTO `django_migrations` VALUES (37, 'auth', '0008_alter_user_username_max_length', '2020-11-09 14:08:41.027322');
INSERT INTO `django_migrations` VALUES (38, 'auth', '0009_alter_user_last_name_max_length', '2020-11-09 14:08:41.212303');
INSERT INTO `django_migrations` VALUES (39, 'auth', '0010_alter_group_name_max_length', '2020-11-09 14:08:41.393832');
INSERT INTO `django_migrations` VALUES (40, 'auth', '0011_update_proxy_permissions', '2020-11-09 14:08:41.592922');
INSERT INTO `django_migrations` VALUES (41, 'auth', '0012_alter_user_first_name_max_length', '2020-11-09 14:08:41.776639');
INSERT INTO `django_migrations` VALUES (42, 'captcha', '0001_initial', '2020-11-09 14:08:42.078259');
INSERT INTO `django_migrations` VALUES (43, 'management', '0001_initial', '2020-11-09 14:08:42.812351');
INSERT INTO `django_migrations` VALUES (44, 'management', '0002_about', '2020-11-09 14:08:43.008515');
INSERT INTO `django_migrations` VALUES (45, 'management', '0003_about_time', '2020-11-09 14:08:43.224504');
INSERT INTO `django_migrations` VALUES (46, 'management', '0004_bloggerinfo_websiteconfig', '2020-11-09 14:08:43.879542');
INSERT INTO `django_migrations` VALUES (47, 'management', '0005_auto_20201021_1013', '2020-11-09 14:08:44.197237');
INSERT INTO `django_migrations` VALUES (48, 'management', '0006_auto_20201021_1016', '2020-11-09 14:08:44.364230');
INSERT INTO `django_migrations` VALUES (49, 'management', '0007_auto_20201022_1259', '2020-11-09 14:08:45.001266');
INSERT INTO `django_migrations` VALUES (50, 'management', '0008_auto_20201022_2144', '2020-11-09 14:08:45.501363');
INSERT INTO `django_migrations` VALUES (51, 'management', '0009_carousel_is_show', '2020-11-09 14:08:45.702387');
INSERT INTO `django_migrations` VALUES (52, 'management', '0010_auto_20201023_1747', '2020-11-09 14:08:45.907918');
INSERT INTO `django_migrations` VALUES (53, 'management', '0011_auto_20201101_2147', '2020-11-09 14:08:45.983797');
INSERT INTO `django_migrations` VALUES (54, 'sessions', '0001_initial', '2020-11-09 14:08:46.216114');
INSERT INTO `django_migrations` VALUES (55, 'management', '0012_auto_20201109_1523', '2020-11-09 15:23:53.536149');
INSERT INTO `django_migrations` VALUES (56, 'blog', '0007_auto_20201110_1502', '2020-11-10 15:03:05.949044');
INSERT INTO `django_migrations` VALUES (57, 'management', '0013_auto_20201110_1558', '2020-11-10 15:58:22.565090');
INSERT INTO `django_migrations` VALUES (58, 'blog', '0008_note', '2020-12-02 20:00:47.130381');
INSERT INTO `django_migrations` VALUES (59, 'blog', '0009_auto_20201120_2305', '2020-12-02 20:00:47.141039');
INSERT INTO `django_migrations` VALUES (60, 'blog', '0010_remove_note_url', '2020-12-02 20:00:47.329388');
INSERT INTO `django_migrations` VALUES (61, 'blog', '0011_catalogue_content', '2020-12-02 20:00:47.438379');
INSERT INTO `django_migrations` VALUES (62, 'blog', '0012_auto_20201122_1340', '2020-12-02 20:00:47.631890');
INSERT INTO `django_migrations` VALUES (63, 'blog', '0013_auto_20201122_1402', '2020-12-02 20:00:47.731324');
INSERT INTO `django_migrations` VALUES (64, 'blog', '0014_auto_20201122_1420', '2020-12-02 20:00:47.803655');
INSERT INTO `django_migrations` VALUES (65, 'blog', '0015_auto_20201122_1457', '2020-12-02 20:00:47.917441');
INSERT INTO `django_migrations` VALUES (66, 'blog', '0016_auto_20201123_1018', '2020-12-02 20:00:48.280405');
INSERT INTO `django_migrations` VALUES (67, 'blog', '0017_note_is_release', '2020-12-02 20:00:48.331357');
INSERT INTO `django_migrations` VALUES (68, 'blog', '0018_remove_note_is_release', '2020-12-02 20:00:48.390791');
INSERT INTO `django_migrations` VALUES (69, 'blog', '0019_auto_20201123_1035', '2020-12-02 20:00:48.594502');
INSERT INTO `django_migrations` VALUES (70, 'blog', '0020_content_collection', '2020-12-02 20:00:48.705593');
INSERT INTO `django_migrations` VALUES (71, 'account', '0019_auto_20201201_2134', '2020-12-02 20:00:48.783273');
INSERT INTO `django_migrations` VALUES (72, 'account', '0020_remove_articleviewhistory_category', '2020-12-02 20:00:49.065578');
INSERT INTO `django_migrations` VALUES (73, 'account', '0021_auto_20201202_1146', '2020-12-02 20:00:49.231634');
INSERT INTO `django_migrations` VALUES (74, 'blog', '0021_auto_20201202_1352', '2020-12-02 20:00:49.715566');
INSERT INTO `django_migrations` VALUES (75, 'account', '0022_auto_20201202_1400', '2020-12-02 20:00:49.794976');
INSERT INTO `django_migrations` VALUES (76, 'account', '0023_auto_20201202_1426', '2020-12-02 20:00:49.908872');
INSERT INTO `django_migrations` VALUES (77, 'account', '0024_auto_20201202_1456', '2020-12-02 20:00:49.994298');
INSERT INTO `django_migrations` VALUES (78, 'blog', '0022_auto_20201206_2253', '2020-12-08 13:24:27.755689');
INSERT INTO `django_migrations` VALUES (79, 'blog', '0023_catalogue_section', '2020-12-08 13:24:27.954634');
INSERT INTO `django_migrations` VALUES (80, 'blog', '0024_auto_20201208_1324', '2020-12-08 13:24:27.999059');
INSERT INTO `django_migrations` VALUES (81, 'blog', '0024_auto_20201219_1601', '2020-12-20 22:56:28.132893');
INSERT INTO `django_migrations` VALUES (82, 'account', '0025_auto_20201219_2043', '2020-12-20 22:56:28.250289');
INSERT INTO `django_migrations` VALUES (83, 'account', '0026_auto_20201220_1901', '2020-12-20 22:56:28.370032');
INSERT INTO `django_migrations` VALUES (84, 'account', '0027_sectionviewhistory_note', '2020-12-20 22:56:28.459364');
INSERT INTO `django_migrations` VALUES (85, 'default', '0001_initial', '2020-12-21 08:54:27.125291');
INSERT INTO `django_migrations` VALUES (86, 'social_auth', '0001_initial', '2020-12-21 08:54:27.131587');
INSERT INTO `django_migrations` VALUES (87, 'default', '0002_add_related_name', '2020-12-21 08:54:27.318690');
INSERT INTO `django_migrations` VALUES (88, 'social_auth', '0002_add_related_name', '2020-12-21 08:54:27.325853');
INSERT INTO `django_migrations` VALUES (89, 'default', '0003_alter_email_max_length', '2020-12-21 08:54:27.384840');
INSERT INTO `django_migrations` VALUES (90, 'social_auth', '0003_alter_email_max_length', '2020-12-21 08:54:27.389599');
INSERT INTO `django_migrations` VALUES (91, 'default', '0004_auto_20160423_0400', '2020-12-21 08:54:27.414691');
INSERT INTO `django_migrations` VALUES (92, 'social_auth', '0004_auto_20160423_0400', '2020-12-21 08:54:27.419561');
INSERT INTO `django_migrations` VALUES (93, 'social_auth', '0005_auto_20160727_2333', '2020-12-21 08:54:27.445277');
INSERT INTO `django_migrations` VALUES (94, 'social_django', '0006_partial', '2020-12-21 08:54:27.504295');
INSERT INTO `django_migrations` VALUES (95, 'social_django', '0007_code_timestamp', '2020-12-21 08:54:27.555432');
INSERT INTO `django_migrations` VALUES (96, 'social_django', '0008_partial_timestamp', '2020-12-21 08:54:27.606896');
INSERT INTO `django_migrations` VALUES (97, 'social_django', '0009_auto_20191118_0520', '2020-12-21 08:54:27.751806');
INSERT INTO `django_migrations` VALUES (98, 'social_django', '0010_uid_db_index', '2020-12-21 08:54:27.815955');
INSERT INTO `django_migrations` VALUES (99, 'social_django', '0002_add_related_name', '2020-12-21 08:54:27.827040');
INSERT INTO `django_migrations` VALUES (100, 'social_django', '0003_alter_email_max_length', '2020-12-21 08:54:27.833439');
INSERT INTO `django_migrations` VALUES (101, 'social_django', '0001_initial', '2020-12-21 08:54:27.843165');
INSERT INTO `django_migrations` VALUES (102, 'social_django', '0004_auto_20160423_0400', '2020-12-21 08:54:27.852968');
INSERT INTO `django_migrations` VALUES (103, 'social_django', '0005_auto_20160727_2333', '2020-12-21 08:54:27.861772');
COMMIT;

-- ----------------------------
-- Table structure for django_session
-- ----------------------------
DROP TABLE IF EXISTS `django_session`;
CREATE TABLE `django_session` (
  `session_key` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `session_data` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `expire_date` datetime(6) NOT NULL,
  PRIMARY KEY (`session_key`) USING BTREE,
  KEY `django_session_expire_date_a5c62663` (`expire_date`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of django_session
-- ----------------------------
BEGIN;
INSERT INTO `django_session` VALUES ('0tpb898zoagpjuiqi4x6ansd3g2ke4v8', 'e30:1kEkn8:wccfD4Fj6wr6OUtXQi6uMQC9l4Ily45BU_KFQk2gBlY', '1900-01-20 10:52:30.498369');
INSERT INTO `django_session` VALUES ('1eid0v0rmrouxvikvrd3t0g8xi5s5ub0', 'eyJlbWFpbF9jb2RlIjoiODg5OTQ5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHijw:q6oFyh3LGoknANzi5S441mHhU5Yuw1RMXqA5cvKDRMg', '1900-01-20 15:20:28.384577');
INSERT INTO `django_session` VALUES ('1lwt0f0cgxtnki9nnds363g45grx8lto', 'eyJlbWFpbF9jb2RlIjoiMjcxMDUyIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGbcS:MA2hoRSGpJcrZ-aiADJkRrtTXPKfoB4MXXYk5VMk4Zs', '1900-01-20 13:32:08.100130');
INSERT INTO `django_session` VALUES ('1p5jpgrbiae3czc06osdx0sb6o9pvkmp', 'eyJlbWFpbF9jb2RlIjoiMzA3NjE5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kISqb:mHe_SIm7KkwKstyT3lJht05LmZqYo5YpKiOoIuuzU9A', '1900-01-20 16:34:25.445237');
INSERT INTO `django_session` VALUES ('22umyeuzjojqxsrqruh9hu00jopivjdn', '.eJxVi0sOwiAQQO_C2phhoA10qQchMAwp0RZTip8Y725N3HT7Pm_Bk89XRyWyGITpjbQgDsJVrjWX2fHzlpeXGKSBjfq2jq5VXlyOW65R7GDwdOH5ZzxRafN6vGd-1OO51bVMp7_dLaOv49ZLo7oOMETpE2iwGDShTiQTUyBUGJICy5ICEFrd9546kJFkQJUSg_h8AZFxQS8:1kHhb4:oCdD9wzactQSR3yhu1O1ZtddY12fhA7KlLUxv4iyFPg', '1900-01-20 14:07:14.796296');
INSERT INTO `django_session` VALUES ('263d8j7ja00g0sgozkkol1h00af5oqln', 'eyJlbWFpbF9jb2RlIjoiNTc0NjEyIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGOMQ:EVq5J24Taj1IkIJIerd1Qd9ikasCbnYH0LrsMwf2GYk', '1900-01-20 23:22:42.576674');
INSERT INTO `django_session` VALUES ('2b0awbv4xdx9xezd8kgzxdggve7hpybz', 'eyJlbWFpbF9jb2RlIjoiMDkxMTA2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNbF:OMkd-GC5ybm_FJKahB55pI7ExFYNt2eqV8Jyb4Scd24', '1900-01-20 22:33:57.752755');
INSERT INTO `django_session` VALUES ('2e8v3hf4waaa0osbxhrp2tv80w48qma0', 'eyJlbWFpbF9jb2RlIjoiMTMxODUzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGJSb:j7eUR9l4vgrtpazDYJlXA1BNaGLMUx0bTgXQRcpg3dE', '1900-01-20 18:08:45.063037');
INSERT INTO `django_session` VALUES ('2stekirw70tk9pg26s16m50og60g7ilt', 'eyJlbWFpbF9jb2RlIjoiOTQ2MTg2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqYm:Krpyh4jr6yOZ2PFcTZW29D43wrbKa9OMcHK9x-DPuFA', '1900-01-20 11:17:12.515594');
INSERT INTO `django_session` VALUES ('3da3et8ot92wu60zwz1d71v3x7p886cc', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kflyC:UTrxrsrjCkGB4dCQyUCf5qF_c_lieCixxQSRTCMoNM4', '2020-12-03 23:35:36.639161');
INSERT INTO `django_session` VALUES ('3ftrms1uxrd8ll5ulxxon98ili7sjfs3', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kfjVJ:gUCbury0Igmd6zXaMEza49umJV0XiQJMNRuBlAoOne4', '2020-12-03 20:57:37.008584');
INSERT INTO `django_session` VALUES ('3gxalnappkxlcb9745um01avsph9oy8m', 'eyJlbWFpbF9jb2RlIjoiMDU1NzU3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHi0x:DNvxd1ajzbuTpMFBPlW-v5SaYfM0sjD-VDzU7Woej3s', '1900-01-20 14:33:59.301538');
INSERT INTO `django_session` VALUES ('3nk335s3m08h9a6su2o9mdbsdpc34d0w', 'eyJlbWFpbF9jb2RlIjoiMjY5MDMwIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kIOd5:fS21Wgg3AKIRnAXB-mpgI3UyLLb1ApW9DPbwD9P1LPY', '1900-01-20 12:04:11.307093');
INSERT INTO `django_session` VALUES ('3rn27l0gzk7wzd245mw8vazeqygd7d17', 'eyJlbWFpbF9jb2RlIjoiNjQwNDAwIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHhux:Irn9W2orCreHL_RT9gET6jy6uy7nz_xeGAMHzGJ9YPc', '1900-01-20 14:27:47.001348');
INSERT INTO `django_session` VALUES ('3w1neaqak53q0y1355trktaagy7b1p2k', '.eJxVTkkOwiAUvQtrY_hMxS71IITC_ynRFlOKQ4x3tyQudPMWb34x5-s6ulpwcSmyngHb_XKDD2ecm-BDyHVe97eE97I_1bLm6fhV_yKjL2Pzc86NJTNIDxQt10FoI60QB1IdAAWpBGEHG2giUBAVdUSKjLFCDn4rxcmniws5YjtmtTCyTRUsJeXZ4eOalifrwfL3B25EQGg:1kMC6I:4PTE4uXgaJJtSgYc3gnulQ7hpipvfST5GfOJwPy86pc', '1900-01-20 23:30:02.840807');
INSERT INTO `django_session` VALUES ('50casj21jxps91339987mgfq6eir5xyd', 'eyJlbWFpbF9jb2RlIjoiNDg2NDc1IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kG1LH:yg1RANGPd_t6wzoPaBKBrnxw4bWDHMsM4cbY9HfwLLc', '1900-01-20 22:47:59.675227');
INSERT INTO `django_session` VALUES ('55zn590z1kk2u9yh1zj69ogmw9p2rohd', 'eyJlbWFpbF9jb2RlIjoiMDQxMjg5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNH4:ztQIaSIFcSsCXC0YeYI5nIU-npdthfJkwplumbpJ12s', '1900-01-20 22:13:06.716440');
INSERT INTO `django_session` VALUES ('591s0u19woz1w7g0skmy789vxb2tlxn7', 'eyJlbWFpbF9jb2RlIjoiMzgyMzY0IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGOGY:nziZzROfWJAqchbqRPORqAfW4RuVPFGxenCZv_AI080', '1900-01-20 23:16:38.460182');
INSERT INTO `django_session` VALUES ('5929yj60dixozm9jbh0inszt6vo016cz', 'eyJlbWFpbF9jb2RlIjoiMTI0NzA0IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFpWH:R-3wm4Dopiw4TWt_D--fUAObjZ4QyRFB24Rx8lMRgLw', '1900-01-20 10:10:33.211236');
INSERT INTO `django_session` VALUES ('59mc2yvp6d8263diw1ioi3hy2oc0zkqv', 'eyJlbWFpbF9jb2RlIjoiMjQ0NjQ4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kIM32:4NytC4ZZuw6NUcf1vnYikHuH6q9F9NzYFxquud8AoDU', '1900-01-20 09:18:48.814643');
INSERT INTO `django_session` VALUES ('5o2jhs61jsssx5mypgk48dsq35mawadl', '.eJxVjMsOwiAQRf-FtSGlPDq4dO83kIEZpGogKe3K-O_apAvd3nPOfYmA21rC1nkJM4mzUOL0u0VMD647oDvWW5Op1XWZo9wVedAur434eTncv4OCvXxrrRA85BEjEY0uemuMHlQaBu1Vssk4yMiWlYFMfpqQDTBYPYFzWSsn3h_aBzdZ:1kDSHF:_QnHZCQ9cQYMLWPBZObytaLYo6YcJL4RmYTxnxFPwE8', '1900-01-20 20:54:13.239692');
INSERT INTO `django_session` VALUES ('5ow4ld2ax7zhy7ecq9ctb7my32j9u7n1', 'eyJlbWFpbF9jb2RlIjoiNTEwMjc4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFrOZ:2pQ1a-HyhgCQAOZLan-RXnmqk3iKOuo9lDiSxbtWZLY', '1900-01-20 12:10:43.595411');
INSERT INTO `django_session` VALUES ('7bew70oscbugjopnk28bzwkkbx0uwhar', 'eyJlbWFpbF9jb2RlIjoiNzU5MDA0IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqy3:Ul9FTl9wIb8gsfCbvdz66dqi08ASIGG1FegEw-q21xs', '1900-01-20 11:43:19.542703');
INSERT INTO `django_session` VALUES ('81e5oxy7piipbhyvnlyzy33uclwk7d1i', 'eyJlbWFpbF9jb2RlIjoiNzc3NDU1IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFrAl:3tFbifxOONMbPGchnm6FkNPsLVh8ym_jd2znthYUJM4', '1900-01-20 11:56:27.635491');
INSERT INTO `django_session` VALUES ('8wns2pflpc0209m1odtim8whbt9nld3v', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kUVJE:RniakHqM80BsO6spz4_4j1clKhF3cgZ2AhN4I2SKiIo', '1900-01-20 21:34:44.312202');
INSERT INTO `django_session` VALUES ('8yd5duin21k63btgcrmq0vmd485735ht', '.eJxVjTkOwjAQRe_iGiGPd1LCQSzHnlEsSIzimEWIu5NIFND84i_vv5gPbRl8qzj7nFjHgO1-vT7EM05bEGIsbVr2t4z3uj-1upTx-E3_JkOow9bnnBtHppcBKDmuo9BGOiEOpCwARakEoYVVNBEoSIoskSJjnJB9WKE4hnzxsSRceRyU1Wq7qlhrLpPHxzXPT9aB4-8PbcVAZA:1kMBxr:qjAWwyCXLAsr36pPNwCyvO3E94oJynhqxXvAqEAaXmE', '1900-01-20 23:21:19.983706');
INSERT INTO `django_session` VALUES ('9aki7bchexiusl6kwkip5fnd7g5y9heo', 'eyJlbWFpbF9jb2RlIjoiMTE4NTE3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFq7W:-VKc-VxsDEbMHmpK2HKTbuZA0A-67ALfR8BsSa5IdZo', '1900-01-20 10:49:02.150614');
INSERT INTO `django_session` VALUES ('9gdn62253jbtenrashonq9v85ih75e6a', 'eyJlbWFpbF9jb2RlIjoiNTE3NzIxIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHhUk:ZT-YoYD1Z02CDztZZLlGwmeCrCXPjGYt-bB2oo6MwC4', '1900-01-20 14:00:42.278103');
INSERT INTO `django_session` VALUES ('a5vd4qeyk1xt7b5v82uiwf2m0nzvqva9', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kR3Su:PMAyVDqL9VhGJB8ztgTkWnZ7w0X20p88g6YPhUF8_DM', '1900-01-20 09:14:28.900578');
INSERT INTO `django_session` VALUES ('akgsjwoa9lydg2808gao9q0bjezrhr2k', '.eJxVjcsOgjAURP_lrg3p67bC0r3f0PRxlSq2hkKiMf67YNiwnXNm5gP0cGmwoUSCDiQKIzQcwFaqNZVs6fVM4xs6ydiSunnq7VxptCn-ddiF3oU75ZXEm8vX0oSSpzH5ZlWajdbmvJwNp83dDfSu9kvbB-l10P4iueM6RGGYNApbVBiIKynIcdaiVu1RMB4MF5EJzhHRCKVJw_cHcrhEOw:1kFSsf:kIzdPSpdOLWwH2rQ5NezpRoUEwzBNLClB8qJOFdiw6U', '1900-01-20 10:02:09.101237');
INSERT INTO `django_session` VALUES ('bhbmwtlay7jrm23zmjugyqnfwki613rt', 'eyJlbWFpbF9jb2RlIjoiODczMzg4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNuc:cvPVGlI2ixaXc1y-WMD7iRSav7DDj-UCpII_0hx_ZR0', '1900-01-20 22:53:58.437411');
INSERT INTO `django_session` VALUES ('bk4p8hefifjhvl03nbbwomm58ixpg1rb', 'eyJlbWFpbF9jb2RlIjoiOTI0OTQxIiwiX3Nlc3Npb25fZXhwaXJ5IjozMDB9:1kFS6R:qpekm5shMJmT_deSUSxZOGsdsO5K_sWt-iJ0mNYP2vk', '1900-01-20 09:12:19.655267');
INSERT INTO `django_session` VALUES ('cycrndkdihicp4svbpan5i5ydevacodq', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kTnip:uWR1Sum7f_LH-sQHCvLPIqbLgXuu3ZICvBOlBWkslo8', '1900-01-20 23:02:15.309504');
INSERT INTO `django_session` VALUES ('d5n5olviqk8zdtdqerc4txmll2dt8x90', 'eyJlbWFpbF9jb2RlIjoiMDAyNjU5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kIOQN:S4mkZA4tr6cS9CGEqOkWXhGrglDayOVLb3y3h9t1Enw', '1900-01-20 11:51:03.391768');
INSERT INTO `django_session` VALUES ('dctuenzm0jwtkjgs7h8g20pxzflrp1rk', 'eyJlbWFpbF9jb2RlIjoiNTQzNTgxIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kISir:JK7ED34FIEU8d9Eegj6EwOHWZ0-jNzMhUKnXmRd9bNw', '1900-01-20 16:26:25.914649');
INSERT INTO `django_session` VALUES ('dq476uetymkq3izj58xyxm5gkeeh4w5c', 'eyJlbWFpbF9jb2RlIjoiMjkyMTQ2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqK7:pP46bntfHZpeeXgRn5sPzkr0RWc4kIswlQS4UIctv58', '1900-01-20 11:02:03.639622');
INSERT INTO `django_session` VALUES ('dwv6ogqyq3vjevi5omq3ojo2i892qzzd', 'eyJlbWFpbF9jb2RlIjoiMTU4MDAxIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGO5P:pDOEHw4Ay-oIHxaSiPNinNdwOc5nU4jrQyRxgeuEzLc', '1900-01-20 23:05:07.518570');
INSERT INTO `django_session` VALUES ('dxqt78r1hqraar3mfffgliytywjd5vsc', 'eyJlbWFpbF9jb2RlIjoiMDA1ODE3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kWa66:xSpVvVBtvQc_1CsR4OyFohCkQtda26LXMUoTNqzgJMw', '1900-01-20 15:08:46.594675');
INSERT INTO `django_session` VALUES ('e2yz42csj6w705132erc1sn46zn2q879', 'eyJlbWFpbF9jb2RlIjoiNTE2MDA3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqSQ:eX3d5Wutcm8y5qsa2qksUN1UbCyul492MZuGkNm8Ibo', '1900-01-20 11:10:38.131666');
INSERT INTO `django_session` VALUES ('e3428doub4t59lu7b76508if8dy8gryt', '.eJxVjcEOgjAQRP-lZ0OWCrTl6N1vaLbbrVSRGgqJxvjvloQL13lvZr6CnxhHS8mz6IU0tTRSnITNnHNMk-X3K84f0Z8BSorrMtg182yjL_q5EYfQIT142oi_43RLFaVpmaOrNqXaaa6u5Wy87O5hYMA8lDbp1ikGjdKEFoLmxgVSBE5Rw8oAoQatQ2c65VoM2JEKNUggLY0PKojfH6UkRn0:1kFSir:Qw60G4QW1tYkTiu9cOI-XtwRb0RkwDsDkQ6taJzcuTM', '1900-01-20 09:52:01.991022');
INSERT INTO `django_session` VALUES ('fnq8wz919td8t7m480zpmpgfj1llgygo', 'eyJlbWFpbF9jb2RlIjoiNTg3NTYzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFrkt:0VY6riPxN9c5knaACXMnrXn5PWN65zfXTJqBQMHJbbM', '1900-01-20 12:33:47.763209');
INSERT INTO `django_session` VALUES ('ftsgzwlsegsje0ndyp5cocryufejiadw', 'eyJlbWFpbF9jb2RlIjoiMTk1MjMzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGMKT:jFwO_9WwX7C-i_op4DfrqpKARTggc33PESR1oy7GxXo', '1900-01-20 21:12:33.907092');
INSERT INTO `django_session` VALUES ('g0ujfqn2e346a4zvjtcexcfwzgncu3f3', '.eJxVi0sOgjAQQO8ya2P6YYCy1IM0M9MSGgUMpX5ivLuYuGH7Pm-II6WrlzlE6MAqRG3gAD7HnNM8-fi8peUFnW7VRqmsgy85Lj6FLa8U7CCTXOL0MyQyl2k93lN85OO55HUeT3-7WwbKw9aH1ihkNKJqbRwFDuSYlWXUVFvdB9ZN1Ox601aoMIhjtM6JmB7RVg18votIQLM:1kGOVp:p4fdkK74Y3sitd7cyWBJVk4H33D3r8ZU02CEX8cCAzY', '1900-01-20 23:32:25.118688');
INSERT INTO `django_session` VALUES ('g4uvvmrrtqdwwltvqexjm1e29fpvhotn', 'eyJlbWFpbF9jb2RlIjoiODc3NzUxIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFrlf:OHo1W3gD0zMA37VXDIub2pp3wcyQrbovbY1I0tjUfps', '1900-01-20 12:34:35.998264');
INSERT INTO `django_session` VALUES ('ggjw1r7wg7jzc4gf4vdhehf0r0coeang', '.eJxVi0sOgjAQQO_StTH9wEzLUg_SlOk0NAoYSv3EeHcxccP2fd6Cx5CvnubIohNOm1a24iB84VLyPHl-3vLyEp2ycqOhroOvhRef45Y3IHawD3Th6WcC0Vyn9XjP_CjHcy3rPJ7-drcMoQxbr3XDhiWoRqNFiAnQgDI9AsQYABMqBUZSdM46sAbBMmGgZPqErSLx-QJfzEAF:1kWaC3:Msuwwd7870ePRmXyx2Mxp17o2mHD8vb7rSKJUWnPZl8', '1900-01-20 15:14:55.187889');
INSERT INTO `django_session` VALUES ('hkcna062e44b94sox2mdx8i4m6chu98z', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kXJRH:fN23nH74W0sGXTpi3ArK1o7lg7OtjHZA_yXO3tHQKlI', '1900-01-20 15:30:39.436287');
INSERT INTO `django_session` VALUES ('hkwg9rpeneuuk1lhpbunlatubbsojg6t', 'eyJlbWFpbF9jb2RlIjoiNTAzMTIxIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqGH:uw8iIhMljra4F0QhbSfuF97U60XzLsvIlCcKR7x9VHw', '1900-01-20 10:58:05.671659');
INSERT INTO `django_session` VALUES ('hreyl8i3heoxssmwf8wcoy8d82s1tj03', 'eyJlbWFpbF9jb2RlIjoiMTM0OTA5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFres:w5xtbkRfGvyRcdiaTHeocf_BEohqnXJjAhKm24SMt6w', '1900-01-20 12:27:34.556585');
INSERT INTO `django_session` VALUES ('ir6vrx56reitms61ea6iw0tpf8lbnos5', 'eyJlbWFpbF9jb2RlIjoiMTY3ODc1IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHgR2:Lxzz1-BlOVoQJAfXCG4fRloYoOU1j1hwNQ9lVLGm4ck', '1900-01-20 12:52:48.903348');
INSERT INTO `django_session` VALUES ('ixrbpq5irnzzi6b0sfu2wu6dhxp02ksz', 'eyJlbWFpbF9jb2RlIjoiMzExMjIwIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFr3x:e5Gh0DFTnzoQW3HZ5Ed-pFdBU8rMS5vcIVWaJpVB02k', '1900-01-20 11:49:25.382119');
INSERT INTO `django_session` VALUES ('izsaydbuyi16tp4hks08fp9ttsmz1pyn', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kZDqo:QG9cBbq490te7_hqlHXdY34DfaobNRlk27c7eQlsO4k', '1900-01-20 21:56:54.995699');
INSERT INTO `django_session` VALUES ('j3cq9m4ylae51mhpopw944et7z5xk556', 'eyJlbWFpbF9jb2RlIjoiMDA3MzE3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqrV:AsErTIL9ABy_RoVV3GRLM1h0NK_hSSa-bEi5gFhzVrc', '1900-01-20 11:36:33.819977');
INSERT INTO `django_session` VALUES ('j3hkzst889e801tw3kc3xduyus0d5lgd', 'eyJlbWFpbF9jb2RlIjoiNjYwNjM5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kMCIH:or1oziuQvBKo6Xv2iyYIjKedVB0V8v3pYYkqu_lxsZQ', '1900-01-20 23:42:25.103126');
INSERT INTO `django_session` VALUES ('j4metnlu82586uqgv4w088apqcaxikvb', 'eyJlbWFpbF9jb2RlIjoiNDg5NjQwIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGO8t:lVfxckY2Qbio1Muas8HHOZTUjlbzMx0xtSovoxqCPTk', '1900-01-20 23:08:43.683676');
INSERT INTO `django_session` VALUES ('jcq0eb421xhmg85m222dm4vh3tjqda4o', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kSKbD:gYyYwNmg1Rvo_XB9Cz1QlzBJtB2VaYsvQXEC2Iorv6k', '1900-01-20 21:44:19.049155');
INSERT INTO `django_session` VALUES ('jlsuz6c5mh1jnhs6gn2hzv4h5gv07jku', 'eyJlbWFpbF9jb2RlIjoiNDE2Mjk4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kMCAa:66SXGPLe9bdKAAaBI5aqEUJNjo54V20KTMNsjclXhBM', '1900-01-20 23:34:28.296696');
INSERT INTO `django_session` VALUES ('k63biwd5f2r2kxtdkdpg8gg7n3s6tm2o', 'eyJlbWFpbF9jb2RlIjoiNTE1NzM0IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kISef:A2OCd9oJkBXgQVPmpKii1V6wicFf-K9dFoM5yXB2fjQ', '1900-01-20 16:22:05.614335');
INSERT INTO `django_session` VALUES ('kfmcgfirzzf6m5jhner9fe6ohaqnxihz', 'eyJlbWFpbF9jb2RlIjoiNDIyMzUzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kG11E:H70eyd4i_UQW0s1pS3Na6VN6CXhNAERjBcKTUZ3ue6M', '1900-01-20 22:27:16.509698');
INSERT INTO `django_session` VALUES ('kjprdty0uxbgwku9grqnrjyc1qerjji2', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kRsaC:nFZtekbwfo3M0A_E-pYrlnVUoJLEiIsHLyt7dmDAKCk', '1900-01-20 15:49:24.299956');
INSERT INTO `django_session` VALUES ('kk9ozlzzzvsr9clj32878c8x58q1ghuy', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kUSmI:b6NJf-OX9QyllZNFe6G7TUk7OkZCbMdvEwdDQ__bErk', '1900-01-20 18:52:34.844649');
INSERT INTO `django_session` VALUES ('km16s59agb0vjdjrpxfueo9uz8rueysx', 'eyJlbWFpbF9jb2RlIjoiMzA1MTc2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFsXm:XUI5rtcVqqZFQzk5WsFaK-TJWbxdMg5fjy55Xo1n2xc', '1900-01-20 13:24:18.138103');
INSERT INTO `django_session` VALUES ('knm8d9zouw6m4mx7uhv7j7kmcgzwxktv', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kMnd9:gcQV-WvuCZKp6l31NCIWO7RkOwrWfCMiT3v8NTpd7AM', '1900-01-20 15:31:27.125706');
INSERT INTO `django_session` VALUES ('l0lepvxm66gyl43m0g11a06164zyj0sp', '.eJxVi80OwiAMgN-Fs1mA1ZZ51AchpSthMW6JgB6M764mHvT6_TxM5N5K7FWvcZnNwTiz-2WJ5azrR7DI1tc23Ba91-HUa9sux6_9WwrX8u6FbAbe48TJetKQHCdSxjCCm3PO6DwG72CSRD4TgwKJhoB-gtEymucLrdMypA:1kmnkR:h0yHwdrxjrrBq87SVx3B_M_eQ2pHK-BbsB8qbfjUcvQ', '2020-12-23 08:54:27.525275');
INSERT INTO `django_session` VALUES ('l5h4qbo7su2w2yucty7okhese2bm75w9', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kg11N:X3iITwk7yvaI-rKGfPbU2D5oS03rymb-a6YnkVc72Ho', '2020-12-04 15:39:53.272764');
INSERT INTO `django_session` VALUES ('l7ewjpjwllhnhn5xia0v0pohimeopg91', 'eyJlbWFpbF9jb2RlIjoiMTIyNjk4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFqdP:QQZKpxxpgtFznvzZMVT5O_U5rAEceXPHyjES6Rns7Hg', '1900-01-20 11:21:59.671402');
INSERT INTO `django_session` VALUES ('li7w9k9pt5zopk0bewf62gk7jprr7jz2', '.eJxVjTsOwyAQRO9CHaE1WHxcps8Z0ALrmHwgMraUKMrdgyU3bue9mfkyemJ6uFAisYH1nTFg2Ym5SrWmkh29X2n-sEECtBTXZXJrpdml2HSp2SH0GO6UNxJvmK-Fh5KXOXm-KXynlV_a2eO8u4eBCevU2gpsGKMGEyyRjlp4E21QUoDxnQFBUWOvwFs5CjQijAqDRSuh8wptBPb7A5lPRg8:1kFfIh:QXDo65rYzss_vFlMUqwuiB8spWv5l_PTVTgqtbc5HnQ', '1900-01-20 23:17:51.735271');
INSERT INTO `django_session` VALUES ('ms9fjcp4bpvg9b0e81optt1b5n5dwaoo', 'eyJlbWFpbF9jb2RlIjoiMTE5NjQ3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGJWT:3TBq6R6vUX_mXGijxFegWRCWvdJPyx-5tNz8GwtLotM', '1900-01-20 18:12:45.778893');
INSERT INTO `django_session` VALUES ('nj09rbg0brifkdos8488j0ykca59yrwo', 'eyJlbWFpbF9jb2RlIjoiNTQ4NTY3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kISn6:5ClXcMbFD4zuxPvEMVgVkkOylFg9AX6Ct3laR-ZBIKs', '1900-01-20 16:30:48.021775');
INSERT INTO `django_session` VALUES ('noijwy0a4boh3d3t82a3gx9o22rbjako', 'eyJlbWFpbF9jb2RlIjoiOTk5MjE5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNYC:Pg_eYPKGBIorSemQfjxn8Tittffz_X8OBjMN_mZy508', '1900-01-20 22:30:48.980621');
INSERT INTO `django_session` VALUES ('nt5b3oogl2vj9z1v10749qj5taokmhvn', '.eJxVjc0OwiAQhN-FsyFgiws9evcZyMJuLf4UU9pEY3x3adJLr_N9M_MV_MT08DETi040yoAGcRC-cCkpj57frzR9RNcoVVNc5sEvhSefaNVPYhcGjHceV0I3HK9ZxjzOUwpyVeRGi7zUs8d5c3cDA5ahtrUDd3TksGUTNYN1RiliDEb3zkK0CNa0NlCooqaWTIDQQG_RAhmO4vcHgitGBg:1kFSyD:WDo8egl2MXR3iLeqa1URfpiZ6cX307AV_uvb3emi5-A', '1900-01-20 10:07:53.378713');
INSERT INTO `django_session` VALUES ('o7ksp82lx346liiw08oxbhsj4jr9i6rk', 'eyJlbWFpbF9jb2RlIjoiMjAwNDM5IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFq0B:LcOp8mrJuwE5BrYcou50bAb4z-a5j-igT-BQSj19R4k', '1900-01-20 10:41:27.755225');
INSERT INTO `django_session` VALUES ('owews100vjno1v5tcb0enm9orvemzqf3', '.eJxVi8sOgyAQRf9l1o1hABFdth9CEGYiadVGpI80_fdq0o2buzjn3A_Q6NPNhTkSdNC2xioBJ3CZck7z5Oh1T8sbOrRio76sgyuZFpfiliMcWO_DlaZd-BDmMq3VI9EzV5eS13k8_-3hMvg87L0Qwlg2vfLI0Yo6yNooK2XLukHkoLRkanCbmhk1Rs0Ns2ZjrFS9h-8PUKJAcg:1kMCXH:0qyMjuZ4Fvlo0mWvuLb0cTrm1oESPoYDUj0x67GvgBk', '1900-01-20 23:57:55.705287');
INSERT INTO `django_session` VALUES ('oz1yxw2luuwxt85g8r6z9uf7uuh5noxy', 'eyJlbWFpbF9jb2RlIjoiMzA4OTEzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kG1Vu:metKxsgjTHoM8X7PNlMHbcSjtsGfcfcSd9aQXqW18mo', '1900-01-20 22:58:58.848493');
INSERT INTO `django_session` VALUES ('p04eoy665t34h4ajjby4tr4jr6n2u5mw', 'eyJlbWFpbF9jb2RlIjoiMDQxODc1IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFrfd:D6qI3DPTiaTOriH9kzfHzt_W4FvvI6LHApnKbNXZL7M', '1900-01-20 12:28:21.920170');
INSERT INTO `django_session` VALUES ('p0w6h234l2ordszav75fbq8f3wuhewt4', '.eJxVjDsOwjAQBe_iGlna-E9Jzxms9e4aB1AixUmFuDtESgHtm5n3Uhm3teWty5JHVmcFUZ1-x4L0kGknfMfpNmuap3UZi94VfdCurzPL83K4fwcNe_vWlXFw5InYga2xDAA2kXFCDAaBq63eiwmuoKQUShJwAU2K0UgltOr9ASuEONc:1kEwZ0:IiS6lJMecNvWY1RE3sxbkhSPdsv6dD6AgfiM65l7S90', '1900-01-20 23:26:42.126208');
INSERT INTO `django_session` VALUES ('qllfbmoiit6vgh190theo01twtcra05a', '.eJxVjMsOgyAQRf-FdUNEHg4uu-83kAEGpTWQiK6a_ns1cdFu7zn3vJnDfZvd3mh1ObKRCXb73TyGF5UTxCeWqfJQy7Zmz0-FX7TxR4203C_3LzBjm4-3FAgWUo8-xtgbb7VSshOh66QVQQdlICFpEgpStMOApIBAywGMSVKYI5qbW-qUCxu3dafPF3vOPRc:1k9WMu:rf3fDB7NvDrVYuX6EG9qwrxvnofc7_lQs0UOE4jl9Y0', '1900-01-20 00:27:48.731707');
INSERT INTO `django_session` VALUES ('r16jscghvwr56axzidtwm3umoh3im402', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kNFsL:4CTPkalZfkbu9bpGBTZ3I2JTW_yw3l_kNdgbExmW368', '1900-01-20 21:41:01.234871');
INSERT INTO `django_session` VALUES ('r3ckbzy0f50c4miu0mejvzpejh1peso0', '.eJxVi0sOwiAQQO_C2hgoUIYu9SAEhiEl2mJK8RPj3a2Jm27f581o8vnqsERiAwOpLRh2YK5SrbnMjp63vLzYIIBv1Ld1dK3S4nLcciXYDgaPF5p_xiOWNq_He6ZHPZ5bXct0-tvdMvo6br3xQibouA4cvCAZZSJudcCu74n33FihOksBkg4KgSQlBG8QUYookmKfL5LQQTc:1kHfVR:x62zJ3IaLCRA9701mwKHQDHb3WGsh5yUmKnbCWDqSqI', '1900-01-20 11:53:17.361774');
INSERT INTO `django_session` VALUES ('rkfjjjvbcdxryk7rd58zf23ck5hsrhq4', 'eyJlbWFpbF9jb2RlIjoiOTc0NzI2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNAB:yLG4gvxRX-efCkc3GC9vcP0d-bNW8fkeTXKH497iJ-Y', '1900-01-20 22:05:59.927983');
INSERT INTO `django_session` VALUES ('s0p7zhj187fbxwpp4nopwl259jp4hpxl', '.eJxVjcsOwiAQRf-FtWkYoJR2qR9CKMykRFtMKT5i_Hdp4kI3d3Ef576YdWWbbMm42hjYwIAdfr3R-TMue-C8T2XZmlvEe25OJW9pPn7Tv8nk8rT3OefakB6lAwqGt160WhohelIdAHmpBGEHVVoiUBAUdUSKtDZCjq5CcXbxYn0KWHm96et-v8qYc0yLxcc1rk82gOHvD2_1QHc:1kMBsW:fi6r-4SfjsuTd6Q8_uESPBqQMSO0vgx30Fqy5WsfcV0', '1900-01-20 23:15:48.655589');
INSERT INTO `django_session` VALUES ('s56nznkniafv77f0xhwnkbjff0109u6n', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kfDtj:R6HHlHTX4YhsTMO0bNN0nP3EmprShvdBeZo2HOyfe2E', '2020-12-02 11:12:43.791859');
INSERT INTO `django_session` VALUES ('s6q7h223m5yxs83vjc3j0tkfrc22keq5', 'e30:1kEklH:pwSDvPSLLQkR9Oi9BVEt3eISAadRWNgtnGYqMrc8Z-Q', '1900-01-20 10:50:35.245136');
INSERT INTO `django_session` VALUES ('tb2x1xuxjjqfbwwcgkr2t3ycykluuwy3', 'eyJlbWFpbF9jb2RlIjoiMTUwMDI1IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHf3s:maTUmDglo1tY_kt1neJep642vTkmGOo2c1ZlXoLcgC8', '1900-01-20 11:24:48.924180');
INSERT INTO `django_session` VALUES ('tc2u1tdxpblwymm1bx6v4kc3xkz3pfcv', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kWb1V:kgAzjeEcVDPIg7HZlrd_awoqv7fM-0_QDNpYx0tWcaI', '1900-01-20 16:05:05.607533');
INSERT INTO `django_session` VALUES ('trhllfk4csb12nguc97afp32aq1e93gv', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kMCS2:etdpKpgCVK57K48NOX67bzjkmZUfn86vqnDxZy2YnIU', '1900-01-20 23:49:30.300247');
INSERT INTO `django_session` VALUES ('u8mvad29fv9xc98teno5etk9p4xnlh56', 'eyJlbWFpbF9jb2RlIjoiMDIyNDU4IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGN1t:gkrpGVNgkyUPO8eXOdRbgLJWJxwnDCUbSfix-D3Ejis', '1900-01-20 21:57:25.035994');
INSERT INTO `django_session` VALUES ('un9gn7pomk4uka3v7c8vu0orn2ectgqm', 'eyJlbWFpbF9jb2RlIjoiNTE4NzkzIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kFsU3:7xODAT1R8Vy8CooOydMRjWJYVp4yZpRYXaxzujJQDW8', '1900-01-20 13:20:27.422799');
INSERT INTO `django_session` VALUES ('v39hlay4jlyw8wyn96az4qo7dtzsuyq1', '.eJxVjcsOgyAURP_lrhuDIAIuu-83kAteKn1AI5q0afrvxcSN2zlnZr5AT4wP6_NIMIDpuWglnMAWKiXmZOn9ivMHBsFYTXFdJrsWmm0cqy44HEKH_k5pI-MN0zU3Pqdljq7ZlGanpbnUs8d5dw8DE5aptpG1SFwFo7GV0jOnNUcpjegJuVLeM6Y6EVRwxmDHBGkteh46oaQjExz8_nzXRX0:1kFSWo:vlEpqMpxGS14OQVmlf5SODbcsGIMyHZHJ2pIvZKWypM', '1900-01-20 09:39:34.047639');
INSERT INTO `django_session` VALUES ('vc5rpf4dxqs4sliiqn74777daqm0e71s', '.eJxVi8sKAiEUht_FdQw23lvWg8jxP4pDNAOptYjevYIWtf0uDxFp9BpHy9e4sDgIbcTuFybCOa8fQ8A21j7dlnxv02m0vl2OX_u3VGr13SutZjhi7bIMQSkjLXvJSWaXCpz1zoI9rC3E2MPMWQIlBONRSgosni_KEjPb:1kNYmX:GEx78P-tyDWHuU9QK3Mk_FwkLC8721jV3P7GIwhar8k', '1900-01-20 17:52:17.909592');
INSERT INTO `django_session` VALUES ('vl3a1vmt2gvsf0qzajwgi1g4zsr2779m', 'eyJlbWFpbF9jb2RlIjoiOTgyMzcwIiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kHfCi:y8unZeRQ83tJHPwSccm85l9o8H_rKOHjmRaM0IWLld4', '1900-01-20 11:33:56.475189');
INSERT INTO `django_session` VALUES ('w3hepz3l921zss910akt30k1ngnqt8p6', '.eJxVi80OwiAMgN-Fs1mA1ZZ51AchpSthMW6JgB6M764mHvT6_TxM5N5K7FWvcZnNwTiz-2WJ5azrR7DI1tc23Ba91-HUa9sux6_9WwrX8u6FbAbe48TJetKQHCdSxjCCm3PO6DwG72CSRD4TgwKJhoB-gtEymucLrdMypA:1kmdvp:bP9I4wsLMz-3N68A7KgDr1ueL_FX6BB8hI29-3y49aI', '2020-12-22 22:25:33.158674');
INSERT INTO `django_session` VALUES ('we8vomyzz1006n0864ym0pqrfsw0x19n', 'eyJlbWFpbF9jb2RlIjoiNzI0NjA0IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kG0je:gPXliEhvgSZndML-QuGcqR5GHE7qXsEhxjVMr-UoCX4', '1900-01-20 22:09:06.111669');
INSERT INTO `django_session` VALUES ('x7moi4x5tp43t6lx7tnurhagtl59ffdc', 'eyJlbWFpbF9jb2RlIjoiNjk4MTM3IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kG0QI:yyJzXW2N1capupAz7ic199rWuUdR-ZijoF4REdjgR3k', '1900-01-20 21:49:06.557517');
INSERT INTO `django_session` VALUES ('x94jbqdw73rokvvxxpwhn7xv9hllom2a', '.eJxVjcsOwiAQRf-FtWkYoIBd2g8hFGZSoi2mFB8x_rtt4kI3d3Ef576Y83UdXS24uBRZx4Adfr3BhzPOe-BDyHVem1vCe2n6WtY8nb7p32T0Zdz7nHNtSQ_SA0XL2yBaLa0QR1IGgIJUgtDAJi0RKIiKDJEira2Qg9-gOPl0cSFH3HjcKGn1flWwlJRnh49rWp6sA8vfH26HQGs:1kMC4a:qW-SVf9kRNrjpPOjI9QTaN9-XIO-Cuh1Fi9tlbv8cLk', '1900-01-20 23:28:16.158430');
INSERT INTO `django_session` VALUES ('xn5nka2o9mcqwmlf4wkl0auf9a0oudfu', '.eJxVjssKwyAUBf_FdQm-oibL9kNEr1cibWKJsQ9K_70Gusl2Zg6cD8HZpZuFHJCMRPKBM01OxBYsJeXF4uue1jcZmaGNurpNthZcbQp7LskBegdXXHbjAHJdtu6R8Fm6Sy1bns9_e5hMrkytpyrKoJXQiglkyA2C4L2gznsfVO-MNCA1YoRIlTTCxHYogEYYqMZ2-fsDiXtA2w:1kHjOG:cTFVTmaJfTUH94lzpqP0OBIpng9GV5myDxrj2ZOZDfg', '1900-01-20 16:02:08.050697');
INSERT INTO `django_session` VALUES ('xposonsv3r5xorg80p1krw4l69ytew7f', '.eJxVi8sOgyAQRf9l1o1hABFdth9CEGYiadVGpI80_fdq0o2buzjn3A_Q6NPNhTkSdIBGtgrhBC5TzmmeHL3uaXlDh1Zs1Jd1cCXT4lLccziw3ocrTbvwIcxlWqtHomeuLiWv83j-28Nl8HnYeyGEsWx65ZGjFXWQtVFWypZ1g8hBacnU4DY1M2qMmhtmzcZYqXoP3x9HM0Bl:1kMCGM:ghP_Stoe5xDVruIvcPgfcHf7i9K6sMW35O9F6owAR_o', '1900-01-20 23:40:26.925470');
INSERT INTO `django_session` VALUES ('xuka6s4uqppansa33afk4xjq0u5o80s3', '.eJxVizsOwjAQBe_iGkVef9aGkhzEWn9WjhCJhG0oEHeHSBTQTPHmzVMEGr2G0cotLFmcBIjD7xYpXcq6C0ppG2uf7kt5tGkerW_X89f-JZVa3f9SSvSMURNw9tImZVF7pY5sHAAnbRQXBx9YZjCQDTtmw4he6Uji9QaRDTKZ:1kRcpN:JVpb8nBwUS0NRwUVzPKE0nvixVotvjmgVfZZ7_ptv0U', '1900-01-20 23:00:01.533194');
INSERT INTO `django_session` VALUES ('yq5ibxkv6qb0ljm1ucptfnr2pkngw7ei', 'eyJlbWFpbF9jb2RlIjoiNTc5NzU2IiwiX3Nlc3Npb25fZXhwaXJ5IjoxODB9:1kGNH2:nFPQm9fO-F72TZvgtTpIau-NiKxsUeTykiV81WKKue4', '1900-01-20 22:13:04.205898');
INSERT INTO `django_session` VALUES ('z7sjni9ne7weir1qil1w6our6mcz6i9p', '.eJxVi80KwjAMgN-lZxlJ1q2NR32QkmaBDnEDu86D-O4qeNDr9_NwSdpWUqt2S_Pkjg7d4Zdl0YstHyGqa1u2bp_tXrtzq9t6PX3t31KklncPDAHHAT35HkjVcEAdIMeAU4wovY6SmZgRQhCLnsmIlZAgsAG55wthJjEK:1kIkmI:KdrGChX4IRUrSu8jydQb1MEzOI_JUfmtrwV60eITGzY', '1900-01-20 11:40:10.533068');
COMMIT;

-- ----------------------------
-- Table structure for management_about
-- ----------------------------
DROP TABLE IF EXISTS `management_about`;
CREATE TABLE `management_about` (
  `id` int NOT NULL AUTO_INCREMENT,
  `body` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `time` datetime(6) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_about
-- ----------------------------
BEGIN;
INSERT INTO `management_about` VALUES (1, '[TOC]\n# 关于博主\n我叫崔亮，老家甘肃，18年开始北漂，目前从事运维工程师岗位，擅长Linux服务器管理、Web服务部署维护、Ansible批量运维工具使用、以及Kubernets和docker容器管理，略懂Python编程开发，爱好是编程、看电影、玩游戏。\n\n# 关于网站\n## 建站目的\n\n作为一名运维工程师，知识的积累是必不可少的，本人在工作和学习中记录了大量的笔记，打算通过这个博客平台，分享工作中遇到的各种问题的解决办法和学习笔记总结，本站所有文章和笔记内容都是本人亲自实践过后总结所得，各种坑已经替大家踩过了，大家可以放心参考。\n\n本站所有文章均为原创，若有引用会在博文底部均注明了参考地址，如有遗漏，欢迎您联系我进行处理。因原创文章均由自己手打而成，即使发表时检查再三也难免有错误或者错别字产生，如果您发现了需要更正的地方或者您有更好的见解和思路，欢迎评论留言。\n\n开发博客网站一方面也是为了督促自己每天学习，发表文章和笔记内容。另一方面，希望能通过这个网站结识更多志趣相同的人，一起交流研究运维技术，提高个人专业能力。\n\n\n## 网站技术\n\n目前的博客网站大多才有HEXO或者WordPress搭建而成，拿来即用。但博客网站这件事，我还是喜欢自己造轮子。可以随心所欲地按需开发功能模块，设计业务逻辑，制作前端样式。同时也是通过博客网站项目将业务产品从需求分析、功能模块设计、数据库设计、代码开发、项目测试、部署运维、SEO优化的整个流程做一个充分了解，提高自己的运维开发能力。\n\n网站前端使用到了layui、jQuery、Bootstrap框架，以及echaets实现数据可视化。\n\n文章编辑器使用markdown，评论留言使用layui富文本实现。\n\n后端使用python语言，django框架开发。\n\n数据库主要使用MySQL，缓存使用Redis。\n\n网站部署在阿里云服务器，使用docker-compose部署，通过nginx反向代理，实现动静分离，css、js、字体、图片等静态资源使用七牛OSS对象存储，并通过华为CDN提供加速服务，动态资源由Django解析并处理。\n\n整个站点流量通过调用百度统计的api接口实现流量统计分析。\n\n## 网站架构\n\n![](media/markdown/2020_10_28_12_42_03_182074.png)\n\n## 功能模块\n博客网站按照用户身份权限共划分为网站前台、用户管理后台、管理员后台三部分，总共二十八个模块。\n\n所有用户都可以通过网站前台浏览到博文、轮播广告、最新博文、热点博文、文章点赞等功能。\n\n用户注册后，通过登陆的用户可以进行发表文章评论、收藏文章、发表留言以及回复留言评论操作，在个人中心可以查看自己的浏览记录，收藏记录，点赞记录，还可以修改个人信息、修改密码、修改邮箱等功能。\n![](media/markdown/2020_12_26_23_45_50_607355.jpg)\n\n![](media/markdown/2020_12_09_10_08_57_273306.png)\n\n![](media/markdown/2020_12_09_10_12_21_292661.png)\n\n后台博客管理系统是为管理员管理博客而设置的，管理员可以查看网站流量数据，查看注册用户信息，可以对文章进行发布、修改、删除操作。以及对文章分类，标签管理。还有设置博客网站的轮播图、友情链接等内容。\n\n![](media/markdown/2020_12_26_23_52_32_791830.jpg)\n\n![](media/markdown/2020_12_09_12_21_06_545641.png)\n\n![](media/markdown/2020_12_09_12_21_55_166338.png)\n\n![](media/markdown/2020_12_09_12_22_36_497601.png)\n\n![](media/markdown/2020_12_09_12_22_59_876194.png)\n\n![](media/markdown/2020_12_09_12_23_22_812681.png)\n\n正如冰山一般，你看到的仅是一小部分模块，网站后台页面同样精彩，也欢迎大家通过邮箱注册账号，登录体验完整功能！\n\n# 关于友链\n> 欢迎各位站长在本站申请友链\n\n1. 申请地址：在留言板留言即可\n\n2. 申请格式：\n- 网站名称：xxxx\n\n- 网站地址：xxxx\n\n- 网站简介：xxxx\n\n- 网站logo尺寸：100*100\n\n# 关于赞赏\n\n## 赞赏说明\n由于网站每个月都需要大量的费用维护，且目前没有有效的盈利方式\n\n如果您感觉本站对您有帮助，欢迎扫取下面的二维码赞助一顿饭钱，让博主不再忍受饥饿~\n\n但是如果您感觉本站对您没有任何帮助，那么一分钱也不要赞助，让博主继续饿着吧！(๑•̀ㅂ•́)و✧\n\n（请赞助后发送邮件或添加本人QQ/微信联系，如果未联系，则默认为匿名赞助。）\n![](media/markdown/2020_10_28_12_05_13_529384.png)\n\n## 已赞赏列表\n赞助名单\n(按照赞助时间先后排名，以*开头的名单均为匿名赞助）：\n\n| 赞助者 |平台 |金额   |时间   |\n| :------------: | :------------: | :------------: | :------------: |\n| * |  支付宝 |5 |  2020.11.04 |\n|  寡人为王 |  微信|10 |  2020.11.08 |\n| *音|微信|  20 |  2020.11.11 |\n|  Marina | 微信| 3 |  2020.11.18 |\n|  *思亲 | 支付宝| 8 |  2020.11.19 |\n|  董小姐| 微信| 1 |  2020.11.19 |\n| 天空大海| 微信 | 88 | 2020.12.14|\n感谢以上所有人对本站的支持，我会更加努力让本站帮助到更多的人，谢谢！\n', '2020-12-26 23:52:42.154577');
COMMIT;

-- ----------------------------
-- Table structure for management_carousel
-- ----------------------------
DROP TABLE IF EXISTS `management_carousel`;
CREATE TABLE `management_carousel` (
  `id` int NOT NULL AUTO_INCREMENT,
  `img` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `url` varchar(300) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `info` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `is_show` tinyint(1) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_carousel
-- ----------------------------
BEGIN;
INSERT INTO `management_carousel` VALUES (1, 'carousel/2021_01_26_19_37_43_316840.jpg', 'https://www.cuiliangblog.cn/blog/note-3/', 'Prometheus笔记', 1);
INSERT INTO `management_carousel` VALUES (2, 'carousel/carousel-1.png', 'https://www.yuque.com/login?platform=wechat&inviteToken=d250cce7a9bfb322880f20b1d1c4cdc482efb9966f48a20bc6fbb66407542ced', '语雀', 1);
INSERT INTO `management_carousel` VALUES (3, 'carousel/carousel-2.png', 'https://www.aliyun.com/1111/new?userCode=gs1gd34d', '阿里云', 1);
INSERT INTO `management_carousel` VALUES (4, 'carousel/carousel-3.png', 'https://portal.qiniu.com/signup?code=1hid00p3r409e', '七牛云', 1);
COMMIT;

-- ----------------------------
-- Table structure for management_imagesconfig
-- ----------------------------
DROP TABLE IF EXISTS `management_imagesconfig`;
CREATE TABLE `management_imagesconfig` (
  `id` int NOT NULL AUTO_INCREMENT,
  `foreground` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `background` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `icon` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `photo` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `cover` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `pay` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_imagesconfig
-- ----------------------------
BEGIN;
INSERT INTO `management_imagesconfig` VALUES (1, 'images/2020_10_22_21_35_56_614469.png', 'images/2020_10_22_21_36_01_011344.png', 'images/2020_10_22_21_36_11_447318.png', 'images/2020_10_22_21_36_17_002878.jpg', 'images/2020_10_22_21_36_20_895536.jpg', 'images/2020_10_22_21_36_25_733879.png');
COMMIT;

-- ----------------------------
-- Table structure for management_info
-- ----------------------------
DROP TABLE IF EXISTS `management_info`;
CREATE TABLE `management_info` (
  `id` int NOT NULL AUTO_INCREMENT,
  `position` varchar(10) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `company` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `location` varchar(10) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `email` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `csdn` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `github` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `qq` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `weixin` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_info
-- ----------------------------
BEGIN;
INSERT INTO `management_info` VALUES (1, '运维开发工程师', 'IBM', '北京', 'cuiliang0302@qq.com', 'https://blog.csdn.net/qq_33816243', 'https://github.com/cuiliang0302', 'images/2020_10_22_22_06_31_596364.png', 'images/2020_10_22_22_06_35_819056.png');
COMMIT;

-- ----------------------------
-- Table structure for management_link
-- ----------------------------
DROP TABLE IF EXISTS `management_link`;
CREATE TABLE `management_link` (
  `id` int NOT NULL AUTO_INCREMENT,
  `logo` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `url` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `describe` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `type` varchar(1) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=21 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_link
-- ----------------------------
BEGIN;
INSERT INTO `management_link` VALUES (1, 'link/2020_10_26_13_03_34_315632.jpg', 'Layui', 'https://www.layui.com/', 'Layui - 经典模块化前端 UI 框架', '2');
INSERT INTO `management_link` VALUES (2, 'link/2020_10_26_13_06_46_083869.jpg', 'Django', 'https://www.djangoproject.com/', 'Django 是一个开放源代码的 Web 应用框架，由 Python 写成。', '2');
INSERT INTO `management_link` VALUES (3, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '2');
INSERT INTO `management_link` VALUES (4, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '2');
INSERT INTO `management_link` VALUES (5, 'link/2020_10_26_13_03_34_315632.jpg', 'Layui', 'https://www.layui.com/', 'Layui - 经典模块化前端 UI 框架', '1');
INSERT INTO `management_link` VALUES (6, 'link/2020_10_26_13_06_46_083869.jpg', 'Django', 'https://www.djangoproject.com/', 'Django 是一个开放源代码的 Web 应用框架，由 Python 写成。', '1');
INSERT INTO `management_link` VALUES (7, 'link/2020_10_26_13_03_34_315632.jpg', 'Layui', 'https://www.layui.com/', 'Layui - 经典模块化前端 UI 框架', '1');
INSERT INTO `management_link` VALUES (8, 'link/2020_10_26_13_06_46_083869.jpg', 'Django', 'https://www.djangoproject.com/', 'Django 是一个开放源代码的 Web 应用框架，由 Python 写成。', '1');
INSERT INTO `management_link` VALUES (9, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '1');
INSERT INTO `management_link` VALUES (10, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '1');
INSERT INTO `management_link` VALUES (11, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '1');
INSERT INTO `management_link` VALUES (12, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '1');
INSERT INTO `management_link` VALUES (13, 'link/2020_10_26_13_03_34_315632.jpg', 'Layui', 'https://www.layui.com/', 'Layui - 经典模块化前端 UI 框架', '1');
INSERT INTO `management_link` VALUES (14, 'link/2020_10_26_13_06_46_083869.jpg', 'Django', 'https://www.djangoproject.com/', 'Django 是一个开放源代码的 Web 应用框架，由 Python 写成。', '1');
INSERT INTO `management_link` VALUES (15, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '1');
INSERT INTO `management_link` VALUES (16, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '2');
INSERT INTO `management_link` VALUES (17, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '1');
INSERT INTO `management_link` VALUES (18, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '1');
INSERT INTO `management_link` VALUES (19, 'link/2020_10_26_13_08_54_694653.jpg', 'jQuery', 'https://jquery.com/', 'jQuery 是一个 JavaScript 库。  jQuery 极大地简化了 JavaScript 编程。', '1');
INSERT INTO `management_link` VALUES (20, 'link/2020_10_26_13_12_25_174157.jpg', 'echarts', 'https://echarts.apache.org/zh/index.html', 'ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。', '1');
COMMIT;

-- ----------------------------
-- Table structure for management_websiteconfig
-- ----------------------------
DROP TABLE IF EXISTS `management_websiteconfig`;
CREATE TABLE `management_websiteconfig` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `domain` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `index_title` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `keywords` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `descript` varchar(300) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `copyright` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of management_websiteconfig
-- ----------------------------
BEGIN;
INSERT INTO `management_websiteconfig` VALUES (1, '崔亮的博客', 'https://www.cuiliangblog.cn/', '崔亮的博客-专注devops自动化运维，传播优秀it运维技术文章', 'docker,虚拟化,云计算,IT运维,运维社区,运维论坛,运维管理,运维服务,自动化运维,运维工具,DevOps,运维工程师,Linux运维,Shell脚本,数据库,高效运维,微服务,互联网运维', '崔亮的个人博客，致力于分享容器运维、DevOps、自动化运维、数据库等运维实践经验，由系统运维、脚本编程、devops以及资源分享等分类组成，涵盖了DevOps、自动化运维、操作系统教程、运维经验、脚本语言、云计算、虚拟化、监控以及网络资源等内容。', 'Copyright  ©  2019-2020  崔亮的博客  All Rights Reserved');
COMMIT;

-- ----------------------------
-- Table structure for social_auth_association
-- ----------------------------
DROP TABLE IF EXISTS `social_auth_association`;
CREATE TABLE `social_auth_association` (
  `id` int NOT NULL AUTO_INCREMENT,
  `server_url` varchar(255) NOT NULL,
  `handle` varchar(255) NOT NULL,
  `secret` varchar(255) NOT NULL,
  `issued` int NOT NULL,
  `lifetime` int NOT NULL,
  `assoc_type` varchar(64) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `social_auth_association_server_url_handle_078befa2_uniq` (`server_url`,`handle`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of social_auth_association
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for social_auth_code
-- ----------------------------
DROP TABLE IF EXISTS `social_auth_code`;
CREATE TABLE `social_auth_code` (
  `id` int NOT NULL AUTO_INCREMENT,
  `email` varchar(254) NOT NULL,
  `code` varchar(32) NOT NULL,
  `verified` tinyint(1) NOT NULL,
  `timestamp` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `social_auth_code_email_code_801b2d02_uniq` (`email`,`code`),
  KEY `social_auth_code_code_a2393167` (`code`),
  KEY `social_auth_code_timestamp_176b341f` (`timestamp`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of social_auth_code
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for social_auth_nonce
-- ----------------------------
DROP TABLE IF EXISTS `social_auth_nonce`;
CREATE TABLE `social_auth_nonce` (
  `id` int NOT NULL AUTO_INCREMENT,
  `server_url` varchar(255) NOT NULL,
  `timestamp` int NOT NULL,
  `salt` varchar(65) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `social_auth_nonce_server_url_timestamp_salt_f6284463_uniq` (`server_url`,`timestamp`,`salt`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of social_auth_nonce
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for social_auth_partial
-- ----------------------------
DROP TABLE IF EXISTS `social_auth_partial`;
CREATE TABLE `social_auth_partial` (
  `id` int NOT NULL AUTO_INCREMENT,
  `token` varchar(32) NOT NULL,
  `next_step` smallint unsigned NOT NULL,
  `backend` varchar(32) NOT NULL,
  `data` longtext NOT NULL,
  `timestamp` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `social_auth_partial_token_3017fea3` (`token`),
  KEY `social_auth_partial_timestamp_50f2119f` (`timestamp`),
  CONSTRAINT `social_auth_partial_chk_1` CHECK ((`next_step` >= 0))
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of social_auth_partial
-- ----------------------------
BEGIN;
COMMIT;

-- ----------------------------
-- Table structure for social_auth_usersocialauth
-- ----------------------------
DROP TABLE IF EXISTS `social_auth_usersocialauth`;
CREATE TABLE `social_auth_usersocialauth` (
  `id` int NOT NULL AUTO_INCREMENT,
  `provider` varchar(32) NOT NULL,
  `uid` varchar(255) NOT NULL,
  `extra_data` longtext NOT NULL,
  `user_id` int NOT NULL,
  `created` datetime(6) NOT NULL,
  `modified` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `social_auth_usersocialauth_provider_uid_e6b5e668_uniq` (`provider`,`uid`),
  KEY `social_auth_usersocialauth_user_id_17d28448_fk_auth_user_id` (`user_id`),
  KEY `social_auth_usersocialauth_uid_796e51dc` (`uid`),
  CONSTRAINT `social_auth_usersocialauth_user_id_17d28448_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of social_auth_usersocialauth
-- ----------------------------
BEGIN;
INSERT INTO `social_auth_usersocialauth` VALUES (18, 'qq', 'A0B9D81D3894C176B4CE747083813D31', '{\"auth_time\": 1608979789, \"username\": \"\\u03b6\\u0e31\\u0361\\u0793\\u0787\\u0787\\u964c\\u5f71\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=B2EsdNd78qoVu4616YoNVg&s=40&t=1581912070\", \"gender\": \"\\u5973\", \"access_token\": \"087C76B41E572274AB854C93056A6DAD\", \"token_type\": null}', 62, '2020-12-26 18:49:49.114142', '2020-12-26 18:49:49.119342');
INSERT INTO `social_auth_usersocialauth` VALUES (19, 'weibo', '6255685200', '{\"auth_time\": 1608979972, \"id\": 6255685200, \"username\": \"\\u7b19\\u6e05\\u521d\\u8273\\u79bb\", \"profile_image_url\": \"https://tvax1.sinaimg.cn/crop.0.0.1002.1002.50/006Pmduoly8gm02fdw0amj30ru0rujta.jpg?KID=imgbed,tva&Expires=1608990772&ssig=q2bvW6tQwK\", \"gender\": \"f\", \"access_token\": \"2.005UN3pGRDyXMB138aaad6750cc_Xv\", \"token_type\": null}', 63, '2020-12-26 18:52:52.066672', '2020-12-26 18:52:52.073322');
INSERT INTO `social_auth_usersocialauth` VALUES (26, 'qq', 'BEEE210D1827ADAA1D901B4B1C30A73A', '{\"auth_time\": 1608987869, \"username\": \"Starry sky\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=4O7XJvT5XeqBBeARzBTdgg&s=40&t=1599925650\", \"gender\": \"\\u7537\", \"access_token\": \"25BD86506D4CB5A8601EB541A5E8AD40\", \"token_type\": null}', 67, '2020-12-26 21:04:29.389584', '2020-12-26 21:04:29.394081');
INSERT INTO `social_auth_usersocialauth` VALUES (27, 'weibo', '7343849074', '{\"auth_time\": 1608988080, \"id\": 7343849074, \"username\": \"\\u946b\\u7a7a\\u7269\\u8bed50880\", \"profile_image_url\": \"https://tvax3.sinaimg.cn/crop.0.0.1002.1002.50/008102CCly8gm1h2n98kdj30ru0rudgw.jpg?KID=imgbed,tva&Expires=1608998880&ssig=mvhd3dZ8Qk\", \"gender\": \"m\", \"access_token\": \"2.00ccCABIRDyXMBabc71f0bbd1uDK2B\", \"token_type\": null}', 68, '2020-12-26 21:08:00.272463', '2020-12-26 21:08:00.276833');
INSERT INTO `social_auth_usersocialauth` VALUES (28, 'qq', '33DA248521878B2C8DF91BAFA9E47128', '{\"auth_time\": 1608988194, \"username\": \"~\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=ibW9qRDvn2DPgibPibtqFZujA&s=40&t=1557182446\", \"gender\": \"\\u7537\", \"access_token\": \"740CBA5A587DC8E780014B51F6AB6BDC\", \"token_type\": null}', 69, '2020-12-26 21:09:53.994664', '2020-12-26 21:09:54.004058');
INSERT INTO `social_auth_usersocialauth` VALUES (29, 'weibo', '5873919976', '{\"auth_time\": 1608988214, \"id\": 5873919976, \"username\": \"cj16201603\", \"profile_image_url\": \"https://tvax1.sinaimg.cn/default/images/default_avatar_male_50.gif?KID=imgbed,tva&Expires=1608999014&ssig=wntI2BMysp\", \"gender\": \"m\", \"access_token\": \"2.00mu3W6GRDyXMB93c0c25781mWSezD\", \"token_type\": null}', 70, '2020-12-26 21:10:14.111773', '2020-12-26 21:10:14.116814');
INSERT INTO `social_auth_usersocialauth` VALUES (30, 'github', '42726109', '{\"auth_time\": 1608988344, \"id\": 42726109, \"expires\": null, \"login\": \"cuiliang0302\", \"access_token\": \"3f34c0c47aa8cd8e68772cb4325fbdfac15eeec2\", \"token_type\": \"bearer\"}', 71, '2020-12-26 21:12:24.413795', '2020-12-26 21:12:24.419170');
INSERT INTO `social_auth_usersocialauth` VALUES (31, 'qq', 'ED81E9D8079C931034C2563E1B39C78D', '{\"auth_time\": 1609063531, \"username\": \"\\u270e\\ufe4f\\u2133\\u0e53\\u20af\\u3395 \\u2661\\u67e0\\u6aac\\u4e0d\\u840c\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=TGX7Iu5wFlTiasg1VOiaeYMA&s=40&t=1598856813\", \"gender\": \"\\u7537\", \"access_token\": \"7C0B0D22983EB8D955BF78595EBD40E6\", \"token_type\": null}', 72, '2020-12-26 21:44:14.893330', '2020-12-27 18:05:31.243166');
INSERT INTO `social_auth_usersocialauth` VALUES (32, 'weibo', '5259703655', '{\"auth_time\": 1608990413, \"id\": 5259703655, \"username\": \"\\u5d14\\u4eae60857\", \"profile_image_url\": \"https://tva4.sinaimg.cn/crop.0.0.100.100.50/005JXbbVjw8enhfu6gdsej302s02smx0.jpg?KID=imgbed,tva&Expires=1609001213&ssig=gDHhbu%2Fprl\", \"gender\": \"m\", \"access_token\": \"2.00vLLxjFRDyXMB68634b6945GBpQCE\", \"token_type\": null}', 73, '2020-12-26 21:46:53.054902', '2020-12-26 21:46:53.069907');
INSERT INTO `social_auth_usersocialauth` VALUES (33, 'github', '22441197', '{\"auth_time\": 1608992485, \"id\": 22441197, \"expires\": null, \"login\": \"SoulSurferdj\", \"access_token\": \"29b5b49cf110065b70b8e0aceed544627c24ea5a\", \"token_type\": \"bearer\"}', 74, '2020-12-26 22:21:25.671196', '2020-12-26 22:21:25.677698');
INSERT INTO `social_auth_usersocialauth` VALUES (34, 'github', '33510343', '{\"auth_time\": 1608994039, \"id\": 33510343, \"expires\": null, \"login\": \"guancongcong\", \"access_token\": \"04692595d32893adcc6d1a53ee4b3404d2ef89a1\", \"token_type\": \"bearer\"}', 75, '2020-12-26 22:47:19.966522', '2020-12-26 22:47:19.973890');
INSERT INTO `social_auth_usersocialauth` VALUES (35, 'qq', '5D637F37557EEFB59B16EFCBF25F24DA', '{\"auth_time\": 1609071817, \"username\": \"\\u8549\\u592a\\u72fc\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=Zk8qzkZr9VN0f8hDn6rH6A&s=40&t=1568016167\", \"gender\": \"\\u7537\", \"access_token\": \"87F3F68E4D5FBCAC966FD6E2CFC591DF\", \"token_type\": null}', 76, '2020-12-27 20:23:37.216748', '2020-12-27 20:23:37.225927');
INSERT INTO `social_auth_usersocialauth` VALUES (36, 'qq', '0D9FAB7F6F5FF6673872C39C5B6B5798', '{\"auth_time\": 1609741810, \"username\": \"bug\", \"profile_image_url\": \"http://thirdqq.qlogo.cn/g?b=oidb&k=LDjJiaUskvsnyjuicpIO6eLQ&s=40&t=1556405128\", \"gender\": \"\\u7537\", \"access_token\": \"C2B0C1E90F8412E529C5E4020BD9B764\", \"token_type\": null}', 77, '2021-01-04 14:30:10.050325', '2021-01-04 14:30:10.054627');
COMMIT;

-- ----------------------------
-- Table structure for sqlite_sequence
-- ----------------------------
DROP TABLE IF EXISTS `sqlite_sequence`;
CREATE TABLE `sqlite_sequence` (
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `seq` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

-- ----------------------------
-- Records of sqlite_sequence
-- ----------------------------
BEGIN;
INSERT INTO `sqlite_sequence` VALUES ('django_migrations', '64');
INSERT INTO `sqlite_sequence` VALUES ('django_admin_log', '372');
INSERT INTO `sqlite_sequence` VALUES ('django_content_type', '22');
INSERT INTO `sqlite_sequence` VALUES ('auth_permission', '96');
INSERT INTO `sqlite_sequence` VALUES ('auth_group', '0');
INSERT INTO `sqlite_sequence` VALUES ('auth_user', '46');
INSERT INTO `sqlite_sequence` VALUES ('blog_category', '21');
INSERT INTO `sqlite_sequence` VALUES ('blog_tag', '11');
INSERT INTO `sqlite_sequence` VALUES ('blog_article_tags', '445');
INSERT INTO `sqlite_sequence` VALUES ('captcha_captchastore', '683');
INSERT INTO `sqlite_sequence` VALUES ('account_leavemessage', '31');
INSERT INTO `sqlite_sequence` VALUES ('account_commentmessage', '24');
INSERT INTO `sqlite_sequence` VALUES ('management_about', '1');
INSERT INTO `sqlite_sequence` VALUES ('blog_article', '122');
INSERT INTO `sqlite_sequence` VALUES ('account_articleviewhistory', '183');
INSERT INTO `sqlite_sequence` VALUES ('account_userinfo', '23');
INSERT INTO `sqlite_sequence` VALUES ('management_info', '1');
INSERT INTO `sqlite_sequence` VALUES ('management_imagesconfig', '1');
INSERT INTO `sqlite_sequence` VALUES ('management_websiteconfig', '1');
INSERT INTO `sqlite_sequence` VALUES ('management_carousel', '25');
INSERT INTO `sqlite_sequence` VALUES ('management_link', '20');
COMMIT;

SET FOREIGN_KEY_CHECKS = 1;
